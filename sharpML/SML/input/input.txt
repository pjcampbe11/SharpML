~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2024-03-01 11:39:50[0m
[92mHash: c3cf139b05d38231db198a25d9f66a1e1bff141c[0m
[92mFilepath: tests/unit/test_session.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: Add AutoMLV2 support (#4461)

* Add AutoMLV2 support

* Improvements of the integration tests

---------

Co-authored-by: Anton Repushko <repuanto@amazon.com>[0m
@@ -55,7 +55,6 @@ from tests.unit import (
     SAGEMAKER_CONFIG_ENDPOINT_ENDPOINT_CONFIG_COMBINED,
     SAGEMAKER_CONFIG_ENDPOINT,
     SAGEMAKER_CONFIG_AUTO_ML,
-    SAGEMAKER_CONFIG_AUTO_ML_V2,
     SAGEMAKER_CONFIG_MODEL_PACKAGE,
     SAGEMAKER_CONFIG_FEATURE_GROUP,
     SAGEMAKER_CONFIG_PROCESSING_JOB,
@@ -4444,83 +4443,6 @@ COMPLETE_EXPECTED_AUTO_ML_JOB_ARGS = {
     "Tags": ["tag"],
 }
 
-DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS = {
-    "AutoMLJobName": JOB_NAME,
-    "AutoMLJobInputDataConfig": [
-        {
-            "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": S3_INPUT_URI}},
-        }
-    ],
-    "AutoMLProblemTypeConfig": {
-        "TabularJobConfig": {
-            "TargetAttributeName": "y",
-            "SampleWeightAttributeName": "sampleWeight",
-            "CompletionCriteria": {
-                "MaxCandidates": 10,
-                "MaxAutoMLJobRuntimeInSeconds": 36000,
-                "MaxRuntimePerTrainingJobInSeconds": 3600 * 2,
-            },
-            "GenerateCandidateDefinitionsOnly": False,
-        }
-    },
-    "OutputDataConfig": {"S3OutputPath": S3_OUTPUT},
-    "RoleArn": EXPANDED_ROLE,
-}
-
-COMPLETE_EXPECTED_AUTO_ML_V2_JOB_ARGS = {
-    "AutoMLJobName": JOB_NAME,
-    "AutoMLJobInputDataConfig": [
-        {
-            "ChannelType": "training",
-            "CompressionType": "Gzip",
-            "DataSource": {
-                "S3DataSource": {
-                    "S3DataType": "S3Prefix",
-                    "S3Uri": S3_INPUT_URI,
-                }
-            },
-        },
-        {
-            "ChannelType": "validation",
-            "CompressionType": "Gzip",
-            "DataSource": {
-                "S3DataSource": {
-                    "S3DataType": "S3Prefix",
-                    "S3Uri": DEFAULT_S3_VALIDATION_DATA,
-                }
-            },
-        },
-    ],
-    "OutputDataConfig": {"S3OutputPath": S3_OUTPUT},
-    "AutoMLProblemTypeConfig": {
-        "TabularJobConfig": {
-            "TargetAttributeName": "y",
-            "SampleWeightAttributeName": "sampleWeight",
-            "CompletionCriteria": {
-                "MaxCandidates": 10,
-                "MaxAutoMLJobRuntimeInSeconds": 36000,
-                "MaxRuntimePerTrainingJobInSeconds": 3600 * 2,
-            },
-            "GenerateCandidateDefinitionsOnly": False,
-            "ProblemType": "Regression",
-            "FeatureSpecificationS3Uri": "s3://mybucket/features.json",
-            "Mode": "ENSEMBLING",
-        },
-    },
-    "AutoMLJobObjective": {"Type": "type", "MetricName": "metric-name"},
-    "SecurityConfig": {
-        "VolumeKmsKeyId": "volume-kms-key-id-string",
-        "EnableInterContainerTrafficEncryption": False,
-        "VpcConfig": {
-            "SecurityGroupIds": ["security-group-id"],
-            "Subnets": ["subnet"],
-        },
-    },
-    "RoleArn": EXPANDED_ROLE,
-    "Tags": ["tag"],
-}
-
-
 COMPLETE_EXPECTED_LIST_CANDIDATES_ARGS = {
     "AutoMLJobName": JOB_NAME,
     "StatusEquals": "Completed",
@@ -4563,48 +4485,6 @@ def test_auto_ml_pack_to_request(sagemaker_session):
     )
 
 
-def test_auto_ml_v2_pack_to_request(sagemaker_session):
-    input_config = [
-        {
-            "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": S3_INPUT_URI}},
-        }
-    ]
-
-    output_config = {"S3OutputPath": S3_OUTPUT}
-
-    problem_config = {
-        "TabularJobConfig": {
-            "TargetAttributeName": "y",
-            "SampleWeightAttributeName": "sampleWeight",
-            "CompletionCriteria": {
-                "MaxCandidates": 10,
-                "MaxAutoMLJobRuntimeInSeconds": 36000,
-                "MaxRuntimePerTrainingJobInSeconds": 3600 * 2,
-            },
-            "GenerateCandidateDefinitionsOnly": False,
-        }
-    }
-
-    job_name = JOB_NAME
-    role = EXPANDED_ROLE
-
-    sagemaker_session.create_auto_ml_v2(
-        input_config=input_config,
-        job_name=job_name,
-        problem_config=problem_config,
-        output_config=output_config,
-        role=role,
-    )
-
-    sagemaker_session.sagemaker_client.create_auto_ml_job_v2.assert_called_with(
-        AutoMLJobName=DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS["AutoMLJobName"],
-        AutoMLJobInputDataConfig=DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS["AutoMLJobInputDataConfig"],
-        OutputDataConfig=DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS["OutputDataConfig"],
-        [93mAutoMLProblemTypeConfig=DEFAULT[0m_EXPECTED_AUTO_ML_V2_JOB_ARGS["AutoMLProblemTypeConfig"],
-        RoleArn=DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS["RoleArn"],
-    )
-
-
 def test_create_auto_ml_with_sagemaker_config_injection(sagemaker_session):
     sagemaker_session.sagemaker_config = SAGEMAKER_CONFIG_AUTO_ML
 
@@ -4663,70 +4543,6 @@ def test_create_auto_ml_with_sagemaker_config_injection(sagemaker_session):
     )
 
 
-def test_create_auto_ml_v2_with_sagemaker_config_injection(sagemaker_session):
-    sagemaker_session.sagemaker_config = SAGEMAKER_CONFIG_AUTO_ML_V2
-
-    input_config = [
-        {
-            "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": S3_INPUT_URI}},
-        }
-    ]
-
-    output_config = {"S3OutputPath": S3_OUTPUT}
-
-    problem_config = {
-        "TabularJobConfig": {
-            "TargetAttributeName": "y",
-            "SampleWeightAttributeName": "sampleWeight",
-            "CompletionCriteria": {
-                "MaxCandidates": 10,
-                "MaxAutoMLJobRuntimeInSeconds": 36000,
-                "MaxRuntimePerTrainingJobInSeconds": 3600 * 2,
-            },
-            "GenerateCandidateDefinitionsOnly": False,
-        }
-    }
-
-    job_name = JOB_NAME
-    sagemaker_session.create_auto_ml_v2(
-        input_config=input_config,
-        job_name=job_name,
-        problem_config=problem_config,
-        output_config=output_config,
-    )
-
-    expected_call_args = copy.deepcopy(DEFAULT_EXPECTED_AUTO_ML_V2_JOB_ARGS)
-    expected_volume_kms_key_id = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"]["AutoMLJobV2"][
-        "SecurityConfig"
-    ]["VolumeKmsKeyId"]
-    expected_role_arn = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"]["AutoMLJobV2"]["RoleArn"]
-    expected_kms_key_id = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"]["AutoMLJobV2"][
-        "OutputDataConfig"
-    ]["KmsKeyId"]
-    expected_vpc_config = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"]["AutoMLJobV2"]["SecurityConfig"][
-        "VpcConfig"
-    ]
-    expected_tags = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"]["AutoMLJobV2"]["Tags"]
-    expected_enable_inter_container_traffic_encryption = SAGEMAKER_CONFIG_AUTO_ML_V2["SageMaker"][
-        "AutoMLJobV2"
-    ]["SecurityConfig"]["EnableInterContainerTrafficEncryption"]
-    expected_call_args["OutputDataConfig"]["KmsKeyId"] = expected_kms_key_id
-    expected_call_args["RoleArn"] = expected_role_arn
-    expected_call_args["SecurityConfig"] = {
-        "EnableInterContainerTrafficEncryption": expected_enable_inter_container_traffic_encryption
-    }
-    expected_call_args["SecurityConfig"]["VpcConfig"] = expected_vpc_config
-    expected_call_args["SecurityConfig"]["VolumeKmsKeyId"] = expected_volume_kms_key_id
-    sagemaker_session.sagemaker_client.create_auto_ml_job_v2.assert_called_with(
-        AutoMLJobName=expected_call_args["AutoMLJobName"],
-        AutoMLJobInputDataConfig=expected_call_args["AutoMLJobInputDataConfig"],
-        OutputDataConfig=expected_call_args["OutputDataConfig"],
-        AutoMLProblemTypeConfig=expected_call_args["AutoMLProblemTypeConfig"],
-        RoleArn=expected_call_args["RoleArn"],
-        Tags=expected_tags,
-    )
-
-
 def test_auto_ml_pack_to_request_with_optional_args(sagemaker_session):
     input_config = [
         {
@@ -4797,78 +4613,6 @@ def test_auto_ml_pack_to_request_with_optional_args(sagemaker_session):
     )
 
 
-def test_auto_ml_v2_pack_to_request_with_optional_args(sagemaker_session):
-    input_config = [
-        {
-            "ChannelType": "training",
-            "CompressionType": "Gzip",
-            "DataSource": {
-                "S3DataSource": {
-                    "S3DataType": "S3Prefix",
-                    "S3Uri": S3_INPUT_URI,
-                }
-            },
-        },
-        {
-            "ChannelType": "validation",
-            "CompressionType": "Gzip",
-            "DataSource": {
-                "S3DataSource": {
-                    "S3DataType": "S3Prefix",
-                    "S3Uri": DEFAULT_S3_VALIDATION_DATA,
-                }
-            },
-        },
-    ]
-
-    output_config = {"S3OutputPath": S3_OUTPUT}
-
-    problem_config = {
-        "TabularJobConfig": {
-            "TargetAttributeName": "y",
-            "SampleWeightAttributeName": "sampleWeight",
-            "CompletionCriteria": {
-                "MaxCandidates": 10,
-                "MaxAutoMLJobRuntimeInSeconds": 36000,
-                "MaxRuntimePerTrainingJobInSeconds": 3600 * 2,
-            },
-            "GenerateCandidateDefinitionsOnly": False,
-            "FeatureSpecificationS3Uri": "s3://mybucket/features.json",
-            "Mode": "ENSEMBLING",
-            "ProblemType": "Regression",
-        }
-    }
-
-    security_config = {
-        "VolumeKmsKeyId": "volume-kms-key-id-string",
-        "EnableInterContainerTrafficEncryption": False,
-        "VpcConfig": {
-            "SecurityGroupIds": ["security-group-id"],
-            "Subnets": ["subnet"],
-        },
-    }
-
-    job_name = JOB_NAME
-    role = EXPANDED_ROLE
-
-    sagemaker_session.create_auto_ml_v2(
-        input_config=input_config,
-        job_name=job_name,
-        problem_config=problem_config,
-        output_config=output_config,
-        role=role,
-        security_config=security_config,
-        job_objective={"Type": "type", "MetricName": "metric-name"},
-        tags=["tag"],
-    )
-
-    assert sagemaker_session.sagemaker_client.method_calls[0] == (
-        "create_auto_ml_job_v2",
-        (),
-        COMPLETE_EXPECTED_AUTO_ML_V2_JOB_ARGS,
-    )
-
-
 def test_list_candidates_for_auto_ml_job_default(sagemaker_session):
     sagemaker_session.list_candidates(job_name=JOB_NAME)
     sagemaker_session.sagemaker_client.list_candidates_for_auto_ml_job.assert_called_once()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-11-29 16:46:09[0m
[92mHash: b380eb357832dac1f8442ab2d62fa7cb1f663c9b[0m
[92mFilepath: tests/data/workflow/notebook_job_step/notebook1_happypath.ipynb[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Add Pipeline step decorator, NotebookJobStep, and scheduler (#1271)

Co-authored-by: Rohan Gujarathi <gujarathi.rohan@gmail.com>
Co-authored-by: svia3 <svia@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Dewen Qi <qidewen@amazon.com>
Co-authored-by: Edward Sun <edward.psun@amazon.com>
Co-authored-by: Stephen Via <51342648+svia3@users.noreply.github.com>
Co-authored-by: Namrata Madan <nmmadan@amazon.com>
Co-authored-by: Stacia Choe <stacicho@amazon.com>
Co-authored-by: Edward Sun <sunp@amazon.com>
Co-authored-by: Edward Sun <83920185+edwardps@users.noreply.github.com>
Co-authored-by: Rohan Gujarathi <gujrohan@amazon.com>
fix: Multiple bug fixes including removing unsupported feature. (#1105)
Fix some problems with pipeline compilation (#1125)
fix: Refactor JsonGet s3 URI and add serialize_output_to_json flag (#1164)
fix: invoke_function circular import (#1262)
fix: pylint (#1264)
fix: Add logging for docker build failures (#1267)
[0m
@@ -1,729 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {
-    "tags": [
-     "parameters"
-    ]
-   },
-   "outputs": [],
-   "source": [
-    "company='zappos'"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "ParentNotebook: ENV_VAR_FROM_INIT_SCRIPT=None\n"
-     ]
-    }
-   ],
-   "source": [
-    "import os\n",
-    "print(f\"ParentNotebook: ENV_VAR_FROM_INIT_SCRIPT={os.getenv('ENV_VAR_FROM_INIT_SCRIPT')}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "ParentNotebook: env_key=None\n"
-     ]
-    }
-   ],
-   "source": [
-    "print(f\"ParentNotebook: env_key={os.getenv('env_key')}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "ParentNotebook: company_is_zappos\n"
-     ]
-    }
-   ],
-   "source": [
-    "print(f'ParentNotebook: company_is_{company}')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "from sub notebook\n",
-      "SubNotebook: ENV_VAR_FROM_INIT_SCRIPT=None\n",
-      "SubNotebook: env_key=None\n",
-      "SubNotebook: company_is_zappos\n"
-     ]
-    }
-   ],
-   "source": [
-    "%run 'subfolder/sub.ipynb'"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 7,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "aws-sagemaker\n",
-      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
-      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
-      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
-      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
-     ]
-    }
-   ],
-   "source": [
-    "%run 'simple.ipynb'"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "availableInstances": [
-   {
-    "_defaultOrder": 0,
-    "_isFastLaunch": true,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 4,
-    "name": "ml.t3.medium",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 1,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 8,
-    "name": "ml.t3.large",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 2,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.t3.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 3,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.t3.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 4,
-    "_isFastLaunch": true,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 8,
-    "name": "ml.m5.large",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 5,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.m5.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 6,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.m5.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 7,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 64,
-    "name": "ml.m5.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 8,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 128,
-    "name": "ml.m5.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 9,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 192,
-    "name": "ml.m5.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 10,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 256,
-    "name": "ml.m5.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 11,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 384,
-    "name": "ml.m5.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 12,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 8,
-    "name": "ml.m5d.large",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 13,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.m5d.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 14,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.m5d.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 15,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 64,
-    "name": "ml.m5d.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 16,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 128,
-    "name": "ml.m5d.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 17,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 192,
-    "name": "ml.m5d.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 18,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 256,
-    "name": "ml.m5d.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 19,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 384,
-    "name": "ml.m5d.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 20,
-    "_isFastLaunch": false,
-    "category": "General purpose",
-    "gpuNum": 0,
-    "hideHardwareSpecs": true,
-    "memoryGiB": 0,
-    "name": "ml.geospatial.interactive",
-    "supportedImageNames": [
-     "sagemaker-geospatial-v1-0"
-    ],
-    "vcpuNum": 0
-   },
-   {
-    "_defaultOrder": 21,
-    "_isFastLaunch": true,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 4,
-    "name": "ml.c5.large",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 22,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 8,
-    "name": "ml.c5.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 23,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.c5.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 24,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.c5.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 25,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 72,
-    "name": "ml.c5.9xlarge",
-    "vcpuNum": 36
-   },
-   {
-    "_defaultOrder": 26,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 96,
-    "name": "ml.c5.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 27,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 144,
-    "name": "ml.c5.18xlarge",
-    "vcpuNum": 72
-   },
-   {
-    "_defaultOrder": 28,
-    "_isFastLaunch": false,
-    "category": "Compute optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 192,
-    "name": "ml.c5.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 29,
-    "_isFastLaunch": true,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.g4dn.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 30,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.g4dn.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 31,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 64,
-    "name": "ml.g4dn.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 32,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 128,
-    "name": "ml.g4dn.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 33,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 4,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 192,
-    "name": "ml.g4dn.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 34,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 256,
-    "name": "ml.g4dn.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 35,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 61,
-    "name": "ml.p3.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 36,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 4,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 244,
-    "name": "ml.p3.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 37,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 8,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 488,
-    "name": "ml.p3.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 38,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 8,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 768,
-    "name": "ml.p3dn.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 39,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.r5.large",
-    "vcpuNum": 2
-   },
-   {
-    "_defaultOrder": 40,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.r5.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 41,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 64,
-    "name": "ml.r5.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 42,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 128,
-    "name": "ml.r5.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 43,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 256,
-    "name": "ml.r5.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 44,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 384,
-    "name": "ml.r5.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 45,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 512,
-    "name": "ml.r5.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 46,
-    "_isFastLaunch": false,
-    "category": "Memory Optimized",
-    "gpuNum": 0,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 768,
-    "name": "ml.r5.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 47,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 16,
-    "name": "ml.g5.xlarge",
-    "vcpuNum": 4
-   },
-   {
-    "_defaultOrder": 48,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 32,
-    "name": "ml.g5.2xlarge",
-    "vcpuNum": 8
-   },
-   {
-    "_defaultOrder": 49,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 64,
-    "name": "ml.g5.4xlarge",
-    "vcpuNum": 16
-   },
-   {
-    "_defaultOrder": 50,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 128,
-    "name": "ml.g5.8xlarge",
-    "vcpuNum": 32
-   },
-   {
-    "_defaultOrder": 51,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 1,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 256,
-    "name": "ml.g5.16xlarge",
-    "vcpuNum": 64
-   },
-   {
-    "_defaultOrder": 52,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 4,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 192,
-    "name": "ml.g5.12xlarge",
-    "vcpuNum": 48
-   },
-   {
-    "_defaultOrder": 53,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 4,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 384,
-    "name": "ml.g5.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 54,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 8,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 768,
-    "name": "ml.g5.48xlarge",
-    "vcpuNum": 192
-   },
-   {
-    "_defaultOrder": 55,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 8,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 1152,
-    "name": "ml.p4d.24xlarge",
-    "vcpuNum": 96
-   },
-   {
-    "_defaultOrder": 56,
-    "_isFastLaunch": false,
-    "category": "Accelerated computing",
-    "gpuNum": 8,
-    "hideHardwareSpecs": false,
-    "memoryGiB": 1152,
-    "name": "ml.p4de.24xlarge",
-    "vcpuNum": 96
-   }
-  ],
-  "instance_type": "ml.t3.medium",
-  "kernelspec": {
-   "display_name": "Python 3 (Data Science 3.0)",
-   "language": "python",
-   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.10.6"
-  },
-  "vscode": {
-   "interpreter": {
-    "hash": "[93maee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49[0m"
-   }
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 4
-}

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-11-29 16:46:09[0m
[92mHash: b380eb357832dac1f8442ab2d62fa7cb1f663c9b[0m
[92mFilepath: tests/data/workflow/notebook_job_step/subfolder/sub.ipynb[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Add Pipeline step decorator, NotebookJobStep, and scheduler (#1271)

Co-authored-by: Rohan Gujarathi <gujarathi.rohan@gmail.com>
Co-authored-by: svia3 <svia@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Dewen Qi <qidewen@amazon.com>
Co-authored-by: Edward Sun <edward.psun@amazon.com>
Co-authored-by: Stephen Via <51342648+svia3@users.noreply.github.com>
Co-authored-by: Namrata Madan <nmmadan@amazon.com>
Co-authored-by: Stacia Choe <stacicho@amazon.com>
Co-authored-by: Edward Sun <sunp@amazon.com>
Co-authored-by: Edward Sun <83920185+edwardps@users.noreply.github.com>
Co-authored-by: Rohan Gujarathi <gujrohan@amazon.com>
fix: Multiple bug fixes including removing unsupported feature. (#1105)
Fix some problems with pipeline compilation (#1125)
fix: Refactor JsonGet s3 URI and add serialize_output_to_json flag (#1164)
fix: invoke_function circular import (#1262)
fix: pylint (#1264)
fix: Add logging for docker build failures (#1267)
[0m
@@ -1,77 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "print(\"from sub notebook\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import os\n",
-    "print(f\"SubNotebook: ENV_VAR_FROM_INIT_SCRIPT={os.getenv('ENV_VAR_FROM_INIT_SCRIPT')}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "print(f\"SubNotebook: env_key={os.getenv('env_key')}\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "print(f'SubNotebook: company_is_{company}')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "%env"
-   ]
-  }
- ],
- "metadata": {
-  "instance_type": "ml.t3.medium",
-  "kernelspec": {
-   "display_name": "Python 3 (Data Science)",
-   "language": "python",
-   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.7.10"
-  },
-  "vscode": {
-   "interpreter": {
-    "hash": "[93maee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49[0m"
-   }
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 4
-}

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-11-29 16:46:09[0m
[92mHash: b380eb357832dac1f8442ab2d62fa7cb1f663c9b[0m
[92mFilepath: tests/integ/sagemaker/workflow/test_notebook_job_step.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Add Pipeline step decorator, NotebookJobStep, and scheduler (#1271)

Co-authored-by: Rohan Gujarathi <gujarathi.rohan@gmail.com>
Co-authored-by: svia3 <svia@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Dewen Qi <qidewen@amazon.com>
Co-authored-by: Edward Sun <edward.psun@amazon.com>
Co-authored-by: Stephen Via <51342648+svia3@users.noreply.github.com>
Co-authored-by: Namrata Madan <nmmadan@amazon.com>
Co-authored-by: Stacia Choe <stacicho@amazon.com>
Co-authored-by: Edward Sun <sunp@amazon.com>
Co-authored-by: Edward Sun <83920185+edwardps@users.noreply.github.com>
Co-authored-by: Rohan Gujarathi <gujrohan@amazon.com>
fix: Multiple bug fixes including removing unsupported feature. (#1105)
Fix some problems with pipeline compilation (#1125)
fix: Refactor JsonGet s3 URI and add serialize_output_to_json flag (#1164)
fix: invoke_function circular import (#1262)
fix: pylint (#1264)
fix: Add logging for docker build failures (#1267)
[0m
@@ -1,630 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-
-from __future__ import absolute_import
-
-import os.path
-import tarfile
-import logging
-import nbformat as nbf
-
-from sagemaker import get_execution_role
-from sagemaker.s3 import S3Downloader
-from sagemaker.s3_utils import s3_path_join
-from sagemaker.utils import name_from_base
-from sagemaker.workflow import ParameterString
-from sagemaker.workflow.notebook_job_step import NotebookJobStep
-from sagemaker.workflow.parameters import ParameterInteger, ParameterBoolean
-from sagemaker.workflow.pipeline import Pipeline
-from sagemaker.utils import _tmpdir
-from tests.integ.retry import retries
-from tests.integ.sagemaker.workflow.helpers import wait_pipeline_execution
-from tests.integ import kms_utils
-from sagemaker.workflow.function_step import step
-
-SAGE_MAKER_ROLE_NAME = "SageMakerRole"
-
-PIPELINE_NAME = "pipeline-test-from-pysdk"
-STEP_NAME = "NotebookStepFromPySDKTest"
-DISPLAY_NAME = "MyNotebookStep From PySDK Test"
-DESCRIPTION = "Description for the step"
-NOTEBOOK_JOB_NAME_Prefix = "PySDKTest"
-
-COSMOS_IMAGE_V0_URI = (
-    "542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod@sha256"
-    ":[93mb49a54ff7ca519dd57cf1a3a7a675f3db5c591fab456f9374e390fb68a99edaf[0m"
-)
-KERNEL_NAME = "python3"
-
-
-def test_happycase_minimum_input(sagemaker_session):
-    pipeline_name = name_from_base(PIPELINE_NAME, max_length=63)
-    input_notebook = "tests/data/workflow/notebook_job_step/simple.ipynb"
-
-    notebook_job_step = NotebookJobStep(
-        input_notebook=input_notebook,
-        image_uri=COSMOS_IMAGE_V0_URI,
-        kernel_name=KERNEL_NAME,
-        role=SAGE_MAKER_ROLE_NAME,
-    )
-
-    # TODO - will be removed when cosmos image is ready
-    notebook_job_step = _config_manual_dependency_installation_short_term_workaround(
-        notebook_job_step
-    )
-
-    pipeline = Pipeline(
-        name=pipeline_name,
-        steps=[notebook_job_step],
-        sagemaker_session=sagemaker_session,
-    )
-    logging.info(f"Notebook job step test pipeline definition: {pipeline.definition()}")
-
-    try:
-        pipeline.create(get_role(sagemaker_session))
-        execution_steps = _start_and_verify_execution_with_retry(
-            pipeline=pipeline,
-            parameters={},
-        )
-
-        job_description = _get_training_job_details(execution_steps[0], sagemaker_session)
-
-        # verify the training job is created consistent with input.
-        training_job_name = job_description["TrainingJobName"]
-        assert training_job_name.startswith("simple-ipynb-")
-        assert job_description["AlgorithmSpecification"]["TrainingImage"] == COSMOS_IMAGE_V0_URI
-        assert SAGE_MAKER_ROLE_NAME in job_description["RoleArn"]
-
-        assert job_description["Environment"]["SM_INPUT_NOTEBOOK_NAME"] == os.path.basename(
-            input_notebook
-        )
-        assert job_description["Environment"]["SM_KERNEL_NAME"] == KERNEL_NAME
-        assert job_description["TrainingJobStatus"] == "Completed"
-
-        # verify the output notebook
-        output_s3_uri = s3_path_join(
-            job_description["OutputDataConfig"]["S3OutputPath"],
-            training_job_name,
-            "output",
-            "output.tar.gz",
-        )
-        output_notebook_name = job_description["Environment"]["SM_OUTPUT_NOTEBOOK_NAME"]
-
-        def verify_notebook_for_happy_case(cells):
-            # happy case is using tests/data/workflow/notebook_job_step/simple.ipynb which print
-            # aws-sagemaker.
-            assert NotebookUtils.search_output_of_cells(cells, "aws-sagemaker")
-
-        _download_and_verify_output_notebook(
-            output_s3_uri, output_notebook_name, verify_notebook_for_happy_case, sagemaker_session
-        )
-
-        # upsert and run a second time
-        pipeline.upsert(get_role(sagemaker_session))
-        execution_steps = _start_and_verify_execution_with_retry(
-            pipeline=pipeline,
-            parameters={},
-        )
-
-        job_description = _get_training_job_details(execution_steps[0], sagemaker_session)
-        assert job_description["TrainingJobStatus"] == "Completed"
-    finally:
-        try:
-            pipeline.delete()
-        except Exception as error:
-            logging.error(error)
-
-
-def test_notebook_job_with_more_configuration(sagemaker_session):
-    """This test case is for more complex job configuration.
-    1. a parent notebook file with %run magic to execute 'subfolder/sub.ipynb' and the
-    simple.ipynb
-    2. the notebook job has parameters used by parent and sub notebooks
-    3. the environment variables are passed from step and init script
-    4. pipeline variables are used for a sanity testing purpose
-    5. sub notebook is uploaded as additional_dependencies
-    6. init script will be used
-    7. additional tags are also attached
-    8. apply kms key
-    """
-    pipeline_name = name_from_base(PIPELINE_NAME, max_length=63)
-    notebook_job_name = f"{NOTEBOOK_JOB_NAME_Prefix}-case2-configs"
-
-    input_notebook = "tests/data/workflow/notebook_job_step/notebook1_happypath.ipynb"
-    folder_with_sub_notebook = "tests/data/workflow/notebook_job_step/subfolder"
-    simple_notebook_path = "tests/data/workflow/notebook_job_step/simple.ipynb"
-    init_script = "tests/data/workflow/notebook_job_step/my-init.sh"
-
-    # prepare parameter
-    company_parameter = ParameterString(name="company", default_value="Amazon_FromParameter")
-    notebook_job_parameters = {
-        "company": company_parameter,
-        "company2": "Amazon2",
-    }
-
-    # prepare env vars
-    env_parameter = ParameterString(name="env_key", default_value="EnvValueDefinedByParameter")
-    environment_variables = {"env_key": env_parameter, "env_key2": "env_value_abc2"}
-
-    # prepare tags
-    tags = {
-        "company": company_parameter,
-        "sagemaker:user-profile-name": "studio-user",
-        "sagemaker:is-scheduling-notebook-job": "true",
-    }
-
-    # ParameterInteger for volume
-    volume_size_parameter = ParameterInteger(name="volume_size", default_value=40)
-
-    # ParameterBoolean
-    encrypt_container_traffic_parameter = ParameterBoolean(
-        name="encrypt_container_traffic", default_value=False
-    )
-
-    kms_key_id = kms_utils.get_or_create_kms_key(
-        sagemaker_session, role_arn=get_role(sagemaker_session)
-    )
-
-    notebook_job_step = NotebookJobStep(
-        name=STEP_NAME,
-        display_name=DISPLAY_NAME,
-        description=DESCRIPTION,
-        notebook_job_name=notebook_job_name,
-        image_uri=COSMOS_IMAGE_V0_URI,
-        kernel_name=KERNEL_NAME,
-        role=SAGE_MAKER_ROLE_NAME,
-        input_notebook=input_notebook,
-        initialization_script=init_script,
-        additional_dependencies=[simple_notebook_path, folder_with_sub_notebook],
-        parameters=notebook_job_parameters,
-        environment_variables=environment_variables,
-        tags=tags,
-        volume_size=volume_size_parameter,
-        encrypt_inter_container_traffic=encrypt_container_traffic_parameter,
-        s3_kms_key=kms_key_id,
-        volume_kms_key=kms_key_id,
-    )
-
-    # TODO - will be removed when cosmos image is ready
-    notebook_job_step = _config_manual_dependency_installation_short_term_workaround(
-        notebook_job_step
-    )
-
-    pipeline = Pipeline(
-        name=pipeline_name,
-        parameters=[
-            company_parameter,
-            env_parameter,
-            volume_size_parameter,
-            encrypt_container_traffic_parameter,
-        ],
-        steps=[notebook_job_step],
-        sagemaker_session=sagemaker_session,
-    )
-    logging.info(f"Notebook job step test pipeline definition: {pipeline.definition()}")
-
-    try:
-        pipeline.create(get_role(sagemaker_session))
-        execution_steps = _start_and_verify_execution_with_retry(
-            pipeline=pipeline,
-            parameters={},
-        )
-
-        job_description = _get_training_job_details(execution_steps[0], sagemaker_session)
-
-        # verify the training job is created consistent with input.
-        assert job_description["AlgorithmSpecification"]["TrainingImage"] == COSMOS_IMAGE_V0_URI
-        assert SAGE_MAKER_ROLE_NAME in job_description["RoleArn"]
-
-        # verify notebook job parameters
-        assert job_description["HyperParameters"]["company"] == "Amazon_FromParameter"
-        assert job_description["HyperParameters"]["company2"] == "Amazon2"
-
-        # verify custom env variables
-        assert job_description["Environment"]["env_key"] == "EnvValueDefinedByParameter"
-        assert job_description["Environment"]["env_key2"] == "env_value_abc2"
-
-        # verify passed in volume number
-        assert job_description["ResourceConfig"]["VolumeSizeInGB"] == 40
-
-        # verify passed in encrypt_container_traffic_parameter
-        assert not job_description["EnableInterContainerTrafficEncryption"]
-
-        # verify kms key
-        assert job_description["OutputDataConfig"]["KmsKeyId"] == kms_key_id
-        assert job_description["ResourceConfig"]["VolumeKmsKeyId"] == kms_key_id
-
-        # verify tags
-        tags_from_job = sagemaker_session.sagemaker_client.list_tags(
-            ResourceArn=job_description["TrainingJobArn"]
-        )["Tags"]
-        tags_dict = _convert_to_flatten_dict(tags_from_job)
-        # tags set by client code
-        assert tags_dict["company"] == "Amazon_FromParameter"
-        assert tags_dict["sagemaker:user-profile-name"] == "studio-user"
-        assert tags_dict["sagemaker:is-scheduling-notebook-job"] == "true"
-
-        # system tags
-        assert tags_dict["sagemaker:is-studio-archived"] == "false"
-        assert tags_dict["sagemaker:name"] == notebook_job_name
-        assert tags_dict["sagemaker:notebook-job-origin"] == "PIPELINE_STEP"
-
-        # verify the job output
-        assert job_description["TrainingJobStatus"] == "Completed"
-
-        output_s3_uri = s3_path_join(
-            job_description["OutputDataConfig"]["S3OutputPath"],
-            job_description["TrainingJobName"],
-            "output",
-            "output.tar.gz",
-        )
-        output_notebook_name = job_description["Environment"]["SM_OUTPUT_NOTEBOOK_NAME"]
-
-        def verify_notebook(cells):
-            # verify the env variable set by the init script
-            # logging for better troubleshooting
-            logging.info(cells)
-            assert NotebookUtils.search_output_of_cells(
-                cells, "ParentNotebook: ENV_VAR_FROM_INIT_SCRIPT=FROM_INIT_SCRIPT"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, "SubNotebook: ENV_VAR_FROM_INIT_SCRIPT=FROM_INIT_SCRIPT"
-            )
-
-            # verify the env from the step def
-            assert NotebookUtils.search_output_of_cells(
-                cells, "ParentNotebook: env_key=EnvValueDefinedByParameter"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, "SubNotebook: env_key=EnvValueDefinedByParameter"
-            )
-
-            # verify parameters
-            assert NotebookUtils.search_output_of_cells(
-                cells, "ParentNotebook: company_is_Amazon_FromParameter"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, "SubNotebook: company_is_Amazon_FromParameter"
-            )
-
-            # verify the output of %run simple.ipynb
-            assert NotebookUtils.search_output_of_cells(cells, "aws-sagemaker")
-
-        _download_and_verify_output_notebook(
-            output_s3_uri,
-            output_notebook_name,
-            verify_notebook,
-            sagemaker_session,
-        )
-    finally:
-        try:
-            pipeline.delete()
-        except Exception as error:
-            logging.error(error)
-
-
-def test_notebook_job_dependencies_and_outputs(sagemaker_session):
-    pipeline_name = name_from_base(PIPELINE_NAME, max_length=63)
-    notebook_job_name_base = f"{NOTEBOOK_JOB_NAME_Prefix}-case3-outputs"
-
-    tags = {
-        "sagemaker:user-profile-name": "studio-user",
-        "sagemaker:is-scheduling-notebook-job": "true",
-    }
-
-    input_notebook = "tests/data/workflow/notebook_job_step/simple.ipynb"
-    notebook_job_step1 = NotebookJobStep(
-        name=f"{STEP_NAME}-1",
-        display_name=DISPLAY_NAME,
-        description=DESCRIPTION,
-        notebook_job_name=f"{notebook_job_name_base}-a",
-        input_notebook=input_notebook,
-        image_uri=COSMOS_IMAGE_V0_URI,
-        kernel_name=KERNEL_NAME,
-        role=SAGE_MAKER_ROLE_NAME,
-        tags=tags,
-    )
-
-    # TODO - will be removed when cosmos image is ready
-    notebook_job_step1 = _config_manual_dependency_installation_short_term_workaround(
-        notebook_job_step1
-    )
-
-    input_notebook2 = "tests/data/workflow/notebook_job_step/step2_notebook.ipynb"
-    notebook_job_step2 = NotebookJobStep(
-        name=f"{STEP_NAME}-2",
-        display_name=DISPLAY_NAME,
-        description=DESCRIPTION,
-        notebook_job_name=f"{notebook_job_name_base}-b",
-        input_notebook=input_notebook2,
-        image_uri=COSMOS_IMAGE_V0_URI,
-        kernel_name=KERNEL_NAME,
-        role=SAGE_MAKER_ROLE_NAME,
-        parameters={
-            "step1_JobName": notebook_job_step1.properties.ComputingJobName,
-            "step1_JobStatus": notebook_job_step1.properties.ComputingJobStatus,
-            "step1_NotebookJobInput": notebook_job_step1.properties.NotebookJobInputLocation,
-            "step1_NotebookJobOutput": notebook_job_step1.properties.NotebookJobOutputLocationPrefix,
-            "step1_InputNotebookName": notebook_job_step1.properties.InputNotebookName,
-            "step1_OutputNotebookName": notebook_job_step1.properties.OutputNotebookName,
-        },
-        tags=tags,
-    )
-    notebook_job_step2.add_depends_on([notebook_job_step1])
-
-    # TODO - will be removed when cosmos image is ready
-    notebook_job_step2 = _config_manual_dependency_installation_short_term_workaround(
-        notebook_job_step2
-    )
-
-    pipeline = Pipeline(
-        name=pipeline_name,
-        steps=[notebook_job_step1, notebook_job_step2],
-        sagemaker_session=sagemaker_session,
-    )
-    logging.info(f"Notebook job step test pipeline definition: {pipeline.definition()}")
-
-    try:
-        pipeline.create(get_role(sagemaker_session))
-        execution_steps = _start_and_verify_execution_with_retry(
-            pipeline=pipeline,
-            parameters={},
-        )
-
-        job_description1 = None
-        job_description2 = None
-        for execution_step in execution_steps:
-            if execution_step["StepName"] == f"{STEP_NAME}-1":
-                job_description1 = _get_training_job_details(execution_step, sagemaker_session)
-            elif execution_step["StepName"] == f"{STEP_NAME}-2":
-                job_description2 = _get_training_job_details(execution_step, sagemaker_session)
-
-        step1_job_name = job_description1["TrainingJobName"]
-        step1_job_status = job_description1["TrainingJobStatus"]
-
-        step1_job_input = job_description1["InputDataConfig"][0]["DataSource"]["S3DataSource"][
-            "S3Uri"
-        ]
-        step1_job_input_notebook = job_description1["Environment"]["SM_INPUT_NOTEBOOK_NAME"]
-        step1_job_output = job_description1["OutputDataConfig"]["S3OutputPath"]
-        step1_job_output_notebook = job_description1["Environment"]["SM_OUTPUT_NOTEBOOK_NAME"]
-
-        # verify output notebook of step 2
-        output_s3_uri = s3_path_join(
-            job_description2["OutputDataConfig"]["S3OutputPath"],
-            job_description2["TrainingJobName"],
-            "output",
-            "output.tar.gz",
-        )
-        output_notebook_name = job_description2["Environment"]["SM_OUTPUT_NOTEBOOK_NAME"]
-
-        def verify_notebook(cells):
-            # logging for better troubleshooting
-            logging.info(f"step1_JobName={step1_job_name}")
-            logging.info(f"step1_JobStatus={step1_job_status}")
-            logging.info(f"step1_NotebookJobInput={step1_job_input}")
-            logging.info(f"step1_InputNotebookName={step1_job_input_notebook}")
-            logging.info(f"step1_NotebookJobOutput={step1_job_output}")
-            logging.info(f"step1_OutputNotebookName={step1_job_output_notebook}")
-            logging.info(f"cells: {cells}")
-
-            assert NotebookUtils.search_output_of_cells(cells, f"step1_JobName={step1_job_name}")
-            assert NotebookUtils.search_output_of_cells(
-                cells, f"step1_JobStatus={step1_job_status}"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, f"step1_NotebookJobInput={step1_job_input}"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, f"step1_InputNotebookName={step1_job_input_notebook}"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, f"step1_NotebookJobOutput={step1_job_output}"
-            )
-            assert NotebookUtils.search_output_of_cells(
-                cells, f"step1_OutputNotebookName={step1_job_output_notebook}"
-            )
-
-        _download_and_verify_output_notebook(
-            output_s3_uri, output_notebook_name, verify_notebook, sagemaker_session
-        )
-    finally:
-        try:
-            pipeline.delete()
-        except Exception as error:
-            logging.error(error)
-
-
-def test_notebook_job_depend_on_function_step(sagemaker_session, dummy_container_without_error):
-
-    pipeline_name = name_from_base(PIPELINE_NAME, max_length=63)
-    notebook_job_name_base = f"{NOTEBOOK_JOB_NAME_Prefix}-case4-depend-on"
-
-    tags = {
-        "sagemaker:user-profile-name": "studio-user",
-        "sagemaker:is-scheduling-notebook-job": "true",
-    }
-
-    @step(
-        role=SAGE_MAKER_ROLE_NAME,
-        instance_type="ml.m5.large",
-        keep_alive_period_in_seconds=60,
-        image_uri=dummy_container_without_error,
-    )
-    def get_env_value():
-        """return a static string"""
-        return "env_value_from_function_step"
-
-    env_value = get_env_value()
-
-    input_notebook = "tests/data/workflow/notebook_job_step/simple.ipynb"
-    notebook_job_step1 = NotebookJobStep(
-        name=f"{STEP_NAME}-1",
-        display_name=DISPLAY_NAME,
-        description=DESCRIPTION,
-        notebook_job_name=f"{notebook_job_name_base}-a",
-        input_notebook=input_notebook,
-        image_uri=COSMOS_IMAGE_V0_URI,
-        kernel_name=KERNEL_NAME,
-        role=SAGE_MAKER_ROLE_NAME,
-        tags=tags,
-        environment_variables={"env_key": env_value},
-    )
-
-    # TODO - will be removed when cosmos image is ready
-    notebook_job_step1 = _config_manual_dependency_installation_short_term_workaround(
-        notebook_job_step1
-    )
-
-    pipeline = Pipeline(
-        name=pipeline_name,
-        steps=[env_value, notebook_job_step1],
-        sagemaker_session=sagemaker_session,
-    )
-
-    logging.info(f"Notebook job step test pipeline definition: {pipeline.definition()}")
-
-    try:
-        pipeline.create(get_role(sagemaker_session))
-        execution_steps = _start_and_verify_execution_with_retry(
-            pipeline=pipeline,
-            parameters={},
-        )
-
-        job_description1 = None
-        for execution_step in execution_steps:
-            if execution_step["StepName"] == f"{STEP_NAME}-1":
-                job_description1 = _get_training_job_details(execution_step, sagemaker_session)
-
-        # verify the output of function step is in notebook job's training job def
-        assert job_description1["Environment"]["env_key"] == "env_value_from_function_step"
-    finally:
-        try:
-            pipeline.delete()
-        except Exception as error:
-            logging.error(error)
-
-
-def _get_training_job_details(notebook_job_step, sagemaker_session):
-    training_job_arn = notebook_job_step["Metadata"]["TrainingJob"]["Arn"]
-
-    return sagemaker_session.sagemaker_client.describe_training_job(
-        TrainingJobName=training_job_arn.split("/")[1]
-    )
-
-
-def _download_and_verify_output_notebook(
-    output_s3_uri, output_notebook_name, verification, sagemaker_session, kms_key=None
-):
-    with _tmpdir() as temp_output_folder:
-        S3Downloader.download(
-            output_s3_uri,
-            temp_output_folder,
-            sagemaker_session=sagemaker_session,
-            kms_key=kms_key,
-        )
-
-        with tarfile.open(os.path.join(temp_output_folder, "output.tar.gz"), "r:gz") as tar:
-            tar.extract(output_notebook_name, temp_output_folder)
-
-        notebook_cells = NotebookUtils.get_notebook_cells(
-            os.path.join(temp_output_folder, output_notebook_name)
-        )
-        verification(notebook_cells)
-
-
-def get_role(sagemaker_session):
-    return get_execution_role(sagemaker_session)
-
-
-def _convert_to_flatten_dict(list_of_tag_dicts):
-    """Method to convert tags format.
-
-    from:
-        [{"Key": "tag1", "Value": "value1"}, {"Key": "tag2", "Value": "value2"}]
-    to:
-        {"tag1": "value1", "tag2":"value2"}
-    """
-    result_dict = {}
-    for entry in list_of_tag_dicts:
-        key = entry["Key"]
-        value = entry["Value"]
-        result_dict[key] = value
-    return result_dict
-
-
-def _start_and_verify_execution_with_retry(pipeline: Pipeline, parameters: dict) -> list:
-    for _ in retries(
-        max_retry_count=3,
-        exception_message_prefix="Waiting for a successful execution of pipeline",
-        seconds_to_sleep=10,
-    ):
-        execution = pipeline.start(parameters=parameters)
-        wait_pipeline_execution(execution=execution)
-        execution_steps = execution.list_steps()
-
-        for execution_step in execution_steps:
-            failure_reason = execution_step.get("FailureReason", "")
-            if failure_reason != "":
-                logging.error(f"Pipeline execution failed with error: {failure_reason}. Retrying..")
-                continue
-
-        for execution_step in execution_steps:
-            if execution_step["StepStatus"] != "Succeeded":
-                logging.error(f"execution_steps: {execution_steps}")
-            assert execution_step["StepStatus"] == "Succeeded"
-        return execution_steps
-
-
-def _config_manual_dependency_installation_short_term_workaround(notebook_step):
-    notebook_step._scheduler_container_entry_point = ["/bin/bash"]
-
-    container_arguments = [
-        "-l",
-        "-c",
-        "pip install sagemaker-headless-execution-driver && exec amazon_sagemaker_scheduler",
-    ]
-
-    notebook_step._scheduler_container_arguments = container_arguments
-
-    return notebook_step
-
-
-# TODO - potentially move to a common place for reuse.
-class NotebookUtils:
-    @classmethod
-    def get_notebook_cells(cls, notebook_path):
-        return nbf.read(notebook_path, nbf.NO_CONVERT).cells
-
-    # return the first cell containing the target string
-    @classmethod
-    def search_output_of_cells(cls, cells, target):
-        for cell in cells:
-            if "outputs" in cell:
-                for output in cell["outputs"]:
-                    if "text" in output and output["text"].find(target) > -1:
-                        return cell
-                    if "output_type" in output and output["output_type"] == "display_data":
-                        if str(output["data"]).find(target) > -1:
-                            return cell
-        return None
-
-    # return true if there is error
-    @classmethod
-    def search_error_from_output_of_cells(cls, cells):
-        for cell in cells:
-            if "outputs" in cell:
-                for output in cell["outputs"]:
-                    if output.output_type == "error":
-                        return True
-        return False

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-10-23 11:28:51[0m
[92mHash: 488adba580378892cfa138efd578bfc7aad2e856[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart gated model artifacts (#4215)

* feat: jumpstart private model artifacts

* chore: use gated instead of private keyword[0m
@@ -807,233 +807,6 @@ SPECIAL_MODEL_SPECS_DICT = {
         "inference_enable_network_isolation": True,
         "training_enable_network_isolation": False,
     },
-    "private-model": {
-        "model_id": "pytorch-ic-mobilenet-v2",
-        "gated_bucket": True,
-        "url": "https://pytorch.org/hub/pytorch_vision_mobilenet_v2/",
-        "version": "1.0.0",
-        "min_sdk_version": "2.49.0",
-        "training_supported": True,
-        "incremental_training_supported": True,
-        "hosting_model_package_arns": {
-            "us-west-2": "arn:aws:sagemaker:us-west-2:594846645681:model-package/ll"
-            "ama2-7b-v3-[93m740347e540da35b4ab9f6fc0ab3fed2c[0m"
-        },
-        "hosting_ecr_specs": {
-            "framework": "pytorch",
-            "framework_version": "1.5.0",
-            "py_version": "py3",
-        },
-        "hosting_instance_type_variants": {
-            "regional_aliases": {
-                "us-west-2": {
-                    "gpu_image_uri": "763104351884.dkr.ecr.us-west-2.amazonaws.com/"
-                    "huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04",
-                    "cpu_image_uri": "867930986793.dkr.us-west-2.amazonaws.com/cpu-blah",
-                    "inf_model_package_arn": "us-west-2/blah/blah/blah/inf",
-                    "gpu_model_package_arn": "us-west-2/blah/blah/blah/gpu",
-                }
-            },
-            "variants": {
-                "p2": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "p3": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "p4": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "g4dn": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "m2": {"regional_properties": {"image_uri": "$cpu_image_uri"}},
-                "c2": {"regional_properties": {"image_uri": "$cpu_image_uri"}},
-                "ml.g5.48xlarge": {
-                    "properties": {"environment_variables": {"TENSOR_PARALLEL_DEGREE": "8"}}
-                },
-                "ml.g5.12xlarge": {
-                    "properties": {"environment_variables": {"TENSOR_PARALLEL_DEGREE": "4"}}
-                },
-                "inf1": {"regional_properties": {"model_package_arn": "$inf_model_package_arn"}},
-                "inf2": {"regional_properties": {"model_package_arn": "$inf_model_package_arn"}},
-            },
-        },
-        "training_ecr_specs": {
-            "framework": "pytorch",
-            "framework_version": "1.5.0",
-            "py_version": "py3",
-        },
-        "training_instance_type_variants": None,
-        "hosting_artifact_key": "pytorch-infer/infer-pytorch-ic-mobilenet-v2.tar.gz",
-        "training_artifact_key": "pytorch-training/train-pytorch-ic-mobilenet-v2.tar.gz",
-        "hosting_script_key": "source-directory-tarballs/pytorch/inference/ic/v1.0.0/sourcedir.tar.gz",
-        "training_script_key": "source-directory-tarballs/pytorch/transfer_learning/ic/v1.0.0/sourcedir.tar.gz",
-        "training_prepacked_script_key": None,
-        "hosting_prepacked_artifact_key": None,
-        "training_model_package_artifact_uris": None,
-        "deprecate_warn_message": None,
-        "deprecated_message": None,
-        "hosting_eula_key": None,
-        "hyperparameters": [
-            {
-                "name": "epochs",
-                "type": "int",
-                "default": 3,
-                "min": 1,
-                "max": 1000,
-                "scope": "algorithm",
-            },
-            {
-                "name": "adam-learning-rate",
-                "type": "float",
-                "default": 0.05,
-                "min": 1e-08,
-                "max": 1,
-                "scope": "algorithm",
-            },
-            {
-                "name": "batch-size",
-                "type": "int",
-                "default": 4,
-                "min": 1,
-                "max": 1024,
-                "scope": "algorithm",
-            },
-            {
-                "name": "sagemaker_submit_directory",
-                "type": "text",
-                "default": "/opt/ml/input/data/code/sourcedir.tar.gz",
-                "scope": "container",
-            },
-            {
-                "name": "sagemaker_program",
-                "type": "text",
-                "default": "transfer_learning.py",
-                "scope": "container",
-            },
-            {
-                "name": "sagemaker_container_log_level",
-                "type": "text",
-                "default": "20",
-                "scope": "container",
-            },
-        ],
-        "inference_environment_variables": [
-            {
-                "name": "SAGEMAKER_PROGRAM",
-                "type": "text",
-                "default": "inference.py",
-                "scope": "container",
-                "required_for_model_class": True,
-            },
-            {
-                "name": "SAGEMAKER_SUBMIT_DIRECTORY",
-                "type": "text",
-                "default": "/opt/ml/model/code",
-                "scope": "container",
-                "required_for_model_class": False,
-            },
-            {
-                "name": "SAGEMAKER_CONTAINER_LOG_LEVEL",
-                "type": "text",
-                "default": "20",
-                "scope": "container",
-                "required_for_model_class": False,
-            },
-            {
-                "name": "SAGEMAKER_MODEL_SERVER_TIMEOUT",
-                "type": "text",
-                "default": "3600",
-                "scope": "container",
-                "required_for_model_class": False,
-            },
-            {
-                "name": "ENDPOINT_SERVER_TIMEOUT",
-                "type": "int",
-                "default": 3600,
-                "scope": "container",
-                "required_for_model_class": True,
-            },
-            {
-                "name": "MODEL_CACHE_ROOT",
-                "type": "text",
-                "default": "/opt/ml/model",
-                "scope": "container",
-                "required_for_model_class": True,
-            },
-            {
-                "name": "SAGEMAKER_ENV",
-                "type": "text",
-                "default": "1",
-                "scope": "container",
-                "required_for_model_class": True,
-            },
-            {
-                "name": "SAGEMAKER_MODEL_SERVER_WORKERS",
-                "type": "int",
-                "default": 1,
-                "scope": "container",
-                "required_for_model_class": True,
-            },
-        ],
-        "inference_vulnerable": False,
-        "inference_dependencies": [],
-        "inference_vulnerabilities": [],
-        "training_vulnerable": False,
-        "training_dependencies": [],
-        "training_vulnerabilities": [],
-        "deprecated": False,
-        "default_inference_instance_type": "ml.p2.xlarge",
-        "supported_inference_instance_types": [
-            "ml.p2.xlarge",
-            "ml.p3.2xlarge",
-            "ml.g4dn.xlarge",
-            "ml.m5.large",
-            "ml.m5.xlarge",
-            "ml.c5.xlarge",
-            "ml.c5.2xlarge",
-        ],
-        "default_training_instance_type": "ml.p3.2xlarge",
-        "supported_training_instance_types": [
-            "ml.p3.2xlarge",
-            "ml.p2.xlarge",
-            "ml.g4dn.2xlarge",
-            "ml.m5.xlarge",
-            "ml.c5.2xlarge",
-        ],
-        "hosting_use_script_uri": True,
-        "metrics": [{"Regex": "val_accuracy: ([0-9\\.]+)", "Name": "pytorch-ic:val-accuracy"}],
-        "model_kwargs": {"some-model-kwarg-key": "some-model-kwarg-value"},
-        "deploy_kwargs": {"some-model-deploy-kwarg-key": "some-model-deploy-kwarg-value"},
-        "estimator_kwargs": {
-            "encrypt_inter_container_traffic": True,
-        },
-        "fit_kwargs": {"some-estimator-fit-key": "some-estimator-fit-value"},
-        "predictor_specs": {
-            "supported_content_types": ["application/x-image"],
-            "supported_accept_types": ["application/json;verbose", "application/json"],
-            "default_content_type": "application/x-image",
-            "default_accept_type": "application/json",
-        },
-        "inference_volume_size": 123,
-        "training_volume_size": 456,
-        "inference_enable_network_isolation": True,
-        "training_enable_network_isolation": False,
-        "resource_name_base": "dfsdfsds",
-    },
     "js-model-package-arn": {
         "model_id": "meta-textgeneration-llama-2-7b-f",
         "url": "https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
@@ -4342,7 +4115,6 @@ BASE_SPEC = {
     "min_sdk_version": "2.49.0",
     "training_supported": True,
     "incremental_training_supported": True,
-    "gated_bucket": False,
     "default_payloads": None,
     "hosting_ecr_specs": {
         "framework": "pytorch",

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-10-17 13:10:59[0m
[92mHash: ef3d2b2cc915d442e50614a6b0b954004e8eb2ce[0m
[92mFilepath: tests/unit/sagemaker/feature_store/feature_processor/lineage/test_constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: Feature Processor event based triggers (#1132) (#4202)

Co-authored-by: jiapinw <jiapinw@amazon.com>
[0m
@@ -33,7 +33,6 @@ from sagemaker.feature_store.feature_processor.lineage._feature_group_contexts i
 from sagemaker.feature_store.feature_processor.lineage._pipeline_schedule import (
     PipelineSchedule,
 )
-from sagemaker.feature_store.feature_processor.lineage._pipeline_trigger import PipelineTrigger
 from sagemaker.feature_store.feature_processor.lineage._transformation_code import (
     TransformationCode,
 )
@@ -113,44 +112,6 @@ PIPELINE_SCHEDULE_2 = PipelineSchedule(
     start_date="234234234",
 )
 
-PIPELINE_TRIGGER = PipelineTrigger(
-    trigger_name="trigger-name",
-    trigger_arn="trigger-arn",
-    pipeline_name="pipeline-name",
-    event_pattern="event-pattern",
-    state="Enabled",
-)
-
-PIPELINE_TRIGGER_2 = PipelineTrigger(
-    trigger_name="trigger-name-2",
-    trigger_arn="trigger-arn",
-    pipeline_name="pipeline-name",
-    event_pattern="event-pattern-2",
-    state="Enabled",
-)
-
-PIPELINE_TRIGGER_ARTIFACT: Artifact = Artifact(
-    artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
-    artifact_name="sm-fs-fe-trigger-trigger-name",
-    artifact_type="PipelineTrigger",
-    source={"source_uri": "trigger-arn"},
-    properties=dict(
-        pipeline_name=PIPELINE_TRIGGER.pipeline_name,
-        event_pattern=PIPELINE_TRIGGER.event_pattern,
-        state=PIPELINE_TRIGGER.state,
-    ),
-)
-
-PIPELINE_TRIGGER_ARTIFACT_SUMMARY: ArtifactSummary = ArtifactSummary(
-    artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
-    artifact_name="sm-fs-fe-trigger-trigger-name",
-    source=ArtifactSource(
-        source_uri="trigger-arn",
-    ),
-    artifact_type="PipelineTrigger",
-    creation_time=datetime.datetime(2023, 4, 27, 21, 4, 17, 926000),
-)
-
 ARTIFACT_RESULT: Artifact = Artifact(
     artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
     artifact_name="sm-fs-fe-raw-data",

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-10-16 11:20:44[0m
[92mHash: a44a7554bcef698f68e07fb1f3652607ed2d7d8e[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart model package arn instance type variants (#4186)

[0m
@@ -181,10 +181,6 @@ SPECIAL_MODEL_SPECS_DICT = {
         "min_sdk_version": "2.49.0",
         "training_supported": True,
         "incremental_training_supported": True,
-        "hosting_model_package_arns": {
-            "us-west-2": "arn:aws:sagemaker:us-west-2:594846645681:model-package/ll"
-            "ama2-7b-v3-[93m740347e540da35b4ab9f6fc0ab3fed2c[0m"
-        },
         "hosting_ecr_specs": {
             "framework": "pytorch",
             "framework_version": "1.5.0",
@@ -196,35 +192,13 @@ SPECIAL_MODEL_SPECS_DICT = {
                     "gpu_image_uri": "763104351884.dkr.ecr.us-west-2.amazonaws.com/"
                     "huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04",
                     "cpu_image_uri": "867930986793.dkr.us-west-2.amazonaws.com/cpu-blah",
-                    "inf_model_package_arn": "us-west-2/blah/blah/blah/inf",
-                    "gpu_model_package_arn": "us-west-2/blah/blah/blah/gpu",
                 }
             },
             "variants": {
-                "p2": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "p3": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "p4": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
-                "g4dn": {
-                    "regional_properties": {
-                        "image_uri": "$gpu_image_uri",
-                        "model_package_arn": "$gpu_model_package_arn",
-                    }
-                },
+                "p2": {"regional_properties": {"image_uri": "$gpu_image_uri"}},
+                "p3": {"regional_properties": {"image_uri": "$gpu_image_uri"}},
+                "p4": {"regional_properties": {"image_uri": "$gpu_image_uri"}},
+                "g4dn": {"regional_properties": {"image_uri": "$gpu_image_uri"}},
                 "m2": {"regional_properties": {"image_uri": "$cpu_image_uri"}},
                 "c2": {"regional_properties": {"image_uri": "$cpu_image_uri"}},
                 "ml.g5.48xlarge": {
@@ -233,8 +207,6 @@ SPECIAL_MODEL_SPECS_DICT = {
                 "ml.g5.12xlarge": {
                     "properties": {"environment_variables": {"TENSOR_PARALLEL_DEGREE": "4"}}
                 },
-                "inf1": {"regional_properties": {"model_package_arn": "$inf_model_package_arn"}},
-                "inf2": {"regional_properties": {"model_package_arn": "$inf_model_package_arn"}},
             },
         },
         "training_ecr_specs": {
@@ -252,6 +224,7 @@ SPECIAL_MODEL_SPECS_DICT = {
         "training_model_package_artifact_uris": None,
         "deprecate_warn_message": None,
         "deprecated_message": None,
+        "hosting_model_package_arns": None,
         "hosting_eula_key": None,
         "hyperparameters": [
             {

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-10-16 11:20:44[0m
[92mHash: a44a7554bcef698f68e07fb1f3652607ed2d7d8e[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/test_artifacts.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart model package arn instance type variants (#4186)

[0m
@@ -12,18 +12,13 @@
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
 import unittest
-from unittest.mock import Mock
 
 
 from mock.mock import patch
-import pytest
 
 from sagemaker.jumpstart import artifacts
-from sagemaker.jumpstart.artifacts.model_packages import _retrieve_model_package_arn
-from sagemaker.jumpstart.enums import JumpStartScriptScope
 
-from tests.unit.sagemaker.jumpstart.utils import get_spec_from_base_spec, get_special_model_spec
-from tests.unit.sagemaker.workflow.conftest import mock_client
+from tests.unit.sagemaker.jumpstart.utils import get_spec_from_base_spec
 
 
 @patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
@@ -134,109 +129,3 @@ class RetrieveKwargsTest(unittest.TestCase):
         )
 
         assert kwargs == {"some-estimator-fit-key": "some-estimator-fit-value"}
-
-
-class RetrieveModelPackageArnTest(unittest.TestCase):
-
-    mock_session = Mock(s3_client=mock_client)
-
-    @patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
-    def test_retrieve_model_package_arn(self, patched_get_model_specs):
-        patched_get_model_specs.side_effect = get_special_model_spec
-
-        model_id = "variant-model"
-        region = "us-west-2"
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.p2.48xlarge",
-            )
-            == "us-west-2/blah/blah/blah/gpu"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.p4.2xlarge",
-            )
-            == "us-west-2/blah/blah/blah/gpu"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.inf1.2xlarge",
-            )
-            == "us-west-2/blah/blah/blah/inf"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.inf2.12xlarge",
-            )
-            == "us-west-2/blah/blah/blah/inf"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.afasfasf.12xlarge",
-            )
-            == "arn:aws:sagemaker:us-west-2:594846645681:model-package/llama2-7b-v3-[93m[93m[93m740347e540da35b4ab9f6fc0ab3fed2c[0m[0m[0m"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.m2.12xlarge",
-            )
-            == "arn:aws:sagemaker:us-west-2:594846645681:model-package/llama2-7b-v3-[93m[93m[93m740347e540da35b4ab9f6fc0ab3fed2c[0m[0m[0m"
-        )
-
-        assert (
-            _retrieve_model_package_arn(
-                region=region,
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="nobodycares",
-            )
-            == "arn:aws:sagemaker:us-west-2:594846645681:model-package/llama2-7b-v3-[93m[93m[93m740347e540da35b4ab9f6fc0ab3fed2c[0m[0m[0m"
-        )
-
-        with pytest.raises(ValueError):
-            _retrieve_model_package_arn(
-                region="cn-north-1",
-                model_id=model_id,
-                scope=JumpStartScriptScope.INFERENCE,
-                model_version="*",
-                sagemaker_session=self.mock_session,
-                instance_type="ml.p2.12xlarge",
-            )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-08-16 15:38:45[0m
[92mHash: 1f3754d9ffa95a60ef325a88805d31405c527bc3[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/model/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: jumpstart cache using sagemaker session s3 client (#4051)

[0m
@@ -18,19 +18,19 @@ import unittest
 from mock import MagicMock
 import pytest
 from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig
-from sagemaker.jumpstart.constants import DEFAULT_JUMPSTART_SAGEMAKER_SESSION
 from sagemaker.jumpstart.enums import JumpStartScriptScope
 
 from sagemaker.jumpstart.model import JumpStartModel
 from sagemaker.model import Model
 from sagemaker.predictor import Predictor
+from sagemaker.session import Session
 
 from tests.unit.sagemaker.jumpstart.utils import get_special_model_spec, overwrite_dictionary
 
 
 execution_role = "fake role! do not use!"
 region = "us-west-2"
-sagemaker_session = DEFAULT_JUMPSTART_SAGEMAKER_SESSION
+sagemaker_session = Session()
 sagemaker_session.get_caller_identity_arn = lambda: execution_role
 default_predictor = Predictor("blah", sagemaker_session)
 default_predictor_with_presets = Predictor(
@@ -39,9 +39,6 @@ default_predictor_with_presets = Predictor(
 
 
 class ModelTest(unittest.TestCase):
-
-    mock_session_empty_config = MagicMock(sagemaker_config={})
-
     @mock.patch("sagemaker.utils.sagemaker_timestamp")
     @mock.patch("sagemaker.jumpstart.model.is_valid_model_id")
     @mock.patch("sagemaker.jumpstart.factory.model.Session")
@@ -419,7 +416,6 @@ class ModelTest(unittest.TestCase):
             region=region,
             tolerate_deprecated_model=False,
             tolerate_vulnerable_model=False,
-            sagemaker_session=model.sagemaker_session,
         )
         self.assertEqual(type(predictor), Predictor)
         self.assertEqual(predictor, default_predictor_with_presets)
@@ -536,14 +532,12 @@ class ModelTest(unittest.TestCase):
                     model_version=None,
                     region=None,
                     script=JumpStartScriptScope.INFERENCE,
-                    sagemaker_session=None,
                 ),
                 mock.call(
                     model_id="js-trainable-model",
                     model_version=None,
                     region=None,
                     script=JumpStartScriptScope.INFERENCE,
-                    sagemaker_session=None,
                 ),
             ]
         )
@@ -564,24 +558,24 @@ class ModelTest(unittest.TestCase):
                     model_version=None,
                     region=None,
                     script=JumpStartScriptScope.INFERENCE,
-                    sagemaker_session=None,
                 ),
                 mock.call(
                     model_id="js-trainable-model",
                     model_version=None,
                     region=None,
                     script=JumpStartScriptScope.INFERENCE,
-                    sagemaker_session=None,
                 ),
             ]
         )
 
     @mock.patch("sagemaker.jumpstart.model.is_valid_model_id")
+    @mock.patch("sagemaker.jumpstart.factory.model.Session")
     @mock.patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
     @mock.patch("sagemaker.jumpstart.factory.model.JUMPSTART_DEFAULT_REGION_NAME", region)
     def test_jumpstart_model_package_arn(
         self,
         mock_get_model_specs: mock.Mock,
+        mock_session: mock.Mock,
         mock_is_valid_model_id: mock.Mock,
     ):
 
@@ -591,9 +585,9 @@ class ModelTest(unittest.TestCase):
 
         mock_get_model_specs.side_effect = get_special_model_spec
 
-        mock_session = MagicMock(sagemaker_config={})
+        mock_session.return_value = MagicMock(sagemaker_config={})
 
-        model = JumpStartModel(model_id=model_id, sagemaker_session=mock_session)
+        model = JumpStartModel(model_id=model_id)
 
         tag = {"Key": "foo", "Value": "bar"}
         tags = [tag]
@@ -601,21 +595,23 @@ class ModelTest(unittest.TestCase):
         model.deploy(tags=tags)
 
         self.assertEqual(
-            mock_session.create_model.call_args[0][2],
+            mock_session.return_value.create_model.call_args[0][2],
             {
                 "ModelPackageName": "arn:aws:sagemaker:us-west-2:594846645681:model-package"
                 "/llama2-7b-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m"
             },
         )
 
-        self.assertIn(tag, mock_session.create_model.call_args[1]["tags"])
+        self.assertIn(tag, mock_session.return_value.create_model.call_args[1]["tags"])
 
     @mock.patch("sagemaker.jumpstart.model.is_valid_model_id")
+    @mock.patch("sagemaker.jumpstart.factory.model.Session")
     @mock.patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
     @mock.patch("sagemaker.jumpstart.factory.model.JUMPSTART_DEFAULT_REGION_NAME", region)
     def test_jumpstart_model_package_arn_override(
         self,
         mock_get_model_specs: mock.Mock,
+        mock_session: mock.Mock,
         mock_is_valid_model_id: mock.Mock,
     ):
 
@@ -626,20 +622,18 @@ class ModelTest(unittest.TestCase):
 
         mock_get_model_specs.side_effect = get_special_model_spec
 
-        mock_session = MagicMock(sagemaker_config={})
+        mock_session.return_value = MagicMock(sagemaker_config={})
 
         model_package_arn = (
             "arn:aws:sagemaker:us-west-2:867530986753:model-package/"
             "llama2-ynnej-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m"
         )
-        model = JumpStartModel(
-            model_id=model_id, model_package_arn=model_package_arn, sagemaker_session=mock_session
-        )
+        model = JumpStartModel(model_id=model_id, model_package_arn=model_package_arn)
 
         model.deploy()
 
         self.assertEqual(
-            mock_session.create_model.call_args[0][2],
+            mock_session.return_value.create_model.call_args[0][2],
             {
                 "ModelPackageName": model_package_arn,
                 "Environment": {

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-08-02 13:04:02[0m
[92mHash: 2345fb0d5afa6081e4356ac1a76c64c517ad384c[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: meta llama fine tuning (#4021)

* feat: meta llama fine tuning

* chore: simplify code

* chore: rename function to _retrieve_model_package_model_artifact_s3_uri

* chore: add unit tests, fixes

* fix: env var collisions, unit test

* chore: de morgans law simplification

* chore: address PR comments

* chore: improve docstring

* fix: better metadata for unit tests

* fix: flake8 issues[0m
@@ -641,108 +641,6 @@ SPECIAL_MODEL_SPECS_DICT = {
             "default_accept_type": "application/json",
         },
     },
-    "js-gated-artifact-trainable-model": {
-        "model_id": "meta-textgeneration-llama-2-7b-f",
-        "url": "https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
-        "version": "2.0.0",
-        "min_sdk_version": "2.173.0",
-        "training_supported": True,
-        "incremental_training_supported": False,
-        "hosting_ecr_specs": {
-            "framework": "djl-deepspeed",
-            "framework_version": "0.21.0",
-            "py_version": "py39",
-        },
-        "hosting_artifact_key": "meta-infer/infer-meta-textgeneration-llama-2-7b-f.tar.gz",
-        "hosting_use_script_uri": False,
-        "hosting_script_key": "source-directory-tarballs/meta/inference/textgeneration/v1.0.0/sourcedir.tar.gz",
-        "hosting_eula_key": "fmhMetadata/eula/llamaEula.txt",
-        "hosting_model_package_arns": {
-            "us-west-2": "arn:aws:sagemaker:us-west-2:594846645681:model-package/"
-            "llama2-7b-f-[93m[93m[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m[0m[0m",
-            "us-east-1": "arn:aws:sagemaker:us-east-1:865070037744:model-package/"
-            "llama2-7b-f-[93m[93m[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m[0m[0m",
-            "eu-west-1": "arn:aws:sagemaker:eu-west-1:985815980388:model-package/"
-            "llama2-7b-f-[93m[93m[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m[0m[0m",
-            "ap-southeast-1": "arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/"
-            "llama2-7b-f-[93m[93m[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m[0m[0m",
-        },
-        "training_model_package_artifact_uris": {
-            "us-west-2": "s3://jumpstart-cache-alpha-us-west-2/dummy.tar.gz",
-            "us-east-1": "s3://jumpstart-cache-alpha-us-west-2/dummy.tar.gz",
-            "eu-west-1": "s3://jumpstart-cache-alpha-us-west-2/dummy.tar.gz",
-            "ap-southeast-1": "s3://jumpstart-cache-alpha-us-west-2/dummy.tar.gz",
-        },
-        "inference_vulnerable": False,
-        "inference_dependencies": [],
-        "inference_vulnerabilities": [],
-        "training_vulnerable": False,
-        "training_dependencies": [],
-        "training_vulnerabilities": [],
-        "deprecated": False,
-        "hyperparameters": [
-            {
-                "name": "sagemaker_submit_directory",
-                "type": "text",
-                "default": "/opt/ml/input/data/code/sourcedir.tar.gz",
-                "scope": "container",
-            },
-            {
-                "name": "sagemaker_program",
-                "type": "text",
-                "default": "transfer_learning.py",
-                "scope": "container",
-            },
-            {
-                "name": "sagemaker_container_log_level",
-                "type": "text",
-                "default": "20",
-                "scope": "container",
-            },
-        ],
-        "training_script_key": "source-directory-tarballs/meta/transfer_learning/"
-        "textgeneration/v1.0.0/sourcedir.tar.gz",
-        "training_ecr_specs": {
-            "framework": "djl-deepspeed",
-            "framework_version": "0.21.0",
-            "py_version": "py39",
-        },
-        "training_artifact_key": "meta-training/train-meta-textgeneration-llama-2-7b-f.tar.gz",
-        "inference_environment_variables": [],
-        "metrics": [],
-        "default_inference_instance_type": "ml.g5.2xlarge",
-        "supported_inference_instance_types": [
-            "ml.g5.2xlarge",
-            "ml.g5.4xlarge",
-            "ml.g5.8xlarge",
-            "ml.g5.12xlarge",
-            "ml.g5.24xlarge",
-            "ml.g5.48xlarge",
-            "ml.p4d.24xlarge",
-        ],
-        "default_training_instance_type": "ml.p3.2xlarge",
-        "supported_training_instance_types": ["ml.p3.2xlarge", "ml.p2.8xlarge", "ml.g4dn.xlarge"],
-        "model_kwargs": {},
-        "deploy_kwargs": {
-            "model_data_download_timeout": 3600,
-            "container_startup_health_check_timeout": 3600,
-        },
-        "estimator_kwargs": {"encrypt_inter_container_traffic": True, "max_run": 360000},
-        "fit_kwargs": {},
-        "predictor_specs": {
-            "supported_content_types": ["application/json"],
-            "supported_accept_types": ["application/json"],
-            "default_content_type": "application/json",
-            "default_accept_type": "application/json",
-        },
-        "inference_volume_size": 256,
-        "inference_enable_network_isolation": True,
-        "training_enable_network_isolation": True,
-        "default_training_dataset_key": "training-datasets/wikitext/",
-        "validation_supported": False,
-        "fine_tuning_supported": True,
-        "resource_name_base": "meta-textgeneration-llama-2-7b-f",
-    },
     "js-trainable-model": {
         "model_id": "autogluon-classification-ensemble",
         "url": "https://auto.gluon.ai/stable/index.html",
@@ -2458,7 +2356,6 @@ BASE_SPEC = {
     "training_script_key": "source-directory-tarballs/pytorch/transfer_learning/ic/v1.0.0/sourcedir.tar.gz",
     "training_prepacked_script_key": None,
     "hosting_prepacked_artifact_key": None,
-    "training_model_package_artifact_uris": None,
     "deprecate_warn_message": None,
     "deprecated_message": None,
     "hosting_model_package_arns": None,
@@ -2590,7 +2487,6 @@ BASE_SPEC = {
         "ml.m5.xlarge",
         "ml.c5.2xlarge",
     ],
-    "hosting_use_script_uri": True,
     "metrics": [{"Regex": "val_accuracy: ([0-9\\.]+)", "Name": "pytorch-ic:val-accuracy"}],
     "model_kwargs": {"some-model-kwarg-key": "some-model-kwarg-value"},
     "deploy_kwargs": {"some-model-deploy-kwarg-key": "some-model-deploy-kwarg-value"},

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-07-18 13:47:35[0m
[92mHash: c6d412f806058f58d5021b858c7c873b2df5b1c1[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: chore: add jumpstart llama 2 tests (#4004)

[0m
@@ -14,63 +14,6 @@ from __future__ import absolute_import
 
 
 SPECIAL_MODEL_SPECS_DICT = {
-    "js-model-package-arn": {
-        "model_id": "meta-textgeneration-llama-2-7b-f",
-        "url": "https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
-        "version": "1.0.0",
-        "min_sdk_version": "2.173.0",
-        "training_supported": False,
-        "incremental_training_supported": False,
-        "hosting_ecr_specs": {
-            "framework": "pytorch",
-            "framework_version": "1.12.0",
-            "py_version": "py38",
-        },
-        "hosting_artifact_key": "meta-infer/infer-meta-textgeneration-llama-2-7b-f.tar.gz",
-        "hosting_script_key": "source-directory-tarballs/meta/inference/textgeneration/v1.0.0/sourcedir.tar.gz",
-        "hosting_eula_key": "fmhMetadata/eula/llamaEula.txt",
-        "hosting_model_package_arns": {
-            "us-west-2": "arn:aws:sagemaker:us-west-2:594846645681:model-package/"
-            "llama2-7b-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m",
-            "us-east-1": "arn:aws:sagemaker:us-east-1:865070037744:model-package/"
-            "llama2-7b-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m",
-        },
-        "inference_vulnerable": False,
-        "inference_dependencies": [],
-        "inference_vulnerabilities": [],
-        "training_vulnerable": False,
-        "training_dependencies": [],
-        "training_vulnerabilities": [],
-        "deprecated": False,
-        "inference_environment_variables": [],
-        "metrics": [],
-        "default_inference_instance_type": "ml.g5.2xlarge",
-        "supported_inference_instance_types": [
-            "ml.g5.2xlarge",
-            "ml.g5.4xlarge",
-            "ml.g5.8xlarge",
-            "ml.g5.12xlarge",
-            "ml.g5.24xlarge",
-            "ml.g5.48xlarge",
-            "ml.p4d.24xlarge",
-        ],
-        "model_kwargs": {},
-        "deploy_kwargs": {
-            "model_data_download_timeout": 3600,
-            "container_startup_health_check_timeout": 3600,
-        },
-        "predictor_specs": {
-            "supported_content_types": ["application/json"],
-            "supported_accept_types": ["application/json"],
-            "default_content_type": "application/json",
-            "default_accept_type": "application/json",
-        },
-        "inference_volume_size": 256,
-        "inference_enable_network_isolation": True,
-        "validation_supported": False,
-        "fine_tuning_supported": False,
-        "resource_name_base": "meta-textgeneration-llama-2-7b-f",
-    },
     "js-trainable-model-prepacked": {
         "model_id": "huggingface-text2text-flan-t5-base",
         "url": "https://huggingface.co/google/flan-t5-base",

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-07-18 13:47:35[0m
[92mHash: c6d412f806058f58d5021b858c7c873b2df5b1c1[0m
[92mFilepath: tests/unit/sagemaker/jumpstart/model/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: chore: add jumpstart llama 2 tests (#4004)

[0m
@@ -15,7 +15,6 @@ from inspect import signature
 from typing import Optional, Set
 from unittest import mock
 import unittest
-from mock import MagicMock
 import pytest
 from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig
 from sagemaker.jumpstart.enums import JumpStartScriptScope
@@ -568,79 +567,6 @@ class ModelTest(unittest.TestCase):
             ]
         )
 
-    @mock.patch("sagemaker.jumpstart.model.is_valid_model_id")
-    @mock.patch("sagemaker.jumpstart.factory.model.Session")
-    @mock.patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
-    @mock.patch("sagemaker.jumpstart.factory.model.JUMPSTART_DEFAULT_REGION_NAME", region)
-    def test_jumpstart_model_package_arn(
-        self,
-        mock_get_model_specs: mock.Mock,
-        mock_session: mock.Mock,
-        mock_is_valid_model_id: mock.Mock,
-    ):
-
-        mock_is_valid_model_id.return_value = True
-
-        model_id, _ = "js-model-package-arn", "*"
-
-        mock_get_model_specs.side_effect = get_special_model_spec
-
-        mock_session.return_value = MagicMock(sagemaker_config={})
-
-        model = JumpStartModel(model_id=model_id)
-
-        model.deploy()
-
-        self.assertEqual(
-            mock_session.return_value.create_model.call_args[0][2],
-            {
-                "ModelPackageName": "arn:aws:sagemaker:us-west-2:594846645681:model-package"
-                "/llama2-7b-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m"
-            },
-        )
-
-    @mock.patch("sagemaker.jumpstart.model.is_valid_model_id")
-    @mock.patch("sagemaker.jumpstart.factory.model.Session")
-    @mock.patch("sagemaker.jumpstart.accessors.JumpStartModelsAccessor.get_model_specs")
-    @mock.patch("sagemaker.jumpstart.factory.model.JUMPSTART_DEFAULT_REGION_NAME", region)
-    def test_jumpstart_model_package_arn_override(
-        self,
-        mock_get_model_specs: mock.Mock,
-        mock_session: mock.Mock,
-        mock_is_valid_model_id: mock.Mock,
-    ):
-
-        mock_is_valid_model_id.return_value = True
-
-        # arbitrary model without model packarn arn
-        model_id, _ = "js-trainable-model", "*"
-
-        mock_get_model_specs.side_effect = get_special_model_spec
-
-        mock_session.return_value = MagicMock(sagemaker_config={})
-
-        model_package_arn = (
-            "arn:aws:sagemaker:us-west-2:867530986753:model-package/"
-            "llama2-ynnej-f-[93m[93me46eb8a833643ed58aaccd81498972c3[0m[0m"
-        )
-        model = JumpStartModel(model_id=model_id, model_package_arn=model_package_arn)
-
-        model.deploy()
-
-        self.assertEqual(
-            mock_session.return_value.create_model.call_args[0][2],
-            {
-                "ModelPackageName": model_package_arn,
-                "Environment": {
-                    "ENDPOINT_SERVER_TIMEOUT": "3600",
-                    "MODEL_CACHE_ROOT": "/opt/ml/model",
-                    "SAGEMAKER_ENV": "1",
-                    "SAGEMAKER_MODEL_SERVER_WORKERS": "1",
-                    "SAGEMAKER_PROGRAM": "inference.py",
-                },
-            },
-        )
-
 
 def test_jumpstart_model_requires_model_id():
     with pytest.raises(ValueError):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-07-15 16:43:26[0m
[92mHash: 0a5b45029cdb26f3746829bd88704d0ad1cdc5c8[0m
[92mFilepath: src/sagemaker/jumpstart/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart EULA models (#3999)

[0m
@@ -14,7 +14,6 @@
 
 from __future__ import absolute_import
 import logging
-import re
 
 from typing import Dict, List, Optional, Union
 from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig
@@ -31,7 +30,7 @@ from sagemaker.jumpstart.factory.model import (
 )
 from sagemaker.jumpstart.utils import is_valid_model_id
 from sagemaker.utils import stringify_object
-from sagemaker.model import MODEL_PACKAGE_ARN_PATTERN, Model
+from sagemaker.model import Model
 from sagemaker.model_monitor.data_capture_config import DataCaptureConfig
 from sagemaker.predictor import PredictorBase
 from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig
@@ -72,7 +71,6 @@ class JumpStartModel(Model):
         container_log_level: Optional[Union[int, PipelineVariable]] = None,
         dependencies: Optional[List[str]] = None,
         git_config: Optional[Dict[str, str]] = None,
-        model_package_arn: Optional[str] = None,
     ):
         """Initializes a ``JumpStartModel``.
 
@@ -251,9 +249,6 @@ class JumpStartModel(Model):
                     >>>               'branch': 'test-branch-git-config',
                     >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
 
-            model_package_arn (Optional[str]): An existing SageMaker Model Package arn,
-                can be just the name if your account owns the Model Package.
-                ``model_data`` is not required. (Default: None).
         Raises:
             ValueError: If the model ID is not recognized by JumpStart.
         """
@@ -296,7 +291,6 @@ class JumpStartModel(Model):
             container_log_level=container_log_level,
             dependencies=dependencies,
             git_config=git_config,
-            model_package_arn=model_package_arn,
         )
 
         self.orig_predictor_cls = predictor_cls
@@ -307,49 +301,9 @@ class JumpStartModel(Model):
         self.tolerate_vulnerable_model = model_init_kwargs.tolerate_vulnerable_model
         self.tolerate_deprecated_model = model_init_kwargs.tolerate_deprecated_model
         self.region = model_init_kwargs.region
-        self.model_package_arn = model_init_kwargs.model_package_arn
 
         super(JumpStartModel, self).__init__(**model_init_kwargs.to_kwargs_dict())
 
-    def _create_sagemaker_model(self, *args, **kwargs):  # pylint: disable=unused-argument
-        """Create a SageMaker Model Entity
-
-        Args:
-            args: Positional arguments coming from the caller. This class does not require
-                any so they are ignored.
-
-            kwargs: Keyword arguments coming from the caller. This class does not require
-                any so they are ignored.
-        """
-        if self.model_package_arn:
-            # When a ModelPackageArn is provided we just create the Model
-            match = re.match(MODEL_PACKAGE_ARN_PATTERN, self.model_package_arn)
-            if match:
-                model_package_name = match.group(3)
-            else:
-                # model_package_arn can be just the name if your account owns the Model Package
-                model_package_name = self.model_package_arn
-            container_def = {"ModelPackageName": self.model_package_arn}
-
-            if self.env != {}:
-                container_def["Environment"] = self.env
-
-            if self.name is None:
-                self._base_name = model_package_name
-
-            self._set_model_name_if_needed()
-
-            self.sagemaker_session.create_model(
-                self.name,
-                self.role,
-                container_def,
-                vpc_config=self.vpc_config,
-                enable_network_isolation=self.enable_network_isolation(),
-                tags=kwargs.get("tags"),
-            )
-        else:
-            super(JumpStartModel, self)._create_sagemaker_model(*args, **kwargs)
-
     def deploy(
         self,
         initial_instance_count: Optional[int] = None,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-06-29 11:04:03[0m
[92mHash: 0bc4f41a5702c1c26f2336a044cf16447587ebde[0m
[92mFilepath: tests/unit/test_model_card.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: model registry integration to model cards to support model packages (#3933)

Co-authored-by: Bikash Shrestha <shgbikas@amazon.com>
[0m
@@ -17,7 +17,6 @@ import os
 import json
 import datetime
 import re
-import logging
 import pytest
 from mock import patch, Mock
 from botocore.exceptions import ClientError
@@ -40,15 +39,8 @@ from sagemaker.model_card import (
     ModelCard,
     Function,
     TrainingJobDetails,
-    ModelPackage,
-)
-from sagemaker.model_card.model_card import (
-    ModelCardExportJob,
-    ModelPackageCreator,
-    SourceAlgorithm,
-    Container,
-    InferenceSpecification,
 )
+from sagemaker.model_card.model_card import ModelCardExportJob
 from sagemaker.model_card.helpers import (
     _MaxSizeArray,
     _IsList,
@@ -56,8 +48,6 @@ from sagemaker.model_card.helpers import (
     _IsModelCardObject,
     _JSONEncoder,
     _hash_content_str,
-    _DefaultToRequestDict,
-    _SkipEncodingDecoding,
 )
 from sagemaker.model_card.evaluation_metric_parsers import (
     EvaluationMetricTypeEnum,
@@ -80,30 +70,6 @@ MODEL_ARTIFACT = ["s3://1", "s3://2"]
 MODEL_IMAGE = "test model container image"
 INFERENCE_ENVRIONMENT = Environment(container_image=[MODEL_IMAGE])
 
-# model package details arguments
-MODEL_PACKAGE_ARN = "arn:aws:sagemaker:us-west-2:001234567890:model-package/testmodelgroup/1"
-MODEL_PACKAGE_DESCRIPTION = "this is test model package"
-MODEL_PACKAGE_STATUS = "Pending"
-MODEL_APPROVAL_STATUS = "PendingManualApproval"
-APPROVAL_DESCRIPTION = "approval_description"
-MODEL_PACKAGE_GROUP_NAME = "testmodelgroup"
-MODEL_PACKAGE_NAME = "model_package_name"
-MODEL_PACKAGE_VERSION = 1
-DOMAIN = "domain"
-TASK = "task"
-USER_PROFILE_NAME = "test-user"
-CREATED_BY = ModelPackageCreator(USER_PROFILE_NAME)
-ALGORITHM_NAME = "test-algorithm-arn"
-MODEL_DATA_URL = "s3://test"
-SOURCE_ALGORITHMS = [SourceAlgorithm(ALGORITHM_NAME, MODEL_DATA_URL)]
-NEAREST_MODEL_NAME = "test-model"
-CONTAINERS = [Container(MODEL_IMAGE, MODEL_DATA_URL, NEAREST_MODEL_NAME)]
-INFERENCE_SPECIFICATION = InferenceSpecification(CONTAINERS)
-CLARIFY_BIAS_JSON_PATH = os.path.join(DATA_DIR, "evaluation_metrics/clarify_bias.json")
-MODEL_METRICS = {
-    "Bias": {"Report": {"ContentType": "application/json", "S3Uri": CLARIFY_BIAS_JSON_PATH}}
-}
-
 # intended uses auguments
 PURPOSE_OF_MODEL = "mock model for testing"
 INTENDED_USES = "this model card is used for development testing"
@@ -158,13 +124,7 @@ ETHICAL_CONSIDERATIONS = "there is no ethical consideration for this model card"
 CAVEATS_AND_RECOMMENDATIONS = "attention: this is a pure test model card"
 CUSTOM_DETAILS = {"custom details1": "details value"}
 MODEL_CARD_NAME = "test_model_card"
-MODEL_CARD_NAME_FOR_CARRY_OVER_ADDITIONAL_CONTENT = (
-    "test_model_card_for_carry_over_additional_content"
-)
 MODEL_CARD_ARN = "test_model_card_arn"
-MODEL_CARD_ARN_FOR_CARRY_OVER_ADDITIONAL_CONTENT = (
-    "test_model_card_arn_for_carry_over_additional_content"
-)
 MODEL_CARD_VERSION = 1
 CREATE_MODEL_CARD_RETURN_EXAMPLE = {
     "ModelCardArn": MODEL_CARD_ARN,
@@ -203,61 +163,6 @@ LOAD_MODEL_CARD_EXMPLE = {
     },
 }
 
-# sample response of model card with model package information in it
-MODEL_CARD_WITH_MODEL_PACKAGE_MOCK_RESPONSE = {
-    "ModelCardArn": MODEL_CARD_ARN,
-    "ModelCardName": MODEL_CARD_NAME,
-    "ModelCardVersion": MODEL_CARD_VERSION,
-    "Content": json.dumps(
-        {
-            "model_overview": {"model_id": MODEL_PACKAGE_ARN},
-            "model_package_details": {
-                "model_package_arn": MODEL_PACKAGE_ARN,
-                "model_package_name": MODEL_PACKAGE_NAME,
-                "model_package_group_name": MODEL_PACKAGE_GROUP_NAME,
-                "model_package_version": MODEL_PACKAGE_VERSION,
-                "model_package_description": MODEL_PACKAGE_DESCRIPTION,
-                "inference_specification": {
-                    "containers": [
-                        {
-                            "image": MODEL_IMAGE,
-                            "model_data_url": MODEL_DATA_URL,
-                            "nearest_model_name": NEAREST_MODEL_NAME,
-                        }
-                    ]
-                },
-                "model_package_status": MODEL_PACKAGE_STATUS,
-                "model_approval_status": MODEL_APPROVAL_STATUS,
-                "approval_description": APPROVAL_DESCRIPTION,
-                "created_by": {
-                    "user_profile_name": USER_PROFILE_NAME,
-                },
-                "domain": DOMAIN,
-                "task": TASK,
-                "source_algorithms": [
-                    {"algorithm_name": ALGORITHM_NAME, "model_data_url": MODEL_DATA_URL}
-                ],
-            },
-        }
-    ),
-    "ModelCardStatus": MODEL_CARD_STATUS,
-    "CreationTime": datetime.datetime(2022, 9, 17, 17, 15, 45, 672000),
-    "CreatedBy": {},
-    "LastModifiedTime": datetime.datetime(2022, 9, 17, 17, 15, 45, 672000),
-    "LastModifiedBy": {},
-    "ResponseMetadata": {
-        "RequestId": "7f317f47-a1e5-45dc-975a-fa4d9df81365",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "7f317f47-a1e5-45dc-975a-fa4d9df81365",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "2429",
-            "date": "Mon, 19 Sep 2022 21:09:05 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-
 SIMPLE_MODEL_CARD_ARN = "simple_test_model_card"
 SIMPLE_MODEL_CARD_NAME = "simple_test_model_card_arn"
 SIMPLE_MODEL_CARD_VERSION = 1
@@ -366,47 +271,6 @@ DESCRIBE_MODEL_EXAMPLE = {
     },
 }
 
-DESCRIBE_MODEL_PACKAGE_EXAMPLE = {
-    "ModelPackageArn": MODEL_PACKAGE_ARN,
-    "ModelPackageName": MODEL_PACKAGE_NAME,
-    "ModelPackageGroupName": MODEL_PACKAGE_GROUP_NAME,
-    "ModelPackageVersion": MODEL_PACKAGE_VERSION,
-    "ModelPackageDescription": MODEL_PACKAGE_DESCRIPTION,
-    "CreationTime": datetime.datetime(2022, 9, 20, 13, 4, 9, 134000),
-    "InferenceSpecification": {
-        "Containers": [
-            {
-                "Image": MODEL_IMAGE,
-                "ImageDigest": "sha256:[93m4814427c3e0a6cf99e637704da3ada04219ac7cd5727ff62284153761d36d7d3[0m",
-                "ModelDataUrl": MODEL_DATA_URL,
-                "NearestModelName": NEAREST_MODEL_NAME,
-            }
-        ],
-        "SupportedContentTypes": [],
-        "SupportedResponseMIMETypes": [],
-    },
-    "ModelPackageStatus": MODEL_PACKAGE_STATUS,
-    "ModelApprovalStatus": MODEL_APPROVAL_STATUS,
-    "CreatedBy": {
-        "UserProfileArn": "arn:aws:sagemaker:us-west-2:001234567890:user-profile/d-crvaptvnkhbq/test",
-        "UserProfileName": USER_PROFILE_NAME,
-        "DomainId": "d-crvaptvnkhbq",
-    },
-    "Domain": DOMAIN,
-    "Task": TASK,
-    "ResponseMetadata": {
-        "RequestId": "43ff67e3-2ae5-479e-bed0-9caccc9e62f0",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "43ff67e3-2ae5-479e-bed0-9caccc9e62f0",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "590",
-            "date": "Tue, 20 Sep 2022 19:55:36 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-
 SEARCH_MODEL_CARD_WITH_MODEL_ID_EXAMPLE = {
     "Results": [
         {
@@ -417,10 +281,7 @@ SEARCH_MODEL_CARD_WITH_MODEL_ID_EXAMPLE = {
                 "Content": json.dumps(
                     {
                         "intended_uses": {"risk_rating": RISK_RATING},
-                        "model_overview": {
-                            "model_id": MODEL_ID,
-                            "model_name": MODEL_NAME,
-                        },
+                        "model_overview": {"model_id": MODEL_ID, "model_name": MODEL_NAME},
                     }
                 ),
                 "ModelCardStatus": SIMPLE_MODEL_CARD_STATUS,
@@ -464,37 +325,12 @@ SEARCH_MODEL_CARD_WITH_MODEL_ID_EMPTY_EXAMPLE = {
 
 MISSING_MODEL_CLIENT_ERROR = ClientError(
     error_response={
-        "Error": {
-            "Code": "BadRequest",
-            "Message": f"Could not find model {MODEL_NAME}",
-        },
+        "Error": {"Code": "BadRequest", "Message": f"Could not find model {MODEL_NAME}"},
         "ResponseMetadata": {"MaxAttemptsReached": True, "RetryAttempts": 4},
     },
     operation_name="DescribeModel",
 )
 
-SEARCH_IAM_PERMISSION_CLIENT_ERROR = ClientError(
-    error_response={
-        "Error": {
-            "Code": "AccessDeniedException",
-            "Message": "An error occurred (AccessDenied) when calling the Search operation",
-        },
-        "ResponseMetadata": {"MaxAttemptsReached": True, "RetryAttempts": 4},
-    },
-    operation_name="Search",
-)
-
-MISSING_MODEL_PACKAGE_CLIENT_ERROR = ClientError(
-    error_response={
-        "Error": {
-            "Code": "BadRequest",
-            "Message": f"Could not find model package {MODEL_PACKAGE_ARN}",
-        },
-        "ResponseMetadata": {"MaxAttemptsReached": True, "RetryAttempts": 4},
-    },
-    operation_name="DescribeModelPackage",
-)
-
 TRAINING_JOB_NAME = MODEL_NAME
 TRAINING_JOB_ARN = "test training job id"
 SEARCH_TRAINING_JOB_EXAMPLE = {
@@ -542,7 +378,7 @@ SEARCH_TRAINING_JOB_EXAMPLE = {
                     "subsample": "0.8828549481113146",
                 },
                 "CreatedBy": {},
-            },
+            }
         }
     ],
     "ResponseMetadata": {
@@ -741,151 +577,6 @@ LIST_MODEL_CARD_VERSION_HISTORY_EXAMPLE = {
         "RetryAttempts": 0,
     },
 }
-RESPONSE_CONTENT_EXAMPLE = {
-    "intended_uses": {"purpose_of_model": PURPOSE_OF_MODEL},
-    "business_details": {
-        "business_problem": BUSINESS_PROBLEM,
-        "business_stakeholders": BUSINESS_STAKEHOLDERS,
-        "line_of_business": LINE_OF_BUSINESS,
-    },
-    "additional_information": {
-        "ethical_considerations": ETHICAL_CONSIDERATIONS,
-        "caveats_and_recommendations": CAVEATS_AND_RECOMMENDATIONS,
-        "custom_details": CUSTOM_DETAILS,
-    },
-}
-ORIGINAL_MOCK_STRING = "Original mock string."
-SEARCH_LATEST_MODEL_CARD_EXAMPLE = {
-    "Results": [
-        {
-            "ModelCard": {
-                "ModelCardArn": MODEL_CARD_ARN_FOR_CARRY_OVER_ADDITIONAL_CONTENT,
-                "ModelCardName": MODEL_CARD_NAME_FOR_CARRY_OVER_ADDITIONAL_CONTENT,
-                "ModelCardVersion": MODEL_CARD_VERSION,
-                "Content": {
-                    "model_overview": {"model_id": MODEL_PACKAGE_ARN},
-                    "intended_uses": {
-                        "purpose_of_model": PURPOSE_OF_MODEL,
-                        "risk_rating": RISK_RATING,
-                        "factors_affecting_model_efficiency": FACTORS_AFFECTING_MODEL_EFFICIENCY,
-                    },
-                    "business_details": {
-                        "business_problem": BUSINESS_PROBLEM,
-                        "business_stakeholders": BUSINESS_STAKEHOLDERS,
-                        "line_of_business": LINE_OF_BUSINESS,
-                    },
-                    "additional_information": {
-                        "ethical_considerations": ETHICAL_CONSIDERATIONS,
-                        "caveats_and_recommendations": CAVEATS_AND_RECOMMENDATIONS,
-                        "custom_details": CUSTOM_DETAILS,
-                    },
-                },
-                "ModelCardStatus": MODEL_CARD_STATUS,
-                "CreationTime": datetime.datetime(2023, 4, 4, 15, 30, 3),
-                "CreatedBy": {},
-                "LastModifiedTime": datetime.datetime(2023, 4, 4, 15, 30, 3),
-                "LastModifiedBy": {},
-                "Tags": [],
-                "ModelId": MODEL_PACKAGE_ARN,
-            }
-        },
-    ],
-    "ResponseMetadata": {
-        "RequestId": "678b50e9-23a5-4ed4-a530-e0635e0fcffd",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "678b50e9-23a5-4ed4-a530-e0635e0fcffd",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "1523",
-            "date": "Wed, 19 Apr 2023 03:32:28 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-SEARCH_LATEST_MODEL_CARD_WITH_EMPTY_RESULT_EXAMPLE = {
-    "Results": [],
-    "ResponseMetadata": {
-        "RequestId": "678b50e9-23a5-4ed4-a530-e0635e0fcffd",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "678b50e9-23a5-4ed4-a530-e0635e0fcffd",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "1523",
-            "date": "Wed, 19 Apr 2023 03:32:28 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-CONTENT_FROM_DESCRIBE_MODEL_CARD = {
-    "model_overview": {
-        "model_id": MODEL_ID,
-        "model_name": MODEL_NAME,
-    },
-    "intended_uses": {
-        "purpose_of_model": PURPOSE_OF_MODEL,
-        "intended_uses": INTENDED_USES,
-        "factors_affecting_model_efficiency": FACTORS_AFFECTING_MODEL_EFFICIENCY,
-        "risk_rating": RISK_RATING,
-        "explanations_for_risk_rating": EXPLANATIONS_FOR_RISK_RATING,
-    },
-    "business_details": {
-        "business_problem": BUSINESS_PROBLEM,
-        "business_stakeholders": BUSINESS_STAKEHOLDERS,
-        "line_of_business": LINE_OF_BUSINESS,
-    },
-    "additional_information": {
-        "ethical_considerations": ETHICAL_CONSIDERATIONS,
-        "caveats_and_recommendations": CAVEATS_AND_RECOMMENDATIONS,
-        "custom_details": CUSTOM_DETAILS,
-    },
-    "model_package_details": {
-        "model_package_arn": MODEL_PACKAGE_ARN,
-        "model_package_name": MODEL_PACKAGE_NAME,
-        "model_package_group_name": MODEL_PACKAGE_GROUP_NAME,
-        "model_package_version": MODEL_PACKAGE_VERSION,
-        "model_package_description": MODEL_PACKAGE_DESCRIPTION,
-        "inference_specification": {
-            "containers": [
-                {
-                    "image": MODEL_IMAGE,
-                    "model_data_url": MODEL_DATA_URL,
-                    "nearest_model_name": NEAREST_MODEL_NAME,
-                }
-            ]
-        },
-        "model_package_status": MODEL_PACKAGE_STATUS,
-        "model_approval_status": MODEL_APPROVAL_STATUS,
-        "approval_description": APPROVAL_DESCRIPTION,
-        "created_by": {
-            "user_profile_name": USER_PROFILE_NAME,
-        },
-        "domain": DOMAIN,
-        "task": TASK,
-        "source_algorithms": [{"algorithm_name": ALGORITHM_NAME, "model_data_url": MODEL_DATA_URL}],
-    },
-}
-DESCRIBE_MODEL_CARD_WITH_ADDITONAL_CONTENT = {
-    "ModelCardArn": MODEL_CARD_ARN_FOR_CARRY_OVER_ADDITIONAL_CONTENT,
-    "ModelCardName": MODEL_CARD_NAME_FOR_CARRY_OVER_ADDITIONAL_CONTENT,
-    "ModelCardVersion": MODEL_CARD_VERSION,
-    "Content": json.dumps(CONTENT_FROM_DESCRIBE_MODEL_CARD),
-    "ModelCardStatus": MODEL_CARD_STATUS,
-    "CreationTime": datetime.datetime(2022, 9, 17, 17, 15, 45, 672000),
-    "CreatedBy": {},
-    "LastModifiedTime": datetime.datetime(2022, 9, 17, 17, 15, 45, 672000),
-    "LastModifiedBy": {},
-    "ResponseMetadata": {
-        "RequestId": "7f317f47-a1e5-45dc-975a-fa4d9df81365",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "7f317f47-a1e5-45dc-975a-fa4d9df81365",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "2429",
-            "date": "Mon, 19 Sep 2022 21:09:05 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
 
 
 @pytest.fixture(name="model_overview_example")
@@ -906,27 +597,6 @@ def fixture_model_overview_example():
     return test_example
 
 
-@pytest.fixture(name="model_package_example")
-def fixture_model_package_example():
-    """Example ModelPackage instance"""
-    test_example = ModelPackage(
-        model_package_arn=MODEL_PACKAGE_ARN,
-        model_package_description=MODEL_PACKAGE_DESCRIPTION,
-        model_package_group_name=MODEL_PACKAGE_GROUP_NAME,
-        model_package_name=MODEL_PACKAGE_NAME,
-        model_approval_status=MODEL_APPROVAL_STATUS,
-        model_package_status=MODEL_PACKAGE_STATUS,
-        model_package_version=MODEL_PACKAGE_VERSION,
-        approval_description=APPROVAL_DESCRIPTION,
-        domain=DOMAIN,
-        task=TASK,
-        created_by=CREATED_BY,
-        source_algorithms=SOURCE_ALGORITHMS,
-        inference_specification=INFERENCE_SPECIFICATION,
-    )
-    return test_example
-
-
 @pytest.fixture(name="intended_uses_example")
 def fixture_fixture_intended_uses_example():
     """Example intended uses instance."""
@@ -1031,72 +701,6 @@ def test_create_model_card(
     assert card.arn == MODEL_CARD_ARN
 
 
-@patch("sagemaker.Session")
-def test_create_model_card_with_model_package(
-    session, model_package_example, training_details_example, caplog
-):
-    session.sagemaker_client.create_model_card = Mock(return_value=CREATE_MODEL_CARD_RETURN_EXAMPLE)
-    session.sagemaker_client.describe_model_card = Mock(
-        return_value=MODEL_CARD_WITH_MODEL_PACKAGE_MOCK_RESPONSE
-    )
-
-    session.sagemaker_client.search.side_effect = [
-        SEARCH_TRAINING_JOB_EXAMPLE,
-        SEARCH_LATEST_MODEL_CARD_WITH_EMPTY_RESULT_EXAMPLE,
-        SEARCH_LATEST_MODEL_CARD_WITH_EMPTY_RESULT_EXAMPLE,
-    ]
-
-    card = ModelCard(
-        name=MODEL_CARD_NAME,
-        status=MODEL_CARD_STATUS,
-        model_package_details=model_package_example,
-        sagemaker_session=session,
-    )
-
-    card.create()
-
-    assert card.arn == MODEL_CARD_ARN
-    assert card.status == MODEL_CARD_STATUS
-    assert card.model_package_details.model_package_arn == MODEL_PACKAGE_ARN
-    assert card.model_package_details.model_approval_status == MODEL_APPROVAL_STATUS
-    assert card.model_package_details.created_by.user_profile_name == USER_PROFILE_NAME
-
-    # testing with existing training details
-    with caplog.at_level(logging.INFO):
-        ModelCard(
-            name=MODEL_CARD_NAME,
-            status=MODEL_CARD_STATUS,
-            training_details=training_details_example,
-            model_package_details=model_package_example,
-            sagemaker_session=session,
-        )
-        assert (
-            "Skipping training details auto discovery. "
-            "Training details already exists for this model card."
-        ) in caplog.text
-
-
-@patch("sagemaker.Session")
-def test_create_model_card_with_multiple_models(
-    session, model_package_example, model_overview_example
-):
-
-    card = ModelCard(
-        name=MODEL_CARD_NAME,
-        status=MODEL_CARD_STATUS,
-        model_overview=model_overview_example,
-        sagemaker_session=session,
-    )
-
-    with pytest.raises(
-        ValueError,
-        match=re.escape(
-            f"The model card has already been associated with a model with model Id {MODEL_ID}"  # noqa E501  # pylint: disable=c0301
-        ),
-    ):
-        card.model_package_details = model_package_example
-
-
 @patch("sagemaker.Session")
 def test_create_model_card_duplicate(session):
     session.sagemaker_client.create_model_card.side_effect = [
@@ -1135,9 +739,7 @@ def test_create_multiple_model_cards_with_same_model(session, model_overview_exa
 
     with pytest.raises(ClientError):
         card2 = ModelCard(
-            name="test2",
-            model_overview=model_overview_example,
-            sagemaker_session=session,
+            name="test2", model_overview=model_overview_example, sagemaker_session=session
         )
         card2.create()
 
@@ -1163,47 +765,11 @@ def test_create_model_card_with_too_long_string(session, model_overview_example)
     with pytest.raises(ClientError):
         model_overview_example.name = "x" * 1025
         card = ModelCard(
-            name=MODEL_CARD_NAME,
-            model_overview=model_overview_example,
-            sagemaker_session=session,
+            name=MODEL_CARD_NAME, model_overview=model_overview_example, sagemaker_session=session
         )
         card.create()
 
 
-@patch("sagemaker.Session")
-def test_carry_over_additional_content_from_model_package_group(session, model_package_example):
-    session.sagemaker_client.describe_model_card = Mock(
-        return_value=DESCRIBE_MODEL_CARD_WITH_ADDITONAL_CONTENT
-    )
-
-    session.sagemaker_client.search.side_effect = [
-        SEARCH_TRAINING_JOB_EXAMPLE,
-        SEARCH_LATEST_MODEL_CARD_EXAMPLE,
-    ]
-
-    mc = ModelCard(
-        name=MODEL_CARD_NAME,
-        status=MODEL_CARD_STATUS,
-        sagemaker_session=session,
-        model_package_details=model_package_example,
-        business_details={
-            "business_problem": ORIGINAL_MOCK_STRING,
-            "business_stakeholders": ORIGINAL_MOCK_STRING,
-        },
-    )
-
-    assert mc.intended_uses.purpose_of_model == PURPOSE_OF_MODEL
-    assert mc.intended_uses.risk_rating == RISK_RATING
-    assert mc.intended_uses.factors_affecting_model_efficiency == FACTORS_AFFECTING_MODEL_EFFICIENCY
-
-    assert mc.business_details.business_problem == ORIGINAL_MOCK_STRING
-    assert mc.business_details.business_stakeholders == ORIGINAL_MOCK_STRING
-
-    assert mc.additional_information.ethical_considerations == ETHICAL_CONSIDERATIONS
-    assert mc.additional_information.caveats_and_recommendations == CAVEATS_AND_RECOMMENDATIONS
-    assert mc.additional_information.custom_details == CUSTOM_DETAILS
-
-
 def test_metric_type_value_mismatch():
     with pytest.raises(
         ValueError,
@@ -1212,9 +778,7 @@ def test_metric_type_value_mismatch():
         ),
     ):
         Metric(
-            name="test_training_metric",
-            type=schema_constraints.MetricTypeEnum.NUMBER,
-            value="123",
+            name="test_training_metric", type=schema_constraints.MetricTypeEnum.NUMBER, value="123"
         )
 
 
@@ -1326,22 +890,6 @@ def test_one_of_descriptor():
         ExampleClass(attr1=schema_constraints.MetricTypeEnum.BAR_CHART)
 
 
-def test_skip_encoding_descriptor():
-    class ExampleClass:
-        attr1 = _SkipEncodingDecoding(dict)
-
-        def __init__(self, attr1):
-            self.attr1 = attr1
-
-    assert ExampleClass({"test": 1})
-
-    with pytest.raises(
-        ValueError,
-        match=re.escape("Please assign a <class 'dict'> to attr1"),
-    ):
-        ExampleClass(attr1="test1")
-
-
 def test_is_model_card_object_descriptor():
     class ExampleClass:  # pylint: disable=C0115
         attr1 = _IsModelCardObject(ModelOverview)
@@ -1439,25 +987,6 @@ def test_model_card_encoder():
     )
 
 
-def test_model_card_encoder_with_skip_encoding():
-    class ExampleClass(_DefaultToRequestDict):
-        """Example class"""
-
-        attr1 = _OneOf(schema_constraints.MetricTypeEnum)
-        attr2 = _SkipEncodingDecoding(dict)
-
-        def __init__(self, attr1: schema_constraints.MetricTypeEnum, attr2: dict):
-            """Initialize an example class"""
-            self.attr1 = attr1
-            self.attr2 = attr2
-
-    my_object = ExampleClass(
-        attr1=schema_constraints.MetricTypeEnum.LINEAR_GRAPH, attr2={"test": 1}
-    )
-
-    assert json.dumps(my_object, cls=_JSONEncoder, sort_keys=True) == '{"attr1": "linear_graph"}'
-
-
 def test_hash_content_str():
     content1 = json.dumps({"key": "value"})
     content2 = json.dumps({"key": "value2"})
@@ -1501,76 +1030,20 @@ def test_model_details_autodiscovery(session):
         ModelOverview.from_model_name(MODEL_NAME, sagemaker_session=session)
 
 
-@patch("sagemaker.Session")
-def test_model_package_autodiscovery(session, model_overview_example, training_details_example):
-    session.sagemaker_client.describe_model_package.side_effect = [
-        DESCRIBE_MODEL_PACKAGE_EXAMPLE,
-        DESCRIBE_MODEL_PACKAGE_EXAMPLE,
-        MISSING_MODEL_PACKAGE_CLIENT_ERROR,
-        DESCRIBE_MODEL_PACKAGE_EXAMPLE,
-        DESCRIBE_MODEL_PACKAGE_EXAMPLE,
-    ]
-
-    session.sagemaker_client.search.side_effect = [
-        SEARCH_MODEL_CARD_WITH_MODEL_ID_EMPTY_EXAMPLE,
-        SEARCH_MODEL_CARD_WITH_MODEL_ID_EXAMPLE,
-        SEARCH_IAM_PERMISSION_CLIENT_ERROR,
-        SEARCH_MODEL_CARD_WITH_MODEL_ID_EMPTY_EXAMPLE,
-    ]
-
-    model_package_details = ModelPackage.from_model_package_arn(
-        MODEL_PACKAGE_ARN, sagemaker_session=session
-    )
-    assert model_package_details.model_package_arn == MODEL_PACKAGE_ARN
-    assert model_package_details.model_package_group_name == MODEL_PACKAGE_GROUP_NAME
-    assert (
-        model_package_details.inference_specification.containers[0].model_data_url == MODEL_DATA_URL
-    )
-    assert model_package_details.created_by.user_profile_name == USER_PROFILE_NAME
-
-    with pytest.raises(
-        ValueError,
-        match=re.escape(
-            f"The model package has already been associated with {[MODEL_CARD_NAME]} model cards."
-        ),
-    ):
-        ModelPackage.from_model_package_arn(MODEL_PACKAGE_ARN, sagemaker_session=session)
-
-    with pytest.raises(
-        ValueError,
-        match=re.escape(
-            f"Model package details for {MODEL_PACKAGE_ARN} could not be found. Make sure the model package name or ARN is valid."  # noqa E501  # pylint: disable=c0301
-        ),
-    ):
-        ModelPackage.from_model_package_arn(MODEL_PACKAGE_ARN, sagemaker_session=session)
-
-    with pytest.raises(
-        ValueError,
-        match=re.escape(
-            "Received AccessDeniedException while calling SageMaker Search operation "
-            "on resource ModelCard. This could mean the IAM role does not "
-            "have the resource permissions, in which case please add resource access "
-            "and retry. For cases where the role has tag based resource policy, "
-            "continuing to wait for tag propagation.."
-        ),
-    ):
-        ModelPackage.from_model_package_arn(MODEL_PACKAGE_ARN, sagemaker_session=session)
-
-
 @patch("sagemaker.Session")
 def test_training_details_autodiscovery_from_model_overview(
     session, model_overview_example, caplog
 ):
     session.sagemaker_client.search.side_effect = [
         SEARCH_TRAINING_JOB_EXAMPLE,
-        SEARCH_IAM_PERMISSION_CLIENT_ERROR,
+        MISSING_TRAINING_JOB_CLIENT_ERROR,
     ]
 
     TrainingDetails.from_model_overview(
         model_overview=model_overview_example, sagemaker_session=session
     )
     assert (
-        "TrainingJobDetails auto-discovery failed. "
+        "TraininigJobDetails auto-discovery failed. "
         "There are 2 associated training jobs. "
         "Further clarification is required. "
         "You could use TrainingDetails.training_job_name after "
@@ -1592,106 +1065,18 @@ def test_training_details_autodiscovery_from_model_overview(
         TRAINING_IMAGE
     ]
 
-    with pytest.raises(
-        ValueError,
-        match=re.escape(
-            "Received AccessDeniedException while calling SageMaker Search operation "
-            "on resource TrainingJob. This could mean the IAM role does not "
-            "have the resource permissions, in which case please add resource access "
-            "and retry. For cases where the role has tag based resource policy, "
-            "continuing to wait for tag propagation.."
-        ),
-    ):
-        TrainingDetails.from_model_overview(
-            model_overview=model_overview_example, sagemaker_session=session
-        )
-
     model_overview_example.model_artifact = []
     TrainingDetails.from_model_overview(
         model_overview=model_overview_example, sagemaker_session=session
     )
     assert (
-        "TrainingJobDetails auto-discovery failed. "
+        "TraininigJobDetails auto-discovery failed. "
         "No associated training job. "
-        "Please create one from scratch with TrainingJobDetails "
+        "Please create one from scrach with TrainingJobDetails "
         "or use from_training_job_name() instead."
     ) in caplog.text
 
 
-@patch("sagemaker.Session")
-def test_training_details_autodiscovery_from_model_package_details(
-    session, model_package_example, caplog
-):
-    session.sagemaker_client.search.side_effect = [
-        SEARCH_TRAINING_JOB_EXAMPLE,
-    ]
-
-    training_details = model_package_example.discover_training_details(sagemaker_session=session)
-    assert training_details.training_job_details.training_arn == TRAINING_JOB_ARN
-    assert len(training_details.training_job_details.training_metrics) == len(
-        SEARCH_TRAINING_JOB_EXAMPLE["Results"][0]["TrainingJob"]["FinalMetricDataList"]
-    )
-    assert len(training_details.training_job_details.hyper_parameters) == len(
-        SEARCH_TRAINING_JOB_EXAMPLE["Results"][0]["TrainingJob"]["HyperParameters"]
-    )
-    assert training_details.training_job_details.training_environment.container_image == [
-        TRAINING_IMAGE
-    ]
-
-    model_package_example.inference_specification.containers = []
-    model_package_example.discover_training_details(sagemaker_session=session)
-    assert (
-        "TrainingJobDetails auto-discovery failed. "
-        "No associated training job. "
-        "Please create one from scratch with TrainingJobDetails "
-        "or use from_training_job_name() instead."
-    ) in caplog.text
-
-    model_package_example.inference_specification = None
-    with caplog.at_level(logging.INFO):
-        model_package_example.discover_training_details(sagemaker_session=session)
-        assert (
-            "TrainingJobDetails auto-discovery was unsuccessful. "
-            "No inference specification found for the given model package."
-            "Please create one from scratch with TrainingJobDetails "
-            "or use from_training_job_name() instead."
-        ) in caplog.text
-
-
-@patch("sagemaker.Session")
-def test_evaluation_details_autodiscovery_from_model_package_details(
-    session, model_package_example, caplog
-):
-    with open(CLARIFY_BIAS_JSON_PATH, "r", encoding="utf-8") as istr:
-        data = json.dumps(json.load(istr))
-        response = {
-            "Body": botocore.response.StreamingBody(
-                io.BytesIO(bytes(data, "utf-8")), content_length=len(data)
-            ),
-            "ContentType": "application/json",
-        }
-    session.boto_session.client.return_value.get_object.side_effect = [
-        response,
-    ]
-
-    with caplog.at_level(logging.INFO):
-        evaluation_details = model_package_example.discover_evaluation_details(
-            sagemaker_session=session
-        )
-        assert (
-            "Evaluation details auto-discovery was unsuccessful. "
-            "ModelMetrics was not found in the given model package. "
-            "Please create one from scratch with EvaluationJob."
-        ) in caplog.text
-
-    model_package_example.model_metrics = MODEL_METRICS
-    evaluation_details = model_package_example.discover_evaluation_details(
-        sagemaker_session=session
-    )
-
-    assert len(evaluation_details[0].metric_groups) == 3
-
-
 @patch("sagemaker.Session")
 def test_training_details_autodiscovery_from_model_overview_autopilot(
     session, model_overview_example, caplog
@@ -1841,9 +1226,7 @@ def test_add_evauation_metrics_from_s3(session, caplog):
 
 def test_metrics_clarify_bias():
     with open(
-        os.path.join(DATA_DIR, "evaluation_metrics/clarify_bias.json"),
-        "r",
-        encoding="utf-8",
+        os.path.join(DATA_DIR, "evaluation_metrics/clarify_bias.json"), "r", encoding="utf-8"
     ) as istr:
         json_data = json.load(istr)
 
@@ -1881,11 +1264,10 @@ def test_metrics_clarify_explanation():
     assert json.dumps(result, sort_keys=True) == json.dumps(expected_translation, sort_keys=True)
 
 
-def test_metrics_model_monitor_model_quality_binary_classification():
+def test_metrics_model_monitor_model_quality():
     with open(
         os.path.join(
-            DATA_DIR,
-            "evaluation_metrics/model_monitor_model_quality_binary_classification.json",
+            DATA_DIR, "evaluation_metrics/model_monitor_model_quality_binary_classification.json"
         ),
         "r",
         encoding="utf-8",
@@ -1902,34 +1284,7 @@ def test_metrics_model_monitor_model_quality_binary_classification():
     ) as istr:
         expected_translation = json.load(istr)
 
-    parser = EVALUATION_METRIC_PARSERS[EvaluationMetricTypeEnum.MODEL_MONITOR_MODEL_QUALITY]
-    result = parser.run(json_data)
-
-    assert json.dumps(result, sort_keys=True) == json.dumps(expected_translation, sort_keys=True)
-
-
-def test_metrics_model_monitor_model_quality_regression():
-    with open(
-        os.path.join(
-            DATA_DIR,
-            "evaluation_metrics/model_monitor_model_quality_regression.json",
-        ),
-        "r",
-        encoding="utf-8",
-    ) as istr:
-        json_data = json.load(istr)
-
-    with open(
-        os.path.join(
-            DATA_DIR,
-            "evaluation_metrics/translated_model_monitor_model_quality_regression.json",
-        ),
-        "r",
-        encoding="utf-8",
-    ) as istr:
-        expected_translation = json.load(istr)
-
-    parser = EVALUATION_METRIC_PARSERS[EvaluationMetricTypeEnum.MODEL_MONITOR_MODEL_QUALITY]
+    parser = EVALUATION_METRIC_PARSERS[EvaluationMetricTypeEnum.BINARY_CLASSIFICATION]
     result = parser.run(json_data)
 
     assert json.dumps(result, sort_keys=True) == json.dumps(expected_translation, sort_keys=True)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-06-20 21:24:09[0m
[92mHash: 2c1b88742d90016a2d300d91ed02fe211ea9a69f[0m
[92mFilepath: tests/unit/sagemaker/feature_store/feature_processor/lineage/test_constants.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add SageMaker FeatureStore feature processing (#3944)

Co-authored-by: Alex Tang <tangalex@amazon.com>
Co-authored-by: cansun <80425164+can-sun@users.noreply.github.com>
Co-authored-by: Alex Tang <alex-tang@users.noreply.github.com>
Co-authored-by: Can Sun <sucan@amazon.com>
Co-authored-by: jiapinw <jiapinw@amazon.com>
Co-authored-by: suryans-commit <80422715+suryans-commit@users.noreply.github.com>
[0m
@@ -1,350 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-"""Contains constants of feature processor to be used for unit tests."""
-from __future__ import absolute_import
-
-import datetime
-from typing import List, Sequence, Union
-
-from botocore.exceptions import ClientError
-from mock import Mock
-
-from sagemaker import Session
-from sagemaker.feature_store.feature_processor._data_source import (
-    CSVDataSource,
-    FeatureGroupDataSource,
-    ParquetDataSource,
-)
-from sagemaker.feature_store.feature_processor.lineage._feature_group_contexts import (
-    FeatureGroupContexts,
-)
-from sagemaker.feature_store.feature_processor.lineage._pipeline_schedule import (
-    PipelineSchedule,
-)
-from sagemaker.feature_store.feature_processor.lineage._transformation_code import (
-    TransformationCode,
-)
-from sagemaker.lineage._api_types import ContextSource
-from sagemaker.lineage.artifact import Artifact, ArtifactSource, ArtifactSummary
-from sagemaker.lineage.context import Context
-
-PIPELINE_NAME = "test-pipeline-01"
-PIPELINE_ARN = "arn:aws:sagemaker:us-west-2:12345789012:pipeline/test-pipeline-01"
-CREATION_TIME = "123123123"
-LAST_UPDATE_TIME = "234234234"
-SAGEMAKER_SESSION_MOCK = Mock(Session)
-CONTEXT_MOCK_01 = Mock(Context)
-CONTEXT_MOCK_02 = Mock(Context)
-
-FEATURE_GROUP_DATA_SOURCE: List[FeatureGroupDataSource] = [
-    FeatureGroupDataSource(
-        name="feature-group-01",
-    ),
-    FeatureGroupDataSource(
-        name="feature-group-02",
-    ),
-]
-
-FEATURE_GROUP_INPUT: List[FeatureGroupContexts] = [
-    FeatureGroupContexts(
-        name="feature-group-01",
-        pipeline_context_arn="feature-group-01-pipeline-context-arn",
-        pipeline_version_context_arn="feature-group-01-pipeline-version-context-arn",
-    ),
-    FeatureGroupContexts(
-        name="feature-group-02",
-        pipeline_context_arn="feature-group-02-pipeline-context-arn",
-        pipeline_version_context_arn="feature-group-02-pipeline-version-context-arn",
-    ),
-]
-
-RAW_DATA_INPUT: Sequence[Union[CSVDataSource, ParquetDataSource]] = [
-    CSVDataSource(s3_uri="raw-data-uri-01"),
-    CSVDataSource(s3_uri="raw-data-uri-02"),
-    ParquetDataSource(s3_uri="raw-data-uri-03"),
-]
-
-RAW_DATA_INPUT_ARTIFACTS: List[Artifact] = [
-    Artifact(artifact_arn="artifact-01-arn"),
-    Artifact(artifact_arn="artifact-02-arn"),
-    Artifact(artifact_arn="artifact-03-arn"),
-]
-
-PIPELINE_SCHEDULE = PipelineSchedule(
-    schedule_name="schedule-name",
-    schedule_arn="schedule-arn",
-    schedule_expression="schedule-expression",
-    pipeline_name="pipeline-name",
-    state="state",
-    start_date="123123123",
-)
-
-PIPELINE_SCHEDULE_2 = PipelineSchedule(
-    schedule_name="schedule-name-2",
-    schedule_arn="schedule-arn",
-    schedule_expression="schedule-expression-2",
-    pipeline_name="pipeline-name",
-    state="state-2",
-    start_date="234234234",
-)
-
-ARTIFACT_RESULT: Artifact = Artifact(
-    artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
-    artifact_name="sm-fs-fe-raw-data",
-    source={
-        "source_uri": "s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz"
-    },
-    artifact_type="DataSet",
-    creation_time=datetime.datetime(2023, 4, 28, 21, 53, 47, 912000),
-)
-
-SCHEDULE_ARTIFACT_RESULT: Artifact = Artifact(
-    artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
-    artifact_name="sm-fs-fe-raw-data",
-    source={
-        "source_uri": "s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz"
-    },
-    properties=dict(
-        pipeline_name=PIPELINE_SCHEDULE.pipeline_name,
-        schedule_expression=PIPELINE_SCHEDULE.schedule_expression,
-        state=PIPELINE_SCHEDULE.state,
-        start_date=PIPELINE_SCHEDULE.start_date,
-    ),
-    artifact_type="DataSet",
-    creation_time=datetime.datetime(2023, 4, 28, 21, 53, 47, 912000),
-)
-
-ARTIFACT_SUMMARY: ArtifactSummary = ArtifactSummary(
-    artifact_arn="arn:aws:sagemaker:us-west-2:789975069016:artifact/[93m[93m[93m7be06af3274fd01d1c18c96f97141f32[0m[0m[0m",
-    artifact_name="sm-fs-fe-raw-data",
-    source=ArtifactSource(
-        source_uri="s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz",
-        source_types=[],
-    ),
-    artifact_type="DataSet",
-    creation_time=datetime.datetime(2023, 4, 27, 21, 4, 17, 926000),
-)
-
-TRANSFORMATION_CODE_ARTIFACT_1 = Artifact(
-    artifact_arn="ts-artifact-01-arn",
-    artifact_name="sm-fs-fe-transformation-code",
-    source={
-        "source_uri": "s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz",
-        "source_types": [{"source_id_type": "Custom", "value": "1684369626"}],
-    },
-    properties={
-        "name": "test-name",
-        "author": "test-author",
-        "inclusive_start_date": "1684369626",
-        "state": "Active",
-    },
-)
-
-TRANSFORMATION_CODE_ARTIFACT_2 = Artifact(
-    artifact_arn="ts-artifact-02-arn",
-    artifact_name="sm-fs-fe-transformation-code",
-    source={
-        "source_uri": "s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz/2",
-        "source_types": [{"source_id_type": "Custom", "value": "1684369626"}],
-    },
-    properties={
-        "name": "test-name",
-        "author": "test-author",
-        "inclusive_start_date": "1684369626",
-        "state": "Active",
-    },
-)
-
-INACTIVE_TRANSFORMATION_CODE_ARTIFACT_1 = Artifact(
-    artifact_arn="ts-artifact-02-arn",
-    artifact_name="sm-fs-fe-transformation-code",
-    source={
-        "source_uri": "s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-        "transform-2023-04-28-21-50-14-616/output/model.tar.gz/2",
-        "source_types": [{"source_id_type": "Custom", "value": "1684369307"}],
-    },
-    Properties={
-        "name": "test-name",
-        "author": "test-author",
-        "exclusive_end_date": "1684369626",
-        "inclusive_start_date": "1684369307",
-        "state": "Inactive",
-    },
-)
-
-VALIDATION_EXCEPTION = ClientError(
-    {"Error": {"Code": "ValidationException", "Message": "AssociationAlreadyExists"}},
-    "Operation",
-)
-
-RESOURCE_NOT_FOUND_EXCEPTION = ClientError(
-    {"Error": {"Code": "ResourceNotFound", "Message": "ResourceDoesNotExists"}},
-    "Operation",
-)
-
-NON_VALIDATION_EXCEPTION = ClientError(
-    {"Error": {"Code": "NonValidationException", "Message": "NonValidationError"}},
-    "Operation",
-)
-
-FEATURE_GROUP_NAME = "feature-group-name-01"
-FEATURE_GROUP = {
-    "FeatureGroupArn": "arn:aws:sagemaker:us-west-2:789975069016:feature-group/feature-group-name-01",
-    "FeatureGroupName": "feature-group-name-01",
-    "RecordIdentifierFeatureName": "model_year_status",
-    "EventTimeFeatureName": "ingest_time",
-    "FeatureDefinitions": [
-        {"FeatureName": "model_year_status", "FeatureType": "String"},
-        {"FeatureName": "avg_mileage", "FeatureType": "String"},
-        {"FeatureName": "max_mileage", "FeatureType": "String"},
-        {"FeatureName": "avg_price", "FeatureType": "String"},
-        {"FeatureName": "max_price", "FeatureType": "String"},
-        {"FeatureName": "avg_msrp", "FeatureType": "String"},
-        {"FeatureName": "max_msrp", "FeatureType": "String"},
-        {"FeatureName": "ingest_time", "FeatureType": "Fractional"},
-    ],
-    "CreationTime": datetime.datetime(2023, 4, 27, 21, 4, 17, 926000),
-    "OnlineStoreConfig": {"EnableOnlineStore": True},
-    "OfflineStoreConfig": {
-        "S3StorageConfig": {
-            "S3Uri": "s3://sagemaker-us-west-2-789975069016/"
-            "feature-store/feature-processor/"
-            "suryans-v2/offline-store",
-            "ResolvedOutputS3Uri": "s3://sagemaker-us-west-2-"
-            "789975069016/feature-store/"
-            "feature-processor/suryans-v2/"
-            "offline-store/789975069016/"
-            "sagemaker/us-west-2/"
-            "offline-store/"
-            "feature-group-name-01-"
-            "1682629457/data",
-        },
-        "DisableGlueTableCreation": False,
-        "DataCatalogConfig": {
-            "TableName": "feature-group-name-01_1682629457",
-            "Catalog": "AwsDataCatalog",
-            "Database": "sagemaker_featurestore",
-        },
-    },
-    "RoleArn": "arn:aws:iam::789975069016:role/service-role/AmazonSageMaker-ExecutionRole-20230421T100744",
-    "FeatureGroupStatus": "Created",
-    "OnlineStoreTotalSizeBytes": 0,
-    "ResponseMetadata": {
-        "RequestId": "8f139791-345d-4388-8d6d-40420495a3c4",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "8f139791-345d-4388-8d6d-40420495a3c4",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "1608",
-            "date": "Mon, 01 May 2023 21:42:59 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-
-PIPELINE = {
-    "PipelineArn": "arn:aws:sagemaker:us-west-2:597217924798:pipeline/test-pipeline-26",
-    "PipelineName": "test-pipeline-26",
-    "PipelineDisplayName": "test-pipeline-26",
-    "PipelineDefinition": '{"Version": "2020-12-01", "Metadata": {}, '
-    '"Parameters": [{"Name": "scheduled-time", "Type": "String"}], '
-    '"PipelineExperimentConfig": {"ExperimentName": {"Get": "Execution.PipelineName"}, '
-    '"TrialName": {"Get": "Execution.PipelineExecutionId"}}, '
-    '"Steps": [{"Name": "test-pipeline-26-training-step", "Type": '
-    '"Training", "Arguments": {"AlgorithmSpecification": {"TrainingInputMode": '
-    '"File", "TrainingImage": "153931337802.dkr.ecr.us-west-2.amazonaws.com/'
-    'sagemaker-spark-processing:3.2-cpu-py39-v1.1", "ContainerEntrypoint": '
-    '["/bin/bash", "/opt/ml/input/data/sagemaker_remote_function_bootstrap/'
-    'job_driver.sh", "--files", "s3://bugbash-schema-update/temp.sh", '
-    '"/opt/ml/input/data/sagemaker_remote_function_bootstrap/spark_app.py"], '
-    '"ContainerArguments": ["--s3_base_uri", '
-    '"s3://bugbash-schema-update-suryans/test-pipeline-26", '
-    '"--region", "us-west-2", "--client_python_version", "3.9"]}, '
-    '"OutputDataConfig": {"S3OutputPath": '
-    '"s3://bugbash-schema-update-suryans/test-pipeline-26"}, '
-    '"StoppingCondition": {"MaxRuntimeInSeconds": 86400}, "ResourceConfig": '
-    '{"VolumeSizeInGB": 30, "InstanceCount": 1, "InstanceType": "ml.m5.xlarge"}, '
-    '"RoleArn": "arn:aws:iam::597217924798:role/Admin", "InputDataConfig": '
-    '[{"DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": '
-    '"s3://bugbash-schema-update-suryans/test-pipeline-26/'
-    'sagemaker_remote_function_bootstrap", "S3DataDistributionType": '
-    '"FullyReplicated"}}, "ChannelName": "sagemaker_remote_function_bootstrap"}, '
-    '{"DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": '
-    '"s3://bugbash-schema-update/sagemaker-2.142.1.dev0-py2.py3-none-any.whl", '
-    '"S3DataDistributionType": "FullyReplicated"}}, "ChannelName": '
-    '"sagemaker_wheel_file"}], "Environment": {"AWS_DEFAULT_REGION": "us-west-2"}, '
-    '"DebugHookConfig": {"S3OutputPath": '
-    '"s3://bugbash-schema-update-suryans/test-pipeline-26", '
-    '"CollectionConfigurations": []},'
-    ' "ProfilerConfig": {"S3OutputPath": '
-    '"s3://bugbash-schema-update-suryans/test-pipeline-26", '
-    '"DisableProfiler": false}, "RetryStrategy": {"MaximumRetryAttempts": 1}}}]}',
-    "RoleArn": "arn:aws:iam::597217924798:role/Admin",
-    "PipelineStatus": "Active",
-    "CreationTime": datetime.datetime(2023, 4, 27, 9, 46, 35, 686000),
-    "LastModifiedTime": datetime.datetime(2023, 4, 27, 20, 27, 36, 648000),
-    "CreatedBy": {},
-    "LastModifiedBy": {},
-    "ResponseMetadata": {
-        "RequestId": "2075bc1c-1b34-4fe5-b7d8-7cfdf784a7d9",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "2075bc1c-1b34-4fe5-b7d8-7cfdf784a7d9",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "2555",
-            "date": "Thu, 04 May 2023 00:28:35 GMT",
-        },
-        "RetryAttempts": 0,
-    },
-}
-
-
-PIPELINE_CONTEXT: Context = Context(
-    context_arn=f"{PIPELINE_NAME}-context-arn",
-    context_name=f"sm-fs-fe-{PIPELINE_NAME}-{CREATION_TIME}-fep",
-    context_type="FeatureEngineeringPipeline",
-    source=ContextSource(source_uri=PIPELINE_ARN, source_types=[]),
-    properties={
-        "PipelineName": PIPELINE_NAME,
-        "PipelineCreationTime": CREATION_TIME,
-        "LastUpdateTime": LAST_UPDATE_TIME,
-    },
-)
-
-PIPELINE_VERSION_CONTEXT: Context = Context(
-    context_arn=f"{PIPELINE_NAME}-version-context-arn",
-    context_name=f"sm-fs-fe-{PIPELINE_NAME}-{LAST_UPDATE_TIME}-fep-ver",
-    context_type=f"FeatureEngineeringPipelineVersion-{PIPELINE_NAME}",
-    source=ContextSource(source_uri=PIPELINE_ARN, source_types=LAST_UPDATE_TIME),
-    properties={"PipelineName": PIPELINE_NAME, "LastUpdateTime": LAST_UPDATE_TIME},
-)
-
-TRANSFORMATION_CODE_INPUT_1: TransformationCode = TransformationCode(
-    s3_uri="s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-    "transform-2023-04-28-21-50-14-616/output/model.tar.gz",
-    author="test-author",
-    name="test-name",
-)
-
-
-TRANSFORMATION_CODE_INPUT_2: TransformationCode = TransformationCode(
-    s3_uri="s3://sagemaker-us-west-2-789975069016/transform-2023-04-28-21-50-14-616/"
-    "transform-2023-04-28-21-50-14-616/output/model.tar.gz/2",
-    author="test-author",
-    name="test-name",
-)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-06-02 00:20:43[0m
[92mHash: b5385256fb89572d394c7c17a76f9c6d61f2c7f5[0m
[92mFilepath: src/sagemaker/jumpstart/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: doc: JumpStart Utility Doc Update (#3866)

[0m
@@ -340,7 +340,6 @@ class JumpStartEstimator(Estimator):
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git
                 repo.
-                (Default: None).
 
                 .. admonition:: Example
 
@@ -354,6 +353,7 @@ class JumpStartEstimator(Estimator):
                     if you need 'train.py'
                     as the entry point and 'test.py' as the training source code, you can assign
                     entry_point='train.py', source_dir='src'.
+                (Default: None).
             git_config (Optional[dict[str, str]]): Git configurations used for cloning
                 files, including ``repo``, ``branch``, ``commit``,
                 ``2FA_enabled``, ``username``, ``password`` and ``token``. The
@@ -363,6 +363,18 @@ class JumpStartEstimator(Estimator):
                 'master' is used. If you don't provide ``commit``, the latest
                 commit in the specified branch is used.
 
+                .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then
+                    checking out the 'master' branch, and checking out the specified
+                    commit.
+
                 ``2FA_enabled``, ``username``, ``password`` and ``token`` are
                 used for authentication. For GitHub (or other Git) accounts, set
                 ``2FA_enabled`` to 'True' if two-factor authentication is
@@ -393,17 +405,6 @@ class JumpStartEstimator(Estimator):
                 the SageMaker Python SDK attempts to use either the CodeCommit
                 credential helper or local credential storage for authentication.
                 (Default: None).
-
-                .. admonition:: Example
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
-                    commit.
             container_log_level (Optional[Union[int, PipelineVariable]]): The log level to use
                 within the container. Valid values are defined in the Python logging module.
                 (Default: None).
@@ -419,9 +420,8 @@ class JumpStartEstimator(Estimator):
                 must point to a file located at the root of ``source_dir``.
                 If 'git_config' is provided, 'entry_point' should be
                 a relative location to the Python source file in the Git repo.
-                (Default: None).
 
-                .. admonition:: Example
+                Example:
                     With the following GitHub repo directory structure:
 
                     >>> |----- README.md
@@ -430,16 +430,18 @@ class JumpStartEstimator(Estimator):
                     >>>         |----- test.py
 
                     You can assign entry_point='src/train.py'.
+
+                (Default: None).
             dependencies (Optional[list[str]]): A list of absolute or relative paths to directories
                 with any additional libraries that should be exported
                 to the container. The library folders are
                 copied to SageMaker in the same folder where the entrypoint is
                 copied. If 'git_config' is provided, 'dependencies' should be a
                 list of relative locations to directories with any additional
-                libraries needed in the Git repo. This is not supported with "local code"
-                in Local Mode. (Default: None).
+                libraries needed in the Git repo.
 
                 .. admonition:: Example
+
                     The following Estimator call:
 
                     >>> Estimator(entry_point='train.py',
@@ -453,6 +455,9 @@ class JumpStartEstimator(Estimator):
                     >>>     |------ train.py
                     >>>     |------ common
                     >>>     |------ virtual-env
+
+                This is not supported with "local code" in Local Mode.
+                (Default: None).
             instance_groups (Optional[list[:class:`sagemaker.instance_group.InstanceGroup`]]):
                 A list of ``InstanceGroup`` objects for launching a training job with a
                 heterogeneous cluster. For example:
@@ -470,7 +475,8 @@ class JumpStartEstimator(Estimator):
                 through the SageMaker generic and framework estimator classes, see
                 `Train Using a Heterogeneous Cluster
                 <https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html>`_
-                in the *Amazon SageMaker developer guide*. (Default: None).
+                in the *Amazon SageMaker developer guide*.
+                (Default: None).
             training_repository_access_mode (Optional[str]): Specifies how SageMaker accesses the
                 Docker image that contains the training algorithm (Default: None).
                 Set this to one of the following values:
@@ -791,7 +797,7 @@ class JumpStartEstimator(Estimator):
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git repo.
                 If the directory points to S3, no code is uploaded and the S3 location
-                is used instead. (Default: None).
+                is used instead.
 
                 .. admonition:: Example
 
@@ -803,6 +809,7 @@ class JumpStartEstimator(Estimator):
                     >>>         |----- test.py
 
                     You can assign entry_point='inference.py', source_dir='src'.
+                (Default: None).
             code_location (Optional[str]): Name of the S3 bucket where custom code is
                 uploaded (Default: None). If not specified, the default bucket
                 created by ``sagemaker.session.Session`` is used. (Default: None).

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-06-02 00:20:43[0m
[92mHash: b5385256fb89572d394c7c17a76f9c6d61f2c7f5[0m
[92mFilepath: src/sagemaker/jumpstart/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: doc: JumpStart Utility Doc Update (#3866)

[0m
@@ -138,7 +138,7 @@ class JumpStartModel(Model):
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git repo.
                 If the directory points to S3, no code is uploaded and the S3 location
-                is used instead. (Default: None).
+                is used instead.
 
                 .. admonition:: Example
 
@@ -150,6 +150,7 @@ class JumpStartModel(Model):
                     >>>         |----- test.py
 
                     You can assign entry_point='inference.py', source_dir='src'.
+                (Default: None).
             code_location (Optional[str]): Name of the S3 bucket where custom code is
                 uploaded (Default: None). If not specified, the default bucket
                 created by ``sagemaker.session.Session`` is used. (Default: None).
@@ -158,9 +159,9 @@ class JumpStartModel(Model):
                 model hosting. (Default: None). If ``source_dir`` is specified, then ``entry_point``
                 must point to a file located at the root of ``source_dir``.
                 If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo. (Default: None).
+                a relative location to the Python source file in the Git repo.
 
-                .. admonition:: Example
+                Example:
                     With the following GitHub repo directory structure:
 
                     >>> |----- README.md
@@ -169,6 +170,8 @@ class JumpStartModel(Model):
                     >>>         |----- test.py
 
                     You can assign entry_point='src/inference.py'.
+
+                (Default: None).
             container_log_level (Optional[Union[int, PipelineVariable]]): Log level to use
                 within the container. Valid values are defined in the Python
                 logging module. (Default: None).
@@ -180,8 +183,7 @@ class JumpStartModel(Model):
                 list of relative locations to directories with any additional
                 libraries needed in the Git repo. If the ```source_dir``` points
                 to S3, code will be uploaded and the S3 location will be used
-                instead. This is not supported with "local code" in Local Mode.
-                (Default: None).
+                instead.
 
                 .. admonition:: Example
 
@@ -198,6 +200,9 @@ class JumpStartModel(Model):
                     >>>     |------ inference.py
                     >>>     |------ common
                     >>>     |------ virtual-env
+
+                This is not supported with "local code" in Local Mode.
+                (Default: None).
             git_config (Optional[dict[str, str]]): Git configurations used for cloning
                 files, including ``repo``, ``branch``, ``commit``,
                 ``2FA_enabled``, ``username``, ``password`` and ``token``. The
@@ -207,6 +212,18 @@ class JumpStartModel(Model):
                 'master' is used. If you don't provide ``commit``, the latest
                 commit in the specified branch is used.
 
+                .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then
+                    checking out the 'master' branch, and checking out the specified
+                    commit.
+
                 ``2FA_enabled``, ``username``, ``password`` and ``token`` are
                 used for authentication. For GitHub (or other Git) accounts, set
                 ``2FA_enabled`` to 'True' if two-factor authentication is
@@ -238,16 +255,6 @@ class JumpStartModel(Model):
                 credential helper or local credential storage for authentication.
                 (Default: None).
 
-                .. admonition:: Example
-
-                    The following config results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
-                    commit.
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
         Raises:
             ValueError: If the model ID is not recognized by JumpStart.
         """

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-05-09 16:36:10[0m
[92mHash: aefdcb7966e8dadf4f020ff46a250493ace5888c[0m
[92mFilepath: src/sagemaker/jumpstart/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart model estimator classes (#3796)

[0m
@@ -1,992 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-"""This module stores JumpStart implementation of Estimator class."""
-from __future__ import absolute_import
-import logging
-
-
-from typing import Dict, List, Optional, Union
-from sagemaker import session
-from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig
-from sagemaker.base_deserializers import BaseDeserializer
-from sagemaker.base_serializers import BaseSerializer
-from sagemaker.debugger.debugger import DebuggerHookConfig, RuleBase, TensorBoardOutputConfig
-from sagemaker.debugger.profiler_config import ProfilerConfig
-
-from sagemaker.estimator import Estimator
-from sagemaker.explainer.explainer_config import ExplainerConfig
-from sagemaker.inputs import FileSystemInput, TrainingInput
-from sagemaker.instance_group import InstanceGroup
-from sagemaker.jumpstart.enums import JumpStartScriptScope
-from sagemaker.jumpstart.exceptions import INVALID_MODEL_ID_ERROR_MSG
-
-from sagemaker.jumpstart.factory.estimator import get_deploy_kwargs, get_fit_kwargs, get_init_kwargs
-from sagemaker.jumpstart.factory.model import get_default_predictor
-from sagemaker.jumpstart.utils import (
-    is_valid_model_id,
-    resolve_model_intelligent_default_field,
-)
-from sagemaker.jumpstart.utils import stringify_object
-from sagemaker.model_monitor.data_capture_config import DataCaptureConfig
-from sagemaker.predictor import PredictorBase
-
-
-from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig
-from sagemaker.workflow.entities import PipelineVariable
-
-logger = logging.getLogger(__name__)
-
-
-class JumpStartEstimator(Estimator):
-    """JumpStartEstimator class.
-
-    This class sets defaults based on the model ID and version.
-    """
-
-    def __init__(
-        self,
-        model_id: Optional[str] = None,
-        model_version: Optional[str] = None,
-        tolerate_vulnerable_model: Optional[bool] = None,
-        tolerate_deprecated_model: Optional[bool] = None,
-        region: Optional[str] = None,
-        image_uri: Optional[Union[str, PipelineVariable]] = None,
-        role: Optional[str] = None,
-        instance_count: Optional[Union[int, PipelineVariable]] = None,
-        instance_type: Optional[Union[str, PipelineVariable]] = None,
-        keep_alive_period_in_seconds: Optional[Union[int, PipelineVariable]] = None,
-        volume_size: Optional[Union[int, PipelineVariable]] = None,
-        volume_kms_key: Optional[Union[str, PipelineVariable]] = None,
-        max_run: Optional[Union[int, PipelineVariable]] = None,
-        input_mode: Optional[Union[str, PipelineVariable]] = None,
-        output_path: Optional[Union[str, PipelineVariable]] = None,
-        output_kms_key: Optional[Union[str, PipelineVariable]] = None,
-        base_job_name: Optional[str] = None,
-        sagemaker_session: Optional[session.Session] = None,
-        hyperparameters: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        tags: Optional[List[Dict[str, Union[str, PipelineVariable]]]] = None,
-        subnets: Optional[List[Union[str, PipelineVariable]]] = None,
-        security_group_ids: Optional[List[Union[str, PipelineVariable]]] = None,
-        model_uri: Optional[str] = None,
-        model_channel_name: Optional[Union[str, PipelineVariable]] = None,
-        metric_definitions: Optional[List[Dict[str, Union[str, PipelineVariable]]]] = None,
-        encrypt_inter_container_traffic: Union[bool, PipelineVariable] = None,
-        use_spot_instances: Optional[Union[bool, PipelineVariable]] = None,
-        max_wait: Optional[Union[int, PipelineVariable]] = None,
-        checkpoint_s3_uri: Optional[Union[str, PipelineVariable]] = None,
-        checkpoint_local_path: Optional[Union[str, PipelineVariable]] = None,
-        enable_network_isolation: Union[bool, PipelineVariable] = None,
-        rules: Optional[List[RuleBase]] = None,
-        debugger_hook_config: Optional[Union[DebuggerHookConfig, bool]] = None,
-        tensorboard_output_config: Optional[TensorBoardOutputConfig] = None,
-        enable_sagemaker_metrics: Optional[Union[bool, PipelineVariable]] = None,
-        profiler_config: Optional[ProfilerConfig] = None,
-        disable_profiler: Optional[bool] = None,
-        environment: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        max_retry_attempts: Optional[Union[int, PipelineVariable]] = None,
-        source_dir: Optional[Union[str, PipelineVariable]] = None,
-        git_config: Optional[Dict[str, str]] = None,
-        container_log_level: Optional[Union[int, PipelineVariable]] = None,
-        code_location: Optional[str] = None,
-        entry_point: Optional[Union[str, PipelineVariable]] = None,
-        dependencies: Optional[List[str]] = None,
-        instance_groups: Optional[List[InstanceGroup]] = None,
-        training_repository_access_mode: Optional[Union[str, PipelineVariable]] = None,
-        training_repository_credentials_provider_arn: Optional[Union[str, PipelineVariable]] = None,
-    ):
-        """Initializes a ``JumpStartEstimator``.
-
-        This method sets model-specific defaults for the ``Estimator.__init__`` method.
-
-        Only model ID is required to instantiate this class, however any field can be overriden.
-        Any field set to ``None`` does not get passed to the parent class method.
-
-
-        Args:
-            model_id (Optional[str]): JumpStart model ID to use. See
-                https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html
-                for list of model IDs.
-            model_version (Optional[str]): Version for JumpStart model to use (Default: None).
-            tolerate_vulnerable_model (Optional[bool]): True if vulnerable versions of model
-                specifications should be tolerated (exception not raised). If False, raises an
-                exception if the script used by this version of the model has dependencies
-                with known security vulnerabilities. (Default: None).
-            tolerate_deprecated_model (Optional[bool]): True if deprecated models should be
-                tolerated (exception not raised). False if these models should raise an exception.
-                (Default: None).
-            region (Optional[str]): The AWS region in which to launch the model. (Default: None).
-            image_uri (Optional[Union[str, PipelineVariable]]): The container image to use for
-                training. (Default: None).
-            role (Optional[str]): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role, if it needs to access an AWS resource. (Default: None).
-            instance_count (Optional[Union[int, PipelineVariable]]): Number of Amazon EC2
-                instances to usefor training. Required if instance_groups is not set.
-                (Default: None).
-            instance_type (Optional[Union[str, PipelineVariable]]): Type of EC2 instance to use
-                for training, for example, ``'ml.c4.xlarge'``. Required if instance_groups is
-                not set. (Default: None).
-            keep_alive_period_in_seconds (Optional[int]): The duration of time in seconds
-                to retain configured resources in a warm pool for subsequent
-                training jobs. (Default: None).
-            volume_size (Optional[int, PipelineVariable]): Size in GB of the storage volume to
-                use for storing input and output data during training.
-
-                Must be large enough to store training data if File mode is
-                used, which is the default mode.
-
-                When you use an ML instance with the EBS-only storage option
-                such as ``ml.c5`` and ``ml.p2``,
-                you must define the size of the EBS
-                volume through the ``volume_size`` parameter in the estimator class.
-
-                .. note::
-
-                    When you use an ML instance with `NVMe SSD volumes
-                    <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssd-instance-store.html#nvme-ssd-volumes>`_
-                    such as ``ml.p4d``, ``ml.g4dn``, and ``ml.g5``,
-                    do not include this parameter in the estimator configuration.
-                    If you use one of those ML instance types,
-                    SageMaker doesn't provision Amazon EBS General Purpose SSD
-                    (gp2) storage nor take this parameter to adjust the NVMe instance storage.
-                    Available storage is fixed to the NVMe instance storage
-                    capacity. SageMaker configures storage paths for training
-                    datasets, checkpoints, model artifacts, and outputs to use the
-                    entire capacity of the instance storage.
-
-                    Note that if you include this parameter and specify a number that
-                    exceeds the size of the NVMe volume attached to the instance type,
-                    SageMaker returns an ``Invalid VolumeSizeInGB`` error.
-
-                To look up instance types and their instance storage types
-                and volumes, see `Amazon EC2 Instance Types
-                <http://aws.amazon.com/ec2/instance-types/>`_.
-
-                To find the default local paths defined by the SageMaker
-                training platform, see `Amazon SageMaker Training Storage
-                Folders for Training Datasets, Checkpoints, Model Artifacts,
-                and Outputs
-                <https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html>`_.
-                (Default: None).
-            volume_kms_key (Optional[Union[str, PipelineVariable]]): KMS key ID for encrypting EBS
-                volume attached to the training instance. (Default: None).
-            max_run (Optional[Union[int, PipelineVariable]]): Timeout in seconds for training.
-                After this amount of time Amazon SageMaker terminates
-                the job regardless of its current status. (Default: None).
-            input_mode (Optional[Union[str, PipelineVariable]]): The input mode that the
-                algorithm supports. Valid modes:
-
-                * 'File' - Amazon SageMaker copies the training dataset from the
-                  S3 location to a local directory.
-                * 'Pipe' - Amazon SageMaker streams data directly from S3 to the
-                  container via a Unix-named pipe.
-
-                This argument can be overriden on a per-channel basis using
-                ``sagemaker.inputs.TrainingInput.input_mode``. (Default: None).
-            output_path (Optional[Union[str, PipelineVariable]]): S3 location for saving
-                the training result (model artifacts and output files). If not specified,
-                results are stored to a default bucket. If the bucket with the specific name
-                does not exist, the estimator creates the bucket during the
-                :meth:`~sagemaker.estimator.EstimatorBase.fit` method execution.
-                (Default: None).
-            output_kms_key (Optional[Union[str, PipelineVariable]]): KMS key ID for encrypting the
-                training output. (Default: None).
-            base_job_name (Optional[str]): Prefix for training job name when the
-                :meth:`~sagemaker.estimator.EstimatorBase.fit` method launches.
-                If not specified, the estimator generates a default job name,
-                based on the training image name and current timestamp. (Default: None).
-            sagemaker_session (Optional[sagemaker.session.Session]): Session object which
-                manages interactions with Amazon SageMaker APIs and any other
-                AWS services needed. If not specified, the estimator creates one
-                using the default AWS configuration chain. (Default: None).
-            hyperparameters (Optional[Union[dict[str, str],dict[str, PipelineVariable]]]):
-                Dictionary containing the hyperparameters to initialize this estimator with.
-
-                .. caution::
-                    You must not include any security-sensitive information, such as
-                    account access IDs, secrets, and tokens, in the dictionary for configuring
-                    hyperparameters. SageMaker rejects the training job request and returns an
-                    validation error for detected credentials, if such user input is found.
-
-                (Default: None).
-            tags (Optional[Union[list[dict[str, str], list[dict[str, PipelineVariable]]]]):
-                List of tags for labeling a training job. For more, see
-                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
-                (Default: None).
-            subnets (Optional[Union[list[str], list[PipelineVariable]]]): List of subnet ids.
-                If not specified training job will be created without VPC config. (Default: None).
-            security_group_ids (Optional[Union[list[str], list[PipelineVariable]]]): List of
-                security group ids. If not specified training job will be created without
-                VPC config. (Default: None).
-            model_uri (Optional[str]): URI where a pre-trained model is stored, either
-                locally or in S3 (Default: None). If specified, the estimator
-                will create a channel pointing to the model so the training job
-                can download it. This model can be a 'model.tar.gz' from a
-                previous training job, or other artifacts coming from a
-                different source.
-
-                In local mode, this should point to the path in which the model
-                is located and not the file itself, as local Docker containers
-                will try to mount the URI as a volume.
-
-                More information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
-
-                (Default: None).
-            model_channel_name (Optional[Union[str, PipelineVariable]]): Name of the channel where
-                'model_uri' will be downloaded. (Default: None).
-            metric_definitions (Optional[Union[list[dict[str, str], list[dict[str,
-                PipelineVariable]]]]): A list of dictionaries that defines the metric(s)
-                used to evaluate the training jobs. Each dictionary contains two keys: 'Name'
-                for the name of the metric, and 'Regex' for the regular expression used to extract
-                the metric from the logs. This should be defined only for jobs that
-                don't use an Amazon algorithm. (Default: None).
-            encrypt_inter_container_traffic (Optional[Union[bool, PipelineVariable]]]): Specifies
-                whether traffic between training containers is encrypted for the training job
-                (Default: None).
-            use_spot_instances (Optional[Union[bool, PipelineVariable]]): Specifies whether to
-                use SageMaker Managed Spot instances for training. If enabled then the
-                ``max_wait`` arg should also be set.
-
-                More information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html
-                (Default: None).
-            max_wait (Optional[Union[int, PipelineVariable]]): Timeout in seconds waiting
-                for spot training job. After this amount of time Amazon
-                SageMaker will stop waiting for managed spot training job to
-                complete. (Default: None).
-            checkpoint_s3_uri (Optional[Union[str, PipelineVariable]]): The S3 URI in which
-                to persist checkpoints that the algorithm persists (if any) during training.
-                (Default: None).
-            checkpoint_local_path (Optional[Union[str, PipelineVariable]]): The local path
-                that the algorithm writes its checkpoints to. SageMaker will persist all
-                files under this path to `checkpoint_s3_uri` continually during
-                training. On job startup the reverse happens - data from the
-                s3 location is downloaded to this path before the algorithm is
-                started. If the path is unset then SageMaker assumes the
-                checkpoints will be provided under `/opt/ml/checkpoints/`.
-                (Default: None).
-            enable_network_isolation (Optional[Union[bool, PipelineVariable]]): Specifies
-                whether container will run in network isolation mode. Network isolation mode
-                restricts the container access to outside networks (such as the Internet).
-                The container does not make any inbound or outbound network calls.
-                Also known as Internet-free mode. (Default: None).
-            rules (Optional[list[:class:`~sagemaker.debugger.RuleBase`]]): A list of
-                :class:`~sagemaker.debugger.RuleBase` objects used to define
-                SageMaker Debugger rules for real-time analysis
-                (Default: None). For more information,
-                see `Continuous analyses through rules
-                <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html
-                #continuous-analyses-through-rules)>`_.
-                (Default: None).
-            debugger_hook_config (Optional[Union[DebuggerHookConfig, bool]]):
-                Configuration for how debugging information is emitted with
-                SageMaker Debugger. If not specified, a default one is created using
-                the estimator's ``output_path``, unless the region does not
-                support SageMaker Debugger. To disable SageMaker Debugger,
-                set this parameter to ``False``. For more information, see
-                `Capture real-time debugging data during model training in Amazon SageMaker
-                <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#
-                capture-real-time-debugging-data-during-model-training-in-amazon-sagemaker>`_.
-                (Default: None).
-            tensorboard_output_config (:class:`~sagemaker.debugger.TensorBoardOutputConfig`):
-                Configuration for customizing debugging visualization using TensorBoard.
-                For more information, see `Capture real time tensorboard data
-                <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#
-                capture-real-time-tensorboard-data-from-the-debugging-hook>`_.
-                (Default: None).
-            enable_sagemaker_metrics (Optional[Union[bool, PipelineVariable]]): enable
-                SageMaker Metrics Time Series. For more information, see `AlgorithmSpecification
-                API <https://docs.aws.amazon.com/sagemaker/latest/dg/
-                API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-
-                EnableSageMakerMetricsTimeSeries>`_.
-                (Default: None).
-            profiler_config (Optional[:class:`~sagemaker.debugger.ProfilerConfig`]):
-                Configuration for how SageMaker Debugger collects
-                monitoring and profiling information from your training job.
-                If not specified, Debugger will be configured with
-                a default configuration and will save system and framework metrics
-                the estimator's default ``output_path`` in Amazon S3.
-                Use :class:`~sagemaker.debugger.ProfilerConfig` to configure this parameter.
-                To disable SageMaker Debugger monitoring and profiling, set the
-                ``disable_profiler`` parameter to ``True``. (Default: None).
-            disable_profiler (Optional[bool]): Specifies whether Debugger monitoring and profiling
-                will be disabled. (Default: None).
-            environment (Optional[Union[dict[str, str], dict[str, PipelineVariable]]]):
-                Environment variables to be set for use during training job. (Default: None).
-            max_retry_attempts (Optional[Union[int, PipelineVariable]]): The number of times
-                to move a job to the STARTING status. You can specify between 1 and 30 attempts.
-                If the value of attempts is greater than zero,
-                the job is retried on InternalServerFailure
-                the same number of attempts as the value.
-                You can cap the total duration for your job by setting ``max_wait`` and ``max_run``.
-                (Default: None).
-            source_dir (Optional[Union[str, PipelineVariable]]): The absolute, relative, or
-                S3 URI Path to a directory with any other training source code dependencies
-                aside from the entry point file. If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory is preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    if you need 'train.py'
-                    as the entry point and 'test.py' as the training source code, you can assign
-                    entry_point='train.py', source_dir='src'.
-                (Default: None).
-            git_config (Optional[dict[str, str]]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
-                for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
-
-                For CodeCommit repos, 2FA is not supported, so ``2FA_enabled``
-                should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
-                (Default: None).
-            container_log_level (Optional[Union[int, PipelineVariable]]): The log level to use
-                within the container. Valid values are defined in the Python logging module.
-                (Default: None).
-            code_location (Optional[str]): The S3 prefix URI where custom code is
-                uploaded (Default: None). You must not include a trailing slash because
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is 's3://output_bucket/job-name/'.
-                (Default: None).
-            entry_point (Optional[Union[str, PipelineVariable]]): The absolute or relative path
-                to the local Python source file that should be executed as the entry point to
-                training. If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-
-                (Default: None).
-            dependencies (Optional[list[str]]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container. The library folders are
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following Estimator call:
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following structure inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-                (Default: None).
-            instance_groups (Optional[list[:class:`sagemaker.instance_group.InstanceGroup`]]):
-                A list of ``InstanceGroup`` objects for launching a training job with a
-                heterogeneous cluster. For example:
-
-                .. code:: python
-
-                    instance_groups=[
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_1', 'ml.p3dn.24xlarge', 64),
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_2', 'ml.c5n.18xlarge', 64)]
-
-                For instructions on how to use ``InstanceGroup`` objects
-                to configure a heterogeneous cluster
-                through the SageMaker generic and framework estimator classes, see
-                `Train Using a Heterogeneous Cluster
-                <https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html>`_
-                in the *Amazon SageMaker developer guide*.
-                (Default: None).
-            training_repository_access_mode (Optional[str]): Specifies how SageMaker accesses the
-                Docker image that contains the training algorithm (Default: None).
-                Set this to one of the following values:
-                * 'Platform' - The training image is hosted in Amazon ECR.
-                * 'Vpc' - The training image is hosted in a private Docker registry in your VPC.
-                When it's default to None, its behavior will be same as 'Platform' - image is hosted
-                in ECR. (Default: None).
-            training_repository_credentials_provider_arn (Optional[str]): The Amazon Resource Name
-                (ARN) of an AWS Lambda function that provides credentials to authenticate to the
-                private Docker registry where your training image is hosted (Default: None).
-                When it's set to None, SageMaker will not do authentication before pulling the image
-                in the private Docker registry. (Default: None).
-
-        Raises:
-            ValueError: If the model ID is not recognized by JumpStart.
-        """
-
-        if not is_valid_model_id(
-            model_id=model_id,
-            model_version=model_version,
-            region=region,
-            script=JumpStartScriptScope.TRAINING,
-        ):
-            raise ValueError(INVALID_MODEL_ID_ERROR_MSG.format(model_id=model_id))
-
-        estimator_init_kwargs = get_init_kwargs(
-            model_id=model_id,
-            model_version=model_version,
-            tolerate_vulnerable_model=tolerate_vulnerable_model,
-            tolerate_deprecated_model=tolerate_deprecated_model,
-            role=role,
-            region=region,
-            instance_count=instance_count,
-            instance_type=instance_type,
-            keep_alive_period_in_seconds=keep_alive_period_in_seconds,
-            volume_size=volume_size,
-            volume_kms_key=volume_kms_key,
-            max_run=max_run,
-            input_mode=input_mode,
-            output_path=output_path,
-            output_kms_key=output_kms_key,
-            base_job_name=base_job_name,
-            sagemaker_session=sagemaker_session,
-            tags=tags,
-            subnets=subnets,
-            security_group_ids=security_group_ids,
-            model_uri=model_uri,
-            model_channel_name=model_channel_name,
-            metric_definitions=metric_definitions,
-            encrypt_inter_container_traffic=encrypt_inter_container_traffic,
-            use_spot_instances=use_spot_instances,
-            max_wait=max_wait,
-            checkpoint_s3_uri=checkpoint_s3_uri,
-            checkpoint_local_path=checkpoint_local_path,
-            rules=rules,
-            debugger_hook_config=debugger_hook_config,
-            tensorboard_output_config=tensorboard_output_config,
-            enable_sagemaker_metrics=enable_sagemaker_metrics,
-            enable_network_isolation=enable_network_isolation,
-            profiler_config=profiler_config,
-            disable_profiler=disable_profiler,
-            environment=environment,
-            max_retry_attempts=max_retry_attempts,
-            source_dir=source_dir,
-            git_config=git_config,
-            hyperparameters=hyperparameters,
-            container_log_level=container_log_level,
-            code_location=code_location,
-            entry_point=entry_point,
-            dependencies=dependencies,
-            instance_groups=instance_groups,
-            training_repository_access_mode=training_repository_access_mode,
-            training_repository_credentials_provider_arn=(
-                training_repository_credentials_provider_arn
-            ),
-            image_uri=image_uri,
-        )
-
-        self.model_id = estimator_init_kwargs.model_id
-        self.model_version = estimator_init_kwargs.model_version
-        self.instance_type = estimator_init_kwargs.instance_type
-        self.tolerate_deprecated_model = estimator_init_kwargs.tolerate_deprecated_model
-        self.tolerate_vulnerable_model = estimator_init_kwargs.tolerate_vulnerable_model
-        self.instance_count = estimator_init_kwargs.instance_count
-        self.region = estimator_init_kwargs.region
-        self.orig_predictor_cls = None
-        self.role = estimator_init_kwargs.role
-        self.sagemaker_session = estimator_init_kwargs.sagemaker_session
-        self._enable_network_isolation = estimator_init_kwargs.enable_network_isolation
-
-        super(JumpStartEstimator, self).__init__(**estimator_init_kwargs.to_kwargs_dict())
-
-    def fit(
-        self,
-        inputs: Optional[Union[str, Dict, TrainingInput, FileSystemInput]] = None,
-        wait: Optional[bool] = True,
-        logs: Optional[str] = None,
-        job_name: Optional[str] = None,
-        experiment_config: Optional[Dict[str, str]] = None,
-    ) -> None:
-        """Start training job by calling base ``Estimator`` class ``fit`` method.
-
-        Any field set to ``None`` does not get passed to the parent class method.
-
-        Args:
-            inputs (Optional[Union[str, dict, sagemaker.inputs.TrainingInput, sagemaker.inputs.FileSystemInput]]):
-                Information about the training data. This can be one of four types:
-
-                * (str) the S3 location where training data is saved, or a file:// path in
-                    local mode.
-                * (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput] or
-                    dict[str, sagemaker.inputs.FileSystemInput]) If using multiple channels for
-                    training data, you can specify a dict mapping channel names to strings or
-                    :func:`~sagemaker.inputs.TrainingInput` objects or
-                    :func:`~sagemaker.inputs.FileSystemInput` objects.
-                * (sagemaker.inputs.TrainingInput) - channel configuration for S3 data sources
-                    that can provide additional information as well as the path to the training
-                    dataset.
-                    See :func:`sagemaker.inputs.TrainingInput` for full details.
-                * (sagemaker.inputs.FileSystemInput) - channel configuration for
-                    a file system data source that can provide additional information as well as
-                    the path to the training dataset.
-
-
-                (Default: None).
-            wait (Optional[bool]): Whether the call should wait until the job completes.
-                (Default: True).
-            logs (Optional[List[str]]): A list of strings specifying which logs to print. Acceptable
-                strings are "All", "None", "Training", or "Rules". To maintain backwards
-                compatibility, boolean values are also accepted and converted to strings.
-                Only meaningful when wait is True. (Default: None).
-            job_name (Optional[str]): Training job name. If not specified, the estimator generates
-                a default job name based on the training image name and current timestamp.
-                (Default: None).
-            experiment_config (Optional[dict[str, str]]): Experiment management configuration.
-                Optionally, the dict can contain four keys:
-                'ExperimentName', 'TrialName', 'TrialComponentDisplayName' and 'RunName'..
-                The behavior of setting these keys is as follows:
-                * If `ExperimentName` is supplied but `TrialName` is not a Trial will be
-                automatically created and the job's Trial Component associated with the Trial.
-                * If `TrialName` is supplied and the Trial already exists the job's Trial Component
-                will be associated with the Trial.
-                * If both `ExperimentName` and `TrialName` are not supplied the trial component
-                will be unassociated.
-                * `TrialComponentDisplayName` is used for display in Studio.
-                * Both `ExperimentName` and `TrialName` will be ignored if the Estimator instance
-                is built with :class:`~sagemaker.workflow.pipeline_context.PipelineSession`.
-                However, the value of `TrialComponentDisplayName` is honored for display in Studio.
-                (Default: None).
-        """
-
-        estimator_fit_kwargs = get_fit_kwargs(
-            model_id=self.model_id,
-            model_version=self.model_version,
-            region=self.region,
-            inputs=inputs,
-            wait=wait,
-            logs=logs,
-            job_name=job_name,
-            experiment_config=experiment_config,
-            tolerate_vulnerable_model=self.tolerate_vulnerable_model,
-            tolerate_deprecated_model=self.tolerate_deprecated_model,
-        )
-
-        return super(JumpStartEstimator, self).fit(**estimator_fit_kwargs.to_kwargs_dict())
-
-    def deploy(
-        self,
-        initial_instance_count: Optional[int] = None,
-        instance_type: Optional[str] = None,
-        serializer: Optional[BaseSerializer] = None,
-        deserializer: Optional[BaseDeserializer] = None,
-        accelerator_type: Optional[str] = None,
-        endpoint_name: Optional[str] = None,
-        tags: List[Dict[str, str]] = None,
-        kms_key: Optional[str] = None,
-        wait: Optional[bool] = True,
-        data_capture_config: Optional[DataCaptureConfig] = None,
-        async_inference_config: Optional[AsyncInferenceConfig] = None,
-        serverless_inference_config: Optional[ServerlessInferenceConfig] = None,
-        volume_size: Optional[int] = None,
-        model_data_download_timeout: Optional[int] = None,
-        container_startup_health_check_timeout: Optional[int] = None,
-        inference_recommendation_id: Optional[str] = None,
-        explainer_config: Optional[ExplainerConfig] = None,
-        image_uri: Optional[Union[str, PipelineVariable]] = None,
-        role: Optional[str] = None,
-        predictor_cls: Optional[callable] = None,
-        env: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        model_name: Optional[str] = None,
-        vpc_config: Optional[Dict[str, List[Union[str, PipelineVariable]]]] = None,
-        sagemaker_session: Optional[session.Session] = None,
-        enable_network_isolation: Union[bool, PipelineVariable] = None,
-        model_kms_key: Optional[str] = None,
-        image_config: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        source_dir: Optional[str] = None,
-        code_location: Optional[str] = None,
-        entry_point: Optional[str] = None,
-        container_log_level: Optional[Union[int, PipelineVariable]] = None,
-        dependencies: Optional[List[str]] = None,
-        git_config: Optional[Dict[str, str]] = None,
-        use_compiled_model: bool = False,
-    ) -> PredictorBase:
-        """Creates endpoint from training job.
-
-        Calls base ``Estimator`` class ``deploy`` method.
-
-        Any field set to ``None`` does not get passed to the parent class method.
-
-        Args:
-            initial_instance_count (Optional[int]): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``. If not using
-                serverless inference or the model has not called ``right_size()``,
-                then it need to be a number larger or equals
-                to 1. (Default: None)
-            instance_type (Optional[str]): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge', or 'local' for local mode. If not using
-                serverless inference or the model has not called ``right_size()``,
-                then it is required to deploy a model.
-                (Default: None)
-            serializer (Optional[:class:`~sagemaker.serializers.BaseSerializer`]): A
-                serializer object, used to encode data for an inference endpoint
-                (Default: None). If ``serializer`` is not None, then
-                ``serializer`` will override the default serializer. The
-                default serializer is set by the ``predictor_cls``. (Default: None).
-            deserializer (Optional[:class:`~sagemaker.deserializers.BaseDeserializer`]): A
-                deserializer object, used to decode data from an inference
-                endpoint (Default: None). If ``deserializer`` is not None, then
-                ``deserializer`` will override the default deserializer. The
-                default deserializer is set by the ``predictor_cls``. (Default: None).
-            accelerator_type (Optional[str]): Type of Elastic Inference accelerator to
-                deploy this model for model loading and inference, for example,
-                'ml.eia1.medium'. If not specified, no Elastic Inference
-                accelerator will be attached to the endpoint. For more
-                information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
-                (Default: None).
-            endpoint_name (Optional[str]): The name of the endpoint to create (default:
-                None). If not specified, a unique endpoint name will be created.
-                (Default: None).
-            tags (Optional[List[dict[str, str]]]): The list of tags to attach to this
-                specific endpoint. (Default: None).
-            kms_key (Optional[str]): The ARN of the KMS key that is used to encrypt the
-                data on the storage volume attached to the instance hosting the
-                endpoint. (Default: None).
-            wait (Optional[bool]): Whether the call should wait until the deployment of
-                this model completes. (Default: True).
-            data_capture_config (Optional[sagemaker.model_monitor.DataCaptureConfig]): Specifies
-                configuration related to Endpoint data capture for use with
-                Amazon SageMaker Model Monitoring. (Default: None).
-            async_inference_config (Optional[sagemaker.model_monitor.AsyncInferenceConfig]):
-                Specifies configuration related to async endpoint. Use this configuration when
-                trying to create async endpoint and make async inference. If empty config object
-                passed through, will use default config to deploy async endpoint. Deploy a
-                real-time endpoint if it's None. (Default: None)
-            serverless_inference_config (Optional[sagemaker.serverless.ServerlessInferenceConfig]):
-                Specifies configuration related to serverless endpoint. Use this configuration
-                when trying to create serverless endpoint and make serverless inference. If
-                empty object passed through, will use pre-defined values in
-                ``ServerlessInferenceConfig`` class to deploy serverless endpoint. Deploy an
-                instance based endpoint if it's None. (Default: None)
-            volume_size (Optional[int]): The size, in GB, of the ML storage volume attached to
-                individual inference instance associated with the production variant.
-                Currenly only Amazon EBS gp2 storage volumes are supported. (Default: None).
-            model_data_download_timeout (Optional[int]): The timeout value, in seconds, to download
-                and extract model data from Amazon S3 to the individual inference instance
-                associated with this production variant. (Default: None).
-            container_startup_health_check_timeout (Optional[int]): The timeout value, in seconds,
-                for your inference container to pass health check by SageMaker Hosting. For more
-                information about health check see:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests
-                (Default: None).
-            inference_recommendation_id (Optional[str]): The recommendation id which specifies the
-                recommendation you picked from inference recommendation job results and
-                would like to deploy the model and endpoint with recommended parameters.
-                (Default: None).
-            explainer_config (Optional[sagemaker.explainer.ExplainerConfig]): Specifies online
-                explainability configuration for use with Amazon SageMaker Clarify. (Default: None).
-            image_uri (Optional[Union[str, PipelineVariable]]): A Docker image URI. (Default: None).
-            role (Optional[str]): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role if it needs to access some AWS resources.
-                It can be null if this is being used to create a Model to pass
-                to a ``PipelineModel`` which has its own Role field. (Default:
-                None).
-            predictor_cls (Optional[callable[string, sagemaker.session.Session]]): A
-                function to call to create a predictor (Default: None). If not
-                None, ``deploy`` will return the result of invoking this
-                function on the created endpoint name. (Default: None).
-            env (Optional[dict[str, str] or dict[str, PipelineVariable]]): Environment variables
-                to run with ``image_uri`` when hosted in SageMaker. (Default: None).
-            model_name (Optional[str]): The model name. If None, a default model name will be
-                selected on each ``deploy``. (Default: None).
-            vpc_config (Optional[Union[dict[str, list[str]],dict[str, list[PipelineVariable]]]]):
-                The VpcConfig set on the model (Default: None)
-                * 'Subnets' (list[str]): List of subnet ids.
-                * 'SecurityGroupIds' (list[str]): List of security group ids. (Default: None).
-            sagemaker_session (Optional[sagemaker.session.Session]): A SageMaker Session
-                object, used for SageMaker interactions (Default: None). If not
-                specified, one is created using the default AWS configuration
-                chain. (Default: None).
-            enable_network_isolation (Optional[Union[bool, PipelineVariable]]): If True,
-                enables network isolation in the endpoint, isolating the model
-                container. No inbound or outbound network calls can be made to
-                or from the model container. (Default: None).
-            model_kms_key (Optional[str]): KMS key ARN used to encrypt the repacked
-                model archive file if the model is repacked. (Default: None).
-            image_config (Optional[Union[dict[str, str], dict[str, PipelineVariable]]]): Specifies
-                whether the image of model container is pulled from ECR, or private
-                registry in your VPC. By default it is set to pull model container
-                image from ECR. (Default: None).
-            source_dir (Optional[str]): The absolute, relative, or S3 URI Path to a directory
-                with any other training source code dependencies aside from the entry
-                point file (Default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory is preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code is uploaded and the S3 location
-                is used instead.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
-                (Default: None).
-            code_location (Optional[str]): Name of the S3 bucket where custom code is
-                uploaded (Default: None). If not specified, the default bucket
-                created by ``sagemaker.session.Session`` is used. (Default: None).
-            entry_point (Optional[str]): The absolute or relative path to the local Python
-                source file that should be executed as the entry point to
-                model hosting. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-
-                (Default: None).
-            container_log_level (Optional[Union[int, PipelineVariable]]): Log level to use within
-                the container. Valid values are defined in the Python logging module.
-                (Default: None).
-            dependencies (Optional[list[str]]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container (default: []). The library folders are
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Model(entry_point='inference.py',
-                    ...       dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following structure inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ inference.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-                (Default: None).
-            git_config (Optional[dict[str, str]]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
-                for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
-
-                For CodeCommit repos, 2FA is not supported, so ``2FA_enabled``
-                should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
-                (Default: None).
-            use_compiled_model (bool): Flag to select whether to use compiled
-                (optimized) model. (Default: False).
-        """
-
-        self.orig_predictor_cls = predictor_cls
-
-        sagemaker_session = sagemaker_session or self.sagemaker_session
-        role = resolve_model_intelligent_default_field(
-            field_name="role",
-            field_val=role,
-            sagemaker_session=sagemaker_session,
-            default_value=self.role,
-        )
-
-        estimator_deploy_kwargs = get_deploy_kwargs(
-            model_id=self.model_id,
-            model_version=self.model_version,
-            region=self.region,
-            tolerate_vulnerable_model=self.tolerate_vulnerable_model,
-            tolerate_deprecated_model=self.tolerate_deprecated_model,
-            initial_instance_count=initial_instance_count,
-            instance_type=instance_type,
-            serializer=serializer,
-            deserializer=deserializer,
-            accelerator_type=accelerator_type,
-            endpoint_name=endpoint_name,
-            tags=tags,
-            kms_key=kms_key,
-            wait=wait,
-            data_capture_config=data_capture_config,
-            async_inference_config=async_inference_config,
-            serverless_inference_config=serverless_inference_config,
-            volume_size=volume_size,
-            model_data_download_timeout=model_data_download_timeout,
-            container_startup_health_check_timeout=container_startup_health_check_timeout,
-            inference_recommendation_id=inference_recommendation_id,
-            explainer_config=explainer_config,
-            image_uri=image_uri,
-            role=role,
-            predictor_cls=predictor_cls,
-            env=env,
-            model_name=model_name,
-            vpc_config=vpc_config,
-            sagemaker_session=sagemaker_session,
-            enable_network_isolation=enable_network_isolation,
-            model_kms_key=model_kms_key,
-            image_config=image_config,
-            source_dir=source_dir,
-            code_location=code_location,
-            entry_point=entry_point,
-            container_log_level=container_log_level,
-            dependencies=dependencies,
-            git_config=git_config,
-            use_compiled_model=use_compiled_model,
-        )
-
-        predictor = super(JumpStartEstimator, self).deploy(
-            **estimator_deploy_kwargs.to_kwargs_dict()
-        )
-
-        # If no predictor class was passed, add defaults to predictor
-        if self.orig_predictor_cls is None:
-            return get_default_predictor(
-                predictor=predictor,
-                model_id=self.model_id,
-                model_version=self.model_version,
-                region=self.region,
-                tolerate_deprecated_model=self.tolerate_deprecated_model,
-                tolerate_vulnerable_model=self.tolerate_vulnerable_model,
-            )
-
-        # If a predictor class was passed, do not mutate predictor
-        return predictor
-
-    def __str__(self) -> str:
-        """Overriding str(*) method to make more human-readable."""
-        return stringify_object(self)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2023-05-09 16:36:10[0m
[92mHash: aefdcb7966e8dadf4f020ff46a250493ace5888c[0m
[92mFilepath: src/sagemaker/jumpstart/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feat: jumpstart model estimator classes (#3796)

[0m
@@ -1,451 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-"""This module stores JumpStart implementation of Model class."""
-
-from __future__ import absolute_import
-import logging
-
-from typing import Dict, List, Optional, Union
-from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig
-from sagemaker.base_deserializers import BaseDeserializer
-from sagemaker.base_serializers import BaseSerializer
-from sagemaker.explainer.explainer_config import ExplainerConfig
-from sagemaker.jumpstart.enums import JumpStartScriptScope
-from sagemaker.jumpstart.exceptions import INVALID_MODEL_ID_ERROR_MSG
-from sagemaker.jumpstart.factory.model import (
-    get_default_predictor,
-    get_deploy_kwargs,
-    get_init_kwargs,
-)
-from sagemaker.jumpstart.utils import is_valid_model_id
-from sagemaker.jumpstart.utils import stringify_object
-from sagemaker.model import Model
-from sagemaker.model_monitor.data_capture_config import DataCaptureConfig
-from sagemaker.predictor import PredictorBase
-from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig
-from sagemaker.session import Session
-from sagemaker.workflow.entities import PipelineVariable
-
-logger = logging.getLogger(__name__)
-
-
-class JumpStartModel(Model):
-    """JumpStartModel class.
-
-    This class sets defaults based on the model ID and version.
-    """
-
-    def __init__(
-        self,
-        model_id: Optional[str] = None,
-        model_version: Optional[str] = None,
-        tolerate_vulnerable_model: Optional[bool] = None,
-        tolerate_deprecated_model: Optional[bool] = None,
-        region: Optional[str] = None,
-        instance_type: Optional[str] = None,
-        image_uri: Optional[Union[str, PipelineVariable]] = None,
-        model_data: Optional[Union[str, PipelineVariable]] = None,
-        role: Optional[str] = None,
-        predictor_cls: Optional[callable] = None,
-        env: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        name: Optional[str] = None,
-        vpc_config: Optional[Dict[str, List[Union[str, PipelineVariable]]]] = None,
-        sagemaker_session: Optional[Session] = None,
-        enable_network_isolation: Union[bool, PipelineVariable] = None,
-        model_kms_key: Optional[str] = None,
-        image_config: Optional[Dict[str, Union[str, PipelineVariable]]] = None,
-        source_dir: Optional[str] = None,
-        code_location: Optional[str] = None,
-        entry_point: Optional[str] = None,
-        container_log_level: Optional[Union[int, PipelineVariable]] = None,
-        dependencies: Optional[List[str]] = None,
-        git_config: Optional[Dict[str, str]] = None,
-    ):
-        """Initializes a ``JumpStartModel``.
-
-        This method sets model-specific defaults for the ``Model.__init__`` method.
-
-        Only model ID is required to instantiate this class, however any field can be overriden.
-
-        Any field set to ``None`` does not get passed to the parent class method.
-
-        Args:
-            model_id (Optional[str]): JumpStart model ID to use. See
-                https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html
-                for list of model IDs.
-            model_version (Optional[str]): Version for JumpStart model to use (Default: None).
-            tolerate_vulnerable_model (Optional[bool]): True if vulnerable versions of model
-                specifications should be tolerated (exception not raised). If False, raises an
-                exception if the script used by this version of the model has dependencies with
-                known security vulnerabilities. (Default: None).
-            tolerate_deprecated_model (Optional[bool]): True if deprecated models should be
-                tolerated (exception not raised). False if these models should raise an exception.
-                (Default: None).
-            region (Optional[str]): The AWS region in which to launch the model. (Default: None).
-            instance_type (Optional[str]): The EC2 instance type to use when provisioning a hosting
-                endpoint. (Default: None).
-            image_uri (Optional[Union[str, PipelineVariable]]): A Docker image URI. (Default: None).
-            model_data (Optional[Union[str, PipelineVariable]]): The S3 location of a SageMaker
-                model data ``.tar.gz`` file. (Default: None).
-            role (Optional[str]): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role if it needs to access some AWS resources.
-                It can be null if this is being used to create a Model to pass
-                to a ``PipelineModel`` which has its own Role field. (Default:
-                None).
-            predictor_cls (Optional[callable[string, sagemaker.session.Session]]): A
-                function to call to create a predictor (Default: None). If not
-                None, ``deploy`` will return the result of invoking this
-                function on the created endpoint name. (Default: None).
-            env (Optional[dict[str, str] or dict[str, PipelineVariable]]): Environment variables
-                to run with ``image_uri`` when hosted in SageMaker. (Default: None).
-            name (Optional[str]): The model name. If None, a default model name will be
-                selected on each ``deploy``. (Default: None).
-            vpc_config (Optional[Union[dict[str, list[str]],dict[str, list[PipelineVariable]]]]):
-                The VpcConfig set on the model (Default: None)
-                * 'Subnets' (list[str]): List of subnet ids.
-                * 'SecurityGroupIds' (list[str]): List of security group ids. (Default: None).
-            sagemaker_session (Optional[sagemaker.session.Session]): A SageMaker Session
-                object, used for SageMaker interactions (Default: None). If not
-                specified, one is created using the default AWS configuration
-                chain. (Default: None).
-            enable_network_isolation (Optional[Union[bool, PipelineVariable]]): If True,
-                enables network isolation in the endpoint, isolating the model
-                container. No inbound or outbound network calls can be made to
-                or from the model container. (Default: None).
-            model_kms_key (Optional[str]): KMS key ARN used to encrypt the repacked
-                model archive file if the model is repacked. (Default: None).
-            image_config (Optional[Union[dict[str, str], dict[str, PipelineVariable]]]): Specifies
-                whether the image of model container is pulled from ECR, or private
-                registry in your VPC. By default it is set to pull model container
-                image from ECR. (Default: None).
-            source_dir (Optional[str]): The absolute, relative, or S3 URI Path to a directory
-                with any other training source code dependencies aside from the entry
-                point file (Default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory is preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code is uploaded and the S3 location
-                is used instead.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
-                (Default: None).
-            code_location (Optional[str]): Name of the S3 bucket where custom code is
-                uploaded (Default: None). If not specified, the default bucket
-                created by ``sagemaker.session.Session`` is used. (Default: None).
-            entry_point (Optional[str]): The absolute or relative path to the local Python
-                source file that should be executed as the entry point to
-                model hosting. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-
-                (Default: None).
-            container_log_level (Optional[Union[int, PipelineVariable]]): Log level to use
-                within the container. Valid values are defined in the Python
-                logging module. (Default: None).
-            dependencies (Optional[list[str]]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container (default: []). The library folders are
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Model(entry_point='inference.py',
-                    ...       dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following structure inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ inference.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-                (Default: None).
-            git_config (Optional[dict[str, str]]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
-                for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
-
-                For CodeCommit repos, 2FA is not supported, so ``2FA_enabled``
-                should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
-                (Default: None).
-
-        Raises:
-            ValueError: If the model ID is not recognized by JumpStart.
-        """
-
-        if not is_valid_model_id(
-            model_id=model_id,
-            model_version=model_version,
-            region=region,
-            script=JumpStartScriptScope.INFERENCE,
-        ):
-            raise ValueError(INVALID_MODEL_ID_ERROR_MSG.format(model_id=model_id))
-
-        model_init_kwargs = get_init_kwargs(
-            model_id=model_id,
-            model_from_estimator=False,
-            model_version=model_version,
-            instance_type=instance_type,
-            tolerate_vulnerable_model=tolerate_vulnerable_model,
-            tolerate_deprecated_model=tolerate_deprecated_model,
-            region=region,
-            image_uri=image_uri,
-            model_data=model_data,
-            source_dir=source_dir,
-            entry_point=entry_point,
-            env=env,
-            predictor_cls=predictor_cls,
-            role=role,
-            name=name,
-            vpc_config=vpc_config,
-            sagemaker_session=sagemaker_session,
-            enable_network_isolation=enable_network_isolation,
-            model_kms_key=model_kms_key,
-            image_config=image_config,
-            code_location=code_location,
-            container_log_level=container_log_level,
-            dependencies=dependencies,
-            git_config=git_config,
-        )
-
-        self.orig_predictor_cls = predictor_cls
-
-        self.model_id = model_init_kwargs.model_id
-        self.model_version = model_init_kwargs.model_version
-        self.instance_type = model_init_kwargs.instance_type
-        self.tolerate_vulnerable_model = model_init_kwargs.tolerate_vulnerable_model
-        self.tolerate_deprecated_model = model_init_kwargs.tolerate_deprecated_model
-        self.region = model_init_kwargs.region
-
-        super(JumpStartModel, self).__init__(**model_init_kwargs.to_kwargs_dict())
-
-    def deploy(
-        self,
-        initial_instance_count: Optional[int] = None,
-        instance_type: Optional[str] = None,
-        serializer: Optional[BaseSerializer] = None,
-        deserializer: Optional[BaseDeserializer] = None,
-        accelerator_type: Optional[str] = None,
-        endpoint_name: Optional[str] = None,
-        tags: List[Dict[str, str]] = None,
-        kms_key: Optional[str] = None,
-        wait: Optional[bool] = True,
-        data_capture_config: Optional[DataCaptureConfig] = None,
-        async_inference_config: Optional[AsyncInferenceConfig] = None,
-        serverless_inference_config: Optional[ServerlessInferenceConfig] = None,
-        volume_size: Optional[int] = None,
-        model_data_download_timeout: Optional[int] = None,
-        container_startup_health_check_timeout: Optional[int] = None,
-        inference_recommendation_id: Optional[str] = None,
-        explainer_config: Optional[ExplainerConfig] = None,
-    ) -> PredictorBase:
-        """Creates endpoint by calling base ``Model`` class `deploy` method.
-
-        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an
-        ``Endpoint`` from this ``Model``.
-
-        Any field set to ``None`` does not get passed to the parent class method.
-
-
-        Args:
-            initial_instance_count (Optional[int]): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``. If not using
-                serverless inference or the model has not called ``right_size()``,
-                then it need to be a number larger or equals
-                to 1. (Default: None)
-            instance_type (Optional[str]): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge', or 'local' for local mode. If not using
-                serverless inference or the model has not called ``right_size()``,
-                then it is required to deploy a model.
-                (Default: None)
-            serializer (Optional[:class:`~sagemaker.serializers.BaseSerializer`]): A
-                serializer object, used to encode data for an inference endpoint
-                (Default: None). If ``serializer`` is not None, then
-                ``serializer`` will override the default serializer. The
-                default serializer is set by the ``predictor_cls``. (Default: None).
-            deserializer (Optional[:class:`~sagemaker.deserializers.BaseDeserializer`]): A
-                deserializer object, used to decode data from an inference
-                endpoint (Default: None). If ``deserializer`` is not None, then
-                ``deserializer`` will override the default deserializer. The
-                default deserializer is set by the ``predictor_cls``. (Default: None).
-            accelerator_type (Optional[str]): Type of Elastic Inference accelerator to
-                deploy this model for model loading and inference, for example,
-                'ml.eia1.medium'. If not specified, no Elastic Inference
-                accelerator will be attached to the endpoint. For more
-                information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
-                (Default: None).
-            endpoint_name (Optional[str]): The name of the endpoint to create (default:
-                None). If not specified, a unique endpoint name will be created.
-                (Default: None).
-            tags (Optional[List[dict[str, str]]]): The list of tags to attach to this
-                specific endpoint. (Default: None).
-            kms_key (Optional[str]): The ARN of the KMS key that is used to encrypt the
-                data on the storage volume attached to the instance hosting the
-                endpoint. (Default: None).
-            wait (Optional[bool]): Whether the call should wait until the deployment of
-                this model completes. (Default: True).
-            data_capture_config (Optional[sagemaker.model_monitor.DataCaptureConfig]): Specifies
-                configuration related to Endpoint data capture for use with
-                Amazon SageMaker Model Monitoring. (Default: None).
-            async_inference_config (Optional[sagemaker.model_monitor.AsyncInferenceConfig]):
-                Specifies configuration related to async endpoint. Use this configuration when
-                trying to create async endpoint and make async inference. If empty config object
-                passed through, will use default config to deploy async endpoint. Deploy a
-                real-time endpoint if it's None. (Default: None)
-            serverless_inference_config (Optional[sagemaker.serverless.ServerlessInferenceConfig]):
-                Specifies configuration related to serverless endpoint. Use this configuration
-                when trying to create serverless endpoint and make serverless inference. If
-                empty object passed through, will use pre-defined values in
-                ``ServerlessInferenceConfig`` class to deploy serverless endpoint. Deploy an
-                instance based endpoint if it's None. (Default: None)
-            volume_size (Optional[int]): The size, in GB, of the ML storage volume attached to
-                individual inference instance associated with the production variant. Currenly only
-                Amazon EBS gp2 storage volumes are supported. (Default: None).
-            model_data_download_timeout (Optional[int]): The timeout value, in seconds, to download
-                and extract model data from Amazon S3 to the individual inference instance
-                associated with this production variant. (Default: None).
-            container_startup_health_check_timeout (Optional[int]): The timeout value, in seconds,
-                for your inference container to pass health check by SageMaker Hosting. For more
-                information about health check see:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests
-                (Default: None).
-            inference_recommendation_id (Optional[str]): The recommendation id which specifies the
-                recommendation you picked from inference recommendation job results and
-                would like to deploy the model and endpoint with recommended parameters.
-                (Default: None).
-            explainer_config (Optional[sagemaker.explainer.ExplainerConfig]): Specifies online
-                explainability configuration for use with Amazon SageMaker Clarify. (Default: None).
-
-        """
-
-        deploy_kwargs = get_deploy_kwargs(
-            model_id=self.model_id,
-            model_version=self.model_version,
-            region=self.region,
-            tolerate_deprecated_model=self.tolerate_deprecated_model,
-            tolerate_vulnerable_model=self.tolerate_vulnerable_model,
-            initial_instance_count=initial_instance_count,
-            instance_type=instance_type or self.instance_type,
-            serializer=serializer,
-            deserializer=deserializer,
-            accelerator_type=accelerator_type,
-            endpoint_name=endpoint_name,
-            tags=tags,
-            kms_key=kms_key,
-            wait=wait,
-            data_capture_config=data_capture_config,
-            async_inference_config=async_inference_config,
-            serverless_inference_config=serverless_inference_config,
-            volume_size=volume_size,
-            model_data_download_timeout=model_data_download_timeout,
-            container_startup_health_check_timeout=container_startup_health_check_timeout,
-            inference_recommendation_id=inference_recommendation_id,
-            explainer_config=explainer_config,
-        )
-
-        predictor = super(JumpStartModel, self).deploy(**deploy_kwargs.to_kwargs_dict())
-
-        # If no predictor class was passed, add defaults to predictor
-        if self.orig_predictor_cls is None:
-            return get_default_predictor(
-                predictor=predictor,
-                model_id=self.model_id,
-                model_version=self.model_version,
-                region=self.region,
-                tolerate_deprecated_model=self.tolerate_deprecated_model,
-                tolerate_vulnerable_model=self.tolerate_vulnerable_model,
-            )
-
-        # If a predictor class was passed, do not mutate predictor
-        return predictor
-
-    def __str__(self) -> str:
-        """Overriding str(*) method to make more human-readable."""
-        return stringify_object(self)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-12-19 18:00:28[0m
[92mHash: f27f975296d543ba77be530ffc9d5d89db27c6c8[0m
[92mFilepath: src/sagemaker/processing.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: support idempotency for framework and spark processors (#3460)

Co-authored-by: Brock Wade <bwayde@amazon.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>[0m
@@ -23,7 +23,6 @@ import pathlib
 import logging
 from textwrap import dedent
 from typing import Dict, List, Optional, Union
-from copy import copy
 
 import attr
 
@@ -1831,17 +1830,14 @@ class FrameworkProcessor(ScriptProcessor):
         #   [93ma7399455f5386d83ddc5cb15c0db00c04bd518ec[0m/src/sagemaker/processing.py#L425-L426
         if inputs is None:
             inputs = []
-
-        # make a shallow copy of user inputs
-        patched_inputs = copy(inputs)
-        patched_inputs.append(
+        inputs.append(
             ProcessingInput(
                 input_name="code",
                 source=s3_payload,
                 destination="/opt/ml/processing/input/code/",
             )
         )
-        return patched_inputs
+        return inputs
 
     def _set_entrypoint(self, command, user_script_name):
         """Framework processor override for setting processing job entrypoint.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-10-21 10:38:22[0m
[92mHash: b267f1e27e294a21a2b0c1d519d25948e23c98ce[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: Fix Repack step auto install behavior (#3419)

* fix: Fix Repack step auto install behavior

* fix doc8 and unit tests

Co-authored-by: Dewen Qi <qidewen@amazon.com>[0m
@@ -26,12 +26,7 @@ from mock import (
 )
 
 from sagemaker.estimator import Estimator
-from sagemaker.workflow._utils import (
-    _RepackModelStep,
-    _RegisterModelStep,
-    REPACK_SCRIPT,
-    REPACK_SCRIPT_LAUNCHER,
-)
+from sagemaker.workflow._utils import _RepackModelStep, _RegisterModelStep
 from sagemaker.workflow.properties import Properties
 from tests.unit.test_utils import FakeS3, list_tar_files
 from tests.unit import DATA_DIR
@@ -120,19 +115,14 @@ def test_repack_model_step(estimator):
         depends_on=["TestStep"],
     )
     request_dict = step.to_request()
-    # No source_dir supplied to _RepackModelStep
-    # so a temp dir will be created and
-    # the repack script and launcher files will be moved/created there
-    assert os.path.isfile(f"{step._source_dir}/{REPACK_SCRIPT}")
-    assert os.path.isfile(f"{step._source_dir}/{REPACK_SCRIPT_LAUNCHER}")
 
     hyperparameters = request_dict["Arguments"]["HyperParameters"]
     assert hyperparameters["inference_script"] == '"dummy_script.py"'
     assert hyperparameters["model_archive"] == '"s3://my-bucket/model.tar.gz"'
-    assert hyperparameters["sagemaker_program"] == f'"{REPACK_SCRIPT_LAUNCHER}"'
+    assert hyperparameters["sagemaker_program"] == '"_repack_model.py"'
     assert (
         hyperparameters["sagemaker_submit_directory"]
-        == '"s3://my-bucket/MyRepackModelStep-[93mb5ea77f701b47a8d075605497462ccc2[0m/source/sourcedir.tar.gz"'
+        == '"s3://my-bucket/MyRepackModelStep-[93m1be10316814854973ed1b445db3ef84e[0m/source/sourcedir.tar.gz"'
     )
 
     del request_dict["Arguments"]["HyperParameters"]
@@ -205,17 +195,14 @@ def test_repack_model_step_with_source_dir(estimator, source_dir):
         source_dir=source_dir,
     )
     request_dict = step.to_request()
-    # The repack script and launcher files will be moved/created to
-    # the specified source_dir
-    assert os.path.isfile(f"{source_dir}/{REPACK_SCRIPT}")
-    assert os.path.isfile(f"{source_dir}/{REPACK_SCRIPT_LAUNCHER}")
+    assert os.path.isfile(f"{source_dir}/_repack_model.py")
 
     hyperparameters = request_dict["Arguments"]["HyperParameters"]
     assert hyperparameters["inference_script"] == '"inference.py"'
     assert hyperparameters["model_archive"].expr == {
         "Std:Join": {"On": "", "Values": [{"Get": "Steps.MyStep"}]}
     }
-    assert hyperparameters["sagemaker_program"] == f'"{REPACK_SCRIPT_LAUNCHER}"'
+    assert hyperparameters["sagemaker_program"] == '"_repack_model.py"'
 
     del request_dict["Arguments"]["HyperParameters"]
     del request_dict["Arguments"]["AlgorithmSpecification"]["TrainingImage"]
@@ -263,6 +250,7 @@ def fake_s3(tmp):
 
 
 def test_inject_repack_script_s3(estimator, tmp, fake_s3):
+
     create_file_tree(
         tmp,
         [
@@ -286,13 +274,12 @@ def test_inject_repack_script_s3(estimator, tmp, fake_s3):
 
     fake_s3.tar_and_upload("model-dir", "s3://fake/location")
 
-    step._prepare_for_repacking()
+    step._inject_repack_script()
 
     assert list_tar_files(fake_s3.fake_upload_path, tmp) == {
         "/aa",
         "/foo/inference.py",
-        f"/{REPACK_SCRIPT}",
-        f"/{REPACK_SCRIPT_LAUNCHER}",
+        "/_repack_model.py",
     }
 
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-08-26 12:37:22[0m
[92mHash: 277f8188beba302aeb89894cbe287385f0fbb8d9[0m
[92mFilepath: CONTRIBUTING.md[0m
[92mBranch: origin/master[0m
[92mCommit: feat: support python 3.10, update airflow dependency (#3327)

[0m
@@ -78,9 +78,9 @@ Before sending us a pull request, please ensure that:
 1. cd into the sagemaker-python-sdk folder: `cd sagemaker-python-sdk` or `cd /environment/sagemaker-python-sdk`
 1. Run the following tox command and verify that all code checks and unit tests pass: `tox tests/unit`
 
-You can also run a single test with the following command: `tox -e py310 -- -s -vv <path_to_file><file_name>::<test_function_name>`
+You can also run a single test with the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`
   * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
-  * Example: `export IGNORE_COVERAGE=- ; tox -e py310 -- -s -vv tests/unit/test_estimator.py::test_sagemaker_model_s3_uri_invalid ; unset IGNORE_COVERAGE`
+  * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/unit/test_estimator.py::test_sagemaker_model_s3_uri_invalid ; unset IGNORE_COVERAGE`
 
 
 ### Run the Integration Tests
@@ -89,9 +89,9 @@ Our CI system runs integration tests (the ones in the `tests/integ` directory),
 You should only worry about manually running any new integration tests that you write, or integration tests that test an area of code that you've modified.
 
 1. Follow the instructions at [Set Up the AWS Command Line Interface (AWS CLI)](https://docs.aws.amazon.com/polly/latest/dg/setup-aws-cli.html).
-1. To run a test, specify the test file and method you want to run per the following command: `tox -e py310 -- -s -vv <path_to_file><file_name>::<test_function_name>`
+1. To run a test, specify the test file and method you want to run per the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`
    * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
-   * Example: `export IGNORE_COVERAGE=- ; tox -e py310 -- -s -vv tests/integ/test_tf_script_mode.py::test_mnist ; unset IGNORE_COVERAGE`
+   * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/integ/test_tf_script_mode.py::test_mnist ; unset IGNORE_COVERAGE`
 
 If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tfs.py#L37) and
 [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-07-07 18:43:15[0m
[92mHash: 5e53a3b37f5306c3437de2acfaeaca7cd8fd0c8c[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: documentation for heterogeneous cluster
[0m
@@ -160,7 +160,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             instance_count (int): Number of Amazon EC2 instances to use
                 for training. Required if instance_groups is not set.
             instance_type (str): Type of EC2 instance to use for training,
-                for example, ``'ml.c4.xlarge'``. Required if instance_groups is
+                for example, 'ml.c4.xlarge'. Required if instance_groups is
                 not set.
             volume_size (int): Size in GB of the EBS volume to use for
                 storing input data during training (default: 30). Must be large
@@ -235,6 +235,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             use_spot_instances (bool): Specifies whether to use SageMaker
                 Managed Spot instances for training. If enabled then the
                 ``max_wait`` arg should also be set.
+
                 More information:
                 https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html
                 (default: ``False``).
@@ -312,18 +313,19 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git
                 repo.
-                With the following GitHub repo directory structure:
 
-                .. code::
+                .. admonition:: Example
+
+                    With the following GitHub repo directory structure:
 
-                    |----- README.md
-                    |----- src
-                             |----- train.py
-                             |----- test.py
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- train.py
+                    >>>         |----- test.py
 
-                if you need 'train.py' as the entry point and 'test.py' as
-                the training source code, you can assign
-                entry_point='train.py' and source_dir='src'.
+                    if you need 'train.py' as the entry point and 'test.py' as
+                    the training source code, you can assign
+                    entry_point='train.py' and source_dir='src'.
             git_config (dict[str, str]): Git configurations used for cloning
                 files, including ``repo``, ``branch``, ``commit``,
                 ``2FA_enabled``, ``username``, ``password``, and ``token``. The
@@ -331,19 +333,20 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 ``repo`` specifies the Git repository where your training script
                 is stored. If you don't provide ``branch``, the default value
                 'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. For example, the following config:
+                commit in the specified branch is used.
 
-                .. code:: python
+                .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
 
-                    git_config = {
-                        'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                        'branch': 'test-branch-git-config',
-                        'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'
-                    }
+                    results in cloning the repo specified in 'repo', then
+                    checking out the 'master' branch, and checking out the specified
+                    commit.
 
-                results in cloning the repo specified in 'repo', then
-                checking out the 'master' branch, and checking out the specified
-                commit.
                 ``2FA_enabled``, ``username``, ``password``, and ``token`` are
                 used for authentication. For GitHub (or other Git) accounts, set
                 ``2FA_enabled`` to 'True' if two-factor authentication is
@@ -424,25 +427,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                     >>>     |------ virtual-env
 
                 This is not supported with "local code" in Local Mode.
-            instance_groups (list[:class:`sagemaker.instance_group.InstanceGroup`]):
-                Optional. A list of ``InstanceGroup`` objects
-                for launching a training job with a heterogeneous cluster.
-                For example:
-
-                .. code:: python
-
-                    instance_groups=[
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_1', 'ml.p3dn.24xlarge', 64),
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_2', 'ml.c5n.18xlarge', 64)]
-
-                For instructions on how to use ``InstanceGroup`` objects
-                to configure a heterogeneous cluster
-                through the SageMaker generic and framework estimator classes, see
-                `Train Using a Heterogeneous Cluster
-                <https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html>`_
-                in the *Amazon SageMaker developer guide*.
+            instance_groups (list[InstanceGroup]): Optional. List of InstanceGroup
+                for specifying different instance groups for heterogeneous cluster.
+                For example: [sagemaker.InstanceGroup('worker','ml.p3dn.24xlarge',64),
+                sagemaker.InstanceGroup('server','ml.c5n.18xlarge',64)]
         """
         instance_count = renamed_kwargs(
             "train_instance_count", "instance_count", instance_count, kwargs
@@ -2430,25 +2418,10 @@ class Estimator(EstimatorBase):
                     >>>     |------ virtual-env
 
                 This is not supported with "local code" in Local Mode.
-            instance_groups (list[:class:`sagemaker.instance_group.InstanceGroup`]):
-                Optional. A list of ``InstanceGroup`` objects
-                for launching a training job with a heterogeneous cluster.
-                For example:
-
-                .. code:: python
-
-                    instance_groups=[
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_1', 'ml.p3dn.24xlarge', 64),
-                        sagemaker.InstanceGroup(
-                            'instance_group_name_2', 'ml.c5n.18xlarge', 64)]
-
-                For instructions on how to use ``InstanceGroup`` objects
-                to configure a heterogeneous cluster
-                through the SageMaker generic and framework estimator classes, see
-                `Train Using a Heterogeneous Cluster
-                <https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html>`_
-                in the *Amazon SageMaker developer guide*.
+            instance_groups (list[InstanceGroup]): Optional. List of InstanceGroup
+                for specifying different instance groups for heterogeneous cluster.
+                For example: [sagemaker.InstanceGroup('worker','ml.p3dn.24xlarge',64),
+                sagemaker.InstanceGroup('server','ml.c5n.18xlarge',64)]
         """
         self.image_uri = image_uri
         self._hyperparameters = hyperparameters.copy() if hyperparameters else {}

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-04-19 16:13:08[0m
[92mHash: a3cb74b1670614881326728614941146d3032886[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_utilities.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: TrainingStep cache misses due to timestamp based job name (#3070)

Co-authored-by: ci <ci>[0m
@@ -14,8 +14,7 @@
 from __future__ import absolute_import
 
 import tempfile
-from sagemaker.workflow.utilities import hash_file, hash_files_or_dirs
-from pathlib import Path
+from sagemaker.workflow.utilities import hash_file
 
 
 def test_hash_file():
@@ -30,70 +29,3 @@ def test_hash_file_uri():
         tmp.write("hashme".encode())
         hash = hash_file(f"file:///{tmp.name}")
         assert hash == "[93md41d8cd98f00b204e9800998ecf8427e[0m"
-
-
-def test_hash_files_or_dirs_with_file():
-    with tempfile.NamedTemporaryFile() as tmp:
-        tmp.write("hashme".encode())
-        hash1 = hash_files_or_dirs([f"file:///{tmp.name}"])
-        # compute hash again with no change to file
-        hash2 = hash_files_or_dirs([f"file:///{tmp.name}"])
-        assert hash1 == hash2
-
-
-def test_hash_files_or_dirs_with_directory():
-    with tempfile.TemporaryDirectory() as tmpdirname:
-        temp_dir = Path(tmpdirname)
-        file_name = temp_dir / "test.txt"
-        file_name.write_text("foo bar")
-        hash1 = hash_files_or_dirs([tmpdirname])
-        # compute hash again with no change to directory
-        hash2 = hash_files_or_dirs([tmpdirname])
-        assert hash1 == hash2
-
-
-def test_hash_files_or_dirs_change_file_content():
-    with tempfile.TemporaryDirectory() as tmpdirname:
-        temp_dir = Path(tmpdirname)
-        file_name = temp_dir / "test.txt"
-        file_name.write_text("foo bar")
-        hash1 = hash_files_or_dirs([tmpdirname])
-        # change file content
-        file_name.write_text("new text")
-        hash2 = hash_files_or_dirs([tmpdirname])
-        assert hash1 != hash2
-
-
-def test_hash_files_or_dirs_rename_file():
-    with tempfile.TemporaryDirectory() as tmpdirname:
-        temp_dir = Path(tmpdirname)
-        file_name = temp_dir / "test.txt"
-        file_name.write_text("foo bar")
-        hash1 = hash_files_or_dirs([tmpdirname])
-        # rename file
-        file_name.rename(temp_dir / "test1.txt")
-        hash2 = hash_files_or_dirs([tmpdirname])
-        assert hash1 != hash2
-
-
-def test_hash_files_or_dirs_add_new_file():
-    with tempfile.TemporaryDirectory() as tmpdirname:
-        temp_dir = Path(tmpdirname)
-        file_name = temp_dir / "test.txt"
-        file_name.write_text("foo bar")
-        hash1 = hash_files_or_dirs([tmpdirname])
-        # add new file
-        file_name2 = temp_dir / "test2.txt"
-        file_name2.write_text("test test")
-        hash2 = hash_files_or_dirs([tmpdirname])
-        assert hash1 != hash2
-
-
-def test_hash_files_or_dirs_unsorted_input_list():
-    with tempfile.NamedTemporaryFile() as tmp1:
-        tmp1.write("hashme".encode())
-        with tempfile.NamedTemporaryFile() as tmp2:
-            tmp2.write("hashme".encode())
-            hash1 = hash_files_or_dirs([tmp1.name, tmp2.name])
-            hash2 = hash_files_or_dirs([tmp2.name, tmp1.name])
-            assert hash1 == hash2

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-04-19 16:13:08[0m
[92mHash: a3cb74b1670614881326728614941146d3032886[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: TrainingStep cache misses due to timestamp based job name (#3070)

Co-authored-by: ci <ci>[0m
@@ -120,10 +120,6 @@ def test_repack_model_step(estimator):
     assert hyperparameters["inference_script"] == '"dummy_script.py"'
     assert hyperparameters["model_archive"] == '"s3://my-bucket/model.tar.gz"'
     assert hyperparameters["sagemaker_program"] == '"_repack_model.py"'
-    assert (
-        hyperparameters["sagemaker_submit_directory"]
-        == '"s3://my-bucket/MyRepackModelStep-[93m1be10316814854973ed1b445db3ef84e[0m/source/sourcedir.tar.gz"'
-    )
 
     del request_dict["Arguments"]["HyperParameters"]
     del request_dict["Arguments"]["AlgorithmSpecification"]["TrainingImage"]

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-04-07 13:34:18[0m
[92mHash: 9e342374c1666e31c0bdf4c853b923de6c6eba3c[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_utilities.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: Support file URIs in ProcessingStep's code parameter (#3051)

* fix: Support file URIs in ProcessingStep's code parameter

* Don't strip leading slash from file uri

Co-authored-by: Payton Staub <pstaub@amazon.com>[0m
@@ -1,31 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-
-import tempfile
-from sagemaker.workflow.utilities import hash_file
-
-
-def test_hash_file():
-    with tempfile.NamedTemporaryFile() as tmp:
-        tmp.write("hashme".encode())
-        hash = hash_file(tmp.name)
-        assert hash == "[93m[93md41d8cd98f00b204e9800998ecf8427e[0m[0m"
-
-
-def test_hash_file_uri():
-    with tempfile.NamedTemporaryFile() as tmp:
-        tmp.write("hashme".encode())
-        hash = hash_file(f"file:///{tmp.name}")
-        assert hash == "[93m[93md41d8cd98f00b204e9800998ecf8427e[0m[0m"

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-03-09 13:37:26[0m
[92mHash: a39b1e753d4d1da9852ee66645a2951a52713bb9[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: container env generation for S3 URI and add test for the same (#2971)

[0m
@@ -26,8 +26,6 @@ from sagemaker.pytorch.model import PyTorchModel
 from sagemaker.sklearn.model import SKLearnModel
 from sagemaker.tensorflow.model import TensorFlowModel
 from sagemaker.xgboost.model import XGBoostModel
-from sagemaker.workflow.properties import Properties
-
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -44,6 +42,7 @@ GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
 ENTRY_POINT_INFERENCE = "inference.py"
+
 SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
 IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
 
@@ -72,23 +71,6 @@ def sagemaker_session():
     return sms
 
 
-@patch("shutil.rmtree", MagicMock())
-@patch("tarfile.open", MagicMock())
-@patch("os.listdir", MagicMock(return_value=[ENTRY_POINT_INFERENCE]))
-def test_prepare_container_def_with_model_src_s3_returns_correct_url(sagemaker_session):
-    model = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=MODEL_IMAGE,
-        model_data=Properties("Steps.MyStep"),
-    )
-    container_def = model.prepare_container_def(INSTANCE_TYPE, "ml.eia.medium")
-
-    assert container_def["Environment"]["SAGEMAKER_SUBMIT_DIRECTORY"] == SCRIPT_URI
-
-
 def test_prepare_container_def_with_model_data():
     model = Model(MODEL_IMAGE)
     container_def = model.prepare_container_def(INSTANCE_TYPE, "ml.eia.medium")

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-04 12:22:10[0m
[92mHash: d8aa976d8e47acdb3ca98f0855e9dd352c13f795[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: Jumpstart doc strings and added new sections (#2893)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Ivy Bazan <nbbazan@amazon.com>
Co-authored-by: IvyBazan <45951687+IvyBazan@users.noreply.github.com>[0m
@@ -165,7 +165,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 the job regardless of its current status.
             input_mode (str): The input mode that the algorithm supports
                 (default: 'File'). Valid modes:
-                'File' - Amazon SageMaker copies the training dataset from the
+                'File' - Amazon SageMaker copiesthe training dataset from the
                 S3 location to a local directory.
                 'Pipe' - Amazon SageMaker streams data directly from S3 to the
                 container via a Unix-named pipe.
@@ -234,7 +234,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             max_wait (int): Timeout in seconds waiting for spot training
                 job (default: None). After this amount of time Amazon
                 SageMaker will stop waiting for managed spot training job to
-                complete (default: None).
+                complete (default: ``None``).
             checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints
                 that the algorithm persists (if any) during training. (default:
                 ``None``).
@@ -245,7 +245,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 s3 location is downloaded to this path before the algorithm is
                 started. If the path is unset then SageMaker assumes the
                 checkpoints will be provided under `/opt/ml/checkpoints/`.
-                (default: None).
+                (default: ``None``).
             rules (list[:class:`~sagemaker.debugger.RuleBase`]): A list of
                 :class:`~sagemaker.debugger.RuleBase` objects used to define
                 SageMaker Debugger rules for real-time analysis
@@ -264,7 +264,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 capture-real-time-debugging-data-during-model-training-in-amazon-sagemaker>`_.
             tensorboard_output_config (:class:`~sagemaker.debugger.TensorBoardOutputConfig`):
                 Configuration for customizing debugging visualization using TensorBoard
-                (default: None). For more information,
+                (default: ``None``). For more information,
                 see `Capture real time tensorboard data
                 <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#
                 capture-real-time-tensorboard-data-from-the-debugging-hook>`_.
@@ -273,7 +273,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 <https://docs.aws.amazon.com/sagemaker/latest/dg/
                 API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-
                 EnableSageMakerMetricsTimeSeries>`_.
-                (default: None).
+                (default: ``None``).
             enable_network_isolation (bool): Specifies whether container will
                 run in network isolation mode (default: ``False``). Network
                 isolation mode restricts the container access to outside networks
@@ -290,18 +290,18 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             disable_profiler (bool): Specifies whether Debugger monitoring and profiling
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
-                use during training job (default: None)
+                use during training job (default: ``None``)
             max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
-                (default: None)
-            source_dir (str): The absolute, relative, or  S3 URI Path to a directory
+                (default: ``None``)
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
                 with any other training source code dependencies aside from the entry
                 point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. The structure within this directory is preserved
+                point to a tar.gz file. Structure within this directory are preserved
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git
                 repo.
@@ -315,19 +315,17 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                     >>>         |----- train.py
                     >>>         |----- test.py
 
-                    if you need 'train.py' as the entry point and 'test.py' as
-                    the training source code, you can assign
-                    entry_point='train.py' and source_dir='src'.
+                    and you need 'train.py' as entry point and 'test.py' as
+                    training source code as well, you can assign
+                    entry_point='train.py', source_dir='src'.
             git_config (dict[str, str]): Git configurations used for cloning
                 files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password``, and ``token``. The
+                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
                 ``repo`` field is required. All other fields are optional.
                 ``repo`` specifies the Git repository where your training script
                 is stored. If you don't provide ``branch``, the default value
                 'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
+                commit in the specified branch is used. .. admonition:: Example
 
                     The following config:
 
@@ -336,10 +334,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                     >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
 
                     results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
+                    checkout the 'master' branch, and checkout the specified
                     commit.
 
-                ``2FA_enabled``, ``username``, ``password``, and ``token`` are
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
                 used for authentication. For GitHub (or other Git) accounts, set
                 ``2FA_enabled`` to 'True' if two-factor authentication is
                 enabled for the account, otherwise set it to 'False'. If you do
@@ -349,37 +347,38 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 repositories.
 
                 For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
                 for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
 
                 For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
                 should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
-            hyperparameters (dict): A dictionary containing the hyperparameters to
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
+            hyperparameters (dict): Dictionary containing the hyperparameters to
                 initialize this estimator with. (Default: None).
-            container_log_level (int): The log level to use within the container
+            container_log_level (int): Log level to use within the container
                 (default: logging.INFO). Valid values are defined in the Python
                 logging module.
-            code_location (str): The S3 prefix URI where custom code is
-                uploaded (default: None). You must not include a trailing slash because
+            code_location (str): The S3 prefix URI where custom code will be
+                uploaded (default: None) - don't include a trailing slash since
                 a string prepended with a "/" is appended to ``code_location``. The code
                 file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is 's3://output_bucket/job-name/'.
-            entry_point (str): The absolute or relative path to the local Python
-                source file that should be executed as the entry point to
+                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
+            entry_point (str): Path (absolute or relative) to the local Python
+                source file which should be executed as the entry point to
                 training. (Default: None). If ``source_dir`` is specified, then ``entry_point``
                 must point to a file located at the root of ``source_dir``.
                 If 'git_config' is provided, 'entry_point' should be
@@ -394,9 +393,9 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                     >>>         |----- test.py
 
                     You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container (default: []). The library folders are
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
                 copied to SageMaker in the same folder where the entrypoint is
                 copied. If 'git_config' is provided, 'dependencies' should be a
                 list of relative locations to directories with any additional
@@ -404,12 +403,12 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
                 .. admonition:: Example
 
-                    The following Estimator call:
+                    The following call
 
                     >>> Estimator(entry_point='train.py',
                     ...           dependencies=['my/libs/common', 'virtual-env'])
 
-                    results in the following structure inside the container:
+                    results in the following inside the container:
 
                     >>> $ ls
 
@@ -590,7 +589,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
     @staticmethod
     def _json_encode_hyperparameters(hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
-        """Applies JSON encoding for certain hyperparameter types, returns hyperparameters.
+        """Applies Json encoding for certain Hyperparameter types, returns hyperparameters.
 
         Args:
             hyperparameters (dict): Dictionary of hyperparameters.
@@ -669,7 +668,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._prepare_profiler_for_training()
 
     def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place updates to hyperparameters required for script mode with training.
+        """Applies in-place update to hyperparameters required for script mode with training.
 
         Args:
             code_dir (str): The directory hosting the training scripts.
@@ -685,9 +684,9 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(hyperparams))
 
     def _stage_user_code_in_s3(self) -> str:
-        """Uploads the user training script to S3 and returns the S3 URI.
+        """Upload the user training script to s3 and return the s3 URI.
 
-        Returns: S3 URI
+        Returns: s3 uri
         """
         local_mode = self.output_path.startswith("file://")
 
@@ -1654,17 +1653,17 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             rules (list[:class:`~sagemaker.debugger.ProfilerRule`]): A list of
                 :class:`~sagemaker.debugger.ProfilerRule` objects to define
                 rules for continuous analysis with SageMaker Debugger. Currently, you can
-                only add new profiler rules during the training job. (default: None)
+                only add new profiler rules during the training job. (default: ``None``)
             s3_output_path (str): The location in S3 to store the output. If profiler is enabled
-                once, s3_output_path cannot be changed. (default: None)
+                once, s3_output_path cannot be changed. (default: ``None``)
             system_monitor_interval_millis (int): How often profiling system metrics are
-                collected; Unit: Milliseconds (default: None)
+                collected; Unit: Milliseconds (default: ``None``)
             framework_profile_params (:class:`~sagemaker.debugger.FrameworkProfile`):
                 A parameter object for framework metrics profiling. Configure it using
                 the :class:`~sagemaker.debugger.FrameworkProfile` class.
                 To use the default framework profile parameters, pass ``FrameworkProfile()``.
                 For more information about the default values,
-                see :class:`~sagemaker.debugger.FrameworkProfile`. (default: None)
+                see :class:`~sagemaker.debugger.FrameworkProfile`. (default: ``None``)
             disable_framework_metrics (bool): Specify whether to disable all the framework metrics.
                 This won't update system metrics and the Debugger built-in rules for monitoring.
                 To stop both monitoring and profiling,
@@ -1889,9 +1888,9 @@ class _TrainingJob(_Job):
         Args:
             estimator (sagemaker.estimator.EstimatorBase): Estimator object created by the user.
             profiler_rule_configs (list): List of profiler rule configurations to be
-                updated in the training job. (default: None).
+                updated in the training job. (default: ``None``).
             profiler_config (dict): Configuration for how profiling information is emitted with
-                SageMaker Debugger. (default: None).
+                SageMaker Debugger. (default: ``None``).
 
         Returns:
             sagemaker.estimator._TrainingJob: Constructed object that captures
@@ -1910,9 +1909,9 @@ class _TrainingJob(_Job):
             estimator (sagemaker.estimator.EstimatorBase): Estimator object
                 created by the user.
             profiler_rule_configs (list): List of profiler rule configurations to be
-                updated in the training job. (default: None).
+                updated in the training job. (default: ``None``).
             profiler_config (dict): Configuration for how profiling information is emitted with
-                SageMaker Debugger. (default: None).
+                SageMaker Debugger. (default: ``None``).
 
         Returns:
             Dict: dict for `sagemaker.session.Session.update_training_job` method
@@ -2102,10 +2101,10 @@ class Estimator(EstimatorBase):
             max_wait (int): Timeout in seconds waiting for spot training
                 instances (default: None). After this amount of time Amazon
                 SageMaker will stop waiting for Spot instances to become
-                available (default: None).
+                available (default: ``None``).
             checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints
                 that the algorithm persists (if any) during training. (default:
-                None).
+                ``None``).
             checkpoint_local_path (str): The local path that the algorithm
                 writes its checkpoints to. SageMaker will persist all files
                 under this path to `checkpoint_s3_uri` continually during
@@ -2113,7 +2112,7 @@ class Estimator(EstimatorBase):
                 s3 location is downloaded to this path before the algorithm is
                 started. If the path is unset then SageMaker assumes the
                 checkpoints will be provided under `/opt/ml/checkpoints/`.
-                (default: None).
+                (default: ``None``).
             enable_network_isolation (bool): Specifies whether container will
                 run in network isolation mode (default: ``False``). Network
                 isolation mode restricts the container access to outside networks
@@ -2122,7 +2121,7 @@ class Estimator(EstimatorBase):
             rules (list[:class:`~sagemaker.debugger.RuleBase`]): A list of
                 :class:`~sagemaker.debugger.RuleBase` objects used to define
                 SageMaker Debugger rules for real-time analysis
-                (default: None). For more information,
+                (default: ``None``). For more information,
                 see `Continuous analyses through rules
                 <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html
                 #continuous-analyses-through-rules)>`_.
@@ -2137,7 +2136,7 @@ class Estimator(EstimatorBase):
                 capture-real-time-debugging-data-during-model-training-in-amazon-sagemaker>`_.
             tensorboard_output_config (:class:`~sagemaker.debugger.TensorBoardOutputConfig`):
                 Configuration for customizing debugging visualization using TensorBoard
-                (default: None). For more information,
+                (default: ``None``). For more information,
                 see `Capture real time tensorboard data
                 <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#
                 capture-real-time-tensorboard-data-from-the-debugging-hook>`_.
@@ -2146,7 +2145,7 @@ class Estimator(EstimatorBase):
                 <https://docs.aws.amazon.com/sagemaker/latest/dg/
                 API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-
                 EnableSageMakerMetricsTimeSeries>`_.
-                (default: None).
+                (default: ``None``).
             profiler_config (:class:`~sagemaker.debugger.ProfilerConfig`):
                 Configuration for how SageMaker Debugger collects
                 monitoring and profiling information from your training job.
@@ -2159,18 +2158,18 @@ class Estimator(EstimatorBase):
             disable_profiler (bool): Specifies whether Debugger monitoring and profiling
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
-                use during training job (default: None)
+                use during training job (default: ``None``)
             max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
-                (default: None)
-            source_dir (str): The absolute, relative, or S3 URI Path to a directory
+                (default: ``None``)
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
                 with any other training source code dependencies aside from the entry
                 point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory is preserved
+                point to a tar.gz file. Structure within this directory are preserved
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git
                 repo.
@@ -2184,8 +2183,8 @@ class Estimator(EstimatorBase):
                     >>>         |----- train.py
                     >>>         |----- test.py
 
-                    if you need 'train.py'
-                    as the entry point and 'test.py' as the training source code, you can assign
+                    and you need 'train.py' as entry point and 'test.py' as
+                    training source code as well, you can assign
                     entry_point='train.py', source_dir='src'.
             git_config (dict[str, str]): Git configurations used for cloning
                 files, including ``repo``, ``branch``, ``commit``,
@@ -2194,9 +2193,7 @@ class Estimator(EstimatorBase):
                 ``repo`` specifies the Git repository where your training script
                 is stored. If you don't provide ``branch``, the default value
                 'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
+                commit in the specified branch is used. .. admonition:: Example
 
                     The following config:
 
@@ -2205,7 +2202,7 @@ class Estimator(EstimatorBase):
                     >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
 
                     results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
+                    checkout the 'master' branch, and checkout the specified
                     commit.
 
                 ``2FA_enabled``, ``username``, ``password`` and ``token`` are
@@ -2218,35 +2215,36 @@ class Estimator(EstimatorBase):
                 repositories.
 
                 For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
                 for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
 
-                For CodeCommit repos, 2FA is not supported, so ``2FA_enabled``
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
                 should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
-            container_log_level (int): The log level to use within the container
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
+            container_log_level (int): Log level to use within the container
                 (default: logging.INFO). Valid values are defined in the Python
                 logging module.
-            code_location (str): The S3 prefix URI where custom code is
-                uploaded (default: None). You must not include a trailing slash because
+            code_location (str): The S3 prefix URI where custom code will be
+                uploaded (default: None) - don't include a trailing slash since
                 a string prepended with a "/" is appended to ``code_location``. The code
                 file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is 's3://output_bucket/job-name/'.
-            entry_point (str): The absolute or relative path to the local Python
-                source file that should be executed as the entry point to
+                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
+            entry_point (str): Path (absolute or relative) to the local Python
+                source file which should be executed as the entry point to
                 training. If ``source_dir`` is specified, then ``entry_point``
                 must point to a file located at the root of ``source_dir``.
                 If 'git_config' is provided, 'entry_point' should be
@@ -2261,9 +2259,9 @@ class Estimator(EstimatorBase):
                     >>>         |----- test.py
 
                     You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container (default: []). The library folders are
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
                 copied to SageMaker in the same folder where the entrypoint is
                 copied. If 'git_config' is provided, 'dependencies' should be a
                 list of relative locations to directories with any additional
@@ -2271,12 +2269,12 @@ class Estimator(EstimatorBase):
 
                 .. admonition:: Example
 
-                    The following Estimator call:
+                    The following call
 
                     >>> Estimator(entry_point='train.py',
                     ...           dependencies=['my/libs/common', 'virtual-env'])
 
-                    results in the following structure inside the container:
+                    results in the following inside the container:
 
                     >>> $ ls
 
@@ -2591,7 +2589,7 @@ class Framework(EstimatorBase):
                 credential storage for authentication.
             checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints
                 that the algorithm persists (if any) during training. (default:
-                None).
+                ``None``).
             checkpoint_local_path (str): The local path that the algorithm
                 writes its checkpoints to. SageMaker will persist all files
                 under this path to `checkpoint_s3_uri` continually during
@@ -2599,11 +2597,11 @@ class Framework(EstimatorBase):
                 s3 location is downloaded to this path before the algorithm is
                 started. If the path is unset then SageMaker assumes the
                 checkpoints will be provided under `/opt/ml/checkpoints/`.
-                (default: None).
+                (default: ``None``).
             enable_sagemaker_metrics (bool): enable SageMaker Metrics Time
                 Series. For more information see:
                 https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-EnableSageMakerMetricsTimeSeries
-                (default: None).
+                (default: ``None``).
             **kwargs: Additional kwargs passed to the ``EstimatorBase``
                 constructor.
 
@@ -2648,7 +2646,7 @@ class Framework(EstimatorBase):
         self._validate_and_set_debugger_configs()
 
     def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place updates to hyperparameters required for script mode with training.
+        """Applies in-place update to hyperparameters required for script mode with training.
 
         Args:
             code_dir (str): The directory hosting the training scripts.
@@ -2722,11 +2720,11 @@ class Framework(EstimatorBase):
         return None
 
     def set_hyperparameters(self, **kwargs):
-        """Escapes the dict argument as JSON, updates the private hyperparameter attribute."""
+        """Escape the dict argument as JSON, update the private hyperparameter attribute."""
         self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(kwargs))
 
     def hyperparameters(self):
-        """Returns the hyperparameters as a dictionary to use for training.
+        """Return the hyperparameters as a dictionary to use for training.
 
         The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which
         trains the model, calls this method to find the hyperparameters.
@@ -2783,8 +2781,8 @@ class Framework(EstimatorBase):
         training.
 
         Args:
-            region (str): Optional. The AWS Region to use for image URI. Default: AWS Region
-            associated with the SageMaker session.
+            region (str): Optional. AWS region to use for image URI. Default: AWS region associated
+                with the SageMaker session.
 
         Returns:
             str: The URI of the Docker image.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-04 12:22:10[0m
[92mHash: d8aa976d8e47acdb3ca98f0855e9dd352c13f795[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: Jumpstart doc strings and added new sections (#2893)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Ivy Bazan <nbbazan@amazon.com>
Co-authored-by: IvyBazan <45951687+IvyBazan@users.noreply.github.com>[0m
@@ -135,14 +135,14 @@ class Model(ModelBase):
                 model container is pulled from ECR, or private registry in your
                 VPC. By default it is set to pull model container image from
                 ECR. (default: None).
-            source_dir (str): The absolute, relative, or S3 URI Path to a directory
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
                 with any other training source code dependencies aside from the entry
                 point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory is preserved
+                point to a tar.gz file. Structure within this directory are preserved
                 when training on Amazon SageMaker. If 'git_config' is provided,
                 'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code is uploaded and the S3 location
-                is used instead.
+                If the directory points to S3, no code will be uploaded and the S3 location
+                will be used instead.
 
                 .. admonition:: Example
 
@@ -155,14 +155,14 @@ class Model(ModelBase):
 
                     You can assign entry_point='inference.py', source_dir='src'.
             code_location (str): Name of the S3 bucket where custom code is
-                uploaded (default: None). If not specified, the default bucket
+                uploaded (default: None). If not specified, default bucket
                 created by ``sagemaker.session.Session`` is used.
-            entry_point (str): The absolute or relative path to the local Python
-                source file that should be executed as the entry point to
-                model hosting. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
+            entry_point (str): Path (absolute or relative) to the Python source
+                file which should be executed as the entry point to model
+                hosting (default: None). If ``source_dir`` is specified,
+                then ``entry_point`` must point to a file located at the root of
+                ``source_dir``. If 'git_config' is provided, 'entry_point' should
+                be a relative location to the Python source file in the Git repo.
 
                 Example:
                     With the following GitHub repo directory structure:
@@ -176,9 +176,9 @@ class Model(ModelBase):
             container_log_level (int): Log level to use within the container
                 (default: logging.INFO). Valid values are defined in the Python
                 logging module.
-            dependencies (list[str]): A list of absolute or relative paths to directories
-                with any additional libraries that should be exported
-                to the container (default: []). The library folders are
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
                 copied to SageMaker in the same folder where the entrypoint is
                 copied. If 'git_config' is provided, 'dependencies' should be a
                 list of relative locations to directories with any additional
@@ -193,7 +193,7 @@ class Model(ModelBase):
                     >>> Model(entry_point='inference.py',
                     ...       dependencies=['my/libs/common', 'virtual-env'])
 
-                    results in the following structure inside the container:
+                    results in the following inside the container:
 
                     >>> $ ls
 
@@ -210,9 +210,7 @@ class Model(ModelBase):
                 ``repo`` specifies the Git repository where your training script
                 is stored. If you don't provide ``branch``, the default value
                 'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
+                commit in the specified branch is used. .. admonition:: Example
 
                     The following config:
 
@@ -221,7 +219,7 @@ class Model(ModelBase):
                     >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
 
                     results in cloning the repo specified in 'repo', then
-                    checking out the 'master' branch, and checking out the specified
+                    checkout the 'master' branch, and checkout the specified
                     commit.
 
                 ``2FA_enabled``, ``username``, ``password`` and ``token`` are
@@ -234,25 +232,26 @@ class Model(ModelBase):
                 repositories.
 
                 For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled. You should
-                either have no passphrase for the SSH key pairs or have the
-                ssh-agent configured so that you will not be prompted for the SSH
-                passphrase when you run the 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided, if 2FA is disabled, then either ``token``
-                or ``username`` and ``password`` are be used for authentication if provided.
-                ``Token`` is prioritized. If 2FA is enabled, only ``token`` is used
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
                 for authentication if provided. If required authentication info
-                is not provided, the SageMaker Python SDK attempts to use local credentials
-                to authenticate. If that fails, an error message is thrown.
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
 
-                For CodeCommit repos, 2FA is not supported, so ``2FA_enabled``
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
                 should not be provided. There is no token in CodeCommit, so
-                ``token`` should also not be provided. When ``repo`` is an SSH URL,
-                the requirements are the same as GitHub  repos. When ``repo``
-                is an HTTPS URL, ``username`` and ``password`` are used for
-                authentication if they are provided. If they are not provided,
-                the SageMaker Python SDK attempts to use either the CodeCommit
-                credential helper or local credential storage for authentication.
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
 
         """
         self.model_data = model_data
@@ -1314,9 +1313,7 @@ class FrameworkModel(Model):
                 ``repo`` specifies the Git repository where your training script
                 is stored. If you don't provide ``branch``, the default value
                 'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used.
-
-                .. admonition:: Example
+                commit in the specified branch is used. .. admonition:: Example
 
                     The following config:
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 17:54:22[0m
[92mHash: 219617ab16e66d7a71df4ce31e03615e408c847a[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: JumpStart Integration (#2870)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -16,7 +16,6 @@ from __future__ import absolute_import, print_function
 import json
 import logging
 import os
-from typing import Any, Dict
 import uuid
 from abc import ABCMeta, abstractmethod
 
@@ -48,10 +47,6 @@ from sagemaker.fw_utils import (
 )
 from sagemaker.inputs import TrainingInput
 from sagemaker.job import _Job
-from sagemaker.jumpstart.utils import (
-    add_jumpstart_tags,
-    update_inference_tags_with_jumpstart_training_tags,
-)
 from sagemaker.local import LocalSession
 from sagemaker.model import (
     CONTAINER_LOG_LEVEL_PARAM_NAME,
@@ -91,15 +86,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
     instance.
     """
 
-    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
-    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
-    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
-    INSTANCE_TYPE = "sagemaker_instance_type"
-    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
-    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
-    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
-    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
-
     def __init__(
         self,
         role,
@@ -133,13 +119,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        hyperparameters=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``EstimatorBase`` instance.
@@ -291,133 +270,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
                 use during training job (default: ``None``)
-            max_retry_attempts (int): The number of times to move a job to the STARTING status.
+             max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            hyperparameters (dict): Dictionary containing the hyperparameters to
-                initialize this estimator with. (Default: None).
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
 
         """
         instance_count = renamed_kwargs(
@@ -446,22 +305,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self.volume_kms_key = volume_kms_key
         self.max_run = max_run
         self.input_mode = input_mode
+        self.tags = tags
         self.metric_definitions = metric_definitions
         self.model_uri = model_uri
         self.model_channel_name = model_channel_name
         self.code_uri = None
         self.code_channel_name = "code"
-        self.source_dir = source_dir
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
-        self.code_location = code_location
-        self.entry_point = entry_point
-        self.dependencies = dependencies
-        self.uploaded_code = None
-        self.tags = add_jumpstart_tags(
-            tags=tags, training_model_uri=self.model_uri, training_script_uri=self.source_dir
-        )
+
         if self.instance_type in ("local", "local_gpu"):
             if self.instance_type == "local_gpu" and self.instance_count > 1:
                 raise RuntimeError("Distributed Training in Local GPU is not supported")
@@ -587,21 +437,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._ensure_base_job_name()
         return name_from_base(self.base_job_name)
 
-    @staticmethod
-    def _json_encode_hyperparameters(hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
-        """Applies Json encoding for certain Hyperparameter types, returns hyperparameters.
-
-        Args:
-            hyperparameters (dict): Dictionary of hyperparameters.
-        """
-        current_hyperparameters = hyperparameters
-        if current_hyperparameters is not None:
-            hyperparameters = {
-                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
-                for (k, v) in current_hyperparameters.items()
-            }
-        return hyperparameters
-
     def _prepare_for_training(self, job_name=None):
         """Set any values in the estimator that need to be set before training.
 
@@ -621,105 +456,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             else:
                 self.output_path = "s3://{}/".format(self.sagemaker_session.default_bucket())
 
-        if self.git_config:
-            updated_paths = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updated_paths["entry_point"]
-            self.source_dir = updated_paths["source_dir"]
-            self.dependencies = updated_paths["dependencies"]
-
-        if self.source_dir or self.entry_point or self.dependencies:
-
-            # validate source dir will raise a ValueError if there is something wrong with
-            # the source directory. We are intentionally not handling it because this is a
-            # critical error.
-            if self.source_dir and not self.source_dir.lower().startswith("s3://"):
-                validate_source_dir(self.entry_point, self.source_dir)
-
-            # if we are in local mode with local_code=True. We want the container to just
-            # mount the source dir instead of uploading to S3.
-            local_code = get_config_value("local.local_code", self.sagemaker_session.config)
-
-            if self.sagemaker_session.local_mode and local_code:
-                # if there is no source dir, use the directory containing the entry point.
-                if self.source_dir is None:
-                    self.source_dir = os.path.dirname(self.entry_point)
-                self.entry_point = os.path.basename(self.entry_point)
-
-                code_dir = "file://" + self.source_dir
-                script = self.entry_point
-            elif self.enable_network_isolation() and self.entry_point:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
-                script = self.uploaded_code.script_name
-                self.code_uri = self.uploaded_code.s3_prefix
-            else:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.uploaded_code.s3_prefix
-                script = self.uploaded_code.script_name
-
-            # Modify hyperparameters in-place to point to the right code directory and
-            # script URIs
-            self._script_mode_hyperparam_update(code_dir, script)
-
         self._prepare_rules()
         self._prepare_debugger_for_training()
         self._prepare_profiler_for_training()
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
-
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
-
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(hyperparams))
-
-    def _stage_user_code_in_s3(self) -> str:
-        """Upload the user training script to s3 and return the s3 URI.
-
-        Returns: s3 uri
-        """
-        local_mode = self.output_path.startswith("file://")
-
-        if self.code_location is None and local_mode:
-            code_bucket = self.sagemaker_session.default_bucket()
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = None
-        elif self.code_location is None:
-            code_bucket, _ = parse_s3_url(self.output_path)
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = self.output_kms_key
-        elif local_mode:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-            kms_key = None
-        else:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-
-            output_bucket, _ = parse_s3_url(self.output_path)
-            kms_key = self.output_kms_key if code_bucket == output_bucket else None
-
-        return tar_and_upload_dir(
-            session=self.sagemaker_session.boto_session,
-            bucket=code_bucket,
-            s3_key_prefix=code_s3_prefix,
-            script=self.entry_point,
-            directory=self.source_dir,
-            dependencies=self.dependencies,
-            kms_key=kms_key,
-            s3_resource=self.sagemaker_session.s3_resource,
-        )
-
     def _prepare_rules(self):
         """Rules list includes both debugger and profiler rules.
 
@@ -1226,10 +966,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
         model.name = model_name
 
-        tags = update_inference_tags_with_jumpstart_training_tags(
-            inference_tags=tags, training_tags=self.tags
-        )
-
         return model.deploy(
             instance_type=instance_type,
             initial_instance_count=initial_instance_count,
@@ -2003,12 +1739,6 @@ class Estimator(EstimatorBase):
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``Estimator`` instance.
@@ -2166,127 +1896,9 @@ class Estimator(EstimatorBase):
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
         """
         self.image_uri = image_uri
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
+        self.hyperparam_dict = hyperparameters.copy() if hyperparameters else {}
         super(Estimator, self).__init__(
             role,
             instance_count,
@@ -2319,13 +1931,6 @@ class Estimator(EstimatorBase):
             disable_profiler=disable_profiler,
             environment=environment,
             max_retry_attempts=max_retry_attempts,
-            container_log_level=container_log_level,
-            source_dir=source_dir,
-            git_config=git_config,
-            code_location=code_location,
-            entry_point=entry_point,
-            dependencies=dependencies,
-            hyperparameters=hyperparameters,
             **kwargs,
         )
 
@@ -2346,7 +1951,7 @@ class Estimator(EstimatorBase):
         training.
         """
         for k, v in kwargs.items():
-            self._hyperparameters[k] = v
+            self.hyperparam_dict[k] = v
 
     def hyperparameters(self):
         """Returns the hyperparameters as a dictionary to use for training.
@@ -2354,7 +1959,7 @@ class Estimator(EstimatorBase):
         The fit() method, that does the model training, calls this method to
         find the hyperparameters you specified.
         """
-        return self._hyperparameters
+        return self.hyperparam_dict
 
     def create_model(
         self,
@@ -2430,6 +2035,15 @@ class Framework(EstimatorBase):
 
     _framework_name = None
 
+    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
+    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
+    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
+    INSTANCE_TYPE = "sagemaker_instance_type"
+    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
+    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
+    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
+    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
+
     def __init__(
         self,
         entry_point,
@@ -2643,23 +2257,48 @@ class Framework(EstimatorBase):
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
-        self._validate_and_set_debugger_configs()
+        if self.git_config:
+            updated_paths = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updated_paths["entry_point"]
+            self.source_dir = updated_paths["source_dir"]
+            self.dependencies = updated_paths["dependencies"]
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
+        # validate source dir will raise a ValueError if there is something wrong with the
+        # source directory. We are intentionally not handling it because this is a critical error.
+        if self.source_dir and not self.source_dir.lower().startswith("s3://"):
+            validate_source_dir(self.entry_point, self.source_dir)
+
+        # if we are in local mode with local_code=True. We want the container to just
+        # mount the source dir instead of uploading to S3.
+        local_code = get_config_value("local.local_code", self.sagemaker_session.config)
+        if self.sagemaker_session.local_mode and local_code:
+            # if there is no source dir, use the directory containing the entry point.
+            if self.source_dir is None:
+                self.source_dir = os.path.dirname(self.entry_point)
+            self.entry_point = os.path.basename(self.entry_point)
+
+            code_dir = "file://" + self.source_dir
+            script = self.entry_point
+        elif self.enable_network_isolation() and self.entry_point:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
+            script = self.uploaded_code.script_name
+            self.code_uri = self.uploaded_code.s3_prefix
+        else:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.uploaded_code.s3_prefix
+            script = self.uploaded_code.script_name
 
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
+        # Modify hyperparameters in-place to point to the right code directory and script URIs
+        self._hyperparameters[DIR_PARAM_NAME] = code_dir
+        self._hyperparameters[SCRIPT_PARAM_NAME] = script
+        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
+        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name
+        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
 
-        self._hyperparameters.update(hyperparams)
+        self._validate_and_set_debugger_configs()
 
     def _validate_and_set_debugger_configs(self):
         """Set defaults for debugging."""
@@ -2689,6 +2328,44 @@ class Framework(EstimatorBase):
                 self.environment = {}
             self.environment[DEBUGGER_FLAG] = "0"
 
+    def _stage_user_code_in_s3(self):
+        """Upload the user training script to s3 and return the location.
+
+        Returns: s3 uri
+        """
+        local_mode = self.output_path.startswith("file://")
+
+        if self.code_location is None and local_mode:
+            code_bucket = self.sagemaker_session.default_bucket()
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = None
+        elif self.code_location is None:
+            code_bucket, _ = parse_s3_url(self.output_path)
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = self.output_kms_key
+        elif local_mode:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+            kms_key = None
+        else:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+
+            output_bucket, _ = parse_s3_url(self.output_path)
+            kms_key = self.output_kms_key if code_bucket == output_bucket else None
+
+        return tar_and_upload_dir(
+            session=self.sagemaker_session.boto_session,
+            bucket=code_bucket,
+            s3_key_prefix=code_s3_prefix,
+            script=self.entry_point,
+            directory=self.source_dir,
+            dependencies=self.dependencies,
+            kms_key=kms_key,
+            s3_resource=self.sagemaker_session.s3_resource,
+            settings=self.sagemaker_session.settings,
+        )
+
     def _model_source_dir(self):
         """Get the appropriate value to pass as ``source_dir`` to a model constructor.
 
@@ -2719,10 +2396,6 @@ class Framework(EstimatorBase):
 
         return None
 
-    def set_hyperparameters(self, **kwargs):
-        """Escape the dict argument as JSON, update the private hyperparameter attribute."""
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(kwargs))
-
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
@@ -2732,7 +2405,7 @@ class Framework(EstimatorBase):
         Returns:
             dict[str, str]: The hyperparameters.
         """
-        return EstimatorBase._json_encode_hyperparameters(self._hyperparameters)
+        return self._json_encode_hyperparameters(self._hyperparameters)
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
@@ -2773,32 +2446,51 @@ class Framework(EstimatorBase):
 
         return init_params
 
-    def training_image_uri(self, region=None):
+    def training_image_uri(self):
         """Return the Docker image to use for training.
 
         The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does
         the model training, calls this method to find the image to use for model
         training.
 
-        Args:
-            region (str): Optional. AWS region to use for image URI. Default: AWS region associated
-                with the SageMaker session.
-
         Returns:
             str: The URI of the Docker image.
         """
+        if self.image_uri:
+            return self.image_uri
+        if hasattr(self, "distribution"):
+            distribution = self.distribution  # pylint: disable=no-member
+        else:
+            distribution = None
+        compiler_config = getattr(self, "compiler_config", None)
+
+        if hasattr(self, "tensorflow_version") or hasattr(self, "pytorch_version"):
+            processor = image_uris._processor(self.instance_type, ["cpu", "gpu"])
+            is_native_huggingface_gpu = processor == "gpu" and not compiler_config
+            container_version = "cu110-ubuntu18.04" if is_native_huggingface_gpu else None
+            if self.tensorflow_version is not None:  # pylint: disable=no-member
+                base_framework_version = (
+                    f"tensorflow{self.tensorflow_version}"  # pylint: disable=no-member
+                )
+            else:
+                base_framework_version = (
+                    f"pytorch{self.pytorch_version}"  # pylint: disable=no-member
+                )
+        else:
+            container_version = None
+            base_framework_version = None
 
-        return image_uris.get_training_image_uri(
-            region=region or self.sagemaker_session.boto_region_name,
-            framework=self._framework_name,
-            framework_version=self.framework_version,  # pylint: disable=no-member
-            py_version=self.py_version,  # pylint: disable=no-member
-            image_uri=self.image_uri,
-            distribution=getattr(self, "distribution", None),
-            compiler_config=getattr(self, "compiler_config", None),
-            tensorflow_version=getattr(self, "tensorflow_version", None),
-            pytorch_version=getattr(self, "pytorch_version", None),
+        return image_uris.retrieve(
+            self._framework_name,
+            self.sagemaker_session.boto_region_name,
             instance_type=self.instance_type,
+            version=self.framework_version,  # pylint: disable=no-member
+            py_version=self.py_version,  # pylint: disable=no-member
+            image_scope="training",
+            distribution=distribution,
+            base_framework_version=base_framework_version,
+            container_version=container_version,
+            training_compiler_config=compiler_config,
         )
 
     @classmethod
@@ -2851,6 +2543,17 @@ class Framework(EstimatorBase):
         )
         return estimator
 
+    @staticmethod
+    def _json_encode_hyperparameters(hyperparameters):
+        """Placeholder docstring"""
+        current_hyperparameters = hyperparameters
+        if current_hyperparameters is not None:
+            hyperparameters = {
+                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
+                for (k, v) in current_hyperparameters.items()
+            }
+        return hyperparameters
+
     @classmethod
     def _update_init_params(cls, hp, tf_arguments):
         """Placeholder docstring"""

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 17:54:22[0m
[92mHash: 219617ab16e66d7a71df4ce31e03615e408c847a[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: JumpStart Integration (#2870)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -18,7 +18,6 @@ import json
 import logging
 import os
 import re
-import copy
 
 import sagemaker
 from sagemaker import (
@@ -35,7 +34,6 @@ from sagemaker.deprecations import removed_kwargs
 from sagemaker.predictor import PredictorBase
 from sagemaker.serverless import ServerlessInferenceConfig
 from sagemaker.transformer import Transformer
-from sagemaker.jumpstart.utils import add_jumpstart_tags
 from sagemaker.utils import unique_name_from_base
 from sagemaker.async_inference import AsyncInferenceConfig
 from sagemaker.predictor_async import AsyncPredictor
@@ -63,15 +61,6 @@ class ModelBase(abc.ABC):
         """Destroy resources associated with this model."""
 
 
-SCRIPT_PARAM_NAME = "sagemaker_program"
-DIR_PARAM_NAME = "sagemaker_submit_directory"
-CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
-JOB_NAME_PARAM_NAME = "sagemaker_job_name"
-MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
-SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
-SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
-
-
 class Model(ModelBase):
     """A SageMaker ``Model`` that can be deployed to an ``Endpoint``."""
 
@@ -88,12 +77,6 @@ class Model(ModelBase):
         enable_network_isolation=False,
         model_kms_key=None,
         image_config=None,
-        source_dir=None,
-        code_location=None,
-        entry_point=None,
-        container_log_level=logging.INFO,
-        dependencies=None,
-        git_config=None,
     ):
         """Initialize an SageMaker ``Model``.
 
@@ -135,124 +118,6 @@ class Model(ModelBase):
                 model container is pulled from ECR, or private registry in your
                 VPC. By default it is set to pull model container image from
                 ECR. (default: None).
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code will be uploaded and the S3 location
-                will be used instead.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
-            code_location (str): Name of the S3 bucket where custom code is
-                uploaded (default: None). If not specified, default bucket
-                created by ``sagemaker.session.Session`` is used.
-            entry_point (str): Path (absolute or relative) to the Python source
-                file which should be executed as the entry point to model
-                hosting (default: None). If ``source_dir`` is specified,
-                then ``entry_point`` must point to a file located at the root of
-                ``source_dir``. If 'git_config' is provided, 'entry_point' should
-                be a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Model(entry_point='inference.py',
-                    ...       dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ inference.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-
         """
         self.model_data = model_data
         self.image_uri = image_uri
@@ -270,24 +135,6 @@ class Model(ModelBase):
         self._enable_network_isolation = enable_network_isolation
         self.model_kms_key = model_kms_key
         self.image_config = image_config
-        self.entry_point = entry_point
-        self.source_dir = source_dir
-        self.dependencies = dependencies or []
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        if code_location:
-            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
-        else:
-            self.bucket, self.key_prefix = None, None
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
-        self.uploaded_code = None
-        self.repacked_model_data = None
 
     def register(
         self,
@@ -399,91 +246,10 @@ class Model(ModelBase):
         Returns:
             dict: A container definition object usable with the CreateModel API.
         """
-        deploy_key_prefix = fw_utils.model_code_key_prefix(
-            self.key_prefix, self.name, self.image_uri
-        )
-        deploy_env = copy.deepcopy(self.env)
-        if self.source_dir or self.dependencies or self.entry_point or self.git_config:
-            is_repack = (
-                self.source_dir and self.entry_point and not (self.key_prefix or self.git_config)
-            )
-            self._upload_code(deploy_key_prefix, repack=is_repack)
-            deploy_env.update(self._script_mode_env_vars())
         return sagemaker.container_def(
-            self.image_uri,
-            self.repacked_model_data or self.model_data,
-            deploy_env,
-            image_config=self.image_config,
+            self.image_uri, self.model_data, self.env, image_config=self.image_config
         )
 
-    def _upload_code(self, key_prefix: str, repack: bool = False) -> None:
-        """Uploads code to S3 to be used with script mode with SageMaker inference.
-
-        Args:
-            key_prefix (str): The S3 key associated with the ``code_location`` parameter of the
-                ``Model`` class.
-            repack (bool): Optional. Set to ``True`` to indicate that the source code and model
-                artifact should be repackaged into a new S3 object. (default: False).
-        """
-        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
-        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
-            self.uploaded_code = None
-        elif not repack:
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            self.uploaded_code = fw_utils.tar_and_upload_dir(
-                session=self.sagemaker_session.boto_session,
-                bucket=bucket,
-                s3_key_prefix=key_prefix,
-                script=self.entry_point,
-                directory=self.source_dir,
-                dependencies=self.dependencies,
-            )
-
-        if repack and self.model_data is not None and self.entry_point is not None:
-            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
-                # model is not yet there, defer repacking to later during pipeline execution
-                return
-
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
-
-            utils.repack_model(
-                inference_script=self.entry_point,
-                source_directory=self.source_dir,
-                dependencies=self.dependencies,
-                model_uri=self.model_data,
-                repacked_model_uri=repacked_model_data,
-                sagemaker_session=self.sagemaker_session,
-                kms_key=self.model_kms_key,
-            )
-
-            self.repacked_model_data = repacked_model_data
-            self.uploaded_code = fw_utils.UploadedCode(
-                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
-            )
-
-    def _script_mode_env_vars(self):
-        """Placeholder docstring"""
-        script_name = None
-        dir_name = None
-        if self.uploaded_code:
-            script_name = self.uploaded_code.script_name
-            if self.enable_network_isolation():
-                dir_name = "/opt/ml/model/code"
-            else:
-                dir_name = self.uploaded_code.s3_prefix
-        elif self.entry_point is not None:
-            script_name = self.entry_point
-            if self.source_dir is not None:
-                dir_name = "file://" + self.source_dir
-
-        return {
-            SCRIPT_PARAM_NAME.upper(): script_name or str(),
-            DIR_PARAM_NAME.upper(): dir_name or str(),
-            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
-            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
-        }
-
     def enable_network_isolation(self):
         """Whether to enable network isolation when creating this Model
 
@@ -1016,10 +782,6 @@ class Model(ModelBase):
         removed_kwargs("update_endpoint", kwargs)
         self._init_sagemaker_session_if_does_not_exist(instance_type)
 
-        tags = add_jumpstart_tags(
-            tags=tags, inference_model_uri=self.model_data, inference_script_uri=self.source_dir
-        )
-
         if self.role is None:
             raise ValueError("Role can not be null for deploying a model")
 
@@ -1194,6 +956,15 @@ class Model(ModelBase):
         self.sagemaker_session.delete_model(self.name)
 
 
+SCRIPT_PARAM_NAME = "sagemaker_program"
+DIR_PARAM_NAME = "sagemaker_submit_directory"
+CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
+JOB_NAME_PARAM_NAME = "sagemaker_job_name"
+MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
+SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
+SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
+
+
 class FrameworkModel(Model):
     """A Model for working with an SageMaker ``Framework``.
 
@@ -1371,14 +1142,113 @@ class FrameworkModel(Model):
             env=env,
             name=name,
             sagemaker_session=sagemaker_session,
-            source_dir=source_dir,
-            code_location=code_location,
-            entry_point=entry_point,
-            container_log_level=container_log_level,
-            dependencies=dependencies,
-            git_config=git_config,
             **kwargs,
         )
+        self.entry_point = entry_point
+        self.source_dir = source_dir
+        self.dependencies = dependencies or []
+        self.git_config = git_config
+        self.container_log_level = container_log_level
+        if code_location:
+            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
+        else:
+            self.bucket, self.key_prefix = None, None
+        if self.git_config:
+            updates = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updates["entry_point"]
+            self.source_dir = updates["source_dir"]
+            self.dependencies = updates["dependencies"]
+        self.uploaded_code = None
+        self.repacked_model_data = None
+
+    def prepare_container_def(self, instance_type=None, accelerator_type=None):
+        """Return a container definition with framework configuration.
+
+        Framework configuration is set in model environment variables.
+        This also uploads user-supplied code to S3.
+
+        Args:
+            instance_type (str): The EC2 instance type to deploy this Model to.
+                For example, 'ml.p2.xlarge'.
+            accelerator_type (str): The Elastic Inference accelerator type to
+                deploy to the instance for loading and making inferences to the
+                model. For example, 'ml.eia1.medium'.
+
+        Returns:
+            dict[str, str]: A container definition object usable with the
+            CreateModel API.
+        """
+        deploy_key_prefix = fw_utils.model_code_key_prefix(
+            self.key_prefix, self.name, self.image_uri
+        )
+        self._upload_code(deploy_key_prefix)
+        deploy_env = dict(self.env)
+        deploy_env.update(self._framework_env_vars())
+        return sagemaker.container_def(self.image_uri, self.model_data, deploy_env)
+
+    def _upload_code(self, key_prefix, repack=False):
+        """Placeholder Docstring"""
+        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
+        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
+            self.uploaded_code = None
+        elif not repack:
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            self.uploaded_code = fw_utils.tar_and_upload_dir(
+                session=self.sagemaker_session.boto_session,
+                bucket=bucket,
+                s3_key_prefix=key_prefix,
+                script=self.entry_point,
+                directory=self.source_dir,
+                dependencies=self.dependencies,
+                settings=self.sagemaker_session.settings,
+            )
+
+        if repack and self.model_data is not None and self.entry_point is not None:
+            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
+                # model is not yet there, defer repacking to later during pipeline execution
+                return
+
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
+
+            utils.repack_model(
+                inference_script=self.entry_point,
+                source_directory=self.source_dir,
+                dependencies=self.dependencies,
+                model_uri=self.model_data,
+                repacked_model_uri=repacked_model_data,
+                sagemaker_session=self.sagemaker_session,
+                kms_key=self.model_kms_key,
+            )
+
+            self.repacked_model_data = repacked_model_data
+            self.uploaded_code = fw_utils.UploadedCode(
+                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
+            )
+
+    def _framework_env_vars(self):
+        """Placeholder docstring"""
+        script_name = None
+        dir_name = None
+        if self.uploaded_code:
+            script_name = self.uploaded_code.script_name
+            if self.enable_network_isolation():
+                dir_name = "/opt/ml/model/code"
+            else:
+                dir_name = self.uploaded_code.s3_prefix
+        elif self.entry_point is not None:
+            script_name = self.entry_point
+            if self.source_dir is not None:
+                dir_name = "file://" + self.source_dir
+
+        return {
+            SCRIPT_PARAM_NAME.upper(): script_name or str(),
+            DIR_PARAM_NAME.upper(): dir_name or str(),
+            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
+            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
+        }
 
 
 class ModelPackage(Model):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 17:54:22[0m
[92mHash: 219617ab16e66d7a71df4ce31e03615e408c847a[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: JumpStart Integration (#2870)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -11,21 +11,12 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
-from unittest.mock import MagicMock
 
 import pytest
 from mock import Mock, patch
 
 import sagemaker
-from sagemaker.model import FrameworkModel, Model
-from sagemaker.huggingface.model import HuggingFaceModel
-from sagemaker.jumpstart.constants import JUMPSTART_BUCKET_NAME_SET
-from sagemaker.jumpstart.enums import JumpStartTag
-from sagemaker.mxnet.model import MXNetModel
-from sagemaker.pytorch.model import PyTorchModel
-from sagemaker.sklearn.model import SKLearnModel
-from sagemaker.tensorflow.model import TensorFlowModel
-from sagemaker.xgboost.model import XGBoostModel
+from sagemaker.model import Model
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -36,39 +27,10 @@ INSTANCE_COUNT = 2
 INSTANCE_TYPE = "ml.c4.4xlarge"
 ROLE = "some-role"
 
-REGION = "us-west-2"
-BUCKET_NAME = "some-bucket-name"
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-ENTRY_POINT_INFERENCE = "inference.py"
 
-SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
-IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
-
-
-class DummyFrameworkModel(FrameworkModel):
-    def __init__(self, **kwargs):
-        super(DummyFrameworkModel, self).__init__(
-            **kwargs,
-        )
-
-
-@pytest.fixture()
+@pytest.fixture
 def sagemaker_session():
-    boto_mock = Mock(name="boto_session", region_name=REGION)
-    sms = MagicMock(
-        name="sagemaker_session",
-        boto_session=boto_mock,
-        boto_region_name=REGION,
-        config=None,
-        local_mode=False,
-        s3_client=None,
-        s3_resource=None,
-    )
-    sms.default_bucket = Mock(name="default_bucket", return_value=BUCKET_NAME)
-
-    return sms
+    return Mock()
 
 
 def test_prepare_container_def_with_model_data():
@@ -383,171 +345,3 @@ def test_delete_model_no_name(sagemaker_session):
     ):
         model.delete_model()
     sagemaker_session.delete_model.assert_not_called()
-
-
-@patch("time.strftime", MagicMock(return_value=TIMESTAMP))
-@patch("sagemaker.utils.repack_model")
-def test_script_mode_model_same_calls_as_framework(repack_model, sagemaker_session):
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert len(sagemaker_session.create_model.call_args_list) == 1
-    assert len(sagemaker_session.endpoint_from_production_variants.call_args_list) == 1
-    assert len(repack_model.call_args_list) == 1
-
-    generic_model_create_model_args = sagemaker_session.create_model.call_args_list
-    generic_model_endpoint_from_production_variants_args = (
-        sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    generic_model_repack_model_args = repack_model.call_args_list
-
-    sagemaker_session.create_model.reset_mock()
-    sagemaker_session.endpoint_from_production_variants.reset_mock()
-    repack_model.reset_mock()
-
-    t = DummyFrameworkModel(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert generic_model_create_model_args == sagemaker_session.create_model.call_args_list
-    assert (
-        generic_model_endpoint_from_production_variants_args
-        == sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    assert generic_model_repack_model_args == repack_model.call_args_list
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_succeed_model_class(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, sourcedir, dependency: {
-        "entry_point": "entry_point",
-        "source_dir": "/tmp/repo_dir/source_dir",
-        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"],
-    }
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    model = Model(
-        sagemaker_session=sagemaker_session,
-        entry_point=entry_point,
-        source_dir=source_dir,
-        dependencies=dependencies,
-        git_config=git_config,
-        image_uri=IMAGE_URI,
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, source_dir, dependencies)
-    assert model.entry_point == "entry_point"
-    assert model.source_dir == "/tmp/repo_dir/source_dir"
-    assert model.dependencies == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
-
-
-@patch("sagemaker.utils.repack_model")
-def test_script_mode_model_tags_jumpstart_models(repack_model, sagemaker_session):
-
-    jumpstart_source_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/source_dirs/source.tar.gz"
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=jumpstart_source_dir,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert sagemaker_session.create_model.call_args_list[0][1]["tags"] == [
-        {
-            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-            "Value": jumpstart_source_dir,
-        },
-    ]
-    assert sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"] == [
-        {
-            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-            "Value": jumpstart_source_dir,
-        },
-    ]
-
-    non_jumpstart_source_dir = "s3://blah/blah/blah"
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=non_jumpstart_source_dir,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert {
-        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-        "Value": non_jumpstart_source_dir,
-    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-    assert {
-        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-        "Value": non_jumpstart_source_dir,
-    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-
-@patch("sagemaker.utils.repack_model")
-@patch("sagemaker.fw_utils.tar_and_upload_dir")
-def test_all_framework_models_add_jumpstart_tags(
-    repack_model, tar_and_uload_dir, sagemaker_session
-):
-    framework_model_classes_to_kwargs = {
-        PyTorchModel: {"framework_version": "1.5.0", "py_version": "py3"},
-        TensorFlowModel: {
-            "framework_version": "2.3",
-        },
-        HuggingFaceModel: {
-            "pytorch_version": "1.7.1",
-            "py_version": "py36",
-            "transformers_version": "4.6.1",
-        },
-        MXNetModel: {"framework_version": "1.7.0", "py_version": "py3"},
-        SKLearnModel: {
-            "framework_version": "0.23-1",
-        },
-        XGBoostModel: {
-            "framework_version": "1.3-1",
-        },
-    }
-    jumpstart_model_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/model_dirs/model.tar.gz"
-    for framework_model_class, kwargs in framework_model_classes_to_kwargs.items():
-        framework_model_class(
-            entry_point=ENTRY_POINT_INFERENCE,
-            role=ROLE,
-            sagemaker_session=sagemaker_session,
-            model_data=jumpstart_model_dir,
-            **kwargs,
-        ).deploy(instance_type="ml.m2.xlarge", initial_instance_count=INSTANCE_COUNT)
-
-        assert {
-            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
-            "Value": jumpstart_model_dir,
-        } in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-        assert {
-            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
-            "Value": jumpstart_model_dir,
-        } in sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"]
-
-        sagemaker_session.create_model.reset_mock()
-        sagemaker_session.endpoint_from_production_variants.reset_mock()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-18 15:31:51[0m
[92mHash: 867f30091fd4639a6f371d6d7619525e696f05a3[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Adds Lineage queries in artifact, context and trial components (#2838)

[0m
@@ -11,11 +11,9 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
-import unittest.mock
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
 from sagemaker.lineage.action import Action
-from sagemaker.lineage.lineage_trial_component import LineageTrialComponent
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -288,49 +286,6 @@ def test_vertex_to_object_context(sagemaker_session):
     assert isinstance(context, Context)
 
 
-def test_vertex_to_object_trial_component(sagemaker_session):
-
-    tc_arn = "arn:aws:sagemaker:us-west-2:963951943925:trial-component/abaloneprocess-ixyt08z3ru-aws-processing-job"
-    vertex = Vertex(
-        arn=tc_arn,
-        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
-        lineage_source=LineageSourceEnum.TRANSFORM_JOB.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_trial_component.return_value = {
-        "TrialComponentName": "MyTrialComponent",
-        "TrialComponentArn": tc_arn,
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/my_trial_component",
-            "SourceType": "ARN",
-            "SourceId": "Thu Dec 17 17:16:24 UTC 2020",
-        },
-        "TrialComponentType": "ModelDeployment",
-        "Properties": {
-            "PipelineExecutionArn": "arn:aws:sagemaker:us-west-2:0123456789012:\
-                pipeline/mypipeline/execution/0irnteql64d0",
-            "PipelineStepName": "MyStep",
-            "Status": "Completed",
-        },
-        "CreationTime": 1608225384.0,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608225384.0,
-        "LastModifiedBy": {},
-    }
-
-    trial_component = vertex.to_lineage_object()
-
-    expected_calls = [
-        unittest.mock.call(TrialComponentName="abaloneprocess-ixyt08z3ru-aws-processing-job"),
-    ]
-    assert expected_calls == sagemaker_session.sagemaker_client.describe_trial_component.mock_calls
-
-    assert trial_component.trial_component_arn == tc_arn
-    assert trial_component.trial_component_name == "MyTrialComponent"
-    assert isinstance(trial_component, LineageTrialComponent)
-
-
 def test_vertex_to_object_model_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -362,37 +317,6 @@ def test_vertex_to_object_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
-def test_vertex_to_object_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.MODEL.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/mymodel",
-            "SourceTypes": [],
-        },
-        "ArtifactType": None,
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, Artifact)
-
-
 def test_vertex_to_dataset_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -455,7 +379,7 @@ def test_vertex_to_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
-def test_vertex_to_object_image_artifact(sagemaker_session):
+def test_vertex_to_object_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
         lineage_entity=LineageEntityEnum.ARTIFACT.value,
@@ -517,7 +441,7 @@ def test_vertex_to_object_action(sagemaker_session):
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.TRIAL.value,
+        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-10 12:28:03[0m
[92mHash: be1eea617e3e085451c673f75b0ef4316d9ca049[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: update master from dev (#2836)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>[0m
@@ -13,7 +13,6 @@
 from __future__ import absolute_import
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
-from sagemaker.lineage.action import Action
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -45,143 +44,6 @@ def test_lineage_query(sagemaker_session):
     assert response.vertices[1].lineage_entity == "Context"
 
 
-def test_lineage_query_cross_account_same_artifact(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-        ],
-        "Edges": [
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-        ],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-    assert len(response.edges) == 0
-    assert len(response.vertices) == 1
-    assert (
-        response.vertices[0].arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-
-
-def test_lineage_query_cross_account(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-        ],
-        "Edges": [
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "ABC",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
-                "AssociationType": "DEF",
-            },
-        ],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-
-    assert len(response.edges) == 2
-    assert (
-        response.edges[0].source_arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert (
-        response.edges[0].destination_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.edges[0].association_type == "ABC"
-
-    assert (
-        response.edges[1].source_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert (
-        response.edges[1].destination_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
-    )
-    assert response.edges[1].association_type == "DEF"
-
-    assert len(response.vertices) == 3
-    assert (
-        response.vertices[0].arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-    assert (
-        response.vertices[1].arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[1].lineage_source == "Endpoint"
-    assert response.vertices[1].lineage_entity == "Artifact"
-    assert (
-        response.vertices[2].arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
-    )
-    assert response.vertices[2].lineage_source == "Endpoint"
-    assert response.vertices[2].lineage_entity == "Artifact"
-
-
 def test_vertex_to_object_endpoint_context(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
@@ -378,38 +240,10 @@ def test_vertex_to_object_artifact(sagemaker_session):
     assert isinstance(artifact, Artifact)
 
 
-def test_vertex_to_object_action(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:action/cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
-        lineage_entity=LineageEntityEnum.ACTION.value,
-        lineage_source="A",
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_action.return_value = {
-        "ActionName": "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
-        "Source": {
-            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
-            "SourceTypes": [],
-        },
-        "ActionType": "A",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    action = vertex.to_lineage_object()
-
-    assert action.action_name == "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint"
-    assert isinstance(action, Action)
-
-
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93me66eef7f19c05e75284089183491bd4f[0m",
-        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
+        lineage_entity=LineageEntityEnum.ACTION.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2021-12-16 21:43:32[0m
[92mHash: 97b71c28ad9498beddbb9452cd293cc71f2248ac[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_steps.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

[0m
@@ -16,7 +16,6 @@ from __future__ import absolute_import
 import pytest
 import sagemaker
 import os
-import warnings
 
 from mock import (
     Mock,
@@ -64,7 +63,8 @@ from sagemaker.workflow.steps import (
 )
 from tests.unit import DATA_DIR
 
-DUMMY_SCRIPT_PATH = os.path.join(DATA_DIR, "dummy_script.py")
+SCRIPT_FILE = "dummy_script.py"
+SCRIPT_PATH = os.path.join(DATA_DIR, SCRIPT_FILE)
 
 REGION = "us-west-2"
 BUCKET = "my-bucket"
@@ -129,31 +129,6 @@ def sagemaker_session(boto_session, client):
     )
 
 
-@pytest.fixture
-def script_processor(sagemaker_session):
-    return ScriptProcessor(
-        role=ROLE,
-        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
-        command=["python3"],
-        instance_type="ml.m4.xlarge",
-        instance_count=1,
-        volume_size_in_gb=100,
-        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
-        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
-        max_runtime_in_seconds=3600,
-        base_job_name="my_sklearn_processor",
-        env={"my_env_variable": "my_env_variable_value"},
-        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
-        network_config=NetworkConfig(
-            subnets=["my_subnet_id"],
-            security_group_ids=["my_security_group_id"],
-            enable_network_isolation=True,
-            encrypt_inter_container_traffic=True,
-        ),
-        sagemaker_session=sagemaker_session,
-    )
-
-
 def test_custom_step():
     step = CustomStep(
         name="MyStep", display_name="CustomStepDisplayName", description="CustomStepDescription"
@@ -351,7 +326,7 @@ def test_training_step_tensorflow(sagemaker_session):
     training_epochs_parameter = ParameterInteger(name="TrainingEpochs", default_value=5)
     training_batch_size_parameter = ParameterInteger(name="TrainingBatchSize", default_value=500)
     estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
+        entry_point=os.path.join(DATA_DIR, SCRIPT_FILE),
         role=ROLE,
         model_dir=False,
         image_uri=IMAGE_URI,
@@ -428,75 +403,6 @@ def test_training_step_tensorflow(sagemaker_session):
     assert step.properties.TrainingJobName.expr == {"Get": "Steps.MyTrainingStep.TrainingJobName"}
 
 
-def test_training_step_profiler_warning(sagemaker_session):
-    estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
-        role=ROLE,
-        model_dir=False,
-        image_uri=IMAGE_URI,
-        source_dir="s3://mybucket/source",
-        framework_version="2.4.1",
-        py_version="py37",
-        disable_profiler=False,
-        instance_count=1,
-        instance_type="ml.p3.16xlarge",
-        sagemaker_session=sagemaker_session,
-        hyperparameters={
-            "batch-size": 500,
-            "epochs": 5,
-        },
-        debugger_hook_config=False,
-        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
-    )
-
-    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    with warnings.catch_warnings(record=True) as w:
-        TrainingStep(
-            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
-        )
-        assert len(w) == 1
-        assert issubclass(w[-1].category, UserWarning)
-        assert "Profiling is enabled on the provided estimator" in str(w[-1].message)
-
-
-def test_training_step_no_profiler_warning(sagemaker_session):
-    estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
-        role=ROLE,
-        model_dir=False,
-        image_uri=IMAGE_URI,
-        source_dir="s3://mybucket/source",
-        framework_version="2.4.1",
-        py_version="py37",
-        disable_profiler=True,
-        instance_count=1,
-        instance_type="ml.p3.16xlarge",
-        sagemaker_session=sagemaker_session,
-        hyperparameters={
-            "batch-size": 500,
-            "epochs": 5,
-        },
-        debugger_hook_config=False,
-        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
-    )
-
-    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    with warnings.catch_warnings(record=True) as w:
-        # profiler disabled, cache config not None
-        TrainingStep(
-            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
-        )
-        assert len(w) == 0
-
-    with warnings.catch_warnings(record=True) as w:
-        # profiler enabled, cache config is None
-        estimator.disable_profiler = False
-        TrainingStep(name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=None)
-        assert len(w) == 0
-
-
 def test_processing_step(sagemaker_session):
     processing_input_data_uri_parameter = ParameterString(
         name="ProcessingInputDataUri", default_value=f"s3://{BUCKET}/processing_manifest"
@@ -567,42 +473,28 @@ def test_processing_step(sagemaker_session):
 
 
 @patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_local_code(mock_normalize_args, script_processor):
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    inputs = [
-        ProcessingInput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    outputs = [
-        ProcessingOutput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    step = ProcessingStep(
-        name="MyProcessingStep",
-        processor=script_processor,
-        code=DUMMY_SCRIPT_PATH,
-        inputs=inputs,
-        outputs=outputs,
-        job_arguments=["arg1", "arg2"],
-        cache_config=cache_config,
-    )
-    mock_normalize_args.return_value = [step.inputs, step.outputs]
-    step.to_request()
-    mock_normalize_args.assert_called_with(
-        job_name="MyProcessingStep-[93m3e89f0c7e101c356cbedf27d9d27e9db[0m",
-        arguments=step.job_arguments,
-        inputs=step.inputs,
-        outputs=step.outputs,
-        code=step.code,
+def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session):
+    processor = ScriptProcessor(
+        role=ROLE,
+        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
+        command=["python3"],
+        instance_type="ml.m4.xlarge",
+        instance_count=1,
+        volume_size_in_gb=100,
+        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
+        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
+        max_runtime_in_seconds=3600,
+        base_job_name="my_sklearn_processor",
+        env={"my_env_variable": "my_env_variable_value"},
+        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
+        network_config=NetworkConfig(
+            subnets=["my_subnet_id"],
+            security_group_ids=["my_security_group_id"],
+            enable_network_isolation=True,
+            encrypt_inter_container_traffic=True,
+        ),
+        sagemaker_session=sagemaker_session,
     )
-
-
-@patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, script_processor):
     cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
     inputs = [
         ProcessingInput(
@@ -618,8 +510,8 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     ]
     step = ProcessingStep(
         name="MyProcessingStep",
-        processor=script_processor,
-        code="s3://foo",
+        processor=processor,
+        code="foo.py",
         inputs=inputs,
         outputs=outputs,
         job_arguments=["arg1", "arg2"],
@@ -628,7 +520,6 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     mock_normalize_args.return_value = [step.inputs, step.outputs]
     step.to_request()
     mock_normalize_args.assert_called_with(
-        job_name=None,
         arguments=step.job_arguments,
         inputs=step.inputs,
         outputs=step.outputs,
@@ -636,40 +527,6 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     )
 
 
-@patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_no_code(mock_normalize_args, script_processor):
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    inputs = [
-        ProcessingInput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    outputs = [
-        ProcessingOutput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    step = ProcessingStep(
-        name="MyProcessingStep",
-        processor=script_processor,
-        inputs=inputs,
-        outputs=outputs,
-        job_arguments=["arg1", "arg2"],
-        cache_config=cache_config,
-    )
-    mock_normalize_args.return_value = [step.inputs, step.outputs]
-    step.to_request()
-    mock_normalize_args.assert_called_with(
-        job_name=None,
-        arguments=step.job_arguments,
-        inputs=step.inputs,
-        outputs=step.outputs,
-        code=None,
-    )
-
-
 def test_create_model_step(sagemaker_session):
     model = Model(
         image_uri=IMAGE_URI,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2021-12-01 21:09:13[0m
[92mHash: 46957fd985350962b4c1e45c50a7147b2dd839a9[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Add support for SageMaker lineage queries

Co-authored-by: Payton Staub <pstaub@amazon.com>
[0m
@@ -1,252 +0,0 @@
-# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
-from sagemaker.lineage.context import EndpointContext, Context
-from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
-import pytest
-
-
-def test_lineage_query(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {"Arn": "arn1", "Type": "Endpoint", "LineageType": "Artifact"},
-            {"Arn": "arn2", "Type": "Model", "LineageType": "Context"},
-        ],
-        "Edges": [{"SourceArn": "arn1", "DestinationArn": "arn2", "AssociationType": "Produced"}],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-
-    assert len(response.edges) == 1
-    assert response.edges[0].source_arn == "arn1"
-    assert response.edges[0].destination_arn == "arn2"
-    assert response.edges[0].association_type == "Produced"
-    assert len(response.vertices) == 2
-    assert response.vertices[0].arn == "arn1"
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-    assert response.vertices[1].arn == "arn2"
-    assert response.vertices[1].lineage_source == "Model"
-    assert response.vertices[1].lineage_entity == "Context"
-
-
-def test_vertex_to_object_endpoint_context(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
-        lineage_entity=LineageEntityEnum.CONTEXT.value,
-        lineage_source=LineageSourceEnum.ENDPOINT.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_context.return_value = {
-        "ContextName": "MyContext",
-        "ContextArn": "arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:endpoint/myendpoint",
-            "SourceType": "ARN",
-            "SourceId": "Thu Dec 17 17:16:24 UTC 2020",
-        },
-        "ContextType": "Endpoint",
-        "Properties": {
-            "PipelineExecutionArn": "arn:aws:sagemaker:us-west-2:0123456789012:\
-                pipeline/mypipeline/execution/0irnteql64d0",
-            "PipelineStepName": "MyStep",
-            "Status": "Completed",
-        },
-        "CreationTime": 1608225384.0,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608225384.0,
-        "LastModifiedBy": {},
-    }
-
-    context = vertex.to_lineage_object()
-
-    assert context.context_arn == "arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"
-    assert context.context_name == "MyContext"
-    assert isinstance(context, EndpointContext)
-
-
-def test_vertex_to_object_context(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
-        lineage_entity=LineageEntityEnum.CONTEXT.value,
-        lineage_source=LineageSourceEnum.MODEL_DEPLOYMENT.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_context.return_value = {
-        "ContextName": "MyContext",
-        "ContextArn": "arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/mymodel",
-            "SourceType": "ARN",
-            "SourceId": "Thu Dec 17 17:16:24 UTC 2020",
-        },
-        "ContextType": "ModelDeployment",
-        "Properties": {
-            "PipelineExecutionArn": "arn:aws:sagemaker:us-west-2:0123456789012:\
-                pipeline/mypipeline/execution/0irnteql64d0",
-            "PipelineStepName": "MyStep",
-            "Status": "Completed",
-        },
-        "CreationTime": 1608225384.0,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608225384.0,
-        "LastModifiedBy": {},
-    }
-
-    context = vertex.to_lineage_object()
-
-    assert context.context_arn == "arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"
-    assert context.context_name == "MyContext"
-    assert isinstance(context, Context)
-
-
-def test_vertex_to_object_model_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.MODEL.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/mymodel",
-            "SourceTypes": [],
-        },
-        "ArtifactType": "Model",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, ModelArtifact)
-
-
-def test_vertex_to_dataset_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.DATASET.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
-            "SourceTypes": [],
-        },
-        "ArtifactType": "Image",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, DatasetArtifact)
-
-
-def test_vertex_to_model_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.MODEL.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
-            "SourceTypes": [],
-        },
-        "ArtifactType": "Image",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, ModelArtifact)
-
-
-def test_vertex_to_object_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.IMAGE.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
-            "SourceTypes": [],
-        },
-        "ArtifactType": "Image",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, Artifact)
-
-
-def test_vertex_to_object_unconvertable(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ACTION.value,
-        lineage_source=LineageSourceEnum.TENSORBOARD.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    with pytest.raises(ValueError):
-        vertex.to_lineage_object()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2021-08-04 19:21:20[0m
[92mHash: b3c8bb1c8aec183ddbe1babdd3ab7a6b8f559178[0m
[92mFilepath: src/sagemaker/processing.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: processors that support multiple Python files, requirements.txt, and dependencies. (#2251)

Co-authored-by: Ajay Karpur <akarpur@amazon.com>
Co-authored-by: Alex Thewsey <thewsey@amazon.com>
Co-authored-by: Panigrahi <panpinak@147dda0b3406.ant.amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -20,10 +20,6 @@ from __future__ import print_function, absolute_import
 
 import os
 import pathlib
-import logging
-from textwrap import dedent
-from typing import Dict, List, Optional
-
 import attr
 
 from six.moves.urllib.parse import urlparse
@@ -32,17 +28,13 @@ from six.moves.urllib.request import url2pathname
 from sagemaker import s3
 from sagemaker.job import _Job
 from sagemaker.local import LocalSession
-from sagemaker.utils import base_name_from_image, get_config_value, name_from_base
+from sagemaker.utils import base_name_from_image, name_from_base
 from sagemaker.session import Session
 from sagemaker.workflow.properties import Properties
 from sagemaker.workflow.parameters import Parameter
 from sagemaker.workflow.entities import Expression
 from sagemaker.dataset_definition.inputs import S3Input, DatasetDefinition
 from sagemaker.apiutils._base_types import ApiObject
-from sagemaker.s3 import S3Uploader
-
-
-logger = logging.getLogger(__name__)
 
 
 class Processor(object):
@@ -128,8 +120,7 @@ class Processor(object):
 
         if self.instance_type in ("local", "local_gpu"):
             if not isinstance(sagemaker_session, LocalSession):
-                # Until Local Mode Processing supports local code, we need to disable it:
-                sagemaker_session = LocalSession(disable_local_code=True)
+                sagemaker_session = LocalSession()
 
         self.sagemaker_session = sagemaker_session or Session()
 
@@ -1230,488 +1221,3 @@ class FeatureStoreOutput(ApiObject):
     """Configuration for processing job outputs in Amazon SageMaker Feature Store."""
 
     feature_group_name = None
-
-
-class FrameworkProcessor(ScriptProcessor):
-    """Handles Amazon SageMaker processing tasks for jobs using a machine learning framework."""
-
-    framework_entrypoint_command = ["/bin/bash"]
-
-    # Added new (kw)args for estimator. The rest are from ScriptProcessor with same defaults.
-    def __init__(
-        self,
-        estimator_cls,
-        framework_version,
-        role,
-        instance_count,
-        instance_type,
-        py_version="py3",
-        image_uri=None,
-        command=None,
-        volume_size_in_gb=30,
-        volume_kms_key=None,
-        output_kms_key=None,
-        code_location=None,
-        max_runtime_in_seconds=None,
-        base_job_name=None,
-        sagemaker_session=None,
-        env=None,
-        tags=None,
-        network_config=None,
-    ):
-        """Initializes a ``FrameworkProcessor`` instance.
-
-        The ``FrameworkProcessor`` handles Amazon SageMaker Processing tasks for jobs
-        using a machine learning framework, which allows for a set of Python scripts
-        to be run as part of the Processing Job.
-
-        Args:
-            estimator_cls (type): A subclass of the :class:`~sagemaker.estimator.Framework`
-                estimator
-            framework_version (str): The version of the framework. Value is ignored when
-                ``image_uri`` is provided.
-            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing uses
-                this role to access AWS resources, such as data stored in Amazon S3.
-            instance_count (int): The number of instances to run a processing job with.
-            instance_type (str): The type of EC2 instance to use for processing, for
-                example, 'ml.c4.xlarge'.
-            py_version (str): Python version you want to use for executing your
-                model training code. One of 'py2' or 'py3'. Defaults to 'py3'. Value
-                is ignored when ``image_uri`` is provided.
-            image_uri (str): The URI of the Docker image to use for the
-                processing jobs (default: None).
-            command ([str]): The command to run, along with any command-line flags
-                to *precede* the ```code script```. Example: ["python3", "-v"]. If not
-                provided, ["python"] will be chosen (default: None).
-            volume_size_in_gb (int): Size in GB of the EBS volume
-                to use for storing data during processing (default: 30).
-            volume_kms_key (str): A KMS key for the processing volume (default: None).
-            output_kms_key (str): The KMS key ID for processing job outputs (default: None).
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None). The code file uploaded to S3 is
-                'code_location/job-name/source/sourcedir.tar.gz'. If not specified, the
-                default ``code location`` is 's3://{sagemaker-default-bucket}'
-            max_runtime_in_seconds (int): Timeout in seconds (default: None).
-                After this amount of time, Amazon SageMaker terminates the job,
-                regardless of its current status. If `max_runtime_in_seconds` is not
-                specified, the default value is 24 hours.
-            base_job_name (str): Prefix for processing name. If not specified,
-                the processor generates a default job name, based on the
-                processing image name and current timestamp (default: None).
-            sagemaker_session (:class:`~sagemaker.session.Session`):
-                Session object which manages interactions with Amazon SageMaker and
-                any other AWS services needed. If not specified, the processor creates
-                one using the default AWS configuration chain (default: None).
-            env (dict[str, str]): Environment variables to be passed to
-                the processing jobs (default: None).
-            tags (list[dict]): List of tags to be passed to the processing job
-                (default: None). For more, see
-                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
-            network_config (:class:`~sagemaker.network.NetworkConfig`):
-                A :class:`~sagemaker.network.NetworkConfig`
-                object that configures network isolation, encryption of
-                inter-container traffic, security group IDs, and subnets (default: None).
-        """
-        if not command:
-            command = ["python"]
-
-        self.estimator_cls = estimator_cls
-        self.framework_version = framework_version
-        self.py_version = py_version
-
-        # 1. To finalize/normalize the image_uri or base_job_name, we need to create an
-        #    estimator_cls instance.
-        # 2. We want to make it easy for children of FrameworkProcessor to override estimator
-        #    creation via a function (to create FrameworkProcessors for Estimators that may have
-        #    different signatures - like HuggingFace or others in future).
-        # 3. Super-class __init__ doesn't (currently) do anything with these params besides
-        #    storing them
-        #
-        # Therefore we'll init the superclass first and then customize the setup after:
-        super().__init__(
-            role=role,
-            image_uri=image_uri,
-            command=command,
-            instance_count=instance_count,
-            instance_type=instance_type,
-            volume_size_in_gb=volume_size_in_gb,
-            volume_kms_key=volume_kms_key,
-            output_kms_key=output_kms_key,
-            max_runtime_in_seconds=max_runtime_in_seconds,
-            base_job_name=base_job_name,
-            sagemaker_session=sagemaker_session,
-            env=env,
-            tags=tags,
-            network_config=network_config,
-        )
-
-        # This subclass uses the "code" input for actual payload and the ScriptProcessor parent's
-        # functionality for uploading just a small entrypoint script to invoke it.
-        self._CODE_CONTAINER_INPUT_NAME = "entrypoint"
-
-        self.code_location = (
-            code_location[:-1] if (code_location and code_location.endswith("/")) else code_location
-        )
-
-        if image_uri is None or base_job_name is None:
-            # For these default configuration purposes, we don't need the optional args:
-            est = self._create_estimator()
-            if image_uri is None:
-                self.image_uri = est.training_image_uri()
-            if base_job_name is None:
-                self.base_job_name = est.base_job_name or estimator_cls._framework_name
-                if base_job_name is None:
-                    base_job_name = "framework-processor"
-
-    def _create_estimator(
-        self,
-        entry_point="",
-        source_dir=None,
-        dependencies=None,
-        git_config=None,
-    ):
-        """Instantiate the Framework Estimator that backs this Processor"""
-        return self.estimator_cls(
-            framework_version=self.framework_version,
-            py_version=self.py_version,
-            entry_point=entry_point,
-            source_dir=source_dir,
-            dependencies=dependencies,
-            git_config=git_config,
-            code_location=self.code_location,
-            enable_network_isolation=False,  # True -> uploads to input channel. Not what we want!
-            image_uri=self.image_uri,
-            role=self.role,
-            # Estimator instance_count doesn't currently matter to FrameworkProcessor, and the
-            # SKLearn Framework Estimator requires instance_type==1. So here we hard-wire it to 1,
-            # but if it matters in future perhaps we could take self.instance_count here and have
-            # SKLearnProcessor override this function instead:
-            instance_count=1,
-            instance_type=self.instance_type,
-            sagemaker_session=self.sagemaker_session,
-            debugger_hook_config=False,
-            disable_profiler=True,
-        )
-
-    def get_run_args(
-        self,
-        code,
-        source_dir=None,
-        dependencies=None,
-        git_config=None,
-        inputs=None,
-        outputs=None,
-        arguments=None,
-        job_name=None,
-    ):
-        """Returns a RunArgs object.
-
-        This object contains the normalized inputs, outputs and arguments needed
-        when using a ``FrameworkProcessor`` in a :class:`~sagemaker.workflow.steps.ProcessingStep`.
-
-        Args:
-            code (str): This can be an S3 URI or a local path to a file with the framework
-                script to run. See the ``code`` argument in
-                `sagemaker.processing.FrameworkProcessor.run()`.
-            source_dir (str): Path (absolute, relative, or an S3 URI) to a directory wit
-                any other processing source code dependencies aside from the entrypoint
-                file (default: None). See the ``source_dir`` argument in
-                `sagemaker.processing.FrameworkProcessor.run()`
-            dependencies (list[str]): A list of paths to directories (absolute or relative)
-                with any additional libraries that will be exported to the container
-                (default: []). See the ``dependencies`` argument in
-                `sagemaker.processing.FrameworkProcessor.run()`.
-            git_config (dict[str, str]): Git configurations used for cloning files. See the
-                `git_config` argument in `sagemaker.processing.FrameworkProcessor.run()`.
-            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
-                the processing job. These must be provided as
-                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
-            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
-                the processing job. These can be specified as either path strings or
-                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
-            arguments (list[str]): A list of string arguments to be passed to a
-                processing job (default: None).
-            job_name (str): Processing job name. If not specified, the processor generates
-                a default job name, based on the base job name and current timestamp.
-        """
-        # When job_name is None, the job_name to upload code (+payload) will
-        # differ from job_name used by run().
-        s3_runproc_sh, inputs, job_name = self._pack_and_upload_code(
-            code, source_dir, dependencies, git_config, job_name, inputs
-        )
-
-        return RunArgs(
-            s3_runproc_sh,
-            inputs=inputs,
-            outputs=outputs,
-            arguments=arguments,
-        )
-
-    def run(  # type: ignore[override]
-        self,
-        code,
-        source_dir=None,
-        dependencies=None,
-        git_config=None,
-        inputs=None,
-        outputs=None,
-        arguments=None,
-        wait=True,
-        logs=True,
-        job_name=None,
-        experiment_config=None,
-        kms_key=None,
-    ):
-        """Runs a processing job.
-
-        Args:
-            code (str): This can be an S3 URI or a local path to a file with the
-                framework script to run.Path (absolute or relative) to the local
-                Python source file which should be executed as the entry point
-                to training. When `code` is an S3 URI, ignore `source_dir`,
-                `dependencies, and `git_config`. If ``source_dir`` is specified,
-                then ``code`` must point to a file located at the root of ``source_dir``.
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other processing source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when processing on Amazon SageMaker (default: None).
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo (default: None).
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
-                the processing job. These must be provided as
-                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
-            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
-                the processing job. These can be specified as either path strings or
-                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
-            arguments (list[str]): A list of string arguments to be passed to a
-                processing job (default: None).
-            wait (bool): Whether the call should wait until the job completes (default: True).
-            logs (bool): Whether to show the logs produced by the job.
-                Only meaningful when wait is True (default: True).
-            job_name (str): Processing job name. If not specified, the processor generates
-                a default job name, based on the base job name and current timestamp.
-            experiment_config (dict[str, str]): Experiment management configuration.
-                Dictionary contains three optional keys:
-                'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.
-            kms_key (str): The ARN of the KMS key that is used to encrypt the
-                user code file (default: None).
-        """
-        s3_runproc_sh, inputs, job_name = self._pack_and_upload_code(
-            code, source_dir, dependencies, git_config, job_name, inputs
-        )
-
-        # Submit a processing job.
-        super().run(
-            code=s3_runproc_sh,
-            inputs=inputs,
-            outputs=outputs,
-            arguments=arguments,
-            wait=wait,
-            logs=logs,
-            job_name=job_name,
-            experiment_config=experiment_config,
-            kms_key=kms_key,
-        )
-
-    def _pack_and_upload_code(self, code, source_dir, dependencies, git_config, job_name, inputs):
-        """Pack local code bundle and upload to Amazon S3."""
-        if code.startswith("s3://"):
-            return code, inputs, job_name
-
-        if job_name is None:
-            job_name = self._generate_current_job_name(job_name)
-
-        estimator = self._upload_payload(
-            code,
-            source_dir,
-            dependencies,
-            git_config,
-            job_name,
-        )
-        inputs = self._patch_inputs_with_payload(
-            inputs,
-            estimator._hyperparameters["sagemaker_submit_directory"],
-        )
-
-        local_code = get_config_value("local.local_code", self.sagemaker_session.config)
-        if self.sagemaker_session.local_mode and local_code:
-            raise RuntimeError(
-                "SageMaker Processing Local Mode does not currently support 'local code' mode. "
-                "Please use a LocalSession created with disable_local_code=True, or leave "
-                "sagemaker_session unspecified when creating your Processor to have one set up "
-                "automatically."
-            )
-
-        # Upload the bootstrapping code as s3://.../jobname/source/runproc.sh.
-        entrypoint_s3_uri = estimator.uploaded_code.s3_prefix.replace(
-            "sourcedir.tar.gz",
-            "runproc.sh",
-        )
-        script = estimator.uploaded_code.script_name
-        s3_runproc_sh = S3Uploader.upload_string_as_file_body(
-            self._generate_framework_script(script),
-            desired_s3_uri=entrypoint_s3_uri,
-            sagemaker_session=self.sagemaker_session,
-        )
-        logger.info("runproc.sh uploaded to %s", s3_runproc_sh)
-
-        return s3_runproc_sh, inputs, job_name
-
-    def _generate_framework_script(self, user_script: str) -> str:
-        """Generate the framework entrypoint file (as text) for a processing job.
-
-        This script implements the "framework" functionality for setting up your code:
-        Untar-ing the sourcedir bundle in the ```code``` input; installing extra
-        runtime dependencies if specified; and then invoking the ```command``` and
-        ```code``` configured for the job.
-
-        Args:
-            user_script (str): Relative path to ```code``` in the source bundle
-                - e.g. 'process.py'.
-        """
-        return dedent(
-            """\
-            #!/bin/bash
-
-            cd /opt/ml/processing/input/code/
-            tar -xzf sourcedir.tar.gz
-
-            # Exit on any error. SageMaker uses error code to mark failed job.
-            set -e
-
-            if [[ -f 'requirements.txt' ]]; then
-                # Some py3 containers has typing, which may breaks pip install
-                pip uninstall --yes typing
-
-                pip install -r requirements.txt
-            fi
-
-            {entry_point_command} {entry_point} "$@"
-        """
-        ).format(
-            entry_point_command=" ".join(self.command),
-            entry_point=user_script,
-        )
-
-    def _upload_payload(
-        self,
-        entry_point: str,
-        source_dir: Optional[str],
-        dependencies: Optional[List[str]],
-        git_config: Optional[Dict[str, str]],
-        job_name: str,
-    ) -> "sagemaker.estimator.Framework":  # type: ignore[name-defined]   # noqa: F821
-        """Upload payload sourcedir.tar.gz to S3."""
-        # A new estimator instance is required, because each call to ScriptProcessor.run() can
-        # use different codes.
-        estimator = self._create_estimator(
-            entry_point=entry_point,
-            source_dir=source_dir,
-            dependencies=dependencies,
-            git_config=git_config,
-        )
-
-        estimator._prepare_for_training(job_name=job_name)
-        logger.info(
-            "Uploaded %s to %s",
-            estimator.source_dir,
-            estimator._hyperparameters["sagemaker_submit_directory"],
-        )
-
-        return estimator
-
-    def _patch_inputs_with_payload(self, inputs, s3_payload) -> List[ProcessingInput]:
-        """Add payload sourcedir.tar.gz to processing input.
-
-        This method follows the same mechanism in ScriptProcessor.
-        """
-        # Follow the exact same mechanism that ScriptProcessor does, which
-        # is to inject the S3 code artifact as a processing input. Note that
-        # framework processor take-over /opt/ml/processing/input/code for
-        # sourcedir.tar.gz, and let ScriptProcessor to place runproc.sh under
-        # /opt/ml/processing/input/{self._CODE_CONTAINER_INPUT_NAME}.
-        #
-        # See:
-        # - ScriptProcessor._CODE_CONTAINER_BASE_PATH, ScriptProcessor._CODE_CONTAINER_INPUT_NAME.
-        # - https://github.com/aws/sagemaker-python-sdk/blob/ \
-        #   [93ma7399455f5386d83ddc5cb15c0db00c04bd518ec[0m/src/sagemaker/processing.py#L425-L426
-        if inputs is None:
-            inputs = []
-        inputs.append(
-            ProcessingInput(
-                input_name="code",
-                source=s3_payload,
-                destination="/opt/ml/processing/input/code/",
-            )
-        )
-        return inputs
-
-    def _set_entrypoint(self, command, user_script_name):
-        """Framework processor override for setting processing job entrypoint.
-
-        Args:
-            command ([str]): Ignored in favor of self.framework_entrypoint_command
-            user_script_name (str): A filename with an extension.
-        """
-
-        user_script_location = str(
-            pathlib.PurePosixPath(
-                self._CODE_CONTAINER_BASE_PATH, self._CODE_CONTAINER_INPUT_NAME, user_script_name
-            )
-        )
-        self.entrypoint = self.framework_entrypoint_command + [user_script_location]

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2021-02-22 15:34:57[0m
[92mHash: 5dc9e58ff0804afa6b7c3a879f6715df18f91dee[0m
[92mFilepath: doc/frameworks/mxnet/using_mxnet.rst[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: update doc for Elastic Inference MXNet 1.7.0 (#2168)

Co-authored-by: Yuchen Nie <yuchnie@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>[0m
@@ -377,7 +377,7 @@ It loads the model parameters from a ``model.params`` file in the SageMaker mode
         return net
 
 MXNet on Amazon SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.
-In order to load and serve your MXNet model through Amazon Elastic Inference, import the ``eimx`` Python package and make one change in the code to partition your model and optimize it for the ``EIA`` back end, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
+In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
 
 Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
 
@@ -392,12 +392,11 @@ Based on the example above, the following code-snippet shows an example custom `
         Returns:
             mxnet.gluon.nn.Block: a Gluon network (for this example)
         """
-        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
-        net.hybridize(backend='EIA', static_alloc=True, static_shape=True)
+        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
         return net
 
-If you are using MXNet 1.5.1 and earlier, the `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ loads and serve your model through Elastic Inference, if applicable, within the Amazon SageMaker MXNet containers.
+The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ loads and serve your model through Elastic Inference, if applicable, within the Amazon SageMaker MXNet containers.
 
 For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-11-19 12:47:29[0m
[92mHash: 3ceaddb7f2dba0fd184686b35cfe597bace515d3[0m
[92mFilepath: src/sagemaker/apiutils/_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add SageMaker lineage, workflow and pipelines support (#448) (#461) (#479) (#485) (#504) (#508) (#513)
[0m
@@ -1,34 +0,0 @@
-# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-"""Provides utilities for instantiating dependencies to boto-python objects."""
-from __future__ import absolute_import
-
-import random
-import time
-from sagemaker.session import Session
-
-
-def suffix():
-    """Generate a random string of length 4."""
-    alph = "[93mabcdefghijklmnopqrstuvwxyz[0m"
-    return "-".join([time.strftime("%Y-%m-%d-%H%M%S"), "".join(random.sample(alph, 4))])
-
-
-def name(prefix):
-    """Generate a new name with the specified prefix."""
-    return "-".join([prefix, suffix()])
-
-
-def default_session():
-    """Create a default session."""
-    return Session()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-09-17 10:34:12[0m
[92mHash: 14380d06a1c6c753b0bd99ccb220b383ac4a69b4[0m
[92mFilepath: doc/workflows/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: removed Kubernetes workflow content (#1900)

* Update README.rst

* Update index.rst

* Delete amazon_sagemaker_components_for_kubeflow_pipelines.rst

* Delete amazon_sagemaker_jobs.rst

* Delete amazon_sagemaker_operators_for_kubernetes.rst

* Delete amazon_sagemaker_operators_for_kubernetes_authentication.png

* Delete index.rst

* Delete using_amazon_sagemaker_components.rst[0m
@@ -0,0 +1,1267 @@
+Using Amazon Sagemaker Jobs
+---------------------------
+
+To run a job using the Amazon Sagemaker Operators for Kubernetes, you can either apply
+a YAML file or use the supplied Helm charts.
+
+All operator sample jobs in the following tutorials use sample data
+taken from a public MNIST dataset. In order to run these samples, download the dataset into your S3 bucket. You can find
+the dataset in `Download the MNIST
+Dataset. <https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data-pull-data.html>`__
+
+.. contents::
+
+TrainingJob operator
+~~~~~~~~~~~~~~~~~~~~
+
+Training job operators reconcile your specified training job spec to
+Amazon SageMaker by launching it for you in Amazon SageMaker. You can
+learn more about Amazon SageMaker training jobs in the Amazon
+SageMaker `CreateTrainingJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html>`__.
+
+Create a TrainingJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for training using the following command:
+
+::
+
+    wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-trainingjob.yaml
+
+Edit the ``xgboost-mnist-trainingjob.yaml`` file to replace the ``roleArn`` parameter with your ``<sagemaker-execution-role>``, and ``outputPath`` with your S3 bucket that the Amazon SageMaker
+execution role has write access to. The ``roleArn`` must have permissions so that Amazon SageMaker
+can access Amazon S3, Amazon CloudWatch, and other services on your
+behalf. For more information on creating an Amazon SageMaker
+ExecutionRole, see `Amazon SageMaker
+Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
+Apply the YAML file using the
+following command:
+
+::
+
+    kubectl apply -f xgboost-mnist-trainingjob.yaml
+
+Create a TrainingJob Using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run TrainingJobs.
+
+Clone the github repo to get the source using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+
+Navigate to the ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
+and edit the ``values.yaml`` file to replace values
+like ``rolearn`` and ``outputpath`` with values that correspond to
+your account. The RoleARN must have permissions so that Amazon SageMaker
+can access Amazon S3, Amazon CloudWatch, and other services on your
+behalf. For more information on creating an Amazon SageMaker
+ExecutionRole, see `Amazon SageMaker
+Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
+
+Create the Training Job
+''''''''''''''''''''''''
+
+With the roles and S3 buckets replaced with appropriate values
+in ``values.yaml``, you can create a training job using the following
+command:
+
+::
+
+    helm install . --generate-name
+
+Your output should look like the following:
+
+::
+
+    NAME: chart-12345678
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-trainingjob.
+
+Verify Your Training Helm Chart
+'''''''''''''''''''''''''''''''
+
+To verify that the Helm Chart was created successfully, run:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    chart-12345678        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-12345678    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+``helm install`` creates a ``TrainingJob`` k8s resource. The operator
+launches the actual training job in Amazon SageMaker and updates
+the ``TrainingJob`` k8s resource to reflect the status of the job in
+Amazon SageMaker. You incur charges for Amazon SageMaker resources used
+during the duration of your job. You do not incur any charges once your
+job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running
+training job. You cannot edit any parameter and re-apply the
+file/config. Either change the metadata name or delete the existing job
+and create a new one. Similar to existing training job operators like
+TFJob in Kubeflow, ``update`` is not supported.
+
+List Training Jobs
+^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+    kubectl get TrainingJob
+
+The output listing all jobs should look like the following:
+
+::
+
+    kubectl get trainingjobs
+    NAME                        STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME
+    xgboost-mnist-from-for-s3   InProgress   Starting           2019-11-20T23:42:35Z   xgboost-mnist-from-for-s3-exampl[93mef11eab94e0ed4671d5a8f[0m
+
+A training job continues to be listed after the job has completed or
+failed. You can remove a ``TrainingJob``  job from the list by
+following the Delete a Training Job steps. Jobs that have completed or
+stopped do not incur any charges for Amazon SageMaker resources.
+
+Training Job Status Values
+''''''''''''''''''''''''''
+
+The ``STATUS`` field can be one of the following values:
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-TrainingJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job.
+
+Secondary Status Values
+'''''''''''''''''''''''
+
+The secondary statuses come directly from the Amazon SageMaker
+official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-SecondaryStatus>`__.
+They contain more granular information about the status of the job.
+
+Describe a Training Job
+^^^^^^^^^^^^^^^^^^^^^^^
+
+You can get more details about the training job by using
+the ``describe`` kubectl verb. This is typically used for debugging a
+problem or checking the parameters of a training job. To get information
+about your training job, use the following command:
+
+::
+
+    kubectl describe trainingjob xgboost-mnist-from-for-s3
+
+The output for your training job should look like the following:
+
+::
+
+    Name:         xgboost-mnist-from-for-s3
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  <none>
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         TrainingJob
+    Metadata:
+      Creation Timestamp:  2019-11-20T23:42:35Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  23119
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-from-for-s3
+      UID:               6d7uiui-0bef-11ea-b94e-0ed467example
+    Spec:
+      Algorithm Specification:
+        Training Image:       8256416981234.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
+        Training Input Mode:  File
+      Hyper Parameters:
+        Name:   eta
+        Value:  0.2
+        Name:   gamma
+        Value:  4
+        Name:   max_depth
+        Value:  5
+        Name:   min_child_weight
+        Value:  6
+        Name:   num_class
+        Value:  10
+        Name:   num_round
+        Value:  10
+        Name:   objective
+        Value:  multi:softmax
+        Name:   silent
+        Value:  0
+      Input Data Config:
+        Channel Name:      train
+        Compression Type:  None
+        Content Type:      text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Distribution Type:  FullyReplicated
+            S 3 Data Type:               S3Prefix
+            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
+        Channel Name:                    validation
+        Compression Type:                None
+        Content Type:                    text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Distribution Type:  FullyReplicated
+            S 3 Data Type:               S3Prefix
+            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
+      Output Data Config:
+        S 3 Output Path:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost/
+      Region:             us-east-2
+      Resource Config:
+        Instance Count:     1
+        Instance Type:      ml.m4.xlarge
+        Volume Size In GB:  5
+      Role Arn:             arn:aws:iam::12345678910:role/service-role/AmazonSageMaker-ExecutionRole
+      Stopping Condition:
+        Max Runtime In Seconds:  86400
+      Training Job Name:         xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0mxample
+    Status:
+      Cloud Watch Log URL:           https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=<example>;streamFilter=typeLogStreamPrefix
+      Last Check Time:               2019-11-20T23:44:29Z
+      Sage Maker Training Job Name:  xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94ee[0mxample
+      Secondary Status:              Downloading
+      Training Job Status:           InProgress
+    Events:                          <none>
+
+View Logs from Training Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to see the logs from the ``kmeans-mnist``
+training job:
+
+::
+
+    kubectl smlogs trainingjob xgboost-mnist-from-for-s3
+
+Your output will look similar to the following. The logs from instances
+are ordered chronologically.
+
+::
+
+    "xgboost-mnist-from-for-s3" has SageMaker TrainingJobName "xgboost-mnist-from-for-s3-123456789" in region "us-east-2", status "InProgress" and secondary status "Starting"
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC Arguments: train
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Running standalone xgboost training.
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 8586.0mb
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Determined delimiter of CSV input is ','
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [23:45:22] S3DistributionType set as FullyReplicated
+
+Delete Training Jobs
+^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a training job on Amazon SageMaker:
+
+::
+
+    kubectl delete trainingjob xgboost-mnist-from-for-s3
+
+This command removes the Amazon SageMaker training job from k8s. This
+command returns the following output:
+
+::
+
+    trainingjob.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+If the job is still in progress on Amazon SageMaker, the job will stop.
+You do not incur any charges for Amazon SageMaker resources after your
+job stops or completes.
+
+**Note**: Amazon SageMaker does not delete training jobs. Stopped jobs
+continue to show on the Amazon SageMaker console. The delete command
+takes about 2 minutes to clean up the resources from Amazon SageMaker.
+
+HyperParameterTuningJobs operator
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Hyperparameter tuning job operators reconcile your
+specified hyperparameter tuning job spec to Amazon SageMaker by
+launching it in Amazon SageMaker. You can learn more about Amazon
+SageMaker hyperparameter tuning jobs in the Amazon
+SageMaker `CreateHyperParameterTuningJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateHyperParameterTuningJob.html>`__.
+
+Create a HyperParameterTuningJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the hyperparameter tuning job using
+the following command:
+
+::
+
+    wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-hpo.yaml
+
+Edit the ``xgboost-mnist-hpo.yaml`` file to replace
+the ``roleArn`` parameter with your <sagemaker-execution-role>. For
+HyperparameterTuningJob to succeed, you must also change
+the ``s3InputPath``  and ``s3OutputPath`` to values that correspond
+to your account. Apply the updates YAML file using the following
+command:
+
+::
+
+    kubectl apply -f xgboost-mnist-hpo.yaml
+
+Create a HyperParameterTuningJob using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run HyperParameterTuningJobs.
+
+Clone the github repo to get the source using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+
+Navigate to the ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
+folder.
+
+Edit the ``values.yaml`` file to replace the ``roleArn`` parameter
+with your <sagemaker-execution-role>. For HyperparameterTuningJob to
+succeed, you must also change the ``s3InputPath``
+and ``s3OutputPath`` to values that correspond to your account.
+
+Create the HPO Job
+''''''''''''''''''
+
+With the roles and Amazon S3 paths replaced with appropriate values
+in ``values.yaml``, you can create a hyperparameter tuning job using
+the following command:
+
+::
+
+    helm install . --generate-name
+
+Your output will look similar to the following:
+
+::
+
+    NAME: chart-1574292948
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-hyperparametertuningjob.
+
+Verify Chart Installation
+'''''''''''''''''''''''''
+
+To verify that the Helm Chart was created successfully, run the
+following command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE       REVISION        UPDATED
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0                               STATUS          CHART                           APP VERSION
+    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+``helm install`` creates a ``HyperParameterTuningJob`` k8s resource.
+The operator launches the actual hyperparameter optimization job in
+Amazon SageMaker and updates the ``HyperParameterTuningJob`` k8s
+resource to reflect the status of the job in Amazon SageMaker. You incur
+charges for Amazon SageMaker resources used during the duration of your
+job. You do not incur any charges once your job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running
+hyperparameter tuning job. You cannot edit any parameter and re-apply
+the file/config. You must either change the metadata name or delete the
+existing job and create a new one. Similar to existing training job
+operators like TFJob in Kubeflow, ``update`` is not supported.
+
+List Hyperparameter Tuning Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+    kubectl get hyperparametertuningjob
+
+Your output will look like the following:
+
+::
+
+    NAME         STATUS      CREATION-TIME          COMPLETED   INPROGRESS   ERRORS   STOPPED   BEST-TRAINING-JOB                               SAGEMAKER-JOB-NAME
+    xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
+
+A hyper parameter tuning job will continue to be listed after the job
+has completed or failed. You can remove a ``hyperparametertuningjob``
+from the list by following the steps in Delete a Hyper Parameter Tuning
+Job. Jobs that have completed or stopped do not incur any charges for
+Amazon SageMaker resources.
+
+Hyperparameter Tuning Job Status Values
+'''''''''''''''''''''''''''''''''''''''
+
+The ``STATUS`` field can be one of the following values:
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job.
+
+Status Counters
+'''''''''''''''
+
+The output has several counters,
+like ``COMPLETED`` and ``INPROGRESS``. These represent how many
+training jobs have completed and are in progress, respectively. For more
+information about how these are determined,
+see `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
+the Amazon SageMaker API documentation.
+
+Best Training Job
+'''''''''''''''''
+
+This column contains the name of the ``TrainingJob`` that best
+optimized the selected metric.
+
+To see a summary of the tuned hyperparameters, run:
+
+::
+
+    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
+
+To see detailed information about the ``TrainingJob``, run:
+
+::
+
+    kubectl describe trainingjobs <job name>
+
+
+Spawned Training Jobs
+'''''''''''''''''''''
+
+You can also track all 10 training jobs in k8s launched by
+``HyperparameterTuningJob`` by running the following command:
+
+::
+
+    kubectl get trainingjobs
+
+Describe a Hyperparameter Tuning Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the ``describe`` kubectl verb
+by running the following command.
+
+::
+
+    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
+
+In addition to information about the tuning job, the Amazon SageMaker
+Operator for Kubernetes also exposes the `best training
+job <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html#automatic-model-tuning-best-training-job>`__\  found
+by the hyperparameter tuning job in the ``describe`` output as
+follows:
+
+::
+
+    Name:         xgboost-mnist-hpo
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HyperparameterTuningJob","metadata":{"annotations":{},"name":"xgboost-mnist-hpo","namespace":...
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         HyperparameterTuningJob
+    Metadata:
+      Creation Timestamp:  2019-10-17T01:15:52Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  8167
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hyperparametertuningjobs/xgboost-mnist-hpo
+      UID:               a92f5e3c-f07b-11e9-bf6c-06d6f303uidu
+    Spec:
+      Hyper Parameter Tuning Job Config:
+        Hyper Parameter Tuning Job Objective:
+          Metric Name:  validation:error
+          Type:         Minimize
+        Parameter Ranges:
+          Integer Parameter Ranges:
+            Max Value:     20
+            Min Value:     10
+            Name:          num_round
+            Scaling Type:  Linear
+        Resource Limits:
+          Max Number Of Training Jobs:     10
+          Max Parallel Training Jobs:      10
+        Strategy:                          Bayesian
+        Training Job Early Stopping Type:  Off
+      Hyper Parameter Tuning Job Name:     xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
+      Region:                              us-east-2
+      Training Job Definition:
+        Algorithm Specification:
+          Training Image:       12345678910.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
+          Training Input Mode:  File
+        Input Data Config:
+          Channel Name:  train
+          Content Type:  text/csv
+          Data Source:
+            s3DataSource:
+              s3DataDistributionType:  FullyReplicated
+              s3DataType:              S3Prefix
+              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
+          Channel Name:                validation
+          Content Type:                text/csv
+          Data Source:
+            s3DataSource:
+              s3DataDistributionType:  FullyReplicated
+              s3DataType:              S3Prefix
+              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
+        Output Data Config:
+          s3OutputPath:  https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/xgboost
+        Resource Config:
+          Instance Count:     1
+          Instance Type:      ml.m4.xlarge
+          Volume Size In GB:  5
+        Role Arn:             arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+        Static Hyper Parameters:
+          Name:   base_score
+          Value:  0.5
+          Name:   booster
+          Value:  gbtree
+          Name:   csv_weights
+          Value:  0
+          Name:   dsplit
+          Value:  row
+          Name:   grow_policy
+          Value:  depthwise
+          Name:   lambda_bias
+          Value:  0.0
+          Name:   max_bin
+          Value:  256
+          Name:   max_leaves
+          Value:  0
+          Name:   normalize_type
+          Value:  tree
+          Name:   objective
+          Value:  reg:linear
+          Name:   one_drop
+          Value:  0
+          Name:   prob_buffer_row
+          Value:  1.0
+          Name:   process_type
+          Value:  default
+          Name:   rate_drop
+          Value:  0.0
+          Name:   refresh_leaf
+          Value:  1
+          Name:   sample_type
+          Value:  uniform
+          Name:   scale_pos_weight
+          Value:  1.0
+          Name:   silent
+          Value:  0
+          Name:   sketch_eps
+          Value:  0.03
+          Name:   skip_drop
+          Value:  0.0
+          Name:   tree_method
+          Value:  auto
+          Name:   tweedie_variance_power
+          Value:  1.5
+        Stopping Condition:
+          Max Runtime In Seconds:  86400
+    Status:
+      Best Training Job:
+        Creation Time:  2019-10-17T01:16:14Z
+        Final Hyper Parameter Tuning Job Objective Metric:
+          Metric Name:        validation:error
+          Value:
+        Objective Status:     Succeeded
+        Training End Time:    2019-10-17T01:20:24Z
+        Training Job Arn:     arn:aws:sagemaker:us-east-2:123456789012:training-job/xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4sample
+        Training Job Name:    xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a3059
+        Training Job Status:  Completed
+        Training Start Time:  2019-10-17T01:18:35Z
+        Tuned Hyper Parameters:
+          Name:                                    num_round
+          Value:                                   18
+      Hyper Parameter Tuning Job Status:           Completed
+      Last Check Time:                             2019-10-17T01:21:01Z
+      Sage Maker Hyper Parameter Tuning Job Name:  xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
+      Training Job Status Counters:
+        Completed:            10
+        In Progress:          0
+        Non Retryable Error:  0
+        Retryable Error:      0
+        Stopped:              0
+        Total Error:          0
+    Events:                   <none>
+
+View Logs from HyperParameterTuning Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Hyperparameter tuning jobs do not have logs, but all training jobs
+launched by them do have logs. These logs can be accessed as if they
+were a normal training job. For more information, see View Logs from
+Training Jobs.
+
+Delete HyperParameterTuning jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a hyperparameter job in
+Amazon SageMaker.
+
+::
+
+    kubectl delete hyperparametertuningjob xgboost-mnist-hpo
+
+This command removes the hyperparameter tuning job and associated
+training jobs from your Kubernetes cluster, as well as stops them in
+Amazon SageMaker. Jobs that have stopped or completed do not incur any
+charges for Amazon SageMaker resources.  Amazon SageMaker does not
+delete hyperparameter tuning jobs. Stopped jobs continue to show on the
+Amazon SageMaker Console.
+
+Your output should look like the following:
+
+::
+
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+**Note**:  The delete command takes about 2 minutes to clean up the
+resources from Amazon SageMaker.
+
+BatchTransformJobs operator
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Batch transform job operators reconcile your specified batch transform
+job spec to Amazon SageMaker by launching it in Amazon SageMaker. You
+can learn more about Amazon SageMaker batch transform job in the Amazon
+SageMaker `CreateTransformJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`__.
+
+Create a BatchTransformJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the batch transform job using the
+following command:
+
+::
+
+    wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-batchtransform.yaml
+
+Edit the file ``xgboost-mnist-batchtransform.yaml`` to change
+necessary parameters to replace the ``inputdataconfig``  with your
+input data and ``s3OutputPath`` with your S3 buckets that the Amazon
+SageMaker execution role has write access to.
+
+Apply the YAML file using the following command:
+
+::
+
+    kubectl apply -f xgboost-mnist-batchtransform.yaml
+
+Create a BatchTransformJob Using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run batch transform jobs.
+
+Get the Helm installer directory
+''''''''''''''''''''''''''''''''
+
+Clone the github repo to get the source using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Configure the Helm Chart
+''''''''''''''''''''''''
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
+folder.
+
+Edit the ``values.yaml`` file to replace the ``inputdataconfig``
+with your input data and outputPath with your S3 buckets that the Amazon
+SageMaker execution role has write access to.
+
+Create a Batch Transform Job
+''''''''''''''''''''''''''''
+
+Use the following command to create a batch transform job:
+
+::
+
+    helm install . --generate-name
+
+Your output should look like the following:
+
+::
+
+    NAME: chart-1574292948
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-batch-transform-job.
+
+To verify that the Helm Chart was created successfully, run the
+following command:
+
+::
+
+    helm ls
+    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-batchtransformjob-0.1.0
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0
+    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+The previous command creates a ``BatchTransformJob`` k8s resource. The
+operator launches the actual transform job in Amazon SageMaker and
+updates the ``BatchTransformJob`` k8s resource to reflect the status
+of the job in Amazon SageMaker. You incur charges for Amazon SageMaker
+resources used during the duration of your job. You do not incur any
+charges once your job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running batch
+transform job. You cannot edit any parameter and re-apply the
+file/config. You must either change the metadata name or delete the
+existing job and create a new one. Similar to existing training job
+operators like TFJob in Kubeflow, ``update`` is not supported.
+
+List Batch Transform Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+     kubectl get batchtransformjob
+
+Your output should look like the following:
+
+::
+
+    NAME                                STATUS      CREATION-TIME          SAGEMAKER-JOB-NAME
+    xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0mxgboost
+
+A batch transform job will continue to be listed after the job has
+completed or failed. You can remove a ``hyperparametertuningjob``
+from the list by following the Delete a Batch Transform Job steps. Jobs
+that have completed or stopped do not incur any charges for
+Amazon SageMaker resources.
+
+Batch Transform Status Values
+'''''''''''''''''''''''''''''
+
+The ``STATUS`` field can be one of the following values:
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job and will get to it
+soon.
+
+Describe a Batch Transform Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the ``describe`` kubectl verb
+by running the following command.
+
+::
+
+    kubectl describe batchtransformjob xgboost-mnist-batch-transform
+
+Your output should look like the following:
+
+::
+
+    Name:         xgboost-mnist-batch-transform
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"BatchTransformJob","metadata":{"annotations":{},"name":"xgboost-mnist","namespace"...
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         BatchTransformJob
+    Metadata:
+      Creation Timestamp:  2019-11-18T03:44:00Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  21990924
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/batchtransformjobs/xgboost-mnist
+      UID:               a88fb198-09b5-11ea-ac44-0aa8a9UIDNUM
+    Spec:
+      Model Name:  TrainingJob-20190814SMJOb-IKEB
+      Region:      us-east-1
+      Transform Input:
+        Content Type:  text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Type:  S3Prefix
+            S 3 Uri:        s3://my-bucket/mnist_kmeans_example/input
+      Transform Job Name:   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0m9SMJOB
+      Transform Output:
+        S 3 Output Path:  s3://my-bucket/mnist_kmeans_example/output
+      Transform Resources:
+        Instance Count:  1
+        Instance Type:   ml.m4.xlarge
+    Status:
+      Last Check Time:                2019-11-19T22:50:40Z
+      Sage Maker Transform Job Name:  xgboost-mnist-[93ma88fb19809b511eaac440aa[0mSMJOB
+      Transform Job Status:           Completed
+    Events:                           <none>
+
+View Logs from Batch Transform Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to see the logs from the ``xgboost-mnist``
+batch transform job:
+
+::
+
+    kubectl smlogs batchtransformjob xgboost-mnist-batch-transform
+
+Delete a Batch Transform Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a batch transform job in
+Amazon SageMaker.
+
+::
+
+    kubectl delete batchTransformJob xgboost-mnist-batch-transform
+
+Your output will look like the following:
+
+::
+
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+This command removes the batch transform job from your Kubernetes
+cluster, as well as stops them in Amazon SageMaker. Jobs that have
+stopped or completed do not incur any charges for Amazon SageMaker
+resources. Delete takes about 2 minutes to clean up the resources from
+Amazon SageMaker.
+
+**Note**: Amazon SageMaker does not delete batch transform jobs. Stopped
+jobs continue to show on the Amazon SageMaker console.
+
+Real-time inference
+~~~~~~~~~~~~~~~~~~~
+
+HostingDeployments support creating and deleting an endpoint, as well as
+updating an existing endpoint. The hosting deployment operator
+reconciles your specified hosting deployment job spec to Amazon
+SageMaker by creating models, endpoint-configs and endpoints in Amazon
+SageMaker. You can learn more about Amazon SageMaker inference in the
+Amazon SageMaker `CreateEndpoint API
+documentaiton <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__.
+
+Configure a HostingDeployment Resource
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the hosting deployment job using the
+following command:
+
+::
+
+    wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-hostingdeployment.yaml
+
+The ``xgboost-mnist-hostingdeployment.yaml`` file has the following components that can be edited as required:
+
+-  ProductionVariants. A production variant is a set of instances
+   serving a single model. Amazon SageMaker will load-balance between
+   all production variants according to set weights.
+
+-  Models. A model is the containers and execution role ARN necessary to
+   serve a model. It requires at least a single container.
+
+-  Containers. A container specifies the dataset and serving image. If
+   you are using your own custom algorithm instead of an algorithm
+   provided by Amazon SageMaker, the inference code must meet Amazon
+   SageMaker requirements. For more information, see `Using Your Own
+   Algorithms with Amazon
+   SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.
+
+Create a HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To create a HostingDeployment, use ``kubectl`` to apply the
+file ``hosting.yaml`` with the following command:
+
+::
+
+    kubectl apply -f hosting.yaml
+
+Amazon SageMaker create an endpoint with the specified
+configuration. You incur charges for Amazon SageMaker resources used
+during the lifetime of your endpoint. You do not incur any charges once
+your endpoint is deleted.
+
+The creation process will take approximately 10 minutes.
+
+List HostingDeployments
+^^^^^^^^^^^^^^^^^^^^^^^
+
+To verify that the HostingDeployment was created, use the following
+command:
+
+::
+
+    kubectl get hostingdeployments
+
+Your output should look like the following:
+
+::
+
+    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   Creating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0mSMLOGS
+
+HostingDeployment Status Values
+'''''''''''''''''''''''''''''''
+
+The status field can be one of several values:
+
+-  ``SynchronizingK8sJobWithSageMaker``: The operator is preparing to
+   create the endpoint.
+
+-  ``ReconcilingEndpoint``: The operator is creating, updating, or
+   deleting endpoint resources. If the HostingDeployment remains in this
+   state, use ``kubectl describe`` to see the reason in the
+   ``Additional`` field.
+
+-  ``OutOfService``: Endpoint is not available to take incoming
+   requests.
+
+-  ``Creating``:
+   `CreateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__
+   is executing.
+
+-  ``Updating``:
+   `UpdateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html>`__
+   or
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   is executing.
+
+-  ``SystemUpdating``: Endpoint is undergoing maintenance and cannot be
+   updated or deleted or re-scaled until it has completed. This
+   maintenance operation does not change any customer-specified values
+   such as VPC config, KMS encryption, model, instance type, or instance
+   count.
+
+-  ``RollingBack``: Endpoint fails to scale up or down or change its
+   variant weight and is in the process of rolling back to its previous
+   configuration. Once the rollback completes, endpoint returns to an
+   ``InService`` status. This transitional status only applies to an
+   endpoint that has autoscaling enabled and is undergoing variant
+   weight or capacity changes as part of an
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   call or when the
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   operation is called explicitly.
+
+-  ``InService``: Endpoint is available to process incoming requests.
+
+-  ``Deleting``:
+   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
+   is executing.
+
+-  ``Failed``: Endpoint could not be created, updated, or re-scaled. Use
+   `DescribeEndpoint:FailureReason <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeEndpoint.html#SageMaker-DescribeEndpoint-response-FailureReason>`__
+   for information about the failure.
+   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
+   is the only operation that can be performed on a failed endpoint.
+
+Describe a Hostingdeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the ``describe`` kubectl verb
+by running the following command.
+
+::
+
+    kubectl describe hostingdeployment
+
+Your output should look like the following:
+
+::
+
+    Name:         host-xgboost
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HostingDeployment","metadata":{"annotations":{},"name":"host-xgboost","namespace":"def..."
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         HostingDeployment
+    Metadata:
+      Creation Timestamp:  2019-11-22T19:40:00Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        1
+      Resource Version:  4258134
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hostingdeployments/host-xgboost
+      UID:               def0e83e-0d5f-11ea-aa45-0a3507uiduid
+    Spec:
+      Containers:
+        Container Hostname:  xgboost
+        Image:               123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
+        Model Data URL:      s3://my-bucket/inference/xgboost-mnist/model.tar.gz
+      Models:
+        Containers:
+          xgboost
+        Execution Role Arn:  arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+        Name:                xgboost-model
+        Primary Container:   xgboost
+      Production Variants:
+        Initial Instance Count:  1
+        Instance Type:           ml.c5.large
+        Model Name:              xgboost-model
+        Variant Name:            all-traffic
+      Region:                    us-east-2
+    Status:
+      Creation Time:         2019-11-22T19:40:04Z
+      Endpoint Arn:          arn:aws:sagemaker:us-east-2:123456789012:endpoint/host-xgboost-def0e83e0d5f11eaaaexample
+      Endpoint Config Name:  host-xgboost-1-def0e83e0d5f11e-[93me08f6c510d5f11eaaa450ae[0mxample
+      Endpoint Name:         host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350733ba06
+      Endpoint Status:       Creating
+      Endpoint URL:          https://runtime.sagemaker.us-east-2.amazonaws.com/endpoints/host-xgboost-def0e83e0d5f11eaaaexample/invocations
+      Last Check Time:       2019-11-22T19:43:57Z
+      Last Modified Time:    2019-11-22T19:40:04Z
+      Model Names:
+        Name:   xgboost-model
+        Value:  xgboost-model-1-def0e83e0d5f11-[93mdf5cc9fd0d5f11eaaa450ae[0mxample
+    Events:     <none>
+
+The status field provides more information using the following fields:
+
+-  ``Additional``: Additional information about the status of the
+   hosting deployment. This field is optional and only gets populated in
+   case of error.
+
+-  ``Creation Time``: When the endpoint was created in Amazon SageMaker.
+
+-  ``Endpoint ARN``: The Amazon SageMaker endpoint ARN.
+
+-  ``Endpoint Config Name``: The Amazon SageMaker name of the endpoint
+   configuration.
+
+-  ``Endpoint Name``: The Amazon SageMaker name of the endpoint.
+
+-  ``Endpoint Status``: The Status of the endpoint.
+
+-  ``Endpoint URL``: The HTTPS URL that can be used to access the
+   endpoint. For more information, see `Deploy a Model on Amazon
+   SageMaker Hosting
+   Services <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>`__.
+
+-  ``FailureReason``: If a create, update, or delete command fails, the
+   cause will be shown here.
+
+-  ``Last Check Time``: The last time the operator checked the status of
+   the endpoint.
+
+-  ``Last Modified Time``: The last time the endpoint was modified.
+
+-  ``Model Names``: A key-value pair of HostingDeployment model names to
+   Amazon SageMaker model names.
+
+Invoking the Endpoint
+^^^^^^^^^^^^^^^^^^^^^
+
+Once the endpoint status is ``InService``, you can invoke the endpoint
+in two ways: using the AWS CLI, which does authentication and URL
+request signing, or using an HTTP client like curl. If you use your own
+client, you will need to do AWSv4 URL signing and authentication on your
+own.
+
+To invoke the endpoint using the AWS CLI, run the following command.
+Make sure to replace the Region and endpoint-name with your endpoint’s
+Region and Amazon SageMaker endpoint name. This information can be
+obtained from the output of ``kubectl describe``.
+
+::
+
+    # Invoke the endpoint with mock input data.
+    aws sagemaker-runtime invoke-endpoint \
+      --region us-east-2 \
+      --endpoint-name <endpoint name> \
+      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
+      >(cat) \
+      --content-type text/csv > /dev/null
+
+For example, if your Region were ``us-east-2`` and your endpoint
+config name were ``host-xgboost-[93mf56b6b280d7511ea824b129926e[0mxample``,
+then the following command would invoke the endpoint:
+
+::
+
+    aws sagemaker-runtime invoke-endpoint \
+      --region us-east-2 \
+      --endpoint-name host-xgboost-[93mf56b6b280d7511ea824b1299e[0mxample \
+      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
+      >(cat) \
+      --content-type text/csv > /dev/null
+    4.95847082138
+
+Here, ``4.95847082138`` is the prediction from the model for the mock
+data.
+
+Update HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^
+
+Once a HostingDeployment has a status of ``InService``, it can be
+updated. It might take about 10 minutes for HostingDeployment to be in
+service. To verify that the status is ``InService``, use the following
+command:
+
+::
+
+    kubectl get hostingdeployments
+
+The HostingDeployment can be updated before the status
+is ``InService``. The operator will wait until the Amazon SageMaker
+endpoint is ``InService`` before applying the update.
+
+To apply an update, modify the ``hosting.yaml`` file. For example,
+change the ``initialInstanceCount`` field from 1 to 2 as follows:
+
+::
+
+    apiVersion: sagemaker.aws.amazon.com/v1
+    kind: HostingDeployment
+    metadata:
+      name: host-xgboost
+    spec:
+        region: us-east-2
+        productionVariants:
+            - variantName: all-traffic
+              modelName: xgboost-model
+              initialInstanceCount: 2
+              instanceType: ml.c5.large
+        models:
+            - name: xgboost-model
+              executionRoleArn: arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+              primaryContainer: xgboost
+              containers:
+                - xgboost
+        containers:
+            - containerHostname: xgboost
+              modelDataUrl: s3://my-bucket/inference/xgboost-mnist/model.tar.gz
+              image: 123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
+
+Save the file, then use ``kubectl`` to apply your update as follows.
+You should see the status change
+from ``InService`` to ``ReconcilingEndpoint``,
+then ``Updating``.
+
+::
+
+    $ kubectl apply -f hosting.yaml
+    hostingdeployment.sagemaker.aws.amazon.com/host-xgboost configured
+
+    $ kubectl get hostingdeployments
+    NAME           STATUS                SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   ReconcilingEndpoint   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350abcdef
+
+    $ kubectl get hostingdeployments
+    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   Updating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m3507abcdef
+
+Amazon SageMaker deploys a new set of instances with your models,
+switches traffic to use the new instances, and drains the old instances.
+As soon as this process begins, the status becomes ``Updating``. After
+the update is complete, your endpoint becomes ``InService``. This
+process takes approximately 10 minutes.
+
+Delete the HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use ``kubectl`` to delete a HostingDeployment with the following
+command:
+
+::
+
+    kubectl delete hostingdeployments host-xgboost
+
+Your output should look like the following:
+
+::
+
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+To verify that the hosting deployment has been deleted, use the
+following command:
+
+::
+
+    kubectl get hostingdeployments
+    No resources found.
+
+Endpoints that have been deleted do not incur any charges for
+Amazon SageMaker resources.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-09-17 10:34:12[0m
[92mHash: 14380d06a1c6c753b0bd99ccb220b383ac4a69b4[0m
[92mFilepath: doc/workflows/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: removed Kubernetes workflow content (#1900)

* Update README.rst

* Update index.rst

* Delete amazon_sagemaker_components_for_kubeflow_pipelines.rst

* Delete amazon_sagemaker_jobs.rst

* Delete amazon_sagemaker_operators_for_kubernetes.rst

* Delete amazon_sagemaker_operators_for_kubernetes_authentication.png

* Delete index.rst

* Delete using_amazon_sagemaker_components.rst[0m
@@ -0,0 +1,965 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --region ${AWS_REGION} --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+If you're using the Amazon SageMaker Operators in China, create a file named ``trust.json``  and insert the following trust
+relationship code block into it instead. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+      {
+        "Version": "2012-10-17",
+        "Statement": [
+          {
+            "Effect": "Allow",
+            "Principal": {
+              "Federated": "arn:aws-cn:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+            },
+            "Action": "sts:AssumeRoleWithWebIdentity",
+            "Condition": {
+              "StringEquals": {
+                "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+                "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+              }
+            }
+          }
+        ]
+      }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --region ${AWS_REGION} --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+If you're using the Amazon SageMaker Operators in China, attach the following policy instead:
+
+::
+
+    aws iam attach-role-policy --region ${AWS_REGION} --role-name <role name> --policy-arn arn:aws-cn:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+   If you're using the Amazon SageMaker Operators in China, download the following installer script instead. Whenever ``installer.yaml`` is referenced, use ``installer_china.yaml`` instead.
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/china/installer_china.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --region ${AWS_REGION} --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+If you're using the Amazon SageMaker Operators in China, create a file named ``trust.json``  and insert the following trust
+relationship code block into it instead. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+      {
+        "Version": "2012-10-17",
+        "Statement": [
+          {
+            "Effect": "Allow",
+            "Principal": {
+              "Federated": "arn:aws-cn:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+            },
+            "Action": "sts:AssumeRoleWithWebIdentity",
+            "Condition": {
+              "StringEquals": {
+                "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+                "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+              }
+            }
+          }
+        ]
+      }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --region ${AWS_REGION} --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+If you're using the Amazon SageMaker Operators in China, attach the following policy instead:
+
+::
+
+    aws iam attach-role-policy --region ${AWS_REGION} --role-name <role name> --policy-arn arn:aws-cn:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+   If you're using the Amazon SageMaker Operators in China, download the following operator script instead. Whenever ``operator.yaml`` is referenced, use ``operator_china.yaml`` instead.
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/china/operator_china.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-08-01 12:36:47[0m
[92mHash: c4bb69572682bb177d2e49aaea156185c661a0ed[0m
[92mFilepath: doc/workflows/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: update KFP full pipeline (#1771)

[0m
@@ -463,24 +463,21 @@ you can create your classification pipeline. To create your pipeline,
 you need to define and compile it. You then deploy it and use it to run
 workflows. You can define your pipeline in Python and use the KFP
 dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows. The full code for the MNIST classification pipeline example is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
+workflows.
 
 Prepare datasets
 ~~~~~~~~~~~~~~~~
 
-To run the pipelines, you need to upload the data extraction pre-processing script to an S3 bucket. This bucket and all resources for this example must be located in the ``us-east-1`` Amazon Region. If you don’t have a bucket, create one
+To run the pipelines, you need to have the datasets in an S3 bucket in
+your account. This bucket must be located in the region where you want
+to run Amazon SageMaker jobs. If you don’t have a bucket, create one
 using the steps in `Creating a
 bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
 
-From the ``mnist-kmeans-sagemaker`` folder of the Kubeflow repository you cloned on your gateway node, run the following command to upload the ``kmeans_preprocessing.py`` file to your S3 bucket. Change ``<bucket-name>`` to the name of the S3 bucket you created.
-
-::
-
-    aws s3 cp mnist-kmeans-sagemaker/kmeans_preprocessing.py s3://<bucket-name>/mnist_kmeans_example/processing_code/kmeans_preprocessing.py
-
+From your gateway node, run the `sample dataset
+creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
+script to copy the datasets into your bucket. Change the bucket name in
+the script to the one you created.
 
 Create a Kubeflow Pipeline using Amazon SageMaker Components
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -499,14 +496,54 @@ parameters for each component of your pipeline. These parameters can
 also be updated when using other pipelines. We have provided default
 values for all parameters in the sample classification pipeline file.
 
-The following are the only parameters you need to pass to run the
-sample pipelines. To pass these parameters, update their entries when creating a new run.
+The following are the only parameters you may need to modify to run the
+sample pipelines. To modify these parameters, update their entries in
+the sample classification pipeline file.
 
 -  **Role-ARN:** This must be the ARN of an IAM role that has full
    Amazon SageMaker access in your AWS account. Use the ARN
    of  ``kfp-example-pod-role``.
 
--  **Bucket**: This is the name of the S3 bucket that you uploaded the ``kmeans_preprocessing.py`` file to.
+-  **The Dataset Buckets**: You must change the S3 bucket with the input
+   data for each of the components. Replace the following with the link
+   to your S3 bucket:
+
+   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **HPO channels for test/HPO channel for
+      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **Batch
+      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
+
+-  **Output buckets:** Replace the output buckets with S3 buckets you
+   have write permission to. Replace the following with the link to your
+   S3 bucket:
+
+   -  **Training/HPO**:
+      ``output_location='s3://<your-s3-bucket-name>/output'``
+
+   -  **Batch Transform**:
+      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
+
+-  **Region:**\ The default pipelines work in us-east-1. If your
+   cluster is in a different region, update the following:
+
+   -  The ``region='us-east-1'`` Parameter in the input list.
+
+   -  The algorithm images for Amazon SageMaker. If you use one of
+      the Amazon SageMaker built-in algorithm images, select the image
+      for your region. Construct the image name using the information
+      in `Common parameters for built-in
+      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
+      For Example:
+
+      ::
+
+          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
+
+   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
+      to copy the data to a bucket in the same region as the cluster.
 
 You can adjust any of the input parameters using the KFP UI and trigger
 your run again.
@@ -595,18 +632,18 @@ currently does not support specifying input parameters while creating
 the run. You need to update your parameters in the Python pipeline file
 before compiling. Replace ``<experiment-name>`` and ``<job-name>``
 with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline. Replace ``<your-role-arn>`` with the ARN of ``kfp-example-pod-role``. Replace ``<your-bucket-name>`` with the name of the S3 bucket you created.
+pipeline.
 
 ::
 
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id> role_arn="<your-role-arn>" bucket_name="<your-bucket-name>"
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
 
 You can also directly submit a run using the compiled pipeline package
 created as the output of the ``dsl-compile`` command.
 
 ::
 
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output> role_arn="<your-role-arn>" bucket_name="<your-bucket-name>"
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
 
 Your output should look like the following:
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-26 01:45:39[0m
[92mHash: 52d01892ba4620454cc8dd036c73a54a07e3949f[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Apache Airflow integration for SageMaker Processing Jobs (#1620)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -58,12 +57,13 @@ passenv =
     AWS_SESSION_TOKEN
     AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
     AWS_DEFAULT_REGION
+
 # {posargs} can be passed in by additional arguments specified when invoking tox.
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86 --omit */tensorflow/tensorflow_serving/*
-deps = .[test]
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+extras = test
 
 [testenv:flake8]
 basepython = python3
@@ -97,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 18:55:13[0m
[92mHash: 7af264a2715849ad7e8cb79e7c7f5118189deff2[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit:  breaking: refactor Predictor attribute endpoint to endpoint_name (#1632)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -57,13 +58,12 @@ passenv =
     AWS_SESSION_TOKEN
     AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
     AWS_DEFAULT_REGION
-
 # {posargs} can be passed in by additional arguments specified when invoking tox.
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
-extras = test
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86 --omit */tensorflow/tensorflow_serving/*
+deps = .[test]
 
 [testenv:flake8]
 basepython = python3
@@ -97,20 +97,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 18:50:44[0m
[92mHash: 1ab06414da08f6a58f36384046ef9c43e0d80d9e[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: fix tox test dependencies and bump coverage threshold to 86% (#1634)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -58,12 +57,13 @@ passenv =
     AWS_SESSION_TOKEN
     AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
     AWS_DEFAULT_REGION
+
 # {posargs} can be passed in by additional arguments specified when invoking tox.
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86 --omit */tensorflow/tensorflow_serving/*
-deps = .[test]
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+extras = test
 
 [testenv:flake8]
 basepython = python3
@@ -97,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 13:00:58[0m
[92mHash: 4f54ab9306a5c26315a3fd55ab393467baa57333[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: make instance_type optional for Airflow model configs (#1627)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 12:18:07[0m
[92mHash: c002dcd590adf6ccda6956ca94478aaf1faef86e[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.66.0
[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -63,7 +62,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
 extras = test
 
 [testenv:flake8]
@@ -98,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 11:19:54[0m
[92mHash: 39c33a27b40c09c4500dddfd6cce33b94f60d715[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: refactor name of RealTimePredictor to Predictor (#1629)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 10:37:58[0m
[92mHash: bd800700f75f2f05a827a2b4ab56321ddc7d1fb9[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: upgrade airflow to latest stable version (#1628)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -63,7 +62,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
 extras = test
 
 [testenv:flake8]
@@ -98,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-25 10:10:48[0m
[92mHash: c24e0b567e29bd0f3a7eb5175904c4aa334581b7[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: use fixture for Python version in TF integ tests (#1617)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-24 14:27:02[0m
[92mHash: ae467e1ca95b791b4637cb92e1f65f1ab6bdd22a[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: update feature request issue template (#1625)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -63,7 +62,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
 extras = test
 
 [testenv:flake8]
@@ -98,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-24 13:19:32[0m
[92mHash: acbe02b0f672fce1748101fd0f7936b205e4d9f5[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove check for Python 2 string in sagemaker.predictor._is_sequence_like() (#1622)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-24 12:18:10[0m
[92mHash: 40a27209322ff0a711c73a997b4ed82b4fa1a4cf[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.65.1.post1
[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -63,7 +62,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
 extras = test
 
 [testenv:flake8]
@@ -98,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-23 16:21:36[0m
[92mHash: c21141762d380713b30ed4a0759e5628c49b095e[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: clean up pickle.load logic in integ tests (#1611)

Because we no longer run our tests with Python 2,
we no longer need the branched logic for pickle.load args.[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-22 12:18:50[0m
[92mHash: 5d74516f17de89dfc2537b9516b38de53f7273ed[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.65.1.post0
[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,7 +19,6 @@ exclude =
     .tox
     tests/data/
     venv/
-    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -63,7 +62,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
 extras = test
 
 [testenv:flake8]
@@ -98,6 +97,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-18 16:43:44[0m
[92mHash: c0134e52f2f301e5be888cf361a8127c560c3f6a[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: remove assumption of Python 2 unit test runs (#1610)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = black-format,flake8,pylint,twine,sphinx,doc8,py36,py37,py38
+envlist = black-format,flake8,pylint,twine,sphinx,doc8,py27,py36,py37,py38
 
 skip_missing_interpreters = False
 
@@ -19,6 +19,7 @@ exclude =
     .tox
     tests/data/
     venv/
+    *service_pb2_grpc.py
 
 max-complexity = 10
 
@@ -62,7 +63,7 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 commands =
     coverage run --source sagemaker -m pytest {posargs}
-    {env:IGNORE_COVERAGE:} coverage report --fail-under=86
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=85 --omit */tensorflow/tensorflow_serving/*
 extras = test
 
 [testenv:flake8]
@@ -97,20 +98,6 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-18 14:59:39[0m
[92mHash: 875abe1f0c17d2e087f97c43b2ae1b0c7a9f3d5c[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: infra: upgrade Sphinx to 3.1.1 (#1605)

This commit also consolidates the doc dependencies to doc/requirements.txt,
removes some unnecessary dependencies, and upgrades sphinx-rtd-theme to 0.5.0.[0m
@@ -98,6 +98,20 @@ commands =
 [testenv:sphinx]
 basepython = python3
 changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-03 15:29:06[0m
[92mHash: 0f5c9b5c007774d50f2e9ba3a9c76b29929f0de4[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Merge branch 'master' into zwei
[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-03 15:29:06[0m
[92mHash: 0f5c9b5c007774d50f2e9ba3a9c76b29929f0de4[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Merge branch 'master' into zwei
[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-03 15:29:06[0m
[92mHash: 0f5c9b5c007774d50f2e9ba3a9c76b29929f0de4[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Merge branch 'master' into zwei
[0m
@@ -1,835 +0,0 @@
-Using Amazon SageMaker Components
-=================================
-
-In this tutorial, you run a pipeline using Amazon SageMaker Components
-for Kubeflow Pipelines to train a classification model using Kmeans with
-the MNIST dataset. This workflow uses Kubeflow pipelines as the
-orchestrator and Amazon SageMaker as the backend to run the steps in the
-workflow. For the full code for this and other pipeline examples, see
-the `Sample AWS SageMaker Kubeflow
-Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
-For information on the components used, see the `KubeFlow Pipelines
-GitHub
-repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
-
-Setup
------
-
-To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
-Service (Amazon EKS) cluster and a gateway node to interact with that
-cluster. The following sections show the steps needed to set up these
-resources.
-
-Set up a gateway node
-~~~~~~~~~~~~~~~~~~~~~
-
-A gateway node is used to create an Amazon EKS cluster and access the
-Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
-as your gateway node. If you want to use a new EC2 instance, create one
-with the latest Ubuntu 18.04 DLAMI version from the AWS console using
-the steps in `Launching and Configuring a
-DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
-
-Complete the following steps to set up your gateway node. Depending on
-your environment, you may have certain requirements already configured.
-
-If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
-AWS
-Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
-you have an existing Amazon EKS cluster, use the credentials of the IAM
-role or user that has access to it.
-
-Add the following permissions to your user using the steps in `Changing
-Permissions for an IAM
-User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
-
--  CloudWatchLogsFullAccess
-
--  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
-
--  IAMFullAccess
-
--  AmazonS3FullAccess
-
--  AmazonEC2FullAccess
-
--  AmazonEKSAdminPolicy - Create this policy using the schema
-   from `Amazon EKS Identity-Based Policy
-   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
-
-Install the following on your gateway node to access the Amazon EKS
-cluster and KFP UI.
-
--  `AWS
-   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
-   If you are using an IAM user, configure your `Access Key ID, Secret
-   Access
-   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
-   preferred AWS Region by running: ``aws configure``
-
--  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-   version 0.1.31 and above.
-
--  ``eksctl`` version above 0.15.
-
--  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-   The version needs to match your Kubernetes version within 1 minor
-   version.
-
-Install \ ``boto3``.
-
-::
-
-    pip install boto3
-
-Set up an Amazon EKS cluster
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-set up an Amazon EKS cluster:
-
-If you do not have an existing Amazon EKS cluster, complete the
-following substeps. If you already have an Amazon EKS cluster, skip this
-step.
-
-Run the following from your command line to create an Amazon EKS Cluster
-with version 1.14 or above. Replace ``<your-cluster-name>`` with any
-name for your cluster.
-
-::
-
-    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
-
-When cluster creation is complete, verify that you have access to the
-cluster using the following command.
-
-::
-
-    kubectl get nodes
-
-Verify that the current kubectl context is the cluster you want to use
-with the following command. The current context is marked with an
-asterisk (\*) in the output.
-
-::
-
-    kubectl config get-contexts
-
-    CURRENT NAME     CLUSTER
-    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
-
-If the desired cluster is not configured as your current default, update
-the default with the following command.
-
-::
-
-    aws eks update-kubeconfig --name <clustername> --region us-east-1
-
-Install Kubeflow Pipelines
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-install Kubeflow Pipelines on your cluster.
-
-Install Kubeflow Pipelines on your cluster by following step 1
-of `Deploying Kubeflow Pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
-Your KFP version must be 0.5.0 or above.
-
-Verify that the Kubeflow Pipelines service and other related resources
-are running.
-
-::
-
-    kubectl -n kubeflow get all | grep pipeline
-
-Your output should look like the following.
-
-::
-
-    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
-    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
-    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
-    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
-    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
-    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
-    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
-    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
-    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
-    deployment.apps/ml-pipeline                       1/1     1            1           2d
-    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
-    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
-    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
-    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
-    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
-    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
-    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
-    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
-    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
-    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
-    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
-
-Access the KFP UI
-~~~~~~~~~~~~~~~~~
-
-The Kubeflow Pipelines UI is used for managing and tracking experiments,
-jobs, and runs on your cluster. You can use port forwarding to access
-the Kubeflow Pipelines UI from your gateway node.
-
-Set up port forwarding to the KFP UI service
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Verify that the KFP UI service is running using the following command:
-
-::
-
-    kubectl -n kubeflow get service ml-pipeline-ui
-
-    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
-    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
-
-Run the following command to setup port forwarding to the KFP UI
-service. This forwards the KFP UI to port 8080 on your gateway node and
-allows you to access the KFP UI from your browser.
-
-    **Note**
-
-    The port-forward from your remote machine drops if there is no
-    activity. Run this command again if your dashboard is unable to get
-    logs or updates. If the commands return an error, ensure that there
-    is no process already running on the port you are trying to use.
-
-::
-
-    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
-
-Your method of accessing the KFP UI depends on your gateway node type.
-
-Local machine as the gateway node
-
-Access the dashboard in your browser as follows:
-
-::
-
-    http://localhost:8080
-
-Click **Pipelines** to access the pipelines UI.
-
-EC2 instance as the gateway node
-
-You need to setup an SSH tunnel on your EC2 instance to access the
-Kubeflow dashboard from your local machine’s browser.
-
-From a new terminal session in your local machine, run the following.
-Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
-instance found on the EC2 console. You can also use the public DNS.
-Replace ``<path_to_key>`` with the path to the pem key used to access
-the gateway node.
-
-::
-
-    public_DNS_address=<public-DNS-of-gateway-node>
-    key=<path_to_key>
-
-    on Ubuntu:
-    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
-
-    or on Amazon Linux:
-    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
-
-Access the dashboard in your browser.
-
-::
-
-    http://localhost:9000
-
-Click **Pipelines** to access the KFP UI.
-
-Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
-SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
-need access to SageMaker. In this section, you create IAM Users/Roles to
-be used by Kubeflow Pipeline pods and Amazon SageMaker.
-
-Create a KFP execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Enable OIDC support on the Amazon EKS cluster with the following
-command. Replace ``<cluster_name>`` with the name of your cluster
-and ``<cluster_region>`` with the region your cluster is in.
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
-            --region <cluster-region> --approve
-
-Run the following to get the `OIDC <https://openid.net/connect/>`__
-issuer URL. This URL is in the
-form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
-
-::
-
-    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
-
-Run the following to create a file named ``trust.json``.
-Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
-include ``https://`` when in your OIDC issuer URL.
-Replace ``<AWS_account_number>`` with your AWS account number.
-
-::
-
-    OIDC_URL="<OIDC-URL>"
-    AWS_ACC_NUM="<AWS-account-number>"
-
-    # Run this to create trust.json file
-    cat <<EOF > trust.json
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "${OIDC_URL}:aud": "sts.amazonaws.com",
-              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
-            }
-          }
-        }
-      ]
-    }
-    EOF
-
-Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
-using the following command. This role is used by KFP pods to create
-Amazon SageMaker jobs from KFP components. Note the ARN returned in the
-output.
-
-::
-
-    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
-    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
-
-Edit your pipeline-runner service account with the following command.
-
-::
-
-    kubectl edit -n kubeflow serviceaccount pipeline-runner
-
-In the file, add the following Amazon EKS role annotation and
-replace ``<role_arn>`` with your role ARN.
-
-::
-
-    eks.amazonaws.com/role-arn: <role-arn>
-
-Your file should look like the following when you’ve added the Amazon
-EKS role annotation. Save the file.
-
-::
-
-    apiVersion: v1
-    kind: ServiceAccount
-    metadata:
-      annotations:
-        eks.amazonaws.com/role-arn: <role-arn>
-        kubectl.kubernetes.io/last-applied-configuration: |
-          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
-      creationTimestamp: "2020-04-16T05:48:06Z"
-      labels:
-        app: pipeline-runner
-        app.kubernetes.io/component: pipelines-runner
-        app.kubernetes.io/instance: pipelines-runner-0.2.0
-        app.kubernetes.io/managed-by: kfctl
-        app.kubernetes.io/name: pipelines-runner
-        app.kubernetes.io/part-of: kubeflow
-        app.kubernetes.io/version: 0.2.0
-      name: pipeline-runner
-      namespace: kubeflow
-      resourceVersion: "11787"
-      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
-      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
-    secrets:
-    - name: pipeline-runner-token-dkjrk
-
-Create an Amazon SageMaker execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The ``kfp-example-sagemaker-execution-role`` IAM role is used
-by Amazon SageMaker jobs to access AWS resources. For more information,
-see the IAM Permissions section. You provide this role as an input
-parameter when running the pipeline.
-
-Run the following to create the role. Note the ARN that is returned in
-your output.
-
-::
-
-    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
-
-    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
-    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
-
-Add access to additional IAM users or roles
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you use an intuitive IDE like Jupyter or want other people in your
-organization to use the cluster you set up, you can also give them
-access. The following steps run through this workflow using Amazon
-SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
-managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
-You use the notebook instance to create and manage Jupyter notebooks to
-create ML workflows. You can define, compile, deploy, and run your
-pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
-SageMaker notebook to run Jupyter, you need to install the `AWS
-CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
-the latest version
-of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-
-Follow the steps in `Create an Amazon SageMaker Notebook
-Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
-to create a Amazon SageMaker notebook instance if you do not already
-have one. Give the IAM role for this instance the ``S3FullAccess``
-permission.
-
-Amazon EKS clusters use IAM users and roles to control access to the
-cluster. The rules are implemented in a config map named ``aws-auth``.
-Only the user/role that has access to the cluster will be able to edit
-this config map. Run the following from the command line of your gateway
-node to get the IAM role of the notebook instance you created.
-Replace ``<instance-name>`` with the name of your instance.
-
-::
-
-    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
-
-This command outputs the IAM role ARN in
-the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
-of this ARN.
-
-Run the following to attach the policies the IAM role.
-Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
-
-::
-
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-``eksctl`` provides commands to read and edit the ``aws-auth`` config
-map. ``system:masters`` is one of the default user groups. You add the
-user to this group. The "system:masters" group has super user
-permissions to the cluster. You can also create a group with more
-restrictive permissions or you can bind permissions directly to users.
-Replace ``<IAM-Role-arn>`` with the ARN of the IAM
-role. ``<your_username>`` can be any unique username.
-
-::
-
-    eksctl create iamidentitymapping \
-        --cluster <cluster-name> \
-        --arn <IAM-Role-arn> \
-        --group system:masters \
-        --username <your-username> \
-        --region <region>
-
-Open the Jupyter notebook on your Amazon SageMaker instance and run the
-following to verify that it has access to the cluster.
-
-::
-
-    aws eks --region <region> update-kubeconfig --name <cluster-name>
-    kubectl -n kubeflow get all | grep pipeline
-
-Running the Kubeflow Pipeline
------------------------------
-
-Now that setup of your gateway node and Amazon EKS cluster is complete,
-you can create your classification pipeline. To create your pipeline,
-you need to define and compile it. You then deploy it and use it to run
-workflows. You can define your pipeline in Python and use the KFP
-dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows.
-
-Prepare datasets
-~~~~~~~~~~~~~~~~
-
-To run the pipelines, you need to have the datasets in an S3 bucket in
-your account. This bucket must be located in the region where you want
-to run Amazon SageMaker jobs. If you don’t have a bucket, create one
-using the steps in `Creating a
-bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
-
-From your gateway node, run the `sample dataset
-creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
-script to copy the datasets into your bucket. Change the bucket name in
-the script to the one you created.
-
-Create a Kubeflow Pipeline using Amazon SageMaker Components
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The full code for the MNIST classification pipeline is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
-
-Input Parameters
-^^^^^^^^^^^^^^^^
-
-The full MNIST classification pipeline has run-specific parameters that
-you must provide values for when creating a run. You must provide these
-parameters for each component of your pipeline. These parameters can
-also be updated when using other pipelines. We have provided default
-values for all parameters in the sample classification pipeline file.
-
-The following are the only parameters you may need to modify to run the
-sample pipelines. To modify these parameters, update their entries in
-the sample classification pipeline file.
-
--  **Role-ARN:** This must be the ARN of an IAM role that has full
-   Amazon SageMaker access in your AWS account. Use the ARN
-   of  ``kfp-example-pod-role``.
-
--  **The Dataset Buckets**: You must change the S3 bucket with the input
-   data for each of the components. Replace the following with the link
-   to your S3 bucket:
-
-   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **HPO channels for test/HPO channel for
-      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **Batch
-      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
-
--  **Output buckets:** Replace the output buckets with S3 buckets you
-   have write permission to. Replace the following with the link to your
-   S3 bucket:
-
-   -  **Training/HPO**:
-      ``output_location='s3://<your-s3-bucket-name>/output'``
-
-   -  **Batch Transform**:
-      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
-
--  **Region:**\ The default pipelines work in us-east-1. If your
-   cluster is in a different region, update the following:
-
-   -  The ``region='us-east-1'`` Parameter in the input list.
-
-   -  The algorithm images for Amazon SageMaker. If you use one of
-      the Amazon SageMaker built-in algorithm images, select the image
-      for your region. Construct the image name using the information
-      in `Common parameters for built-in
-      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
-      For Example:
-
-      ::
-
-          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
-
-   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
-      to copy the data to a bucket in the same region as the cluster.
-
-You can adjust any of the input parameters using the KFP UI and trigger
-your run again.
-
-Compile and deploy your pipeline
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After defining the pipeline in Python, you must compile the pipeline to
-an intermediate representation before you can submit it to the Kubeflow
-Pipelines service. The intermediate representation is a workflow
-specification in the form of a YAML file compressed into a tar.gz
-file. You need the KFP SDK to compile your pipeline.
-
-Install KFP SDK
-^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Install the KFP SDK following the instructions in the \ `Kubeflow
-pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
-
-Verify that the KFP SDK is installed with the following command:
-
-::
-
-    pip show kfp
-
-Verify that ``dsl-compile`` has been installed correctly as follows:
-
-::
-
-    which dsl-compile
-
-Compile your pipeline
-^^^^^^^^^^^^^^^^^^^^^
-
-You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
-CLI, or the KFP SDK. The following sections illustrate the workflow
-using the KFP UI and CLI.
-
-Complete the following from your gateway node to compile your pipeline.
-
-Modify your Python file with your S3 bucket name and IAM role ARN.
-
-Use the ``dsl-compile`` command from the command line to compile your
-pipeline as follows. Replace ``<path-to-python-file>`` with the path
-to your pipeline and ``<path-to-output>`` with the location where you
-want your tar.gz file to be.
-
-::
-
-    dsl-compile --py <path-to-python-file> --output <path-to-output>
-
-Upload and run the pipeline using the KFP CLI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Complete the following steps from the command line of your gateway node.
-KFP organizes runs of your pipeline as experiments. You have the option
-to specify an experiment name. If you do not specify one, the run will
-be listed under ‘Default’ experiment.
-
-Upload your pipeline as follows:
-
-::
-
-    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
-
-Your output should look like the following. Take note of the \ ``ID``.
-
-::
-
-    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
-
-    Pipeline Details
-    ------------------
-    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
-    Name         sm-pipeline
-    Description
-    Uploaded at  2020-04-30T20:22:39+00:00
-    ...
-    ...
-
-Create a run using the following command. The KFP CLI run command
-currently does not support specifying input parameters while creating
-the run. You need to update your parameters in the Python pipeline file
-before compiling. Replace ``<experiment-name>`` and ``<job-name>``
-with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
-
-You can also directly submit a run using the compiled pipeline package
-created as the output of the ``dsl-compile`` command.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
-
-Your output should look like the following:
-
-::
-
-    Creating experiment aws.
-    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
-    +--------------------------------------+--------+----------+---------------------------+
-    | run id                               | name   | status   | created at                |
-    +======================================+========+==========+===========================+
-    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
-    +--------------------------------------+--------+----------+---------------------------+
-
-Navigate to the UI to check the progress of the job
-
-Upload and run the pipeline using the KFP UI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
--  On the left panel, choose the **Pipelines** tab.
-
--  In the upper-right corner, choose ``+UploadPipeline``.
-
--  Enter the pipeline name and description.
-
--  Choose ``Upload a file`` and enter the path to the tar.gz file you
-   created using the CLI or with the Python SDK.
-
--  On the left panel, choose the **Pipelines** tab.
-
--  Find the pipeline you created.
-
--  Choose ``+CreateRun``.
-
--  Enter your input parameters.
-
--  Choose ``Run``.
-
-Running predictions
-~~~~~~~~~~~~~~~~~~~
-
-Once your classification pipeline is deployed, you can run
-classification predictions against the endpoint that was created by the
-Deploy component. Use the KFP UI to check the output artifacts
-for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
-file to extract the endpoint name or check the Amazon SageMaker console
-in the region you used.
-
-Configure permissions to run predictions
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-If you want to run predictions from your gateway node, skip this
-section.
-
-If you want to use any other machine to run predictions, assign
-the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
-user used by the client machine. This permission is used to run
-predictions.
-
-On your gateway node, run the following to create a policy file:
-
-::
-
-    cat <<EoF > ./sagemaker-invoke.json
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "sagemaker:InvokeEndpoint"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
-    EoF
-
-Attach the policy to the client node’s IAM role or IAM user.
-
-If your client machine has an IAM role attached, run the following.
-Replace ``<your-instance-IAM-role>`` with the name of the client
-node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-If your client machine has IAM user credentials configured, run the
-following. Replace ``<your_IAM_user_name>`` with the name of the client
-node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-Run predictions
-^^^^^^^^^^^^^^^
-
-Create a Python file from your client machine
-named ``mnist-predictions.py`` with the following content . Replace
-the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
-MNIST dataset, then creates a CSV from those digits and sends it to the
-endpoint for prediction. It then outputs the results.
-
-::
-
-    import pickle, gzip, numpy, urllib.request, json
-    from urllib.parse import urlparse
-    import json
-    import io
-    import boto3
-
-    ENDPOINT_NAME='<endpoint-name>'
-    REGION = '<region>'
-
-    # Load the dataset
-    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
-    with gzip.open('mnist.pkl.gz', 'rb') as f:
-        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
-
-    # Simple function to create a csv from our numpy array
-    def np2csv(arr):
-        csv = io.BytesIO()
-        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
-        return csv.getvalue().decode().rstrip()
-
-    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
-
-    payload = np2csv(train_set[0][30:31])
-
-    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
-                                       ContentType='text/csv',
-                                       Body=payload)
-    result = json.loads(response['Body'].read().decode())
-    print(result)
-
-Run the Python file as follows:
-
-::
-
-    python mnist-predictions.py
-
-View results and logs
-~~~~~~~~~~~~~~~~~~~~~
-
-When the pipeline is running, you can click on any component to check
-execution details, such as inputs and outputs. This will list the names
-of created resources.
-
-If the KFP request is successfully processed and an Amazon SageMaker job
-is created, the component logs in the KFP UI will provide a link to the
-job created in Amazon SageMaker. The CloudWatch logs will also be
-provided if the job is successfully created.
-
-If you run too many pipeline jobs on the same cluster, you may see an
-error message that indicates you do not have enough pods available. To
-fix this, log in to your gateway node and delete the pods created by the
-pipelines you are not using as follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Cleanup
-~~~~~~~
-
-When you’re finished with your pipeline, you need to cleanup your
-resources.
-
-From the KFP dashboard, terminate your pipeline runs if they do not exit
-properly by clicking ``Terminate``.
-
-If the ``Terminate`` option doesn’t work, log in to your gateway node
-and terminate all the pods created by your pipeline run manually as
-follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Using your AWS account, log in to the Amazon SageMaker service. Manually
-stop all training, batch transform, and HPO jobs. Delete models, data
-buckets and endpoints to avoid incurring any additional
-costs. Terminating the pipeline runs does not stop the jobs in Amazon
-SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 14:16:43[0m
[92mHash: d0eb4a21ff3a746c8431fdd74104b48bd2d17e54[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: force image_uri to be passed for legacy TF images (#1539)

This change also reorganizes the TF unit tests a bit,
and updates the tf_version fixture to include recent versions.[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 14:16:43[0m
[92mHash: d0eb4a21ff3a746c8431fdd74104b48bd2d17e54[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: force image_uri to be passed for legacy TF images (#1539)

This change also reorganizes the TF unit tests a bit,
and updates the tf_version fixture to include recent versions.[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 14:16:43[0m
[92mHash: d0eb4a21ff3a746c8431fdd74104b48bd2d17e54[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: force image_uri to be passed for legacy TF images (#1539)

This change also reorganizes the TF unit tests a bit,
and updates the tf_version fixture to include recent versions.[0m
@@ -0,0 +1,835 @@
+Using Amazon SageMaker Components
+=================================
+
+In this tutorial, you run a pipeline using Amazon SageMaker Components
+for Kubeflow Pipelines to train a classification model using Kmeans with
+the MNIST dataset. This workflow uses Kubeflow pipelines as the
+orchestrator and Amazon SageMaker as the backend to run the steps in the
+workflow. For the full code for this and other pipeline examples, see
+the `Sample AWS SageMaker Kubeflow
+Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
+For information on the components used, see the `KubeFlow Pipelines
+GitHub
+repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
+
+Setup
+-----
+
+To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
+Service (Amazon EKS) cluster and a gateway node to interact with that
+cluster. The following sections show the steps needed to set up these
+resources.
+
+Set up a gateway node
+~~~~~~~~~~~~~~~~~~~~~
+
+A gateway node is used to create an Amazon EKS cluster and access the
+Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
+as your gateway node. If you want to use a new EC2 instance, create one
+with the latest Ubuntu 18.04 DLAMI version from the AWS console using
+the steps in `Launching and Configuring a
+DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
+
+Complete the following steps to set up your gateway node. Depending on
+your environment, you may have certain requirements already configured.
+
+If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
+AWS
+Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
+you have an existing Amazon EKS cluster, use the credentials of the IAM
+role or user that has access to it.
+
+Add the following permissions to your user using the steps in `Changing
+Permissions for an IAM
+User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
+
+-  CloudWatchLogsFullAccess
+
+-  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
+
+-  IAMFullAccess
+
+-  AmazonS3FullAccess
+
+-  AmazonEC2FullAccess
+
+-  AmazonEKSAdminPolicy - Create this policy using the schema
+   from `Amazon EKS Identity-Based Policy
+   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
+
+Install the following on your gateway node to access the Amazon EKS
+cluster and KFP UI.
+
+-  `AWS
+   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
+   If you are using an IAM user, configure your `Access Key ID, Secret
+   Access
+   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
+   preferred AWS Region by running: ``aws configure``
+
+-  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+   version 0.1.31 and above.
+
+-  ``eksctl`` version above 0.15.
+
+-  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+   The version needs to match your Kubernetes version within 1 minor
+   version.
+
+Install \ ``boto3``.
+
+::
+
+    pip install boto3
+
+Set up an Amazon EKS cluster
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+set up an Amazon EKS cluster:
+
+If you do not have an existing Amazon EKS cluster, complete the
+following substeps. If you already have an Amazon EKS cluster, skip this
+step.
+
+Run the following from your command line to create an Amazon EKS Cluster
+with version 1.14 or above. Replace ``<your-cluster-name>`` with any
+name for your cluster.
+
+::
+
+    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
+
+When cluster creation is complete, verify that you have access to the
+cluster using the following command.
+
+::
+
+    kubectl get nodes
+
+Verify that the current kubectl context is the cluster you want to use
+with the following command. The current context is marked with an
+asterisk (\*) in the output.
+
+::
+
+    kubectl config get-contexts
+
+    CURRENT NAME     CLUSTER
+    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
+
+If the desired cluster is not configured as your current default, update
+the default with the following command.
+
+::
+
+    aws eks update-kubeconfig --name <clustername> --region us-east-1
+
+Install Kubeflow Pipelines
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+install Kubeflow Pipelines on your cluster.
+
+Install Kubeflow Pipelines on your cluster by following step 1
+of `Deploying Kubeflow Pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
+Your KFP version must be 0.5.0 or above.
+
+Verify that the Kubeflow Pipelines service and other related resources
+are running.
+
+::
+
+    kubectl -n kubeflow get all | grep pipeline
+
+Your output should look like the following.
+
+::
+
+    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
+    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
+    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
+    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
+    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
+    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
+    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
+    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
+    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
+    deployment.apps/ml-pipeline                       1/1     1            1           2d
+    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
+    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
+    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
+    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
+    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
+    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
+    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
+    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
+    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
+    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
+    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
+
+Access the KFP UI
+~~~~~~~~~~~~~~~~~
+
+The Kubeflow Pipelines UI is used for managing and tracking experiments,
+jobs, and runs on your cluster. You can use port forwarding to access
+the Kubeflow Pipelines UI from your gateway node.
+
+Set up port forwarding to the KFP UI service
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Verify that the KFP UI service is running using the following command:
+
+::
+
+    kubectl -n kubeflow get service ml-pipeline-ui
+
+    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
+    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
+
+Run the following command to setup port forwarding to the KFP UI
+service. This forwards the KFP UI to port 8080 on your gateway node and
+allows you to access the KFP UI from your browser.
+
+    **Note**
+
+    The port-forward from your remote machine drops if there is no
+    activity. Run this command again if your dashboard is unable to get
+    logs or updates. If the commands return an error, ensure that there
+    is no process already running on the port you are trying to use.
+
+::
+
+    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
+
+Your method of accessing the KFP UI depends on your gateway node type.
+
+Local machine as the gateway node
+
+Access the dashboard in your browser as follows:
+
+::
+
+    http://localhost:8080
+
+Click **Pipelines** to access the pipelines UI.
+
+EC2 instance as the gateway node
+
+You need to setup an SSH tunnel on your EC2 instance to access the
+Kubeflow dashboard from your local machine’s browser.
+
+From a new terminal session in your local machine, run the following.
+Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
+instance found on the EC2 console. You can also use the public DNS.
+Replace ``<path_to_key>`` with the path to the pem key used to access
+the gateway node.
+
+::
+
+    public_DNS_address=<public-DNS-of-gateway-node>
+    key=<path_to_key>
+
+    on Ubuntu:
+    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
+
+    or on Amazon Linux:
+    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
+
+Access the dashboard in your browser.
+
+::
+
+    http://localhost:9000
+
+Click **Pipelines** to access the KFP UI.
+
+Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
+SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
+need access to SageMaker. In this section, you create IAM Users/Roles to
+be used by Kubeflow Pipeline pods and Amazon SageMaker.
+
+Create a KFP execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Enable OIDC support on the Amazon EKS cluster with the following
+command. Replace ``<cluster_name>`` with the name of your cluster
+and ``<cluster_region>`` with the region your cluster is in.
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
+            --region <cluster-region> --approve
+
+Run the following to get the `OIDC <https://openid.net/connect/>`__
+issuer URL. This URL is in the
+form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
+
+::
+
+    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
+
+Run the following to create a file named ``trust.json``.
+Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
+include ``https://`` when in your OIDC issuer URL.
+Replace ``<AWS_account_number>`` with your AWS account number.
+
+::
+
+    OIDC_URL="<OIDC-URL>"
+    AWS_ACC_NUM="<AWS-account-number>"
+
+    # Run this to create trust.json file
+    cat <<EOF > trust.json
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "${OIDC_URL}:aud": "sts.amazonaws.com",
+              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
+            }
+          }
+        }
+      ]
+    }
+    EOF
+
+Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
+using the following command. This role is used by KFP pods to create
+Amazon SageMaker jobs from KFP components. Note the ARN returned in the
+output.
+
+::
+
+    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
+    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
+
+Edit your pipeline-runner service account with the following command.
+
+::
+
+    kubectl edit -n kubeflow serviceaccount pipeline-runner
+
+In the file, add the following Amazon EKS role annotation and
+replace ``<role_arn>`` with your role ARN.
+
+::
+
+    eks.amazonaws.com/role-arn: <role-arn>
+
+Your file should look like the following when you’ve added the Amazon
+EKS role annotation. Save the file.
+
+::
+
+    apiVersion: v1
+    kind: ServiceAccount
+    metadata:
+      annotations:
+        eks.amazonaws.com/role-arn: <role-arn>
+        kubectl.kubernetes.io/last-applied-configuration: |
+          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
+      creationTimestamp: "2020-04-16T05:48:06Z"
+      labels:
+        app: pipeline-runner
+        app.kubernetes.io/component: pipelines-runner
+        app.kubernetes.io/instance: pipelines-runner-0.2.0
+        app.kubernetes.io/managed-by: kfctl
+        app.kubernetes.io/name: pipelines-runner
+        app.kubernetes.io/part-of: kubeflow
+        app.kubernetes.io/version: 0.2.0
+      name: pipeline-runner
+      namespace: kubeflow
+      resourceVersion: "11787"
+      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
+      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
+    secrets:
+    - name: pipeline-runner-token-dkjrk
+
+Create an Amazon SageMaker execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The ``kfp-example-sagemaker-execution-role`` IAM role is used
+by Amazon SageMaker jobs to access AWS resources. For more information,
+see the IAM Permissions section. You provide this role as an input
+parameter when running the pipeline.
+
+Run the following to create the role. Note the ARN that is returned in
+your output.
+
+::
+
+    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
+
+    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
+    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
+
+Add access to additional IAM users or roles
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If you use an intuitive IDE like Jupyter or want other people in your
+organization to use the cluster you set up, you can also give them
+access. The following steps run through this workflow using Amazon
+SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
+managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
+You use the notebook instance to create and manage Jupyter notebooks to
+create ML workflows. You can define, compile, deploy, and run your
+pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
+SageMaker notebook to run Jupyter, you need to install the `AWS
+CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
+the latest version
+of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+
+Follow the steps in `Create an Amazon SageMaker Notebook
+Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
+to create a Amazon SageMaker notebook instance if you do not already
+have one. Give the IAM role for this instance the ``S3FullAccess``
+permission.
+
+Amazon EKS clusters use IAM users and roles to control access to the
+cluster. The rules are implemented in a config map named ``aws-auth``.
+Only the user/role that has access to the cluster will be able to edit
+this config map. Run the following from the command line of your gateway
+node to get the IAM role of the notebook instance you created.
+Replace ``<instance-name>`` with the name of your instance.
+
+::
+
+    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
+
+This command outputs the IAM role ARN in
+the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
+of this ARN.
+
+Run the following to attach the policies the IAM role.
+Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
+
+::
+
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+``eksctl`` provides commands to read and edit the ``aws-auth`` config
+map. ``system:masters`` is one of the default user groups. You add the
+user to this group. The "system:masters" group has super user
+permissions to the cluster. You can also create a group with more
+restrictive permissions or you can bind permissions directly to users.
+Replace ``<IAM-Role-arn>`` with the ARN of the IAM
+role. ``<your_username>`` can be any unique username.
+
+::
+
+    eksctl create iamidentitymapping \
+        --cluster <cluster-name> \
+        --arn <IAM-Role-arn> \
+        --group system:masters \
+        --username <your-username> \
+        --region <region>
+
+Open the Jupyter notebook on your Amazon SageMaker instance and run the
+following to verify that it has access to the cluster.
+
+::
+
+    aws eks --region <region> update-kubeconfig --name <cluster-name>
+    kubectl -n kubeflow get all | grep pipeline
+
+Running the Kubeflow Pipeline
+-----------------------------
+
+Now that setup of your gateway node and Amazon EKS cluster is complete,
+you can create your classification pipeline. To create your pipeline,
+you need to define and compile it. You then deploy it and use it to run
+workflows. You can define your pipeline in Python and use the KFP
+dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
+workflows.
+
+Prepare datasets
+~~~~~~~~~~~~~~~~
+
+To run the pipelines, you need to have the datasets in an S3 bucket in
+your account. This bucket must be located in the region where you want
+to run Amazon SageMaker jobs. If you don’t have a bucket, create one
+using the steps in `Creating a
+bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
+
+From your gateway node, run the `sample dataset
+creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
+script to copy the datasets into your bucket. Change the bucket name in
+the script to the one you created.
+
+Create a Kubeflow Pipeline using Amazon SageMaker Components
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The full code for the MNIST classification pipeline is available in the
+`Kubeflow Github
+repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
+To use it, clone the example Python files to your gateway node.
+
+Input Parameters
+^^^^^^^^^^^^^^^^
+
+The full MNIST classification pipeline has run-specific parameters that
+you must provide values for when creating a run. You must provide these
+parameters for each component of your pipeline. These parameters can
+also be updated when using other pipelines. We have provided default
+values for all parameters in the sample classification pipeline file.
+
+The following are the only parameters you may need to modify to run the
+sample pipelines. To modify these parameters, update their entries in
+the sample classification pipeline file.
+
+-  **Role-ARN:** This must be the ARN of an IAM role that has full
+   Amazon SageMaker access in your AWS account. Use the ARN
+   of  ``kfp-example-pod-role``.
+
+-  **The Dataset Buckets**: You must change the S3 bucket with the input
+   data for each of the components. Replace the following with the link
+   to your S3 bucket:
+
+   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **HPO channels for test/HPO channel for
+      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **Batch
+      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
+
+-  **Output buckets:** Replace the output buckets with S3 buckets you
+   have write permission to. Replace the following with the link to your
+   S3 bucket:
+
+   -  **Training/HPO**:
+      ``output_location='s3://<your-s3-bucket-name>/output'``
+
+   -  **Batch Transform**:
+      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
+
+-  **Region:**\ The default pipelines work in us-east-1. If your
+   cluster is in a different region, update the following:
+
+   -  The ``region='us-east-1'`` Parameter in the input list.
+
+   -  The algorithm images for Amazon SageMaker. If you use one of
+      the Amazon SageMaker built-in algorithm images, select the image
+      for your region. Construct the image name using the information
+      in `Common parameters for built-in
+      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
+      For Example:
+
+      ::
+
+          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
+
+   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
+      to copy the data to a bucket in the same region as the cluster.
+
+You can adjust any of the input parameters using the KFP UI and trigger
+your run again.
+
+Compile and deploy your pipeline
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After defining the pipeline in Python, you must compile the pipeline to
+an intermediate representation before you can submit it to the Kubeflow
+Pipelines service. The intermediate representation is a workflow
+specification in the form of a YAML file compressed into a tar.gz
+file. You need the KFP SDK to compile your pipeline.
+
+Install KFP SDK
+^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Install the KFP SDK following the instructions in the \ `Kubeflow
+pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
+
+Verify that the KFP SDK is installed with the following command:
+
+::
+
+    pip show kfp
+
+Verify that ``dsl-compile`` has been installed correctly as follows:
+
+::
+
+    which dsl-compile
+
+Compile your pipeline
+^^^^^^^^^^^^^^^^^^^^^
+
+You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
+CLI, or the KFP SDK. The following sections illustrate the workflow
+using the KFP UI and CLI.
+
+Complete the following from your gateway node to compile your pipeline.
+
+Modify your Python file with your S3 bucket name and IAM role ARN.
+
+Use the ``dsl-compile`` command from the command line to compile your
+pipeline as follows. Replace ``<path-to-python-file>`` with the path
+to your pipeline and ``<path-to-output>`` with the location where you
+want your tar.gz file to be.
+
+::
+
+    dsl-compile --py <path-to-python-file> --output <path-to-output>
+
+Upload and run the pipeline using the KFP CLI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Complete the following steps from the command line of your gateway node.
+KFP organizes runs of your pipeline as experiments. You have the option
+to specify an experiment name. If you do not specify one, the run will
+be listed under ‘Default’ experiment.
+
+Upload your pipeline as follows:
+
+::
+
+    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
+
+Your output should look like the following. Take note of the \ ``ID``.
+
+::
+
+    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
+
+    Pipeline Details
+    ------------------
+    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
+    Name         sm-pipeline
+    Description
+    Uploaded at  2020-04-30T20:22:39+00:00
+    ...
+    ...
+
+Create a run using the following command. The KFP CLI run command
+currently does not support specifying input parameters while creating
+the run. You need to update your parameters in the Python pipeline file
+before compiling. Replace ``<experiment-name>`` and ``<job-name>``
+with any names. Replace ``<pipeline-id>`` with the ID of your submitted
+pipeline.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
+
+You can also directly submit a run using the compiled pipeline package
+created as the output of the ``dsl-compile`` command.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
+
+Your output should look like the following:
+
+::
+
+    Creating experiment aws.
+    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
+    +--------------------------------------+--------+----------+---------------------------+
+    | run id                               | name   | status   | created at                |
+    +======================================+========+==========+===========================+
+    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
+    +--------------------------------------+--------+----------+---------------------------+
+
+Navigate to the UI to check the progress of the job
+
+Upload and run the pipeline using the KFP UI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  In the upper-right corner, choose ``+UploadPipeline``.
+
+-  Enter the pipeline name and description.
+
+-  Choose ``Upload a file`` and enter the path to the tar.gz file you
+   created using the CLI or with the Python SDK.
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  Find the pipeline you created.
+
+-  Choose ``+CreateRun``.
+
+-  Enter your input parameters.
+
+-  Choose ``Run``.
+
+Running predictions
+~~~~~~~~~~~~~~~~~~~
+
+Once your classification pipeline is deployed, you can run
+classification predictions against the endpoint that was created by the
+Deploy component. Use the KFP UI to check the output artifacts
+for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
+file to extract the endpoint name or check the Amazon SageMaker console
+in the region you used.
+
+Configure permissions to run predictions
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+If you want to run predictions from your gateway node, skip this
+section.
+
+If you want to use any other machine to run predictions, assign
+the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
+user used by the client machine. This permission is used to run
+predictions.
+
+On your gateway node, run the following to create a policy file:
+
+::
+
+    cat <<EoF > ./sagemaker-invoke.json
+    {
+        "Version": "2012-10-17",
+        "Statement": [
+            {
+                "Effect": "Allow",
+                "Action": [
+                    "sagemaker:InvokeEndpoint"
+                ],
+                "Resource": "*"
+            }
+        ]
+    }
+    EoF
+
+Attach the policy to the client node’s IAM role or IAM user.
+
+If your client machine has an IAM role attached, run the following.
+Replace ``<your-instance-IAM-role>`` with the name of the client
+node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+If your client machine has IAM user credentials configured, run the
+following. Replace ``<your_IAM_user_name>`` with the name of the client
+node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+Run predictions
+^^^^^^^^^^^^^^^
+
+Create a Python file from your client machine
+named ``mnist-predictions.py`` with the following content . Replace
+the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
+MNIST dataset, then creates a CSV from those digits and sends it to the
+endpoint for prediction. It then outputs the results.
+
+::
+
+    import pickle, gzip, numpy, urllib.request, json
+    from urllib.parse import urlparse
+    import json
+    import io
+    import boto3
+
+    ENDPOINT_NAME='<endpoint-name>'
+    REGION = '<region>'
+
+    # Load the dataset
+    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
+    with gzip.open('mnist.pkl.gz', 'rb') as f:
+        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
+
+    # Simple function to create a csv from our numpy array
+    def np2csv(arr):
+        csv = io.BytesIO()
+        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
+        return csv.getvalue().decode().rstrip()
+
+    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
+
+    payload = np2csv(train_set[0][30:31])
+
+    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
+                                       ContentType='text/csv',
+                                       Body=payload)
+    result = json.loads(response['Body'].read().decode())
+    print(result)
+
+Run the Python file as follows:
+
+::
+
+    python mnist-predictions.py
+
+View results and logs
+~~~~~~~~~~~~~~~~~~~~~
+
+When the pipeline is running, you can click on any component to check
+execution details, such as inputs and outputs. This will list the names
+of created resources.
+
+If the KFP request is successfully processed and an Amazon SageMaker job
+is created, the component logs in the KFP UI will provide a link to the
+job created in Amazon SageMaker. The CloudWatch logs will also be
+provided if the job is successfully created.
+
+If you run too many pipeline jobs on the same cluster, you may see an
+error message that indicates you do not have enough pods available. To
+fix this, log in to your gateway node and delete the pods created by the
+pipelines you are not using as follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Cleanup
+~~~~~~~
+
+When you’re finished with your pipeline, you need to cleanup your
+resources.
+
+From the KFP dashboard, terminate your pipeline runs if they do not exit
+properly by clicking ``Terminate``.
+
+If the ``Terminate`` option doesn’t work, log in to your gateway node
+and terminate all the pods created by your pipeline run manually as
+follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Using your AWS account, log in to the Amazon SageMaker service. Manually
+stop all training, batch transform, and HPO jobs. Delete models, data
+buckets and endpoints to avoid incurring any additional
+costs. Terminating the pipeline runs does not stop the jobs in Amazon
+SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 12:30:03[0m
[92mHash: 9b32c194ff38eab5dabf9dd258df8faca9ae8b3e[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: upgrade smdebug-rulesconfig to 0.1.4 (#1538)

[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 12:30:03[0m
[92mHash: 9b32c194ff38eab5dabf9dd258df8faca9ae8b3e[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: upgrade smdebug-rulesconfig to 0.1.4 (#1538)

[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-02 12:30:03[0m
[92mHash: 9b32c194ff38eab5dabf9dd258df8faca9ae8b3e[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: upgrade smdebug-rulesconfig to 0.1.4 (#1538)

[0m
@@ -1,835 +0,0 @@
-Using Amazon SageMaker Components
-=================================
-
-In this tutorial, you run a pipeline using Amazon SageMaker Components
-for Kubeflow Pipelines to train a classification model using Kmeans with
-the MNIST dataset. This workflow uses Kubeflow pipelines as the
-orchestrator and Amazon SageMaker as the backend to run the steps in the
-workflow. For the full code for this and other pipeline examples, see
-the `Sample AWS SageMaker Kubeflow
-Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
-For information on the components used, see the `KubeFlow Pipelines
-GitHub
-repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
-
-Setup
------
-
-To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
-Service (Amazon EKS) cluster and a gateway node to interact with that
-cluster. The following sections show the steps needed to set up these
-resources.
-
-Set up a gateway node
-~~~~~~~~~~~~~~~~~~~~~
-
-A gateway node is used to create an Amazon EKS cluster and access the
-Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
-as your gateway node. If you want to use a new EC2 instance, create one
-with the latest Ubuntu 18.04 DLAMI version from the AWS console using
-the steps in `Launching and Configuring a
-DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
-
-Complete the following steps to set up your gateway node. Depending on
-your environment, you may have certain requirements already configured.
-
-If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
-AWS
-Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
-you have an existing Amazon EKS cluster, use the credentials of the IAM
-role or user that has access to it.
-
-Add the following permissions to your user using the steps in `Changing
-Permissions for an IAM
-User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
-
--  CloudWatchLogsFullAccess
-
--  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
-
--  IAMFullAccess
-
--  AmazonS3FullAccess
-
--  AmazonEC2FullAccess
-
--  AmazonEKSAdminPolicy - Create this policy using the schema
-   from `Amazon EKS Identity-Based Policy
-   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
-
-Install the following on your gateway node to access the Amazon EKS
-cluster and KFP UI.
-
--  `AWS
-   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
-   If you are using an IAM user, configure your `Access Key ID, Secret
-   Access
-   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
-   preferred AWS Region by running: ``aws configure``
-
--  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-   version 0.1.31 and above.
-
--  ``eksctl`` version above 0.15.
-
--  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-   The version needs to match your Kubernetes version within 1 minor
-   version.
-
-Install \ ``boto3``.
-
-::
-
-    pip install boto3
-
-Set up an Amazon EKS cluster
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-set up an Amazon EKS cluster:
-
-If you do not have an existing Amazon EKS cluster, complete the
-following substeps. If you already have an Amazon EKS cluster, skip this
-step.
-
-Run the following from your command line to create an Amazon EKS Cluster
-with version 1.14 or above. Replace ``<your-cluster-name>`` with any
-name for your cluster.
-
-::
-
-    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
-
-When cluster creation is complete, verify that you have access to the
-cluster using the following command.
-
-::
-
-    kubectl get nodes
-
-Verify that the current kubectl context is the cluster you want to use
-with the following command. The current context is marked with an
-asterisk (\*) in the output.
-
-::
-
-    kubectl config get-contexts
-
-    CURRENT NAME     CLUSTER
-    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
-
-If the desired cluster is not configured as your current default, update
-the default with the following command.
-
-::
-
-    aws eks update-kubeconfig --name <clustername> --region us-east-1
-
-Install Kubeflow Pipelines
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-install Kubeflow Pipelines on your cluster.
-
-Install Kubeflow Pipelines on your cluster by following step 1
-of `Deploying Kubeflow Pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
-Your KFP version must be 0.5.0 or above.
-
-Verify that the Kubeflow Pipelines service and other related resources
-are running.
-
-::
-
-    kubectl -n kubeflow get all | grep pipeline
-
-Your output should look like the following.
-
-::
-
-    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
-    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
-    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
-    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
-    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
-    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
-    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
-    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
-    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
-    deployment.apps/ml-pipeline                       1/1     1            1           2d
-    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
-    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
-    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
-    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
-    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
-    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
-    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
-    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
-    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
-    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
-    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
-
-Access the KFP UI
-~~~~~~~~~~~~~~~~~
-
-The Kubeflow Pipelines UI is used for managing and tracking experiments,
-jobs, and runs on your cluster. You can use port forwarding to access
-the Kubeflow Pipelines UI from your gateway node.
-
-Set up port forwarding to the KFP UI service
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Verify that the KFP UI service is running using the following command:
-
-::
-
-    kubectl -n kubeflow get service ml-pipeline-ui
-
-    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
-    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
-
-Run the following command to setup port forwarding to the KFP UI
-service. This forwards the KFP UI to port 8080 on your gateway node and
-allows you to access the KFP UI from your browser.
-
-    **Note**
-
-    The port-forward from your remote machine drops if there is no
-    activity. Run this command again if your dashboard is unable to get
-    logs or updates. If the commands return an error, ensure that there
-    is no process already running on the port you are trying to use.
-
-::
-
-    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
-
-Your method of accessing the KFP UI depends on your gateway node type.
-
-Local machine as the gateway node
-
-Access the dashboard in your browser as follows:
-
-::
-
-    http://localhost:8080
-
-Click **Pipelines** to access the pipelines UI.
-
-EC2 instance as the gateway node
-
-You need to setup an SSH tunnel on your EC2 instance to access the
-Kubeflow dashboard from your local machine’s browser.
-
-From a new terminal session in your local machine, run the following.
-Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
-instance found on the EC2 console. You can also use the public DNS.
-Replace ``<path_to_key>`` with the path to the pem key used to access
-the gateway node.
-
-::
-
-    public_DNS_address=<public-DNS-of-gateway-node>
-    key=<path_to_key>
-
-    on Ubuntu:
-    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
-
-    or on Amazon Linux:
-    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
-
-Access the dashboard in your browser.
-
-::
-
-    http://localhost:9000
-
-Click **Pipelines** to access the KFP UI.
-
-Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
-SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
-need access to SageMaker. In this section, you create IAM Users/Roles to
-be used by Kubeflow Pipeline pods and Amazon SageMaker.
-
-Create a KFP execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Enable OIDC support on the Amazon EKS cluster with the following
-command. Replace ``<cluster_name>`` with the name of your cluster
-and ``<cluster_region>`` with the region your cluster is in.
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
-            --region <cluster-region> --approve
-
-Run the following to get the `OIDC <https://openid.net/connect/>`__
-issuer URL. This URL is in the
-form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
-
-::
-
-    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
-
-Run the following to create a file named ``trust.json``.
-Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
-include ``https://`` when in your OIDC issuer URL.
-Replace ``<AWS_account_number>`` with your AWS account number.
-
-::
-
-    OIDC_URL="<OIDC-URL>"
-    AWS_ACC_NUM="<AWS-account-number>"
-
-    # Run this to create trust.json file
-    cat <<EOF > trust.json
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "${OIDC_URL}:aud": "sts.amazonaws.com",
-              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
-            }
-          }
-        }
-      ]
-    }
-    EOF
-
-Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
-using the following command. This role is used by KFP pods to create
-Amazon SageMaker jobs from KFP components. Note the ARN returned in the
-output.
-
-::
-
-    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
-    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
-
-Edit your pipeline-runner service account with the following command.
-
-::
-
-    kubectl edit -n kubeflow serviceaccount pipeline-runner
-
-In the file, add the following Amazon EKS role annotation and
-replace ``<role_arn>`` with your role ARN.
-
-::
-
-    eks.amazonaws.com/role-arn: <role-arn>
-
-Your file should look like the following when you’ve added the Amazon
-EKS role annotation. Save the file.
-
-::
-
-    apiVersion: v1
-    kind: ServiceAccount
-    metadata:
-      annotations:
-        eks.amazonaws.com/role-arn: <role-arn>
-        kubectl.kubernetes.io/last-applied-configuration: |
-          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
-      creationTimestamp: "2020-04-16T05:48:06Z"
-      labels:
-        app: pipeline-runner
-        app.kubernetes.io/component: pipelines-runner
-        app.kubernetes.io/instance: pipelines-runner-0.2.0
-        app.kubernetes.io/managed-by: kfctl
-        app.kubernetes.io/name: pipelines-runner
-        app.kubernetes.io/part-of: kubeflow
-        app.kubernetes.io/version: 0.2.0
-      name: pipeline-runner
-      namespace: kubeflow
-      resourceVersion: "11787"
-      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
-      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
-    secrets:
-    - name: pipeline-runner-token-dkjrk
-
-Create an Amazon SageMaker execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The ``kfp-example-sagemaker-execution-role`` IAM role is used
-by Amazon SageMaker jobs to access AWS resources. For more information,
-see the IAM Permissions section. You provide this role as an input
-parameter when running the pipeline.
-
-Run the following to create the role. Note the ARN that is returned in
-your output.
-
-::
-
-    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
-
-    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
-    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
-
-Add access to additional IAM users or roles
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you use an intuitive IDE like Jupyter or want other people in your
-organization to use the cluster you set up, you can also give them
-access. The following steps run through this workflow using Amazon
-SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
-managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
-You use the notebook instance to create and manage Jupyter notebooks to
-create ML workflows. You can define, compile, deploy, and run your
-pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
-SageMaker notebook to run Jupyter, you need to install the `AWS
-CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
-the latest version
-of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-
-Follow the steps in `Create an Amazon SageMaker Notebook
-Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
-to create a Amazon SageMaker notebook instance if you do not already
-have one. Give the IAM role for this instance the ``S3FullAccess``
-permission.
-
-Amazon EKS clusters use IAM users and roles to control access to the
-cluster. The rules are implemented in a config map named ``aws-auth``.
-Only the user/role that has access to the cluster will be able to edit
-this config map. Run the following from the command line of your gateway
-node to get the IAM role of the notebook instance you created.
-Replace ``<instance-name>`` with the name of your instance.
-
-::
-
-    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
-
-This command outputs the IAM role ARN in
-the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
-of this ARN.
-
-Run the following to attach the policies the IAM role.
-Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
-
-::
-
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-``eksctl`` provides commands to read and edit the ``aws-auth`` config
-map. ``system:masters`` is one of the default user groups. You add the
-user to this group. The "system:masters" group has super user
-permissions to the cluster. You can also create a group with more
-restrictive permissions or you can bind permissions directly to users.
-Replace ``<IAM-Role-arn>`` with the ARN of the IAM
-role. ``<your_username>`` can be any unique username.
-
-::
-
-    eksctl create iamidentitymapping \
-        --cluster <cluster-name> \
-        --arn <IAM-Role-arn> \
-        --group system:masters \
-        --username <your-username> \
-        --region <region>
-
-Open the Jupyter notebook on your Amazon SageMaker instance and run the
-following to verify that it has access to the cluster.
-
-::
-
-    aws eks --region <region> update-kubeconfig --name <cluster-name>
-    kubectl -n kubeflow get all | grep pipeline
-
-Running the Kubeflow Pipeline
------------------------------
-
-Now that setup of your gateway node and Amazon EKS cluster is complete,
-you can create your classification pipeline. To create your pipeline,
-you need to define and compile it. You then deploy it and use it to run
-workflows. You can define your pipeline in Python and use the KFP
-dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows.
-
-Prepare datasets
-~~~~~~~~~~~~~~~~
-
-To run the pipelines, you need to have the datasets in an S3 bucket in
-your account. This bucket must be located in the region where you want
-to run Amazon SageMaker jobs. If you don’t have a bucket, create one
-using the steps in `Creating a
-bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
-
-From your gateway node, run the `sample dataset
-creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
-script to copy the datasets into your bucket. Change the bucket name in
-the script to the one you created.
-
-Create a Kubeflow Pipeline using Amazon SageMaker Components
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The full code for the MNIST classification pipeline is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
-
-Input Parameters
-^^^^^^^^^^^^^^^^
-
-The full MNIST classification pipeline has run-specific parameters that
-you must provide values for when creating a run. You must provide these
-parameters for each component of your pipeline. These parameters can
-also be updated when using other pipelines. We have provided default
-values for all parameters in the sample classification pipeline file.
-
-The following are the only parameters you may need to modify to run the
-sample pipelines. To modify these parameters, update their entries in
-the sample classification pipeline file.
-
--  **Role-ARN:** This must be the ARN of an IAM role that has full
-   Amazon SageMaker access in your AWS account. Use the ARN
-   of  ``kfp-example-pod-role``.
-
--  **The Dataset Buckets**: You must change the S3 bucket with the input
-   data for each of the components. Replace the following with the link
-   to your S3 bucket:
-
-   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **HPO channels for test/HPO channel for
-      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **Batch
-      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
-
--  **Output buckets:** Replace the output buckets with S3 buckets you
-   have write permission to. Replace the following with the link to your
-   S3 bucket:
-
-   -  **Training/HPO**:
-      ``output_location='s3://<your-s3-bucket-name>/output'``
-
-   -  **Batch Transform**:
-      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
-
--  **Region:**\ The default pipelines work in us-east-1. If your
-   cluster is in a different region, update the following:
-
-   -  The ``region='us-east-1'`` Parameter in the input list.
-
-   -  The algorithm images for Amazon SageMaker. If you use one of
-      the Amazon SageMaker built-in algorithm images, select the image
-      for your region. Construct the image name using the information
-      in `Common parameters for built-in
-      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
-      For Example:
-
-      ::
-
-          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
-
-   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
-      to copy the data to a bucket in the same region as the cluster.
-
-You can adjust any of the input parameters using the KFP UI and trigger
-your run again.
-
-Compile and deploy your pipeline
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After defining the pipeline in Python, you must compile the pipeline to
-an intermediate representation before you can submit it to the Kubeflow
-Pipelines service. The intermediate representation is a workflow
-specification in the form of a YAML file compressed into a tar.gz
-file. You need the KFP SDK to compile your pipeline.
-
-Install KFP SDK
-^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Install the KFP SDK following the instructions in the \ `Kubeflow
-pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
-
-Verify that the KFP SDK is installed with the following command:
-
-::
-
-    pip show kfp
-
-Verify that ``dsl-compile`` has been installed correctly as follows:
-
-::
-
-    which dsl-compile
-
-Compile your pipeline
-^^^^^^^^^^^^^^^^^^^^^
-
-You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
-CLI, or the KFP SDK. The following sections illustrate the workflow
-using the KFP UI and CLI.
-
-Complete the following from your gateway node to compile your pipeline.
-
-Modify your Python file with your S3 bucket name and IAM role ARN.
-
-Use the ``dsl-compile`` command from the command line to compile your
-pipeline as follows. Replace ``<path-to-python-file>`` with the path
-to your pipeline and ``<path-to-output>`` with the location where you
-want your tar.gz file to be.
-
-::
-
-    dsl-compile --py <path-to-python-file> --output <path-to-output>
-
-Upload and run the pipeline using the KFP CLI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Complete the following steps from the command line of your gateway node.
-KFP organizes runs of your pipeline as experiments. You have the option
-to specify an experiment name. If you do not specify one, the run will
-be listed under ‘Default’ experiment.
-
-Upload your pipeline as follows:
-
-::
-
-    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
-
-Your output should look like the following. Take note of the \ ``ID``.
-
-::
-
-    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
-
-    Pipeline Details
-    ------------------
-    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
-    Name         sm-pipeline
-    Description
-    Uploaded at  2020-04-30T20:22:39+00:00
-    ...
-    ...
-
-Create a run using the following command. The KFP CLI run command
-currently does not support specifying input parameters while creating
-the run. You need to update your parameters in the Python pipeline file
-before compiling. Replace ``<experiment-name>`` and ``<job-name>``
-with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
-
-You can also directly submit a run using the compiled pipeline package
-created as the output of the ``dsl-compile`` command.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
-
-Your output should look like the following:
-
-::
-
-    Creating experiment aws.
-    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
-    +--------------------------------------+--------+----------+---------------------------+
-    | run id                               | name   | status   | created at                |
-    +======================================+========+==========+===========================+
-    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
-    +--------------------------------------+--------+----------+---------------------------+
-
-Navigate to the UI to check the progress of the job
-
-Upload and run the pipeline using the KFP UI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
--  On the left panel, choose the **Pipelines** tab.
-
--  In the upper-right corner, choose ``+UploadPipeline``.
-
--  Enter the pipeline name and description.
-
--  Choose ``Upload a file`` and enter the path to the tar.gz file you
-   created using the CLI or with the Python SDK.
-
--  On the left panel, choose the **Pipelines** tab.
-
--  Find the pipeline you created.
-
--  Choose ``+CreateRun``.
-
--  Enter your input parameters.
-
--  Choose ``Run``.
-
-Running predictions
-~~~~~~~~~~~~~~~~~~~
-
-Once your classification pipeline is deployed, you can run
-classification predictions against the endpoint that was created by the
-Deploy component. Use the KFP UI to check the output artifacts
-for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
-file to extract the endpoint name or check the Amazon SageMaker console
-in the region you used.
-
-Configure permissions to run predictions
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-If you want to run predictions from your gateway node, skip this
-section.
-
-If you want to use any other machine to run predictions, assign
-the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
-user used by the client machine. This permission is used to run
-predictions.
-
-On your gateway node, run the following to create a policy file:
-
-::
-
-    cat <<EoF > ./sagemaker-invoke.json
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "sagemaker:InvokeEndpoint"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
-    EoF
-
-Attach the policy to the client node’s IAM role or IAM user.
-
-If your client machine has an IAM role attached, run the following.
-Replace ``<your-instance-IAM-role>`` with the name of the client
-node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-If your client machine has IAM user credentials configured, run the
-following. Replace ``<your_IAM_user_name>`` with the name of the client
-node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-Run predictions
-^^^^^^^^^^^^^^^
-
-Create a Python file from your client machine
-named ``mnist-predictions.py`` with the following content . Replace
-the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
-MNIST dataset, then creates a CSV from those digits and sends it to the
-endpoint for prediction. It then outputs the results.
-
-::
-
-    import pickle, gzip, numpy, urllib.request, json
-    from urllib.parse import urlparse
-    import json
-    import io
-    import boto3
-
-    ENDPOINT_NAME='<endpoint-name>'
-    REGION = '<region>'
-
-    # Load the dataset
-    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
-    with gzip.open('mnist.pkl.gz', 'rb') as f:
-        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
-
-    # Simple function to create a csv from our numpy array
-    def np2csv(arr):
-        csv = io.BytesIO()
-        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
-        return csv.getvalue().decode().rstrip()
-
-    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
-
-    payload = np2csv(train_set[0][30:31])
-
-    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
-                                       ContentType='text/csv',
-                                       Body=payload)
-    result = json.loads(response['Body'].read().decode())
-    print(result)
-
-Run the Python file as follows:
-
-::
-
-    python mnist-predictions.py
-
-View results and logs
-~~~~~~~~~~~~~~~~~~~~~
-
-When the pipeline is running, you can click on any component to check
-execution details, such as inputs and outputs. This will list the names
-of created resources.
-
-If the KFP request is successfully processed and an Amazon SageMaker job
-is created, the component logs in the KFP UI will provide a link to the
-job created in Amazon SageMaker. The CloudWatch logs will also be
-provided if the job is successfully created.
-
-If you run too many pipeline jobs on the same cluster, you may see an
-error message that indicates you do not have enough pods available. To
-fix this, log in to your gateway node and delete the pods created by the
-pipelines you are not using as follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Cleanup
-~~~~~~~
-
-When you’re finished with your pipeline, you need to cleanup your
-resources.
-
-From the KFP dashboard, terminate your pipeline runs if they do not exit
-properly by clicking ``Terminate``.
-
-If the ``Terminate`` option doesn’t work, log in to your gateway node
-and terminate all the pods created by your pipeline run manually as
-follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Using your AWS account, log in to the Amazon SageMaker service. Manually
-stop all training, batch transform, and HPO jobs. Delete models, data
-buckets and endpoints to avoid incurring any additional
-costs. Terminating the pipeline runs does not stop the jobs in Amazon
-SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:39:27[0m
[92mHash: 614fe7ec8dc206e680031e090ab00640ecb20a13[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: infra: add unit tests for v2 migration script file updaters and modifiers (#1536)

[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:39:27[0m
[92mHash: 614fe7ec8dc206e680031e090ab00640ecb20a13[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: infra: add unit tests for v2 migration script file updaters and modifiers (#1536)

[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:39:27[0m
[92mHash: 614fe7ec8dc206e680031e090ab00640ecb20a13[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: infra: add unit tests for v2 migration script file updaters and modifiers (#1536)

[0m
@@ -0,0 +1,835 @@
+Using Amazon SageMaker Components
+=================================
+
+In this tutorial, you run a pipeline using Amazon SageMaker Components
+for Kubeflow Pipelines to train a classification model using Kmeans with
+the MNIST dataset. This workflow uses Kubeflow pipelines as the
+orchestrator and Amazon SageMaker as the backend to run the steps in the
+workflow. For the full code for this and other pipeline examples, see
+the `Sample AWS SageMaker Kubeflow
+Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
+For information on the components used, see the `KubeFlow Pipelines
+GitHub
+repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
+
+Setup
+-----
+
+To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
+Service (Amazon EKS) cluster and a gateway node to interact with that
+cluster. The following sections show the steps needed to set up these
+resources.
+
+Set up a gateway node
+~~~~~~~~~~~~~~~~~~~~~
+
+A gateway node is used to create an Amazon EKS cluster and access the
+Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
+as your gateway node. If you want to use a new EC2 instance, create one
+with the latest Ubuntu 18.04 DLAMI version from the AWS console using
+the steps in `Launching and Configuring a
+DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
+
+Complete the following steps to set up your gateway node. Depending on
+your environment, you may have certain requirements already configured.
+
+If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
+AWS
+Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
+you have an existing Amazon EKS cluster, use the credentials of the IAM
+role or user that has access to it.
+
+Add the following permissions to your user using the steps in `Changing
+Permissions for an IAM
+User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
+
+-  CloudWatchLogsFullAccess
+
+-  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
+
+-  IAMFullAccess
+
+-  AmazonS3FullAccess
+
+-  AmazonEC2FullAccess
+
+-  AmazonEKSAdminPolicy - Create this policy using the schema
+   from `Amazon EKS Identity-Based Policy
+   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
+
+Install the following on your gateway node to access the Amazon EKS
+cluster and KFP UI.
+
+-  `AWS
+   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
+   If you are using an IAM user, configure your `Access Key ID, Secret
+   Access
+   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
+   preferred AWS Region by running: ``aws configure``
+
+-  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+   version 0.1.31 and above.
+
+-  ``eksctl`` version above 0.15.
+
+-  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+   The version needs to match your Kubernetes version within 1 minor
+   version.
+
+Install \ ``boto3``.
+
+::
+
+    pip install boto3
+
+Set up an Amazon EKS cluster
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+set up an Amazon EKS cluster:
+
+If you do not have an existing Amazon EKS cluster, complete the
+following substeps. If you already have an Amazon EKS cluster, skip this
+step.
+
+Run the following from your command line to create an Amazon EKS Cluster
+with version 1.14 or above. Replace ``<your-cluster-name>`` with any
+name for your cluster.
+
+::
+
+    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
+
+When cluster creation is complete, verify that you have access to the
+cluster using the following command.
+
+::
+
+    kubectl get nodes
+
+Verify that the current kubectl context is the cluster you want to use
+with the following command. The current context is marked with an
+asterisk (\*) in the output.
+
+::
+
+    kubectl config get-contexts
+
+    CURRENT NAME     CLUSTER
+    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
+
+If the desired cluster is not configured as your current default, update
+the default with the following command.
+
+::
+
+    aws eks update-kubeconfig --name <clustername> --region us-east-1
+
+Install Kubeflow Pipelines
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+install Kubeflow Pipelines on your cluster.
+
+Install Kubeflow Pipelines on your cluster by following step 1
+of `Deploying Kubeflow Pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
+Your KFP version must be 0.5.0 or above.
+
+Verify that the Kubeflow Pipelines service and other related resources
+are running.
+
+::
+
+    kubectl -n kubeflow get all | grep pipeline
+
+Your output should look like the following.
+
+::
+
+    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
+    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
+    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
+    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
+    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
+    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
+    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
+    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
+    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
+    deployment.apps/ml-pipeline                       1/1     1            1           2d
+    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
+    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
+    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
+    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
+    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
+    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
+    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
+    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
+    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
+    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
+    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
+
+Access the KFP UI
+~~~~~~~~~~~~~~~~~
+
+The Kubeflow Pipelines UI is used for managing and tracking experiments,
+jobs, and runs on your cluster. You can use port forwarding to access
+the Kubeflow Pipelines UI from your gateway node.
+
+Set up port forwarding to the KFP UI service
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Verify that the KFP UI service is running using the following command:
+
+::
+
+    kubectl -n kubeflow get service ml-pipeline-ui
+
+    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
+    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
+
+Run the following command to setup port forwarding to the KFP UI
+service. This forwards the KFP UI to port 8080 on your gateway node and
+allows you to access the KFP UI from your browser.
+
+    **Note**
+
+    The port-forward from your remote machine drops if there is no
+    activity. Run this command again if your dashboard is unable to get
+    logs or updates. If the commands return an error, ensure that there
+    is no process already running on the port you are trying to use.
+
+::
+
+    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
+
+Your method of accessing the KFP UI depends on your gateway node type.
+
+Local machine as the gateway node
+
+Access the dashboard in your browser as follows:
+
+::
+
+    http://localhost:8080
+
+Click **Pipelines** to access the pipelines UI.
+
+EC2 instance as the gateway node
+
+You need to setup an SSH tunnel on your EC2 instance to access the
+Kubeflow dashboard from your local machine’s browser.
+
+From a new terminal session in your local machine, run the following.
+Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
+instance found on the EC2 console. You can also use the public DNS.
+Replace ``<path_to_key>`` with the path to the pem key used to access
+the gateway node.
+
+::
+
+    public_DNS_address=<public-DNS-of-gateway-node>
+    key=<path_to_key>
+
+    on Ubuntu:
+    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
+
+    or on Amazon Linux:
+    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
+
+Access the dashboard in your browser.
+
+::
+
+    http://localhost:9000
+
+Click **Pipelines** to access the KFP UI.
+
+Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
+SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
+need access to SageMaker. In this section, you create IAM Users/Roles to
+be used by Kubeflow Pipeline pods and Amazon SageMaker.
+
+Create a KFP execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Enable OIDC support on the Amazon EKS cluster with the following
+command. Replace ``<cluster_name>`` with the name of your cluster
+and ``<cluster_region>`` with the region your cluster is in.
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
+            --region <cluster-region> --approve
+
+Run the following to get the `OIDC <https://openid.net/connect/>`__
+issuer URL. This URL is in the
+form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
+
+::
+
+    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
+
+Run the following to create a file named ``trust.json``.
+Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
+include ``https://`` when in your OIDC issuer URL.
+Replace ``<AWS_account_number>`` with your AWS account number.
+
+::
+
+    OIDC_URL="<OIDC-URL>"
+    AWS_ACC_NUM="<AWS-account-number>"
+
+    # Run this to create trust.json file
+    cat <<EOF > trust.json
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "${OIDC_URL}:aud": "sts.amazonaws.com",
+              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
+            }
+          }
+        }
+      ]
+    }
+    EOF
+
+Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
+using the following command. This role is used by KFP pods to create
+Amazon SageMaker jobs from KFP components. Note the ARN returned in the
+output.
+
+::
+
+    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
+    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
+
+Edit your pipeline-runner service account with the following command.
+
+::
+
+    kubectl edit -n kubeflow serviceaccount pipeline-runner
+
+In the file, add the following Amazon EKS role annotation and
+replace ``<role_arn>`` with your role ARN.
+
+::
+
+    eks.amazonaws.com/role-arn: <role-arn>
+
+Your file should look like the following when you’ve added the Amazon
+EKS role annotation. Save the file.
+
+::
+
+    apiVersion: v1
+    kind: ServiceAccount
+    metadata:
+      annotations:
+        eks.amazonaws.com/role-arn: <role-arn>
+        kubectl.kubernetes.io/last-applied-configuration: |
+          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
+      creationTimestamp: "2020-04-16T05:48:06Z"
+      labels:
+        app: pipeline-runner
+        app.kubernetes.io/component: pipelines-runner
+        app.kubernetes.io/instance: pipelines-runner-0.2.0
+        app.kubernetes.io/managed-by: kfctl
+        app.kubernetes.io/name: pipelines-runner
+        app.kubernetes.io/part-of: kubeflow
+        app.kubernetes.io/version: 0.2.0
+      name: pipeline-runner
+      namespace: kubeflow
+      resourceVersion: "11787"
+      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
+      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
+    secrets:
+    - name: pipeline-runner-token-dkjrk
+
+Create an Amazon SageMaker execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The ``kfp-example-sagemaker-execution-role`` IAM role is used
+by Amazon SageMaker jobs to access AWS resources. For more information,
+see the IAM Permissions section. You provide this role as an input
+parameter when running the pipeline.
+
+Run the following to create the role. Note the ARN that is returned in
+your output.
+
+::
+
+    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
+
+    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
+    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
+
+Add access to additional IAM users or roles
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If you use an intuitive IDE like Jupyter or want other people in your
+organization to use the cluster you set up, you can also give them
+access. The following steps run through this workflow using Amazon
+SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
+managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
+You use the notebook instance to create and manage Jupyter notebooks to
+create ML workflows. You can define, compile, deploy, and run your
+pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
+SageMaker notebook to run Jupyter, you need to install the `AWS
+CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
+the latest version
+of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+
+Follow the steps in `Create an Amazon SageMaker Notebook
+Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
+to create a Amazon SageMaker notebook instance if you do not already
+have one. Give the IAM role for this instance the ``S3FullAccess``
+permission.
+
+Amazon EKS clusters use IAM users and roles to control access to the
+cluster. The rules are implemented in a config map named ``aws-auth``.
+Only the user/role that has access to the cluster will be able to edit
+this config map. Run the following from the command line of your gateway
+node to get the IAM role of the notebook instance you created.
+Replace ``<instance-name>`` with the name of your instance.
+
+::
+
+    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
+
+This command outputs the IAM role ARN in
+the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
+of this ARN.
+
+Run the following to attach the policies the IAM role.
+Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
+
+::
+
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+``eksctl`` provides commands to read and edit the ``aws-auth`` config
+map. ``system:masters`` is one of the default user groups. You add the
+user to this group. The "system:masters" group has super user
+permissions to the cluster. You can also create a group with more
+restrictive permissions or you can bind permissions directly to users.
+Replace ``<IAM-Role-arn>`` with the ARN of the IAM
+role. ``<your_username>`` can be any unique username.
+
+::
+
+    eksctl create iamidentitymapping \
+        --cluster <cluster-name> \
+        --arn <IAM-Role-arn> \
+        --group system:masters \
+        --username <your-username> \
+        --region <region>
+
+Open the Jupyter notebook on your Amazon SageMaker instance and run the
+following to verify that it has access to the cluster.
+
+::
+
+    aws eks --region <region> update-kubeconfig --name <cluster-name>
+    kubectl -n kubeflow get all | grep pipeline
+
+Running the Kubeflow Pipeline
+-----------------------------
+
+Now that setup of your gateway node and Amazon EKS cluster is complete,
+you can create your classification pipeline. To create your pipeline,
+you need to define and compile it. You then deploy it and use it to run
+workflows. You can define your pipeline in Python and use the KFP
+dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
+workflows.
+
+Prepare datasets
+~~~~~~~~~~~~~~~~
+
+To run the pipelines, you need to have the datasets in an S3 bucket in
+your account. This bucket must be located in the region where you want
+to run Amazon SageMaker jobs. If you don’t have a bucket, create one
+using the steps in `Creating a
+bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
+
+From your gateway node, run the `sample dataset
+creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
+script to copy the datasets into your bucket. Change the bucket name in
+the script to the one you created.
+
+Create a Kubeflow Pipeline using Amazon SageMaker Components
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The full code for the MNIST classification pipeline is available in the
+`Kubeflow Github
+repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
+To use it, clone the example Python files to your gateway node.
+
+Input Parameters
+^^^^^^^^^^^^^^^^
+
+The full MNIST classification pipeline has run-specific parameters that
+you must provide values for when creating a run. You must provide these
+parameters for each component of your pipeline. These parameters can
+also be updated when using other pipelines. We have provided default
+values for all parameters in the sample classification pipeline file.
+
+The following are the only parameters you may need to modify to run the
+sample pipelines. To modify these parameters, update their entries in
+the sample classification pipeline file.
+
+-  **Role-ARN:** This must be the ARN of an IAM role that has full
+   Amazon SageMaker access in your AWS account. Use the ARN
+   of  ``kfp-example-pod-role``.
+
+-  **The Dataset Buckets**: You must change the S3 bucket with the input
+   data for each of the components. Replace the following with the link
+   to your S3 bucket:
+
+   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **HPO channels for test/HPO channel for
+      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **Batch
+      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
+
+-  **Output buckets:** Replace the output buckets with S3 buckets you
+   have write permission to. Replace the following with the link to your
+   S3 bucket:
+
+   -  **Training/HPO**:
+      ``output_location='s3://<your-s3-bucket-name>/output'``
+
+   -  **Batch Transform**:
+      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
+
+-  **Region:**\ The default pipelines work in us-east-1. If your
+   cluster is in a different region, update the following:
+
+   -  The ``region='us-east-1'`` Parameter in the input list.
+
+   -  The algorithm images for Amazon SageMaker. If you use one of
+      the Amazon SageMaker built-in algorithm images, select the image
+      for your region. Construct the image name using the information
+      in `Common parameters for built-in
+      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
+      For Example:
+
+      ::
+
+          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
+
+   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
+      to copy the data to a bucket in the same region as the cluster.
+
+You can adjust any of the input parameters using the KFP UI and trigger
+your run again.
+
+Compile and deploy your pipeline
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After defining the pipeline in Python, you must compile the pipeline to
+an intermediate representation before you can submit it to the Kubeflow
+Pipelines service. The intermediate representation is a workflow
+specification in the form of a YAML file compressed into a tar.gz
+file. You need the KFP SDK to compile your pipeline.
+
+Install KFP SDK
+^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Install the KFP SDK following the instructions in the \ `Kubeflow
+pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
+
+Verify that the KFP SDK is installed with the following command:
+
+::
+
+    pip show kfp
+
+Verify that ``dsl-compile`` has been installed correctly as follows:
+
+::
+
+    which dsl-compile
+
+Compile your pipeline
+^^^^^^^^^^^^^^^^^^^^^
+
+You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
+CLI, or the KFP SDK. The following sections illustrate the workflow
+using the KFP UI and CLI.
+
+Complete the following from your gateway node to compile your pipeline.
+
+Modify your Python file with your S3 bucket name and IAM role ARN.
+
+Use the ``dsl-compile`` command from the command line to compile your
+pipeline as follows. Replace ``<path-to-python-file>`` with the path
+to your pipeline and ``<path-to-output>`` with the location where you
+want your tar.gz file to be.
+
+::
+
+    dsl-compile --py <path-to-python-file> --output <path-to-output>
+
+Upload and run the pipeline using the KFP CLI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Complete the following steps from the command line of your gateway node.
+KFP organizes runs of your pipeline as experiments. You have the option
+to specify an experiment name. If you do not specify one, the run will
+be listed under ‘Default’ experiment.
+
+Upload your pipeline as follows:
+
+::
+
+    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
+
+Your output should look like the following. Take note of the \ ``ID``.
+
+::
+
+    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
+
+    Pipeline Details
+    ------------------
+    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
+    Name         sm-pipeline
+    Description
+    Uploaded at  2020-04-30T20:22:39+00:00
+    ...
+    ...
+
+Create a run using the following command. The KFP CLI run command
+currently does not support specifying input parameters while creating
+the run. You need to update your parameters in the Python pipeline file
+before compiling. Replace ``<experiment-name>`` and ``<job-name>``
+with any names. Replace ``<pipeline-id>`` with the ID of your submitted
+pipeline.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
+
+You can also directly submit a run using the compiled pipeline package
+created as the output of the ``dsl-compile`` command.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
+
+Your output should look like the following:
+
+::
+
+    Creating experiment aws.
+    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
+    +--------------------------------------+--------+----------+---------------------------+
+    | run id                               | name   | status   | created at                |
+    +======================================+========+==========+===========================+
+    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
+    +--------------------------------------+--------+----------+---------------------------+
+
+Navigate to the UI to check the progress of the job
+
+Upload and run the pipeline using the KFP UI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  In the upper-right corner, choose ``+UploadPipeline``.
+
+-  Enter the pipeline name and description.
+
+-  Choose ``Upload a file`` and enter the path to the tar.gz file you
+   created using the CLI or with the Python SDK.
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  Find the pipeline you created.
+
+-  Choose ``+CreateRun``.
+
+-  Enter your input parameters.
+
+-  Choose ``Run``.
+
+Running predictions
+~~~~~~~~~~~~~~~~~~~
+
+Once your classification pipeline is deployed, you can run
+classification predictions against the endpoint that was created by the
+Deploy component. Use the KFP UI to check the output artifacts
+for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
+file to extract the endpoint name or check the Amazon SageMaker console
+in the region you used.
+
+Configure permissions to run predictions
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+If you want to run predictions from your gateway node, skip this
+section.
+
+If you want to use any other machine to run predictions, assign
+the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
+user used by the client machine. This permission is used to run
+predictions.
+
+On your gateway node, run the following to create a policy file:
+
+::
+
+    cat <<EoF > ./sagemaker-invoke.json
+    {
+        "Version": "2012-10-17",
+        "Statement": [
+            {
+                "Effect": "Allow",
+                "Action": [
+                    "sagemaker:InvokeEndpoint"
+                ],
+                "Resource": "*"
+            }
+        ]
+    }
+    EoF
+
+Attach the policy to the client node’s IAM role or IAM user.
+
+If your client machine has an IAM role attached, run the following.
+Replace ``<your-instance-IAM-role>`` with the name of the client
+node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+If your client machine has IAM user credentials configured, run the
+following. Replace ``<your_IAM_user_name>`` with the name of the client
+node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+Run predictions
+^^^^^^^^^^^^^^^
+
+Create a Python file from your client machine
+named ``mnist-predictions.py`` with the following content . Replace
+the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
+MNIST dataset, then creates a CSV from those digits and sends it to the
+endpoint for prediction. It then outputs the results.
+
+::
+
+    import pickle, gzip, numpy, urllib.request, json
+    from urllib.parse import urlparse
+    import json
+    import io
+    import boto3
+
+    ENDPOINT_NAME='<endpoint-name>'
+    REGION = '<region>'
+
+    # Load the dataset
+    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
+    with gzip.open('mnist.pkl.gz', 'rb') as f:
+        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
+
+    # Simple function to create a csv from our numpy array
+    def np2csv(arr):
+        csv = io.BytesIO()
+        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
+        return csv.getvalue().decode().rstrip()
+
+    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
+
+    payload = np2csv(train_set[0][30:31])
+
+    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
+                                       ContentType='text/csv',
+                                       Body=payload)
+    result = json.loads(response['Body'].read().decode())
+    print(result)
+
+Run the Python file as follows:
+
+::
+
+    python mnist-predictions.py
+
+View results and logs
+~~~~~~~~~~~~~~~~~~~~~
+
+When the pipeline is running, you can click on any component to check
+execution details, such as inputs and outputs. This will list the names
+of created resources.
+
+If the KFP request is successfully processed and an Amazon SageMaker job
+is created, the component logs in the KFP UI will provide a link to the
+job created in Amazon SageMaker. The CloudWatch logs will also be
+provided if the job is successfully created.
+
+If you run too many pipeline jobs on the same cluster, you may see an
+error message that indicates you do not have enough pods available. To
+fix this, log in to your gateway node and delete the pods created by the
+pipelines you are not using as follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Cleanup
+~~~~~~~
+
+When you’re finished with your pipeline, you need to cleanup your
+resources.
+
+From the KFP dashboard, terminate your pipeline runs if they do not exit
+properly by clicking ``Terminate``.
+
+If the ``Terminate`` option doesn’t work, log in to your gateway node
+and terminate all the pods created by your pipeline run manually as
+follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Using your AWS account, log in to the Amazon SageMaker service. Manually
+stop all training, batch transform, and HPO jobs. Delete models, data
+buckets and endpoints to avoid incurring any additional
+costs. Terminating the pipeline runs does not stop the jobs in Amazon
+SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:11:22[0m
[92mHash: fe8f6731bc308b21caf088769d8ed4832706015f[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: fix: explicitly handle arguments in create_model for sklearn and xgboost (#1535)

[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:11:22[0m
[92mHash: fe8f6731bc308b21caf088769d8ed4832706015f[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: fix: explicitly handle arguments in create_model for sklearn and xgboost (#1535)

[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-06-01 12:11:22[0m
[92mHash: fe8f6731bc308b21caf088769d8ed4832706015f[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: fix: explicitly handle arguments in create_model for sklearn and xgboost (#1535)

[0m
@@ -1,835 +0,0 @@
-Using Amazon SageMaker Components
-=================================
-
-In this tutorial, you run a pipeline using Amazon SageMaker Components
-for Kubeflow Pipelines to train a classification model using Kmeans with
-the MNIST dataset. This workflow uses Kubeflow pipelines as the
-orchestrator and Amazon SageMaker as the backend to run the steps in the
-workflow. For the full code for this and other pipeline examples, see
-the `Sample AWS SageMaker Kubeflow
-Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
-For information on the components used, see the `KubeFlow Pipelines
-GitHub
-repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
-
-Setup
------
-
-To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
-Service (Amazon EKS) cluster and a gateway node to interact with that
-cluster. The following sections show the steps needed to set up these
-resources.
-
-Set up a gateway node
-~~~~~~~~~~~~~~~~~~~~~
-
-A gateway node is used to create an Amazon EKS cluster and access the
-Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
-as your gateway node. If you want to use a new EC2 instance, create one
-with the latest Ubuntu 18.04 DLAMI version from the AWS console using
-the steps in `Launching and Configuring a
-DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
-
-Complete the following steps to set up your gateway node. Depending on
-your environment, you may have certain requirements already configured.
-
-If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
-AWS
-Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
-you have an existing Amazon EKS cluster, use the credentials of the IAM
-role or user that has access to it.
-
-Add the following permissions to your user using the steps in `Changing
-Permissions for an IAM
-User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
-
--  CloudWatchLogsFullAccess
-
--  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
-
--  IAMFullAccess
-
--  AmazonS3FullAccess
-
--  AmazonEC2FullAccess
-
--  AmazonEKSAdminPolicy - Create this policy using the schema
-   from `Amazon EKS Identity-Based Policy
-   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
-
-Install the following on your gateway node to access the Amazon EKS
-cluster and KFP UI.
-
--  `AWS
-   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
-   If you are using an IAM user, configure your `Access Key ID, Secret
-   Access
-   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
-   preferred AWS Region by running: ``aws configure``
-
--  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-   version 0.1.31 and above.
-
--  ``eksctl`` version above 0.15.
-
--  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-   The version needs to match your Kubernetes version within 1 minor
-   version.
-
-Install \ ``boto3``.
-
-::
-
-    pip install boto3
-
-Set up an Amazon EKS cluster
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-set up an Amazon EKS cluster:
-
-If you do not have an existing Amazon EKS cluster, complete the
-following substeps. If you already have an Amazon EKS cluster, skip this
-step.
-
-Run the following from your command line to create an Amazon EKS Cluster
-with version 1.14 or above. Replace ``<your-cluster-name>`` with any
-name for your cluster.
-
-::
-
-    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
-
-When cluster creation is complete, verify that you have access to the
-cluster using the following command.
-
-::
-
-    kubectl get nodes
-
-Verify that the current kubectl context is the cluster you want to use
-with the following command. The current context is marked with an
-asterisk (\*) in the output.
-
-::
-
-    kubectl config get-contexts
-
-    CURRENT NAME     CLUSTER
-    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
-
-If the desired cluster is not configured as your current default, update
-the default with the following command.
-
-::
-
-    aws eks update-kubeconfig --name <clustername> --region us-east-1
-
-Install Kubeflow Pipelines
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-install Kubeflow Pipelines on your cluster.
-
-Install Kubeflow Pipelines on your cluster by following step 1
-of `Deploying Kubeflow Pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
-Your KFP version must be 0.5.0 or above.
-
-Verify that the Kubeflow Pipelines service and other related resources
-are running.
-
-::
-
-    kubectl -n kubeflow get all | grep pipeline
-
-Your output should look like the following.
-
-::
-
-    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
-    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
-    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
-    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
-    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
-    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
-    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
-    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
-    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
-    deployment.apps/ml-pipeline                       1/1     1            1           2d
-    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
-    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
-    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
-    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
-    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
-    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
-    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
-    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
-    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
-    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
-    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
-
-Access the KFP UI
-~~~~~~~~~~~~~~~~~
-
-The Kubeflow Pipelines UI is used for managing and tracking experiments,
-jobs, and runs on your cluster. You can use port forwarding to access
-the Kubeflow Pipelines UI from your gateway node.
-
-Set up port forwarding to the KFP UI service
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Verify that the KFP UI service is running using the following command:
-
-::
-
-    kubectl -n kubeflow get service ml-pipeline-ui
-
-    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
-    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
-
-Run the following command to setup port forwarding to the KFP UI
-service. This forwards the KFP UI to port 8080 on your gateway node and
-allows you to access the KFP UI from your browser.
-
-    **Note**
-
-    The port-forward from your remote machine drops if there is no
-    activity. Run this command again if your dashboard is unable to get
-    logs or updates. If the commands return an error, ensure that there
-    is no process already running on the port you are trying to use.
-
-::
-
-    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
-
-Your method of accessing the KFP UI depends on your gateway node type.
-
-Local machine as the gateway node
-
-Access the dashboard in your browser as follows:
-
-::
-
-    http://localhost:8080
-
-Click **Pipelines** to access the pipelines UI.
-
-EC2 instance as the gateway node
-
-You need to setup an SSH tunnel on your EC2 instance to access the
-Kubeflow dashboard from your local machine’s browser.
-
-From a new terminal session in your local machine, run the following.
-Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
-instance found on the EC2 console. You can also use the public DNS.
-Replace ``<path_to_key>`` with the path to the pem key used to access
-the gateway node.
-
-::
-
-    public_DNS_address=<public-DNS-of-gateway-node>
-    key=<path_to_key>
-
-    on Ubuntu:
-    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
-
-    or on Amazon Linux:
-    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
-
-Access the dashboard in your browser.
-
-::
-
-    http://localhost:9000
-
-Click **Pipelines** to access the KFP UI.
-
-Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
-SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
-need access to SageMaker. In this section, you create IAM Users/Roles to
-be used by Kubeflow Pipeline pods and Amazon SageMaker.
-
-Create a KFP execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Enable OIDC support on the Amazon EKS cluster with the following
-command. Replace ``<cluster_name>`` with the name of your cluster
-and ``<cluster_region>`` with the region your cluster is in.
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
-            --region <cluster-region> --approve
-
-Run the following to get the `OIDC <https://openid.net/connect/>`__
-issuer URL. This URL is in the
-form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
-
-::
-
-    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
-
-Run the following to create a file named ``trust.json``.
-Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
-include ``https://`` when in your OIDC issuer URL.
-Replace ``<AWS_account_number>`` with your AWS account number.
-
-::
-
-    OIDC_URL="<OIDC-URL>"
-    AWS_ACC_NUM="<AWS-account-number>"
-
-    # Run this to create trust.json file
-    cat <<EOF > trust.json
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "${OIDC_URL}:aud": "sts.amazonaws.com",
-              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
-            }
-          }
-        }
-      ]
-    }
-    EOF
-
-Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
-using the following command. This role is used by KFP pods to create
-Amazon SageMaker jobs from KFP components. Note the ARN returned in the
-output.
-
-::
-
-    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
-    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
-
-Edit your pipeline-runner service account with the following command.
-
-::
-
-    kubectl edit -n kubeflow serviceaccount pipeline-runner
-
-In the file, add the following Amazon EKS role annotation and
-replace ``<role_arn>`` with your role ARN.
-
-::
-
-    eks.amazonaws.com/role-arn: <role-arn>
-
-Your file should look like the following when you’ve added the Amazon
-EKS role annotation. Save the file.
-
-::
-
-    apiVersion: v1
-    kind: ServiceAccount
-    metadata:
-      annotations:
-        eks.amazonaws.com/role-arn: <role-arn>
-        kubectl.kubernetes.io/last-applied-configuration: |
-          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
-      creationTimestamp: "2020-04-16T05:48:06Z"
-      labels:
-        app: pipeline-runner
-        app.kubernetes.io/component: pipelines-runner
-        app.kubernetes.io/instance: pipelines-runner-0.2.0
-        app.kubernetes.io/managed-by: kfctl
-        app.kubernetes.io/name: pipelines-runner
-        app.kubernetes.io/part-of: kubeflow
-        app.kubernetes.io/version: 0.2.0
-      name: pipeline-runner
-      namespace: kubeflow
-      resourceVersion: "11787"
-      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
-      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
-    secrets:
-    - name: pipeline-runner-token-dkjrk
-
-Create an Amazon SageMaker execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The ``kfp-example-sagemaker-execution-role`` IAM role is used
-by Amazon SageMaker jobs to access AWS resources. For more information,
-see the IAM Permissions section. You provide this role as an input
-parameter when running the pipeline.
-
-Run the following to create the role. Note the ARN that is returned in
-your output.
-
-::
-
-    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
-
-    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
-    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
-
-Add access to additional IAM users or roles
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you use an intuitive IDE like Jupyter or want other people in your
-organization to use the cluster you set up, you can also give them
-access. The following steps run through this workflow using Amazon
-SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
-managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
-You use the notebook instance to create and manage Jupyter notebooks to
-create ML workflows. You can define, compile, deploy, and run your
-pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
-SageMaker notebook to run Jupyter, you need to install the `AWS
-CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
-the latest version
-of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-
-Follow the steps in `Create an Amazon SageMaker Notebook
-Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
-to create a Amazon SageMaker notebook instance if you do not already
-have one. Give the IAM role for this instance the ``S3FullAccess``
-permission.
-
-Amazon EKS clusters use IAM users and roles to control access to the
-cluster. The rules are implemented in a config map named ``aws-auth``.
-Only the user/role that has access to the cluster will be able to edit
-this config map. Run the following from the command line of your gateway
-node to get the IAM role of the notebook instance you created.
-Replace ``<instance-name>`` with the name of your instance.
-
-::
-
-    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
-
-This command outputs the IAM role ARN in
-the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
-of this ARN.
-
-Run the following to attach the policies the IAM role.
-Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
-
-::
-
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-``eksctl`` provides commands to read and edit the ``aws-auth`` config
-map. ``system:masters`` is one of the default user groups. You add the
-user to this group. The "system:masters" group has super user
-permissions to the cluster. You can also create a group with more
-restrictive permissions or you can bind permissions directly to users.
-Replace ``<IAM-Role-arn>`` with the ARN of the IAM
-role. ``<your_username>`` can be any unique username.
-
-::
-
-    eksctl create iamidentitymapping \
-        --cluster <cluster-name> \
-        --arn <IAM-Role-arn> \
-        --group system:masters \
-        --username <your-username> \
-        --region <region>
-
-Open the Jupyter notebook on your Amazon SageMaker instance and run the
-following to verify that it has access to the cluster.
-
-::
-
-    aws eks --region <region> update-kubeconfig --name <cluster-name>
-    kubectl -n kubeflow get all | grep pipeline
-
-Running the Kubeflow Pipeline
------------------------------
-
-Now that setup of your gateway node and Amazon EKS cluster is complete,
-you can create your classification pipeline. To create your pipeline,
-you need to define and compile it. You then deploy it and use it to run
-workflows. You can define your pipeline in Python and use the KFP
-dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows.
-
-Prepare datasets
-~~~~~~~~~~~~~~~~
-
-To run the pipelines, you need to have the datasets in an S3 bucket in
-your account. This bucket must be located in the region where you want
-to run Amazon SageMaker jobs. If you don’t have a bucket, create one
-using the steps in `Creating a
-bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
-
-From your gateway node, run the `sample dataset
-creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
-script to copy the datasets into your bucket. Change the bucket name in
-the script to the one you created.
-
-Create a Kubeflow Pipeline using Amazon SageMaker Components
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The full code for the MNIST classification pipeline is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
-
-Input Parameters
-^^^^^^^^^^^^^^^^
-
-The full MNIST classification pipeline has run-specific parameters that
-you must provide values for when creating a run. You must provide these
-parameters for each component of your pipeline. These parameters can
-also be updated when using other pipelines. We have provided default
-values for all parameters in the sample classification pipeline file.
-
-The following are the only parameters you may need to modify to run the
-sample pipelines. To modify these parameters, update their entries in
-the sample classification pipeline file.
-
--  **Role-ARN:** This must be the ARN of an IAM role that has full
-   Amazon SageMaker access in your AWS account. Use the ARN
-   of  ``kfp-example-pod-role``.
-
--  **The Dataset Buckets**: You must change the S3 bucket with the input
-   data for each of the components. Replace the following with the link
-   to your S3 bucket:
-
-   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **HPO channels for test/HPO channel for
-      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **Batch
-      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
-
--  **Output buckets:** Replace the output buckets with S3 buckets you
-   have write permission to. Replace the following with the link to your
-   S3 bucket:
-
-   -  **Training/HPO**:
-      ``output_location='s3://<your-s3-bucket-name>/output'``
-
-   -  **Batch Transform**:
-      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
-
--  **Region:**\ The default pipelines work in us-east-1. If your
-   cluster is in a different region, update the following:
-
-   -  The ``region='us-east-1'`` Parameter in the input list.
-
-   -  The algorithm images for Amazon SageMaker. If you use one of
-      the Amazon SageMaker built-in algorithm images, select the image
-      for your region. Construct the image name using the information
-      in `Common parameters for built-in
-      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
-      For Example:
-
-      ::
-
-          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
-
-   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
-      to copy the data to a bucket in the same region as the cluster.
-
-You can adjust any of the input parameters using the KFP UI and trigger
-your run again.
-
-Compile and deploy your pipeline
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After defining the pipeline in Python, you must compile the pipeline to
-an intermediate representation before you can submit it to the Kubeflow
-Pipelines service. The intermediate representation is a workflow
-specification in the form of a YAML file compressed into a tar.gz
-file. You need the KFP SDK to compile your pipeline.
-
-Install KFP SDK
-^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Install the KFP SDK following the instructions in the \ `Kubeflow
-pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
-
-Verify that the KFP SDK is installed with the following command:
-
-::
-
-    pip show kfp
-
-Verify that ``dsl-compile`` has been installed correctly as follows:
-
-::
-
-    which dsl-compile
-
-Compile your pipeline
-^^^^^^^^^^^^^^^^^^^^^
-
-You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
-CLI, or the KFP SDK. The following sections illustrate the workflow
-using the KFP UI and CLI.
-
-Complete the following from your gateway node to compile your pipeline.
-
-Modify your Python file with your S3 bucket name and IAM role ARN.
-
-Use the ``dsl-compile`` command from the command line to compile your
-pipeline as follows. Replace ``<path-to-python-file>`` with the path
-to your pipeline and ``<path-to-output>`` with the location where you
-want your tar.gz file to be.
-
-::
-
-    dsl-compile --py <path-to-python-file> --output <path-to-output>
-
-Upload and run the pipeline using the KFP CLI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Complete the following steps from the command line of your gateway node.
-KFP organizes runs of your pipeline as experiments. You have the option
-to specify an experiment name. If you do not specify one, the run will
-be listed under ‘Default’ experiment.
-
-Upload your pipeline as follows:
-
-::
-
-    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
-
-Your output should look like the following. Take note of the \ ``ID``.
-
-::
-
-    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
-
-    Pipeline Details
-    ------------------
-    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
-    Name         sm-pipeline
-    Description
-    Uploaded at  2020-04-30T20:22:39+00:00
-    ...
-    ...
-
-Create a run using the following command. The KFP CLI run command
-currently does not support specifying input parameters while creating
-the run. You need to update your parameters in the Python pipeline file
-before compiling. Replace ``<experiment-name>`` and ``<job-name>``
-with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
-
-You can also directly submit a run using the compiled pipeline package
-created as the output of the ``dsl-compile`` command.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
-
-Your output should look like the following:
-
-::
-
-    Creating experiment aws.
-    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
-    +--------------------------------------+--------+----------+---------------------------+
-    | run id                               | name   | status   | created at                |
-    +======================================+========+==========+===========================+
-    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
-    +--------------------------------------+--------+----------+---------------------------+
-
-Navigate to the UI to check the progress of the job
-
-Upload and run the pipeline using the KFP UI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
--  On the left panel, choose the **Pipelines** tab.
-
--  In the upper-right corner, choose ``+UploadPipeline``.
-
--  Enter the pipeline name and description.
-
--  Choose ``Upload a file`` and enter the path to the tar.gz file you
-   created using the CLI or with the Python SDK.
-
--  On the left panel, choose the **Pipelines** tab.
-
--  Find the pipeline you created.
-
--  Choose ``+CreateRun``.
-
--  Enter your input parameters.
-
--  Choose ``Run``.
-
-Running predictions
-~~~~~~~~~~~~~~~~~~~
-
-Once your classification pipeline is deployed, you can run
-classification predictions against the endpoint that was created by the
-Deploy component. Use the KFP UI to check the output artifacts
-for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
-file to extract the endpoint name or check the Amazon SageMaker console
-in the region you used.
-
-Configure permissions to run predictions
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-If you want to run predictions from your gateway node, skip this
-section.
-
-If you want to use any other machine to run predictions, assign
-the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
-user used by the client machine. This permission is used to run
-predictions.
-
-On your gateway node, run the following to create a policy file:
-
-::
-
-    cat <<EoF > ./sagemaker-invoke.json
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "sagemaker:InvokeEndpoint"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
-    EoF
-
-Attach the policy to the client node’s IAM role or IAM user.
-
-If your client machine has an IAM role attached, run the following.
-Replace ``<your-instance-IAM-role>`` with the name of the client
-node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-If your client machine has IAM user credentials configured, run the
-following. Replace ``<your_IAM_user_name>`` with the name of the client
-node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-Run predictions
-^^^^^^^^^^^^^^^
-
-Create a Python file from your client machine
-named ``mnist-predictions.py`` with the following content . Replace
-the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
-MNIST dataset, then creates a CSV from those digits and sends it to the
-endpoint for prediction. It then outputs the results.
-
-::
-
-    import pickle, gzip, numpy, urllib.request, json
-    from urllib.parse import urlparse
-    import json
-    import io
-    import boto3
-
-    ENDPOINT_NAME='<endpoint-name>'
-    REGION = '<region>'
-
-    # Load the dataset
-    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
-    with gzip.open('mnist.pkl.gz', 'rb') as f:
-        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
-
-    # Simple function to create a csv from our numpy array
-    def np2csv(arr):
-        csv = io.BytesIO()
-        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
-        return csv.getvalue().decode().rstrip()
-
-    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
-
-    payload = np2csv(train_set[0][30:31])
-
-    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
-                                       ContentType='text/csv',
-                                       Body=payload)
-    result = json.loads(response['Body'].read().decode())
-    print(result)
-
-Run the Python file as follows:
-
-::
-
-    python mnist-predictions.py
-
-View results and logs
-~~~~~~~~~~~~~~~~~~~~~
-
-When the pipeline is running, you can click on any component to check
-execution details, such as inputs and outputs. This will list the names
-of created resources.
-
-If the KFP request is successfully processed and an Amazon SageMaker job
-is created, the component logs in the KFP UI will provide a link to the
-job created in Amazon SageMaker. The CloudWatch logs will also be
-provided if the job is successfully created.
-
-If you run too many pipeline jobs on the same cluster, you may see an
-error message that indicates you do not have enough pods available. To
-fix this, log in to your gateway node and delete the pods created by the
-pipelines you are not using as follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Cleanup
-~~~~~~~
-
-When you’re finished with your pipeline, you need to cleanup your
-resources.
-
-From the KFP dashboard, terminate your pipeline runs if they do not exit
-properly by clicking ``Terminate``.
-
-If the ``Terminate`` option doesn’t work, log in to your gateway node
-and terminate all the pods created by your pipeline run manually as
-follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Using your AWS account, log in to the Amazon SageMaker service. Manually
-stop all training, batch transform, and HPO jobs. Delete models, data
-buckets and endpoints to avoid incurring any additional
-costs. Terminating the pipeline runs does not stop the jobs in Amazon
-SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 18:17:28[0m
[92mHash: a680be1d717d9bf5fff36ac1089c11cc978fe436[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: convert TF legacy mode parameters to hyperparameters in v2 migration script (#1534)

[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 18:17:28[0m
[92mHash: a680be1d717d9bf5fff36ac1089c11cc978fe436[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: convert TF legacy mode parameters to hyperparameters in v2 migration script (#1534)

[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 18:17:28[0m
[92mHash: a680be1d717d9bf5fff36ac1089c11cc978fe436[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: convert TF legacy mode parameters to hyperparameters in v2 migration script (#1534)

[0m
@@ -0,0 +1,835 @@
+Using Amazon SageMaker Components
+=================================
+
+In this tutorial, you run a pipeline using Amazon SageMaker Components
+for Kubeflow Pipelines to train a classification model using Kmeans with
+the MNIST dataset. This workflow uses Kubeflow pipelines as the
+orchestrator and Amazon SageMaker as the backend to run the steps in the
+workflow. For the full code for this and other pipeline examples, see
+the `Sample AWS SageMaker Kubeflow
+Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
+For information on the components used, see the `KubeFlow Pipelines
+GitHub
+repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
+
+Setup
+-----
+
+To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
+Service (Amazon EKS) cluster and a gateway node to interact with that
+cluster. The following sections show the steps needed to set up these
+resources.
+
+Set up a gateway node
+~~~~~~~~~~~~~~~~~~~~~
+
+A gateway node is used to create an Amazon EKS cluster and access the
+Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
+as your gateway node. If you want to use a new EC2 instance, create one
+with the latest Ubuntu 18.04 DLAMI version from the AWS console using
+the steps in `Launching and Configuring a
+DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
+
+Complete the following steps to set up your gateway node. Depending on
+your environment, you may have certain requirements already configured.
+
+If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
+AWS
+Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
+you have an existing Amazon EKS cluster, use the credentials of the IAM
+role or user that has access to it.
+
+Add the following permissions to your user using the steps in `Changing
+Permissions for an IAM
+User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
+
+-  CloudWatchLogsFullAccess
+
+-  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
+
+-  IAMFullAccess
+
+-  AmazonS3FullAccess
+
+-  AmazonEC2FullAccess
+
+-  AmazonEKSAdminPolicy - Create this policy using the schema
+   from `Amazon EKS Identity-Based Policy
+   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
+
+Install the following on your gateway node to access the Amazon EKS
+cluster and KFP UI.
+
+-  `AWS
+   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
+   If you are using an IAM user, configure your `Access Key ID, Secret
+   Access
+   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
+   preferred AWS Region by running: ``aws configure``
+
+-  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+   version 0.1.31 and above.
+
+-  ``eksctl`` version above 0.15.
+
+-  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+   The version needs to match your Kubernetes version within 1 minor
+   version.
+
+Install \ ``boto3``.
+
+::
+
+    pip install boto3
+
+Set up an Amazon EKS cluster
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+set up an Amazon EKS cluster:
+
+If you do not have an existing Amazon EKS cluster, complete the
+following substeps. If you already have an Amazon EKS cluster, skip this
+step.
+
+Run the following from your command line to create an Amazon EKS Cluster
+with version 1.14 or above. Replace ``<your-cluster-name>`` with any
+name for your cluster.
+
+::
+
+    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
+
+When cluster creation is complete, verify that you have access to the
+cluster using the following command.
+
+::
+
+    kubectl get nodes
+
+Verify that the current kubectl context is the cluster you want to use
+with the following command. The current context is marked with an
+asterisk (\*) in the output.
+
+::
+
+    kubectl config get-contexts
+
+    CURRENT NAME     CLUSTER
+    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
+
+If the desired cluster is not configured as your current default, update
+the default with the following command.
+
+::
+
+    aws eks update-kubeconfig --name <clustername> --region us-east-1
+
+Install Kubeflow Pipelines
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+install Kubeflow Pipelines on your cluster.
+
+Install Kubeflow Pipelines on your cluster by following step 1
+of `Deploying Kubeflow Pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
+Your KFP version must be 0.5.0 or above.
+
+Verify that the Kubeflow Pipelines service and other related resources
+are running.
+
+::
+
+    kubectl -n kubeflow get all | grep pipeline
+
+Your output should look like the following.
+
+::
+
+    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
+    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
+    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
+    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
+    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
+    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
+    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
+    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
+    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
+    deployment.apps/ml-pipeline                       1/1     1            1           2d
+    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
+    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
+    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
+    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
+    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
+    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
+    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
+    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
+    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
+    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
+    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
+
+Access the KFP UI
+~~~~~~~~~~~~~~~~~
+
+The Kubeflow Pipelines UI is used for managing and tracking experiments,
+jobs, and runs on your cluster. You can use port forwarding to access
+the Kubeflow Pipelines UI from your gateway node.
+
+Set up port forwarding to the KFP UI service
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Verify that the KFP UI service is running using the following command:
+
+::
+
+    kubectl -n kubeflow get service ml-pipeline-ui
+
+    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
+    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
+
+Run the following command to setup port forwarding to the KFP UI
+service. This forwards the KFP UI to port 8080 on your gateway node and
+allows you to access the KFP UI from your browser.
+
+    **Note**
+
+    The port-forward from your remote machine drops if there is no
+    activity. Run this command again if your dashboard is unable to get
+    logs or updates. If the commands return an error, ensure that there
+    is no process already running on the port you are trying to use.
+
+::
+
+    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
+
+Your method of accessing the KFP UI depends on your gateway node type.
+
+Local machine as the gateway node
+
+Access the dashboard in your browser as follows:
+
+::
+
+    http://localhost:8080
+
+Click **Pipelines** to access the pipelines UI.
+
+EC2 instance as the gateway node
+
+You need to setup an SSH tunnel on your EC2 instance to access the
+Kubeflow dashboard from your local machine’s browser.
+
+From a new terminal session in your local machine, run the following.
+Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
+instance found on the EC2 console. You can also use the public DNS.
+Replace ``<path_to_key>`` with the path to the pem key used to access
+the gateway node.
+
+::
+
+    public_DNS_address=<public-DNS-of-gateway-node>
+    key=<path_to_key>
+
+    on Ubuntu:
+    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
+
+    or on Amazon Linux:
+    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
+
+Access the dashboard in your browser.
+
+::
+
+    http://localhost:9000
+
+Click **Pipelines** to access the KFP UI.
+
+Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
+SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
+need access to SageMaker. In this section, you create IAM Users/Roles to
+be used by Kubeflow Pipeline pods and Amazon SageMaker.
+
+Create a KFP execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Enable OIDC support on the Amazon EKS cluster with the following
+command. Replace ``<cluster_name>`` with the name of your cluster
+and ``<cluster_region>`` with the region your cluster is in.
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
+            --region <cluster-region> --approve
+
+Run the following to get the `OIDC <https://openid.net/connect/>`__
+issuer URL. This URL is in the
+form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
+
+::
+
+    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
+
+Run the following to create a file named ``trust.json``.
+Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
+include ``https://`` when in your OIDC issuer URL.
+Replace ``<AWS_account_number>`` with your AWS account number.
+
+::
+
+    OIDC_URL="<OIDC-URL>"
+    AWS_ACC_NUM="<AWS-account-number>"
+
+    # Run this to create trust.json file
+    cat <<EOF > trust.json
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "${OIDC_URL}:aud": "sts.amazonaws.com",
+              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
+            }
+          }
+        }
+      ]
+    }
+    EOF
+
+Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
+using the following command. This role is used by KFP pods to create
+Amazon SageMaker jobs from KFP components. Note the ARN returned in the
+output.
+
+::
+
+    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
+    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
+
+Edit your pipeline-runner service account with the following command.
+
+::
+
+    kubectl edit -n kubeflow serviceaccount pipeline-runner
+
+In the file, add the following Amazon EKS role annotation and
+replace ``<role_arn>`` with your role ARN.
+
+::
+
+    eks.amazonaws.com/role-arn: <role-arn>
+
+Your file should look like the following when you’ve added the Amazon
+EKS role annotation. Save the file.
+
+::
+
+    apiVersion: v1
+    kind: ServiceAccount
+    metadata:
+      annotations:
+        eks.amazonaws.com/role-arn: <role-arn>
+        kubectl.kubernetes.io/last-applied-configuration: |
+          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
+      creationTimestamp: "2020-04-16T05:48:06Z"
+      labels:
+        app: pipeline-runner
+        app.kubernetes.io/component: pipelines-runner
+        app.kubernetes.io/instance: pipelines-runner-0.2.0
+        app.kubernetes.io/managed-by: kfctl
+        app.kubernetes.io/name: pipelines-runner
+        app.kubernetes.io/part-of: kubeflow
+        app.kubernetes.io/version: 0.2.0
+      name: pipeline-runner
+      namespace: kubeflow
+      resourceVersion: "11787"
+      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
+      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
+    secrets:
+    - name: pipeline-runner-token-dkjrk
+
+Create an Amazon SageMaker execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The ``kfp-example-sagemaker-execution-role`` IAM role is used
+by Amazon SageMaker jobs to access AWS resources. For more information,
+see the IAM Permissions section. You provide this role as an input
+parameter when running the pipeline.
+
+Run the following to create the role. Note the ARN that is returned in
+your output.
+
+::
+
+    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
+
+    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
+    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
+
+Add access to additional IAM users or roles
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If you use an intuitive IDE like Jupyter or want other people in your
+organization to use the cluster you set up, you can also give them
+access. The following steps run through this workflow using Amazon
+SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
+managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
+You use the notebook instance to create and manage Jupyter notebooks to
+create ML workflows. You can define, compile, deploy, and run your
+pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
+SageMaker notebook to run Jupyter, you need to install the `AWS
+CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
+the latest version
+of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+
+Follow the steps in `Create an Amazon SageMaker Notebook
+Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
+to create a Amazon SageMaker notebook instance if you do not already
+have one. Give the IAM role for this instance the ``S3FullAccess``
+permission.
+
+Amazon EKS clusters use IAM users and roles to control access to the
+cluster. The rules are implemented in a config map named ``aws-auth``.
+Only the user/role that has access to the cluster will be able to edit
+this config map. Run the following from the command line of your gateway
+node to get the IAM role of the notebook instance you created.
+Replace ``<instance-name>`` with the name of your instance.
+
+::
+
+    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
+
+This command outputs the IAM role ARN in
+the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
+of this ARN.
+
+Run the following to attach the policies the IAM role.
+Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
+
+::
+
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+``eksctl`` provides commands to read and edit the ``aws-auth`` config
+map. ``system:masters`` is one of the default user groups. You add the
+user to this group. The "system:masters" group has super user
+permissions to the cluster. You can also create a group with more
+restrictive permissions or you can bind permissions directly to users.
+Replace ``<IAM-Role-arn>`` with the ARN of the IAM
+role. ``<your_username>`` can be any unique username.
+
+::
+
+    eksctl create iamidentitymapping \
+        --cluster <cluster-name> \
+        --arn <IAM-Role-arn> \
+        --group system:masters \
+        --username <your-username> \
+        --region <region>
+
+Open the Jupyter notebook on your Amazon SageMaker instance and run the
+following to verify that it has access to the cluster.
+
+::
+
+    aws eks --region <region> update-kubeconfig --name <cluster-name>
+    kubectl -n kubeflow get all | grep pipeline
+
+Running the Kubeflow Pipeline
+-----------------------------
+
+Now that setup of your gateway node and Amazon EKS cluster is complete,
+you can create your classification pipeline. To create your pipeline,
+you need to define and compile it. You then deploy it and use it to run
+workflows. You can define your pipeline in Python and use the KFP
+dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
+workflows.
+
+Prepare datasets
+~~~~~~~~~~~~~~~~
+
+To run the pipelines, you need to have the datasets in an S3 bucket in
+your account. This bucket must be located in the region where you want
+to run Amazon SageMaker jobs. If you don’t have a bucket, create one
+using the steps in `Creating a
+bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
+
+From your gateway node, run the `sample dataset
+creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
+script to copy the datasets into your bucket. Change the bucket name in
+the script to the one you created.
+
+Create a Kubeflow Pipeline using Amazon SageMaker Components
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The full code for the MNIST classification pipeline is available in the
+`Kubeflow Github
+repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
+To use it, clone the example Python files to your gateway node.
+
+Input Parameters
+^^^^^^^^^^^^^^^^
+
+The full MNIST classification pipeline has run-specific parameters that
+you must provide values for when creating a run. You must provide these
+parameters for each component of your pipeline. These parameters can
+also be updated when using other pipelines. We have provided default
+values for all parameters in the sample classification pipeline file.
+
+The following are the only parameters you may need to modify to run the
+sample pipelines. To modify these parameters, update their entries in
+the sample classification pipeline file.
+
+-  **Role-ARN:** This must be the ARN of an IAM role that has full
+   Amazon SageMaker access in your AWS account. Use the ARN
+   of  ``kfp-example-pod-role``.
+
+-  **The Dataset Buckets**: You must change the S3 bucket with the input
+   data for each of the components. Replace the following with the link
+   to your S3 bucket:
+
+   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **HPO channels for test/HPO channel for
+      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **Batch
+      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
+
+-  **Output buckets:** Replace the output buckets with S3 buckets you
+   have write permission to. Replace the following with the link to your
+   S3 bucket:
+
+   -  **Training/HPO**:
+      ``output_location='s3://<your-s3-bucket-name>/output'``
+
+   -  **Batch Transform**:
+      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
+
+-  **Region:**\ The default pipelines work in us-east-1. If your
+   cluster is in a different region, update the following:
+
+   -  The ``region='us-east-1'`` Parameter in the input list.
+
+   -  The algorithm images for Amazon SageMaker. If you use one of
+      the Amazon SageMaker built-in algorithm images, select the image
+      for your region. Construct the image name using the information
+      in `Common parameters for built-in
+      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
+      For Example:
+
+      ::
+
+          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
+
+   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
+      to copy the data to a bucket in the same region as the cluster.
+
+You can adjust any of the input parameters using the KFP UI and trigger
+your run again.
+
+Compile and deploy your pipeline
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After defining the pipeline in Python, you must compile the pipeline to
+an intermediate representation before you can submit it to the Kubeflow
+Pipelines service. The intermediate representation is a workflow
+specification in the form of a YAML file compressed into a tar.gz
+file. You need the KFP SDK to compile your pipeline.
+
+Install KFP SDK
+^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Install the KFP SDK following the instructions in the \ `Kubeflow
+pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
+
+Verify that the KFP SDK is installed with the following command:
+
+::
+
+    pip show kfp
+
+Verify that ``dsl-compile`` has been installed correctly as follows:
+
+::
+
+    which dsl-compile
+
+Compile your pipeline
+^^^^^^^^^^^^^^^^^^^^^
+
+You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
+CLI, or the KFP SDK. The following sections illustrate the workflow
+using the KFP UI and CLI.
+
+Complete the following from your gateway node to compile your pipeline.
+
+Modify your Python file with your S3 bucket name and IAM role ARN.
+
+Use the ``dsl-compile`` command from the command line to compile your
+pipeline as follows. Replace ``<path-to-python-file>`` with the path
+to your pipeline and ``<path-to-output>`` with the location where you
+want your tar.gz file to be.
+
+::
+
+    dsl-compile --py <path-to-python-file> --output <path-to-output>
+
+Upload and run the pipeline using the KFP CLI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Complete the following steps from the command line of your gateway node.
+KFP organizes runs of your pipeline as experiments. You have the option
+to specify an experiment name. If you do not specify one, the run will
+be listed under ‘Default’ experiment.
+
+Upload your pipeline as follows:
+
+::
+
+    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
+
+Your output should look like the following. Take note of the \ ``ID``.
+
+::
+
+    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
+
+    Pipeline Details
+    ------------------
+    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
+    Name         sm-pipeline
+    Description
+    Uploaded at  2020-04-30T20:22:39+00:00
+    ...
+    ...
+
+Create a run using the following command. The KFP CLI run command
+currently does not support specifying input parameters while creating
+the run. You need to update your parameters in the Python pipeline file
+before compiling. Replace ``<experiment-name>`` and ``<job-name>``
+with any names. Replace ``<pipeline-id>`` with the ID of your submitted
+pipeline.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
+
+You can also directly submit a run using the compiled pipeline package
+created as the output of the ``dsl-compile`` command.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
+
+Your output should look like the following:
+
+::
+
+    Creating experiment aws.
+    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
+    +--------------------------------------+--------+----------+---------------------------+
+    | run id                               | name   | status   | created at                |
+    +======================================+========+==========+===========================+
+    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
+    +--------------------------------------+--------+----------+---------------------------+
+
+Navigate to the UI to check the progress of the job
+
+Upload and run the pipeline using the KFP UI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  In the upper-right corner, choose ``+UploadPipeline``.
+
+-  Enter the pipeline name and description.
+
+-  Choose ``Upload a file`` and enter the path to the tar.gz file you
+   created using the CLI or with the Python SDK.
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  Find the pipeline you created.
+
+-  Choose ``+CreateRun``.
+
+-  Enter your input parameters.
+
+-  Choose ``Run``.
+
+Running predictions
+~~~~~~~~~~~~~~~~~~~
+
+Once your classification pipeline is deployed, you can run
+classification predictions against the endpoint that was created by the
+Deploy component. Use the KFP UI to check the output artifacts
+for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
+file to extract the endpoint name or check the Amazon SageMaker console
+in the region you used.
+
+Configure permissions to run predictions
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+If you want to run predictions from your gateway node, skip this
+section.
+
+If you want to use any other machine to run predictions, assign
+the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
+user used by the client machine. This permission is used to run
+predictions.
+
+On your gateway node, run the following to create a policy file:
+
+::
+
+    cat <<EoF > ./sagemaker-invoke.json
+    {
+        "Version": "2012-10-17",
+        "Statement": [
+            {
+                "Effect": "Allow",
+                "Action": [
+                    "sagemaker:InvokeEndpoint"
+                ],
+                "Resource": "*"
+            }
+        ]
+    }
+    EoF
+
+Attach the policy to the client node’s IAM role or IAM user.
+
+If your client machine has an IAM role attached, run the following.
+Replace ``<your-instance-IAM-role>`` with the name of the client
+node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+If your client machine has IAM user credentials configured, run the
+following. Replace ``<your_IAM_user_name>`` with the name of the client
+node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+Run predictions
+^^^^^^^^^^^^^^^
+
+Create a Python file from your client machine
+named ``mnist-predictions.py`` with the following content . Replace
+the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
+MNIST dataset, then creates a CSV from those digits and sends it to the
+endpoint for prediction. It then outputs the results.
+
+::
+
+    import pickle, gzip, numpy, urllib.request, json
+    from urllib.parse import urlparse
+    import json
+    import io
+    import boto3
+
+    ENDPOINT_NAME='<endpoint-name>'
+    REGION = '<region>'
+
+    # Load the dataset
+    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
+    with gzip.open('mnist.pkl.gz', 'rb') as f:
+        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
+
+    # Simple function to create a csv from our numpy array
+    def np2csv(arr):
+        csv = io.BytesIO()
+        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
+        return csv.getvalue().decode().rstrip()
+
+    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
+
+    payload = np2csv(train_set[0][30:31])
+
+    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
+                                       ContentType='text/csv',
+                                       Body=payload)
+    result = json.loads(response['Body'].read().decode())
+    print(result)
+
+Run the Python file as follows:
+
+::
+
+    python mnist-predictions.py
+
+View results and logs
+~~~~~~~~~~~~~~~~~~~~~
+
+When the pipeline is running, you can click on any component to check
+execution details, such as inputs and outputs. This will list the names
+of created resources.
+
+If the KFP request is successfully processed and an Amazon SageMaker job
+is created, the component logs in the KFP UI will provide a link to the
+job created in Amazon SageMaker. The CloudWatch logs will also be
+provided if the job is successfully created.
+
+If you run too many pipeline jobs on the same cluster, you may see an
+error message that indicates you do not have enough pods available. To
+fix this, log in to your gateway node and delete the pods created by the
+pipelines you are not using as follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Cleanup
+~~~~~~~
+
+When you’re finished with your pipeline, you need to cleanup your
+resources.
+
+From the KFP dashboard, terminate your pipeline runs if they do not exit
+properly by clicking ``Terminate``.
+
+If the ``Terminate`` option doesn’t work, log in to your gateway node
+and terminate all the pods created by your pipeline run manually as
+follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Using your AWS account, log in to the Amazon SageMaker service. Manually
+stop all training, batch transform, and HPO jobs. Delete models, data
+buckets and endpoints to avoid incurring any additional
+costs. Terminating the pipeline runs does not stop the jobs in Amazon
+SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:19:08[0m
[92mHash: cb4e0befc18479f40b264f8af605249426ce20c7[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.60.2
[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:19:08[0m
[92mHash: cb4e0befc18479f40b264f8af605249426ce20c7[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.60.2
[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:19:08[0m
[92mHash: cb4e0befc18479f40b264f8af605249426ce20c7[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.60.2
[0m
@@ -1,835 +0,0 @@
-Using Amazon SageMaker Components
-=================================
-
-In this tutorial, you run a pipeline using Amazon SageMaker Components
-for Kubeflow Pipelines to train a classification model using Kmeans with
-the MNIST dataset. This workflow uses Kubeflow pipelines as the
-orchestrator and Amazon SageMaker as the backend to run the steps in the
-workflow. For the full code for this and other pipeline examples, see
-the `Sample AWS SageMaker Kubeflow
-Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
-For information on the components used, see the `KubeFlow Pipelines
-GitHub
-repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
-
-Setup
------
-
-To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
-Service (Amazon EKS) cluster and a gateway node to interact with that
-cluster. The following sections show the steps needed to set up these
-resources.
-
-Set up a gateway node
-~~~~~~~~~~~~~~~~~~~~~
-
-A gateway node is used to create an Amazon EKS cluster and access the
-Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
-as your gateway node. If you want to use a new EC2 instance, create one
-with the latest Ubuntu 18.04 DLAMI version from the AWS console using
-the steps in `Launching and Configuring a
-DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
-
-Complete the following steps to set up your gateway node. Depending on
-your environment, you may have certain requirements already configured.
-
-If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
-AWS
-Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
-you have an existing Amazon EKS cluster, use the credentials of the IAM
-role or user that has access to it.
-
-Add the following permissions to your user using the steps in `Changing
-Permissions for an IAM
-User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
-
--  CloudWatchLogsFullAccess
-
--  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
-
--  IAMFullAccess
-
--  AmazonS3FullAccess
-
--  AmazonEC2FullAccess
-
--  AmazonEKSAdminPolicy - Create this policy using the schema
-   from `Amazon EKS Identity-Based Policy
-   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
-
-Install the following on your gateway node to access the Amazon EKS
-cluster and KFP UI.
-
--  `AWS
-   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
-   If you are using an IAM user, configure your `Access Key ID, Secret
-   Access
-   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
-   preferred AWS Region by running: ``aws configure``
-
--  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-   version 0.1.31 and above.
-
--  ``eksctl`` version above 0.15.
-
--  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-   The version needs to match your Kubernetes version within 1 minor
-   version.
-
-Install \ ``boto3``.
-
-::
-
-    pip install boto3
-
-Set up an Amazon EKS cluster
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-set up an Amazon EKS cluster:
-
-If you do not have an existing Amazon EKS cluster, complete the
-following substeps. If you already have an Amazon EKS cluster, skip this
-step.
-
-Run the following from your command line to create an Amazon EKS Cluster
-with version 1.14 or above. Replace ``<your-cluster-name>`` with any
-name for your cluster.
-
-::
-
-    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
-
-When cluster creation is complete, verify that you have access to the
-cluster using the following command.
-
-::
-
-    kubectl get nodes
-
-Verify that the current kubectl context is the cluster you want to use
-with the following command. The current context is marked with an
-asterisk (\*) in the output.
-
-::
-
-    kubectl config get-contexts
-
-    CURRENT NAME     CLUSTER
-    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
-
-If the desired cluster is not configured as your current default, update
-the default with the following command.
-
-::
-
-    aws eks update-kubeconfig --name <clustername> --region us-east-1
-
-Install Kubeflow Pipelines
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-install Kubeflow Pipelines on your cluster.
-
-Install Kubeflow Pipelines on your cluster by following step 1
-of `Deploying Kubeflow Pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
-Your KFP version must be 0.5.0 or above.
-
-Verify that the Kubeflow Pipelines service and other related resources
-are running.
-
-::
-
-    kubectl -n kubeflow get all | grep pipeline
-
-Your output should look like the following.
-
-::
-
-    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
-    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
-    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
-    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
-    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
-    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
-    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
-    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
-    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
-    deployment.apps/ml-pipeline                       1/1     1            1           2d
-    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
-    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
-    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
-    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
-    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
-    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
-    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
-    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
-    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
-    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
-    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
-
-Access the KFP UI
-~~~~~~~~~~~~~~~~~
-
-The Kubeflow Pipelines UI is used for managing and tracking experiments,
-jobs, and runs on your cluster. You can use port forwarding to access
-the Kubeflow Pipelines UI from your gateway node.
-
-Set up port forwarding to the KFP UI service
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Verify that the KFP UI service is running using the following command:
-
-::
-
-    kubectl -n kubeflow get service ml-pipeline-ui
-
-    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
-    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
-
-Run the following command to setup port forwarding to the KFP UI
-service. This forwards the KFP UI to port 8080 on your gateway node and
-allows you to access the KFP UI from your browser.
-
-    **Note**
-
-    The port-forward from your remote machine drops if there is no
-    activity. Run this command again if your dashboard is unable to get
-    logs or updates. If the commands return an error, ensure that there
-    is no process already running on the port you are trying to use.
-
-::
-
-    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
-
-Your method of accessing the KFP UI depends on your gateway node type.
-
-Local machine as the gateway node
-
-Access the dashboard in your browser as follows:
-
-::
-
-    http://localhost:8080
-
-Click **Pipelines** to access the pipelines UI.
-
-EC2 instance as the gateway node
-
-You need to setup an SSH tunnel on your EC2 instance to access the
-Kubeflow dashboard from your local machine’s browser.
-
-From a new terminal session in your local machine, run the following.
-Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
-instance found on the EC2 console. You can also use the public DNS.
-Replace ``<path_to_key>`` with the path to the pem key used to access
-the gateway node.
-
-::
-
-    public_DNS_address=<public-DNS-of-gateway-node>
-    key=<path_to_key>
-
-    on Ubuntu:
-    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
-
-    or on Amazon Linux:
-    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
-
-Access the dashboard in your browser.
-
-::
-
-    http://localhost:9000
-
-Click **Pipelines** to access the KFP UI.
-
-Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
-SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
-need access to SageMaker. In this section, you create IAM Users/Roles to
-be used by Kubeflow Pipeline pods and Amazon SageMaker.
-
-Create a KFP execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Enable OIDC support on the Amazon EKS cluster with the following
-command. Replace ``<cluster_name>`` with the name of your cluster
-and ``<cluster_region>`` with the region your cluster is in.
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
-            --region <cluster-region> --approve
-
-Run the following to get the `OIDC <https://openid.net/connect/>`__
-issuer URL. This URL is in the
-form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
-
-::
-
-    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
-
-Run the following to create a file named ``trust.json``.
-Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
-include ``https://`` when in your OIDC issuer URL.
-Replace ``<AWS_account_number>`` with your AWS account number.
-
-::
-
-    OIDC_URL="<OIDC-URL>"
-    AWS_ACC_NUM="<AWS-account-number>"
-
-    # Run this to create trust.json file
-    cat <<EOF > trust.json
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "${OIDC_URL}:aud": "sts.amazonaws.com",
-              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
-            }
-          }
-        }
-      ]
-    }
-    EOF
-
-Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
-using the following command. This role is used by KFP pods to create
-Amazon SageMaker jobs from KFP components. Note the ARN returned in the
-output.
-
-::
-
-    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
-    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
-
-Edit your pipeline-runner service account with the following command.
-
-::
-
-    kubectl edit -n kubeflow serviceaccount pipeline-runner
-
-In the file, add the following Amazon EKS role annotation and
-replace ``<role_arn>`` with your role ARN.
-
-::
-
-    eks.amazonaws.com/role-arn: <role-arn>
-
-Your file should look like the following when you’ve added the Amazon
-EKS role annotation. Save the file.
-
-::
-
-    apiVersion: v1
-    kind: ServiceAccount
-    metadata:
-      annotations:
-        eks.amazonaws.com/role-arn: <role-arn>
-        kubectl.kubernetes.io/last-applied-configuration: |
-          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
-      creationTimestamp: "2020-04-16T05:48:06Z"
-      labels:
-        app: pipeline-runner
-        app.kubernetes.io/component: pipelines-runner
-        app.kubernetes.io/instance: pipelines-runner-0.2.0
-        app.kubernetes.io/managed-by: kfctl
-        app.kubernetes.io/name: pipelines-runner
-        app.kubernetes.io/part-of: kubeflow
-        app.kubernetes.io/version: 0.2.0
-      name: pipeline-runner
-      namespace: kubeflow
-      resourceVersion: "11787"
-      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
-      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
-    secrets:
-    - name: pipeline-runner-token-dkjrk
-
-Create an Amazon SageMaker execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The ``kfp-example-sagemaker-execution-role`` IAM role is used
-by Amazon SageMaker jobs to access AWS resources. For more information,
-see the IAM Permissions section. You provide this role as an input
-parameter when running the pipeline.
-
-Run the following to create the role. Note the ARN that is returned in
-your output.
-
-::
-
-    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
-
-    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
-    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
-
-Add access to additional IAM users or roles
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you use an intuitive IDE like Jupyter or want other people in your
-organization to use the cluster you set up, you can also give them
-access. The following steps run through this workflow using Amazon
-SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
-managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
-You use the notebook instance to create and manage Jupyter notebooks to
-create ML workflows. You can define, compile, deploy, and run your
-pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
-SageMaker notebook to run Jupyter, you need to install the `AWS
-CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
-the latest version
-of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-
-Follow the steps in `Create an Amazon SageMaker Notebook
-Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
-to create a Amazon SageMaker notebook instance if you do not already
-have one. Give the IAM role for this instance the ``S3FullAccess``
-permission.
-
-Amazon EKS clusters use IAM users and roles to control access to the
-cluster. The rules are implemented in a config map named ``aws-auth``.
-Only the user/role that has access to the cluster will be able to edit
-this config map. Run the following from the command line of your gateway
-node to get the IAM role of the notebook instance you created.
-Replace ``<instance-name>`` with the name of your instance.
-
-::
-
-    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
-
-This command outputs the IAM role ARN in
-the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
-of this ARN.
-
-Run the following to attach the policies the IAM role.
-Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
-
-::
-
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-``eksctl`` provides commands to read and edit the ``aws-auth`` config
-map. ``system:masters`` is one of the default user groups. You add the
-user to this group. The "system:masters" group has super user
-permissions to the cluster. You can also create a group with more
-restrictive permissions or you can bind permissions directly to users.
-Replace ``<IAM-Role-arn>`` with the ARN of the IAM
-role. ``<your_username>`` can be any unique username.
-
-::
-
-    eksctl create iamidentitymapping \
-        --cluster <cluster-name> \
-        --arn <IAM-Role-arn> \
-        --group system:masters \
-        --username <your-username> \
-        --region <region>
-
-Open the Jupyter notebook on your Amazon SageMaker instance and run the
-following to verify that it has access to the cluster.
-
-::
-
-    aws eks --region <region> update-kubeconfig --name <cluster-name>
-    kubectl -n kubeflow get all | grep pipeline
-
-Running the Kubeflow Pipeline
------------------------------
-
-Now that setup of your gateway node and Amazon EKS cluster is complete,
-you can create your classification pipeline. To create your pipeline,
-you need to define and compile it. You then deploy it and use it to run
-workflows. You can define your pipeline in Python and use the KFP
-dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows.
-
-Prepare datasets
-~~~~~~~~~~~~~~~~
-
-To run the pipelines, you need to have the datasets in an S3 bucket in
-your account. This bucket must be located in the region where you want
-to run Amazon SageMaker jobs. If you don’t have a bucket, create one
-using the steps in `Creating a
-bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
-
-From your gateway node, run the `sample dataset
-creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
-script to copy the datasets into your bucket. Change the bucket name in
-the script to the one you created.
-
-Create a Kubeflow Pipeline using Amazon SageMaker Components
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The full code for the MNIST classification pipeline is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
-
-Input Parameters
-^^^^^^^^^^^^^^^^
-
-The full MNIST classification pipeline has run-specific parameters that
-you must provide values for when creating a run. You must provide these
-parameters for each component of your pipeline. These parameters can
-also be updated when using other pipelines. We have provided default
-values for all parameters in the sample classification pipeline file.
-
-The following are the only parameters you may need to modify to run the
-sample pipelines. To modify these parameters, update their entries in
-the sample classification pipeline file.
-
--  **Role-ARN:** This must be the ARN of an IAM role that has full
-   Amazon SageMaker access in your AWS account. Use the ARN
-   of  ``kfp-example-pod-role``.
-
--  **The Dataset Buckets**: You must change the S3 bucket with the input
-   data for each of the components. Replace the following with the link
-   to your S3 bucket:
-
-   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **HPO channels for test/HPO channel for
-      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **Batch
-      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
-
--  **Output buckets:** Replace the output buckets with S3 buckets you
-   have write permission to. Replace the following with the link to your
-   S3 bucket:
-
-   -  **Training/HPO**:
-      ``output_location='s3://<your-s3-bucket-name>/output'``
-
-   -  **Batch Transform**:
-      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
-
--  **Region:**\ The default pipelines work in us-east-1. If your
-   cluster is in a different region, update the following:
-
-   -  The ``region='us-east-1'`` Parameter in the input list.
-
-   -  The algorithm images for Amazon SageMaker. If you use one of
-      the Amazon SageMaker built-in algorithm images, select the image
-      for your region. Construct the image name using the information
-      in `Common parameters for built-in
-      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
-      For Example:
-
-      ::
-
-          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
-
-   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
-      to copy the data to a bucket in the same region as the cluster.
-
-You can adjust any of the input parameters using the KFP UI and trigger
-your run again.
-
-Compile and deploy your pipeline
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After defining the pipeline in Python, you must compile the pipeline to
-an intermediate representation before you can submit it to the Kubeflow
-Pipelines service. The intermediate representation is a workflow
-specification in the form of a YAML file compressed into a tar.gz
-file. You need the KFP SDK to compile your pipeline.
-
-Install KFP SDK
-^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Install the KFP SDK following the instructions in the \ `Kubeflow
-pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
-
-Verify that the KFP SDK is installed with the following command:
-
-::
-
-    pip show kfp
-
-Verify that ``dsl-compile`` has been installed correctly as follows:
-
-::
-
-    which dsl-compile
-
-Compile your pipeline
-^^^^^^^^^^^^^^^^^^^^^
-
-You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
-CLI, or the KFP SDK. The following sections illustrate the workflow
-using the KFP UI and CLI.
-
-Complete the following from your gateway node to compile your pipeline.
-
-Modify your Python file with your S3 bucket name and IAM role ARN.
-
-Use the ``dsl-compile`` command from the command line to compile your
-pipeline as follows. Replace ``<path-to-python-file>`` with the path
-to your pipeline and ``<path-to-output>`` with the location where you
-want your tar.gz file to be.
-
-::
-
-    dsl-compile --py <path-to-python-file> --output <path-to-output>
-
-Upload and run the pipeline using the KFP CLI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Complete the following steps from the command line of your gateway node.
-KFP organizes runs of your pipeline as experiments. You have the option
-to specify an experiment name. If you do not specify one, the run will
-be listed under ‘Default’ experiment.
-
-Upload your pipeline as follows:
-
-::
-
-    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
-
-Your output should look like the following. Take note of the \ ``ID``.
-
-::
-
-    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
-
-    Pipeline Details
-    ------------------
-    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
-    Name         sm-pipeline
-    Description
-    Uploaded at  2020-04-30T20:22:39+00:00
-    ...
-    ...
-
-Create a run using the following command. The KFP CLI run command
-currently does not support specifying input parameters while creating
-the run. You need to update your parameters in the Python pipeline file
-before compiling. Replace ``<experiment-name>`` and ``<job-name>``
-with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
-
-You can also directly submit a run using the compiled pipeline package
-created as the output of the ``dsl-compile`` command.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
-
-Your output should look like the following:
-
-::
-
-    Creating experiment aws.
-    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
-    +--------------------------------------+--------+----------+---------------------------+
-    | run id                               | name   | status   | created at                |
-    +======================================+========+==========+===========================+
-    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
-    +--------------------------------------+--------+----------+---------------------------+
-
-Navigate to the UI to check the progress of the job
-
-Upload and run the pipeline using the KFP UI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
--  On the left panel, choose the **Pipelines** tab.
-
--  In the upper-right corner, choose ``+UploadPipeline``.
-
--  Enter the pipeline name and description.
-
--  Choose ``Upload a file`` and enter the path to the tar.gz file you
-   created using the CLI or with the Python SDK.
-
--  On the left panel, choose the **Pipelines** tab.
-
--  Find the pipeline you created.
-
--  Choose ``+CreateRun``.
-
--  Enter your input parameters.
-
--  Choose ``Run``.
-
-Running predictions
-~~~~~~~~~~~~~~~~~~~
-
-Once your classification pipeline is deployed, you can run
-classification predictions against the endpoint that was created by the
-Deploy component. Use the KFP UI to check the output artifacts
-for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
-file to extract the endpoint name or check the Amazon SageMaker console
-in the region you used.
-
-Configure permissions to run predictions
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-If you want to run predictions from your gateway node, skip this
-section.
-
-If you want to use any other machine to run predictions, assign
-the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
-user used by the client machine. This permission is used to run
-predictions.
-
-On your gateway node, run the following to create a policy file:
-
-::
-
-    cat <<EoF > ./sagemaker-invoke.json
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "sagemaker:InvokeEndpoint"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
-    EoF
-
-Attach the policy to the client node’s IAM role or IAM user.
-
-If your client machine has an IAM role attached, run the following.
-Replace ``<your-instance-IAM-role>`` with the name of the client
-node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-If your client machine has IAM user credentials configured, run the
-following. Replace ``<your_IAM_user_name>`` with the name of the client
-node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-Run predictions
-^^^^^^^^^^^^^^^
-
-Create a Python file from your client machine
-named ``mnist-predictions.py`` with the following content . Replace
-the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
-MNIST dataset, then creates a CSV from those digits and sends it to the
-endpoint for prediction. It then outputs the results.
-
-::
-
-    import pickle, gzip, numpy, urllib.request, json
-    from urllib.parse import urlparse
-    import json
-    import io
-    import boto3
-
-    ENDPOINT_NAME='<endpoint-name>'
-    REGION = '<region>'
-
-    # Load the dataset
-    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
-    with gzip.open('mnist.pkl.gz', 'rb') as f:
-        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
-
-    # Simple function to create a csv from our numpy array
-    def np2csv(arr):
-        csv = io.BytesIO()
-        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
-        return csv.getvalue().decode().rstrip()
-
-    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
-
-    payload = np2csv(train_set[0][30:31])
-
-    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
-                                       ContentType='text/csv',
-                                       Body=payload)
-    result = json.loads(response['Body'].read().decode())
-    print(result)
-
-Run the Python file as follows:
-
-::
-
-    python mnist-predictions.py
-
-View results and logs
-~~~~~~~~~~~~~~~~~~~~~
-
-When the pipeline is running, you can click on any component to check
-execution details, such as inputs and outputs. This will list the names
-of created resources.
-
-If the KFP request is successfully processed and an Amazon SageMaker job
-is created, the component logs in the KFP UI will provide a link to the
-job created in Amazon SageMaker. The CloudWatch logs will also be
-provided if the job is successfully created.
-
-If you run too many pipeline jobs on the same cluster, you may see an
-error message that indicates you do not have enough pods available. To
-fix this, log in to your gateway node and delete the pods created by the
-pipelines you are not using as follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Cleanup
-~~~~~~~
-
-When you’re finished with your pipeline, you need to cleanup your
-resources.
-
-From the KFP dashboard, terminate your pipeline runs if they do not exit
-properly by clicking ``Terminate``.
-
-If the ``Terminate`` option doesn’t work, log in to your gateway node
-and terminate all the pods created by your pipeline run manually as
-follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Using your AWS account, log in to the Amazon SageMaker service. Manually
-stop all training, batch transform, and HPO jobs. Delete models, data
-buckets and endpoints to avoid incurring any additional
-costs. Terminating the pipeline runs does not stop the jobs in Amazon
-SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:11:49[0m
[92mHash: c65c80f65eb7b5044fd9f56f56999c472f6a6907[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove legacy TensorFlowModel and TensorFlowPredictor classes (#1531)

This change also removes the associated serialization/deserialization code
used by TensorFlowPredictor and the locally copied TFS APIs.[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:11:49[0m
[92mHash: c65c80f65eb7b5044fd9f56f56999c472f6a6907[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove legacy TensorFlowModel and TensorFlowPredictor classes (#1531)

This change also removes the associated serialization/deserialization code
used by TensorFlowPredictor and the locally copied TFS APIs.[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-29 12:11:49[0m
[92mHash: c65c80f65eb7b5044fd9f56f56999c472f6a6907[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove legacy TensorFlowModel and TensorFlowPredictor classes (#1531)

This change also removes the associated serialization/deserialization code
used by TensorFlowPredictor and the locally copied TFS APIs.[0m
@@ -0,0 +1,835 @@
+Using Amazon SageMaker Components
+=================================
+
+In this tutorial, you run a pipeline using Amazon SageMaker Components
+for Kubeflow Pipelines to train a classification model using Kmeans with
+the MNIST dataset. This workflow uses Kubeflow pipelines as the
+orchestrator and Amazon SageMaker as the backend to run the steps in the
+workflow. For the full code for this and other pipeline examples, see
+the `Sample AWS SageMaker Kubeflow
+Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
+For information on the components used, see the `KubeFlow Pipelines
+GitHub
+repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
+
+Setup
+-----
+
+To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
+Service (Amazon EKS) cluster and a gateway node to interact with that
+cluster. The following sections show the steps needed to set up these
+resources.
+
+Set up a gateway node
+~~~~~~~~~~~~~~~~~~~~~
+
+A gateway node is used to create an Amazon EKS cluster and access the
+Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
+as your gateway node. If you want to use a new EC2 instance, create one
+with the latest Ubuntu 18.04 DLAMI version from the AWS console using
+the steps in `Launching and Configuring a
+DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
+
+Complete the following steps to set up your gateway node. Depending on
+your environment, you may have certain requirements already configured.
+
+If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
+AWS
+Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
+you have an existing Amazon EKS cluster, use the credentials of the IAM
+role or user that has access to it.
+
+Add the following permissions to your user using the steps in `Changing
+Permissions for an IAM
+User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
+
+-  CloudWatchLogsFullAccess
+
+-  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
+
+-  IAMFullAccess
+
+-  AmazonS3FullAccess
+
+-  AmazonEC2FullAccess
+
+-  AmazonEKSAdminPolicy - Create this policy using the schema
+   from `Amazon EKS Identity-Based Policy
+   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
+
+Install the following on your gateway node to access the Amazon EKS
+cluster and KFP UI.
+
+-  `AWS
+   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
+   If you are using an IAM user, configure your `Access Key ID, Secret
+   Access
+   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
+   preferred AWS Region by running: ``aws configure``
+
+-  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+   version 0.1.31 and above.
+
+-  ``eksctl`` version above 0.15.
+
+-  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+   The version needs to match your Kubernetes version within 1 minor
+   version.
+
+Install \ ``boto3``.
+
+::
+
+    pip install boto3
+
+Set up an Amazon EKS cluster
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+set up an Amazon EKS cluster:
+
+If you do not have an existing Amazon EKS cluster, complete the
+following substeps. If you already have an Amazon EKS cluster, skip this
+step.
+
+Run the following from your command line to create an Amazon EKS Cluster
+with version 1.14 or above. Replace ``<your-cluster-name>`` with any
+name for your cluster.
+
+::
+
+    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
+
+When cluster creation is complete, verify that you have access to the
+cluster using the following command.
+
+::
+
+    kubectl get nodes
+
+Verify that the current kubectl context is the cluster you want to use
+with the following command. The current context is marked with an
+asterisk (\*) in the output.
+
+::
+
+    kubectl config get-contexts
+
+    CURRENT NAME     CLUSTER
+    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
+
+If the desired cluster is not configured as your current default, update
+the default with the following command.
+
+::
+
+    aws eks update-kubeconfig --name <clustername> --region us-east-1
+
+Install Kubeflow Pipelines
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Run the following steps from the command line of your gateway node to
+install Kubeflow Pipelines on your cluster.
+
+Install Kubeflow Pipelines on your cluster by following step 1
+of `Deploying Kubeflow Pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
+Your KFP version must be 0.5.0 or above.
+
+Verify that the Kubeflow Pipelines service and other related resources
+are running.
+
+::
+
+    kubectl -n kubeflow get all | grep pipeline
+
+Your output should look like the following.
+
+::
+
+    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
+    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
+    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
+    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
+    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
+    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
+    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
+    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
+    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
+    deployment.apps/ml-pipeline                       1/1     1            1           2d
+    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
+    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
+    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
+    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
+    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
+    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
+    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
+    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
+    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
+    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
+    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
+
+Access the KFP UI
+~~~~~~~~~~~~~~~~~
+
+The Kubeflow Pipelines UI is used for managing and tracking experiments,
+jobs, and runs on your cluster. You can use port forwarding to access
+the Kubeflow Pipelines UI from your gateway node.
+
+Set up port forwarding to the KFP UI service
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Verify that the KFP UI service is running using the following command:
+
+::
+
+    kubectl -n kubeflow get service ml-pipeline-ui
+
+    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
+    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
+
+Run the following command to setup port forwarding to the KFP UI
+service. This forwards the KFP UI to port 8080 on your gateway node and
+allows you to access the KFP UI from your browser.
+
+    **Note**
+
+    The port-forward from your remote machine drops if there is no
+    activity. Run this command again if your dashboard is unable to get
+    logs or updates. If the commands return an error, ensure that there
+    is no process already running on the port you are trying to use.
+
+::
+
+    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
+
+Your method of accessing the KFP UI depends on your gateway node type.
+
+Local machine as the gateway node
+
+Access the dashboard in your browser as follows:
+
+::
+
+    http://localhost:8080
+
+Click **Pipelines** to access the pipelines UI.
+
+EC2 instance as the gateway node
+
+You need to setup an SSH tunnel on your EC2 instance to access the
+Kubeflow dashboard from your local machine’s browser.
+
+From a new terminal session in your local machine, run the following.
+Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
+instance found on the EC2 console. You can also use the public DNS.
+Replace ``<path_to_key>`` with the path to the pem key used to access
+the gateway node.
+
+::
+
+    public_DNS_address=<public-DNS-of-gateway-node>
+    key=<path_to_key>
+
+    on Ubuntu:
+    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
+
+    or on Amazon Linux:
+    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
+
+Access the dashboard in your browser.
+
+::
+
+    http://localhost:9000
+
+Click **Pipelines** to access the KFP UI.
+
+Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
+SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
+need access to SageMaker. In this section, you create IAM Users/Roles to
+be used by Kubeflow Pipeline pods and Amazon SageMaker.
+
+Create a KFP execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Enable OIDC support on the Amazon EKS cluster with the following
+command. Replace ``<cluster_name>`` with the name of your cluster
+and ``<cluster_region>`` with the region your cluster is in.
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
+            --region <cluster-region> --approve
+
+Run the following to get the `OIDC <https://openid.net/connect/>`__
+issuer URL. This URL is in the
+form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
+
+::
+
+    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
+
+Run the following to create a file named ``trust.json``.
+Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
+include ``https://`` when in your OIDC issuer URL.
+Replace ``<AWS_account_number>`` with your AWS account number.
+
+::
+
+    OIDC_URL="<OIDC-URL>"
+    AWS_ACC_NUM="<AWS-account-number>"
+
+    # Run this to create trust.json file
+    cat <<EOF > trust.json
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "${OIDC_URL}:aud": "sts.amazonaws.com",
+              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
+            }
+          }
+        }
+      ]
+    }
+    EOF
+
+Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
+using the following command. This role is used by KFP pods to create
+Amazon SageMaker jobs from KFP components. Note the ARN returned in the
+output.
+
+::
+
+    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
+    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
+
+Edit your pipeline-runner service account with the following command.
+
+::
+
+    kubectl edit -n kubeflow serviceaccount pipeline-runner
+
+In the file, add the following Amazon EKS role annotation and
+replace ``<role_arn>`` with your role ARN.
+
+::
+
+    eks.amazonaws.com/role-arn: <role-arn>
+
+Your file should look like the following when you’ve added the Amazon
+EKS role annotation. Save the file.
+
+::
+
+    apiVersion: v1
+    kind: ServiceAccount
+    metadata:
+      annotations:
+        eks.amazonaws.com/role-arn: <role-arn>
+        kubectl.kubernetes.io/last-applied-configuration: |
+          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
+      creationTimestamp: "2020-04-16T05:48:06Z"
+      labels:
+        app: pipeline-runner
+        app.kubernetes.io/component: pipelines-runner
+        app.kubernetes.io/instance: pipelines-runner-0.2.0
+        app.kubernetes.io/managed-by: kfctl
+        app.kubernetes.io/name: pipelines-runner
+        app.kubernetes.io/part-of: kubeflow
+        app.kubernetes.io/version: 0.2.0
+      name: pipeline-runner
+      namespace: kubeflow
+      resourceVersion: "11787"
+      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
+      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
+    secrets:
+    - name: pipeline-runner-token-dkjrk
+
+Create an Amazon SageMaker execution role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The ``kfp-example-sagemaker-execution-role`` IAM role is used
+by Amazon SageMaker jobs to access AWS resources. For more information,
+see the IAM Permissions section. You provide this role as an input
+parameter when running the pipeline.
+
+Run the following to create the role. Note the ARN that is returned in
+your output.
+
+::
+
+    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
+
+    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
+    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
+
+Add access to additional IAM users or roles
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If you use an intuitive IDE like Jupyter or want other people in your
+organization to use the cluster you set up, you can also give them
+access. The following steps run through this workflow using Amazon
+SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
+managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
+You use the notebook instance to create and manage Jupyter notebooks to
+create ML workflows. You can define, compile, deploy, and run your
+pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
+SageMaker notebook to run Jupyter, you need to install the `AWS
+CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
+the latest version
+of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
+
+Follow the steps in `Create an Amazon SageMaker Notebook
+Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
+to create a Amazon SageMaker notebook instance if you do not already
+have one. Give the IAM role for this instance the ``S3FullAccess``
+permission.
+
+Amazon EKS clusters use IAM users and roles to control access to the
+cluster. The rules are implemented in a config map named ``aws-auth``.
+Only the user/role that has access to the cluster will be able to edit
+this config map. Run the following from the command line of your gateway
+node to get the IAM role of the notebook instance you created.
+Replace ``<instance-name>`` with the name of your instance.
+
+::
+
+    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
+
+This command outputs the IAM role ARN in
+the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
+of this ARN.
+
+Run the following to attach the policies the IAM role.
+Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
+
+::
+
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
+    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
+
+``eksctl`` provides commands to read and edit the ``aws-auth`` config
+map. ``system:masters`` is one of the default user groups. You add the
+user to this group. The "system:masters" group has super user
+permissions to the cluster. You can also create a group with more
+restrictive permissions or you can bind permissions directly to users.
+Replace ``<IAM-Role-arn>`` with the ARN of the IAM
+role. ``<your_username>`` can be any unique username.
+
+::
+
+    eksctl create iamidentitymapping \
+        --cluster <cluster-name> \
+        --arn <IAM-Role-arn> \
+        --group system:masters \
+        --username <your-username> \
+        --region <region>
+
+Open the Jupyter notebook on your Amazon SageMaker instance and run the
+following to verify that it has access to the cluster.
+
+::
+
+    aws eks --region <region> update-kubeconfig --name <cluster-name>
+    kubectl -n kubeflow get all | grep pipeline
+
+Running the Kubeflow Pipeline
+-----------------------------
+
+Now that setup of your gateway node and Amazon EKS cluster is complete,
+you can create your classification pipeline. To create your pipeline,
+you need to define and compile it. You then deploy it and use it to run
+workflows. You can define your pipeline in Python and use the KFP
+dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
+workflows.
+
+Prepare datasets
+~~~~~~~~~~~~~~~~
+
+To run the pipelines, you need to have the datasets in an S3 bucket in
+your account. This bucket must be located in the region where you want
+to run Amazon SageMaker jobs. If you don’t have a bucket, create one
+using the steps in `Creating a
+bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
+
+From your gateway node, run the `sample dataset
+creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
+script to copy the datasets into your bucket. Change the bucket name in
+the script to the one you created.
+
+Create a Kubeflow Pipeline using Amazon SageMaker Components
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The full code for the MNIST classification pipeline is available in the
+`Kubeflow Github
+repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
+To use it, clone the example Python files to your gateway node.
+
+Input Parameters
+^^^^^^^^^^^^^^^^
+
+The full MNIST classification pipeline has run-specific parameters that
+you must provide values for when creating a run. You must provide these
+parameters for each component of your pipeline. These parameters can
+also be updated when using other pipelines. We have provided default
+values for all parameters in the sample classification pipeline file.
+
+The following are the only parameters you may need to modify to run the
+sample pipelines. To modify these parameters, update their entries in
+the sample classification pipeline file.
+
+-  **Role-ARN:** This must be the ARN of an IAM role that has full
+   Amazon SageMaker access in your AWS account. Use the ARN
+   of  ``kfp-example-pod-role``.
+
+-  **The Dataset Buckets**: You must change the S3 bucket with the input
+   data for each of the components. Replace the following with the link
+   to your S3 bucket:
+
+   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **HPO channels for test/HPO channel for
+      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
+
+   -  **Batch
+      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
+
+-  **Output buckets:** Replace the output buckets with S3 buckets you
+   have write permission to. Replace the following with the link to your
+   S3 bucket:
+
+   -  **Training/HPO**:
+      ``output_location='s3://<your-s3-bucket-name>/output'``
+
+   -  **Batch Transform**:
+      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
+
+-  **Region:**\ The default pipelines work in us-east-1. If your
+   cluster is in a different region, update the following:
+
+   -  The ``region='us-east-1'`` Parameter in the input list.
+
+   -  The algorithm images for Amazon SageMaker. If you use one of
+      the Amazon SageMaker built-in algorithm images, select the image
+      for your region. Construct the image name using the information
+      in `Common parameters for built-in
+      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
+      For Example:
+
+      ::
+
+          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
+
+   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
+      to copy the data to a bucket in the same region as the cluster.
+
+You can adjust any of the input parameters using the KFP UI and trigger
+your run again.
+
+Compile and deploy your pipeline
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After defining the pipeline in Python, you must compile the pipeline to
+an intermediate representation before you can submit it to the Kubeflow
+Pipelines service. The intermediate representation is a workflow
+specification in the form of a YAML file compressed into a tar.gz
+file. You need the KFP SDK to compile your pipeline.
+
+Install KFP SDK
+^^^^^^^^^^^^^^^
+
+Run the following from the command line of your gateway node:
+
+Install the KFP SDK following the instructions in the \ `Kubeflow
+pipelines
+documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
+
+Verify that the KFP SDK is installed with the following command:
+
+::
+
+    pip show kfp
+
+Verify that ``dsl-compile`` has been installed correctly as follows:
+
+::
+
+    which dsl-compile
+
+Compile your pipeline
+^^^^^^^^^^^^^^^^^^^^^
+
+You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
+CLI, or the KFP SDK. The following sections illustrate the workflow
+using the KFP UI and CLI.
+
+Complete the following from your gateway node to compile your pipeline.
+
+Modify your Python file with your S3 bucket name and IAM role ARN.
+
+Use the ``dsl-compile`` command from the command line to compile your
+pipeline as follows. Replace ``<path-to-python-file>`` with the path
+to your pipeline and ``<path-to-output>`` with the location where you
+want your tar.gz file to be.
+
+::
+
+    dsl-compile --py <path-to-python-file> --output <path-to-output>
+
+Upload and run the pipeline using the KFP CLI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Complete the following steps from the command line of your gateway node.
+KFP organizes runs of your pipeline as experiments. You have the option
+to specify an experiment name. If you do not specify one, the run will
+be listed under ‘Default’ experiment.
+
+Upload your pipeline as follows:
+
+::
+
+    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
+
+Your output should look like the following. Take note of the \ ``ID``.
+
+::
+
+    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
+
+    Pipeline Details
+    ------------------
+    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
+    Name         sm-pipeline
+    Description
+    Uploaded at  2020-04-30T20:22:39+00:00
+    ...
+    ...
+
+Create a run using the following command. The KFP CLI run command
+currently does not support specifying input parameters while creating
+the run. You need to update your parameters in the Python pipeline file
+before compiling. Replace ``<experiment-name>`` and ``<job-name>``
+with any names. Replace ``<pipeline-id>`` with the ID of your submitted
+pipeline.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
+
+You can also directly submit a run using the compiled pipeline package
+created as the output of the ``dsl-compile`` command.
+
+::
+
+    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
+
+Your output should look like the following:
+
+::
+
+    Creating experiment aws.
+    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
+    +--------------------------------------+--------+----------+---------------------------+
+    | run id                               | name   | status   | created at                |
+    +======================================+========+==========+===========================+
+    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
+    +--------------------------------------+--------+----------+---------------------------+
+
+Navigate to the UI to check the progress of the job
+
+Upload and run the pipeline using the KFP UI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  In the upper-right corner, choose ``+UploadPipeline``.
+
+-  Enter the pipeline name and description.
+
+-  Choose ``Upload a file`` and enter the path to the tar.gz file you
+   created using the CLI or with the Python SDK.
+
+-  On the left panel, choose the **Pipelines** tab.
+
+-  Find the pipeline you created.
+
+-  Choose ``+CreateRun``.
+
+-  Enter your input parameters.
+
+-  Choose ``Run``.
+
+Running predictions
+~~~~~~~~~~~~~~~~~~~
+
+Once your classification pipeline is deployed, you can run
+classification predictions against the endpoint that was created by the
+Deploy component. Use the KFP UI to check the output artifacts
+for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
+file to extract the endpoint name or check the Amazon SageMaker console
+in the region you used.
+
+Configure permissions to run predictions
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+If you want to run predictions from your gateway node, skip this
+section.
+
+If you want to use any other machine to run predictions, assign
+the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
+user used by the client machine. This permission is used to run
+predictions.
+
+On your gateway node, run the following to create a policy file:
+
+::
+
+    cat <<EoF > ./sagemaker-invoke.json
+    {
+        "Version": "2012-10-17",
+        "Statement": [
+            {
+                "Effect": "Allow",
+                "Action": [
+                    "sagemaker:InvokeEndpoint"
+                ],
+                "Resource": "*"
+            }
+        ]
+    }
+    EoF
+
+Attach the policy to the client node’s IAM role or IAM user.
+
+If your client machine has an IAM role attached, run the following.
+Replace ``<your-instance-IAM-role>`` with the name of the client
+node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+If your client machine has IAM user credentials configured, run the
+following. Replace ``<your_IAM_user_name>`` with the name of the client
+node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
+path to the policy file you created.
+
+::
+
+    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
+
+Run predictions
+^^^^^^^^^^^^^^^
+
+Create a Python file from your client machine
+named ``mnist-predictions.py`` with the following content . Replace
+the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
+MNIST dataset, then creates a CSV from those digits and sends it to the
+endpoint for prediction. It then outputs the results.
+
+::
+
+    import pickle, gzip, numpy, urllib.request, json
+    from urllib.parse import urlparse
+    import json
+    import io
+    import boto3
+
+    ENDPOINT_NAME='<endpoint-name>'
+    REGION = '<region>'
+
+    # Load the dataset
+    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
+    with gzip.open('mnist.pkl.gz', 'rb') as f:
+        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
+
+    # Simple function to create a csv from our numpy array
+    def np2csv(arr):
+        csv = io.BytesIO()
+        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
+        return csv.getvalue().decode().rstrip()
+
+    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
+
+    payload = np2csv(train_set[0][30:31])
+
+    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
+                                       ContentType='text/csv',
+                                       Body=payload)
+    result = json.loads(response['Body'].read().decode())
+    print(result)
+
+Run the Python file as follows:
+
+::
+
+    python mnist-predictions.py
+
+View results and logs
+~~~~~~~~~~~~~~~~~~~~~
+
+When the pipeline is running, you can click on any component to check
+execution details, such as inputs and outputs. This will list the names
+of created resources.
+
+If the KFP request is successfully processed and an Amazon SageMaker job
+is created, the component logs in the KFP UI will provide a link to the
+job created in Amazon SageMaker. The CloudWatch logs will also be
+provided if the job is successfully created.
+
+If you run too many pipeline jobs on the same cluster, you may see an
+error message that indicates you do not have enough pods available. To
+fix this, log in to your gateway node and delete the pods created by the
+pipelines you are not using as follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Cleanup
+~~~~~~~
+
+When you’re finished with your pipeline, you need to cleanup your
+resources.
+
+From the KFP dashboard, terminate your pipeline runs if they do not exit
+properly by clicking ``Terminate``.
+
+If the ``Terminate`` option doesn’t work, log in to your gateway node
+and terminate all the pods created by your pipeline run manually as
+follows:
+
+::
+
+    kubectl get pods -n kubeflow
+    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
+
+Using your AWS account, log in to the Amazon SageMaker service. Manually
+stop all training, batch transform, and HPO jobs. Delete models, data
+buckets and endpoints to avoid incurring any additional
+costs. Terminating the pipeline runs does not stop the jobs in Amazon
+SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-28 21:14:55[0m
[92mHash: 151de9a4f4abefaab634b0bcf35524d0d97a7c89[0m
[92mFilepath: doc/kubernetes/using_amazon_sagemaker_components.rst[0m
[92mBranch: origin/master[0m
[92mCommit: [doc] Added Amazon Components for Kubeflow Pipelines (#1533)

* Create amazon_sagemaker_components_for_kubeflow_pipelines.rst

* Create using_amazon_sagemaker_components.rst

* Update index.rst

* fixed formatting issues

* fixed formatting issues[0m
@@ -1,835 +0,0 @@
-Using Amazon SageMaker Components
-=================================
-
-In this tutorial, you run a pipeline using Amazon SageMaker Components
-for Kubeflow Pipelines to train a classification model using Kmeans with
-the MNIST dataset. This workflow uses Kubeflow pipelines as the
-orchestrator and Amazon SageMaker as the backend to run the steps in the
-workflow. For the full code for this and other pipeline examples, see
-the `Sample AWS SageMaker Kubeflow
-Pipelines <https://github.com/kubeflow/pipelines/tree/master/samples/contrib/aws-samples>`__.
-For information on the components used, see the `KubeFlow Pipelines
-GitHub
-repository <https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker>`__.
-
-Setup
------
-
-To use Kubeflow Pipelines (KFP), you need an Amazon Elastic Kubernetes
-Service (Amazon EKS) cluster and a gateway node to interact with that
-cluster. The following sections show the steps needed to set up these
-resources.
-
-Set up a gateway node
-~~~~~~~~~~~~~~~~~~~~~
-
-A gateway node is used to create an Amazon EKS cluster and access the
-Kubeflow Pipelines UI. Use your local machine or an Amazon EC2 instance
-as your gateway node. If you want to use a new EC2 instance, create one
-with the latest Ubuntu 18.04 DLAMI version from the AWS console using
-the steps in `Launching and Configuring a
-DLAMI <https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>`__.
-
-Complete the following steps to set up your gateway node. Depending on
-your environment, you may have certain requirements already configured.
-
-If you don’t have an existing Amazon EKS cluster, create a user named ``your_credentials`` using the steps in `Creating an IAM User in Your
-AWS
-Account <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html>`__. If
-you have an existing Amazon EKS cluster, use the credentials of the IAM
-role or user that has access to it.
-
-Add the following permissions to your user using the steps in `Changing
-Permissions for an IAM
-User: <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html#users_change_permissions-add-console>`__
-
--  CloudWatchLogsFullAccess
-
--  `AWSCloudFormationFullAccess <https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAWSCloudFormationFullAccess>`__
-
--  IAMFullAccess
-
--  AmazonS3FullAccess
-
--  AmazonEC2FullAccess
-
--  AmazonEKSAdminPolicy - Create this policy using the schema
-   from `Amazon EKS Identity-Based Policy
-   Examples <https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html>`__
-
-Install the following on your gateway node to access the Amazon EKS
-cluster and KFP UI.
-
--  `AWS
-   CLI <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__.
-   If you are using an IAM user, configure your `Access Key ID, Secret
-   Access
-   Key <https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>`__ and
-   preferred AWS Region by running: ``aws configure``
-
--  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-   version 0.1.31 and above.
-
--  ``eksctl`` version above 0.15.
-
--  `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-   The version needs to match your Kubernetes version within 1 minor
-   version.
-
-Install \ ``boto3``.
-
-::
-
-    pip install boto3
-
-Set up an Amazon EKS cluster
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-set up an Amazon EKS cluster:
-
-If you do not have an existing Amazon EKS cluster, complete the
-following substeps. If you already have an Amazon EKS cluster, skip this
-step.
-
-Run the following from your command line to create an Amazon EKS Cluster
-with version 1.14 or above. Replace ``<your-cluster-name>`` with any
-name for your cluster.
-
-::
-
-    eksctl create cluster --name <your-cluster-name> --region us-east-1 --auto-kubeconfig --timeout=50m --managed --nodes=1
-
-When cluster creation is complete, verify that you have access to the
-cluster using the following command.
-
-::
-
-    kubectl get nodes
-
-Verify that the current kubectl context is the cluster you want to use
-with the following command. The current context is marked with an
-asterisk (\*) in the output.
-
-::
-
-    kubectl config get-contexts
-
-    CURRENT NAME     CLUSTER
-    *   <username>@<clustername>.us-east-1.eksctl.io   <clustername>.us-east-1.eksctl.io
-
-If the desired cluster is not configured as your current default, update
-the default with the following command.
-
-::
-
-    aws eks update-kubeconfig --name <clustername> --region us-east-1
-
-Install Kubeflow Pipelines
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Run the following steps from the command line of your gateway node to
-install Kubeflow Pipelines on your cluster.
-
-Install Kubeflow Pipelines on your cluster by following step 1
-of `Deploying Kubeflow Pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines>`__.
-Your KFP version must be 0.5.0 or above.
-
-Verify that the Kubeflow Pipelines service and other related resources
-are running.
-
-::
-
-    kubectl -n kubeflow get all | grep pipeline
-
-Your output should look like the following.
-
-::
-
-    pod/ml-pipeline-6b88c67994-kdtjv                      1/1     Running            0          2d
-    pod/ml-pipeline-persistenceagent-64d74dfdbf-66stk     1/1     Running            0          2d
-    pod/ml-pipeline-scheduledworkflow-65bdf46db7-5x9qj    1/1     Running            0          2d
-    pod/ml-pipeline-ui-66cc4cffb6-cmsdb                   1/1     Running            0          2d
-    pod/ml-pipeline-viewer-crd-6db65ccc4-wqlzj            1/1     Running            0          2d
-    pod/ml-pipeline-visualizationserver-9c47576f4-bqmx4   1/1     Running            0          2d
-    service/ml-pipeline                       ClusterIP   10.100.170.170   <none>        8888/TCP,8887/TCP   2d
-    service/ml-pipeline-ui                    ClusterIP   10.100.38.71     <none>        80/TCP              2d
-    service/ml-pipeline-visualizationserver   ClusterIP   10.100.61.47     <none>        8888/TCP            2d
-    deployment.apps/ml-pipeline                       1/1     1            1           2d
-    deployment.apps/ml-pipeline-persistenceagent      1/1     1            1           2d
-    deployment.apps/ml-pipeline-scheduledworkflow     1/1     1            1           2d
-    deployment.apps/ml-pipeline-ui                    1/1     1            1           2d
-    deployment.apps/ml-pipeline-viewer-crd            1/1     1            1           2d
-    deployment.apps/ml-pipeline-visualizationserver   1/1     1            1           2d
-    replicaset.apps/ml-pipeline-6b88c67994                      1         1         1       2d
-    replicaset.apps/ml-pipeline-persistenceagent-64d74dfdbf     1         1         1       2d
-    replicaset.apps/ml-pipeline-scheduledworkflow-65bdf46db7    1         1         1       2d
-    replicaset.apps/ml-pipeline-ui-66cc4cffb6                   1         1         1       2d
-    replicaset.apps/ml-pipeline-viewer-crd-6db65ccc4            1         1         1       2d
-    replicaset.apps/ml-pipeline-visualizationserver-9c47576f4   1         1         1       2d
-
-Access the KFP UI
-~~~~~~~~~~~~~~~~~
-
-The Kubeflow Pipelines UI is used for managing and tracking experiments,
-jobs, and runs on your cluster. You can use port forwarding to access
-the Kubeflow Pipelines UI from your gateway node.
-
-Set up port forwarding to the KFP UI service
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Verify that the KFP UI service is running using the following command:
-
-::
-
-    kubectl -n kubeflow get service ml-pipeline-ui
-
-    NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
-    ml-pipeline-ui   ClusterIP   10.100.38.71   <none>        80/TCP    2d22h
-
-Run the following command to setup port forwarding to the KFP UI
-service. This forwards the KFP UI to port 8080 on your gateway node and
-allows you to access the KFP UI from your browser.
-
-    **Note**
-
-    The port-forward from your remote machine drops if there is no
-    activity. Run this command again if your dashboard is unable to get
-    logs or updates. If the commands return an error, ensure that there
-    is no process already running on the port you are trying to use.
-
-::
-
-    kubectl port-forward -n kubeflow service/ml-pipeline-ui 8080:80
-
-Your method of accessing the KFP UI depends on your gateway node type.
-
-Local machine as the gateway node
-
-Access the dashboard in your browser as follows:
-
-::
-
-    http://localhost:8080
-
-Click **Pipelines** to access the pipelines UI.
-
-EC2 instance as the gateway node
-
-You need to setup an SSH tunnel on your EC2 instance to access the
-Kubeflow dashboard from your local machine’s browser.
-
-From a new terminal session in your local machine, run the following.
-Replace ``<public-DNS-of-gateway-node>`` with the IP address of your
-instance found on the EC2 console. You can also use the public DNS.
-Replace ``<path_to_key>`` with the path to the pem key used to access
-the gateway node.
-
-::
-
-    public_DNS_address=<public-DNS-of-gateway-node>
-    key=<path_to_key>
-
-    on Ubuntu:
-    ssh -i ${key} -L 9000:localhost:8080 ubuntu@${public_DNS_address}
-
-    or on Amazon Linux:
-    ssh -i ${key} -L 9000:localhost:8080 ec2-user@${public_DNS_address}
-
-Access the dashboard in your browser.
-
-::
-
-    http://localhost:9000
-
-Click **Pipelines** to access the KFP UI.
-
-Create IAM Users/Roles for KFP pods and the Amazon SageMaker service
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You now have a Kubernetes cluster with Kubeflow set up. To run Amazon
-SageMaker Components for Kubeflow Pipelines, the Kubeflow Pipeline pods
-need access to SageMaker. In this section, you create IAM Users/Roles to
-be used by Kubeflow Pipeline pods and Amazon SageMaker.
-
-Create a KFP execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Enable OIDC support on the Amazon EKS cluster with the following
-command. Replace ``<cluster_name>`` with the name of your cluster
-and ``<cluster_region>`` with the region your cluster is in.
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster <cluster-name> \
-            --region <cluster-region> --approve
-
-Run the following to get the `OIDC <https://openid.net/connect/>`__
-issuer URL. This URL is in the
-form ``https://oidc.eks.<region>.amazonaws.com/id/<OIDC_ID>`` .
-
-::
-
-    aws eks describe-cluster --region <cluster-region> --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
-
-Run the following to create a file named ``trust.json``.
-Replace ``<OIDC_URL>`` with your OIDC issuer URL. Don’t
-include ``https://`` when in your OIDC issuer URL.
-Replace ``<AWS_account_number>`` with your AWS account number.
-
-::
-
-    OIDC_URL="<OIDC-URL>"
-    AWS_ACC_NUM="<AWS-account-number>"
-
-    # Run this to create trust.json file
-    cat <<EOF > trust.json
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::${AWS_ACC_NUM}:oidc-provider/${OIDC_URL}"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "${OIDC_URL}:aud": "sts.amazonaws.com",
-              "${OIDC_URL}:sub": "system:serviceaccount:kubeflow:pipeline-runner"
-            }
-          }
-        }
-      ]
-    }
-    EOF
-
-Create an IAM role named ``kfp-example-pod-role`` using ``trust.json``
-using the following command. This role is used by KFP pods to create
-Amazon SageMaker jobs from KFP components. Note the ARN returned in the
-output.
-
-::
-
-    aws iam create-role --role-name kfp-example-pod-role --assume-role-policy-document file://trust.json
-    aws iam attach-role-policy --role-name kfp-example-pod-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam get-role --role-name kfp-example-pod-role --output text --query 'Role.Arn'
-
-Edit your pipeline-runner service account with the following command.
-
-::
-
-    kubectl edit -n kubeflow serviceaccount pipeline-runner
-
-In the file, add the following Amazon EKS role annotation and
-replace ``<role_arn>`` with your role ARN.
-
-::
-
-    eks.amazonaws.com/role-arn: <role-arn>
-
-Your file should look like the following when you’ve added the Amazon
-EKS role annotation. Save the file.
-
-::
-
-    apiVersion: v1
-    kind: ServiceAccount
-    metadata:
-      annotations:
-        eks.amazonaws.com/role-arn: <role-arn>
-        kubectl.kubernetes.io/last-applied-configuration: |
-          {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app":"pipeline-runner","app.kubernetes.io/component":"pipelines-runner","app.kubernetes.io/instance":"pipelines-runner-0.2.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"pipelines-runner","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.2.0"},"name":"pipeline-runner","namespace":"kubeflow"}}
-      creationTimestamp: "2020-04-16T05:48:06Z"
-      labels:
-        app: pipeline-runner
-        app.kubernetes.io/component: pipelines-runner
-        app.kubernetes.io/instance: pipelines-runner-0.2.0
-        app.kubernetes.io/managed-by: kfctl
-        app.kubernetes.io/name: pipelines-runner
-        app.kubernetes.io/part-of: kubeflow
-        app.kubernetes.io/version: 0.2.0
-      name: pipeline-runner
-      namespace: kubeflow
-      resourceVersion: "11787"
-      selfLink: /api/v1/namespaces/kubeflow/serviceaccounts/pipeline-runner
-      uid: d86234bd-7fa5-11ea-a8f2-02934be6dc88
-    secrets:
-    - name: pipeline-runner-token-dkjrk
-
-Create an Amazon SageMaker execution role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The ``kfp-example-sagemaker-execution-role`` IAM role is used
-by Amazon SageMaker jobs to access AWS resources. For more information,
-see the IAM Permissions section. You provide this role as an input
-parameter when running the pipeline.
-
-Run the following to create the role. Note the ARN that is returned in
-your output.
-
-::
-
-    SAGEMAKER_EXECUTION_ROLE_NAME=kfp-example-sagemaker-execution-role
-
-    TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
-    aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-    aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn'
-
-Add access to additional IAM users or roles
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you use an intuitive IDE like Jupyter or want other people in your
-organization to use the cluster you set up, you can also give them
-access. The following steps run through this workflow using Amazon
-SageMaker notebooks. An Amazon SageMaker notebook instance is a fully
-managed Amazon EC2 compute instance that runs the Jupyter Notebook App.
-You use the notebook instance to create and manage Jupyter notebooks to
-create ML workflows. You can define, compile, deploy, and run your
-pipeline using the KFP Python SDK or CLI. If you’re not using an Amazon
-SageMaker notebook to run Jupyter, you need to install the `AWS
-CLI  <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html>`__\ and
-the latest version
-of `kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>`__.
-
-Follow the steps in `Create an Amazon SageMaker Notebook
-Instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__
-to create a Amazon SageMaker notebook instance if you do not already
-have one. Give the IAM role for this instance the ``S3FullAccess``
-permission.
-
-Amazon EKS clusters use IAM users and roles to control access to the
-cluster. The rules are implemented in a config map named ``aws-auth``.
-Only the user/role that has access to the cluster will be able to edit
-this config map. Run the following from the command line of your gateway
-node to get the IAM role of the notebook instance you created.
-Replace ``<instance-name>`` with the name of your instance.
-
-::
-
-    aws sagemaker describe-notebook-instance --notebook-instance-name <instance-name> --region <region> --output text --query 'RoleArn'
-
-This command outputs the IAM role ARN in
-the ``arn:aws:iam::<account-id>:role/<role-name>`` format. Take note
-of this ARN.
-
-Run the following to attach the policies the IAM role.
-Replace ``<role-name>`` with the ``<role-name>`` in your ARN.
-
-::
-
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
-    aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
-
-``eksctl`` provides commands to read and edit the ``aws-auth`` config
-map. ``system:masters`` is one of the default user groups. You add the
-user to this group. The "system:masters" group has super user
-permissions to the cluster. You can also create a group with more
-restrictive permissions or you can bind permissions directly to users.
-Replace ``<IAM-Role-arn>`` with the ARN of the IAM
-role. ``<your_username>`` can be any unique username.
-
-::
-
-    eksctl create iamidentitymapping \
-        --cluster <cluster-name> \
-        --arn <IAM-Role-arn> \
-        --group system:masters \
-        --username <your-username> \
-        --region <region>
-
-Open the Jupyter notebook on your Amazon SageMaker instance and run the
-following to verify that it has access to the cluster.
-
-::
-
-    aws eks --region <region> update-kubeconfig --name <cluster-name>
-    kubectl -n kubeflow get all | grep pipeline
-
-Running the Kubeflow Pipeline
------------------------------
-
-Now that setup of your gateway node and Amazon EKS cluster is complete,
-you can create your classification pipeline. To create your pipeline,
-you need to define and compile it. You then deploy it and use it to run
-workflows. You can define your pipeline in Python and use the KFP
-dashboard, KFP CLI, or Python SDK to compile, deploy, and run your
-workflows.
-
-Prepare datasets
-~~~~~~~~~~~~~~~~
-
-To run the pipelines, you need to have the datasets in an S3 bucket in
-your account. This bucket must be located in the region where you want
-to run Amazon SageMaker jobs. If you don’t have a bucket, create one
-using the steps in `Creating a
-bucket <https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html>`__.
-
-From your gateway node, run the `sample dataset
-creation <https://github.[93mcom/kubeflow/pipelines/tree/[93m34615cb19edfacf9f4d9f2417e9254d52dd53474[0m/samples/contrib/aws[0m-samples/mnist-kmeans-sagemaker#the-sample-dataset>`__
-script to copy the datasets into your bucket. Change the bucket name in
-the script to the one you created.
-
-Create a Kubeflow Pipeline using Amazon SageMaker Components
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The full code for the MNIST classification pipeline is available in the
-`Kubeflow Github
-repository <https://github.com/kubeflow/pipelines/blob/master/samples/contrib/aws-samples/mnist-kmeans-sagemaker>`__.
-To use it, clone the example Python files to your gateway node.
-
-Input Parameters
-^^^^^^^^^^^^^^^^
-
-The full MNIST classification pipeline has run-specific parameters that
-you must provide values for when creating a run. You must provide these
-parameters for each component of your pipeline. These parameters can
-also be updated when using other pipelines. We have provided default
-values for all parameters in the sample classification pipeline file.
-
-The following are the only parameters you may need to modify to run the
-sample pipelines. To modify these parameters, update their entries in
-the sample classification pipeline file.
-
--  **Role-ARN:** This must be the ARN of an IAM role that has full
-   Amazon SageMaker access in your AWS account. Use the ARN
-   of  ``kfp-example-pod-role``.
-
--  **The Dataset Buckets**: You must change the S3 bucket with the input
-   data for each of the components. Replace the following with the link
-   to your S3 bucket:
-
-   -  **Train channel:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **HPO channels for test/HPO channel for
-      train:** ``"S3Uri": "s3://<your-s3-bucket-name>/data"``
-
-   -  **Batch
-      transform:** ``"batch-input": "s3://<your-s3-bucket-name>/data"``
-
--  **Output buckets:** Replace the output buckets with S3 buckets you
-   have write permission to. Replace the following with the link to your
-   S3 bucket:
-
-   -  **Training/HPO**:
-      ``output_location='s3://<your-s3-bucket-name>/output'``
-
-   -  **Batch Transform**:
-      ``batch_transform_ouput='s3://<your-s3-bucket-name>/output'``
-
--  **Region:**\ The default pipelines work in us-east-1. If your
-   cluster is in a different region, update the following:
-
-   -  The ``region='us-east-1'`` Parameter in the input list.
-
-   -  The algorithm images for Amazon SageMaker. If you use one of
-      the Amazon SageMaker built-in algorithm images, select the image
-      for your region. Construct the image name using the information
-      in `Common parameters for built-in
-      algorithms <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.
-      For Example:
-
-      ::
-
-          382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1
-
-   -  The S3 buckets with the dataset. Use the steps in Prepare datasets
-      to copy the data to a bucket in the same region as the cluster.
-
-You can adjust any of the input parameters using the KFP UI and trigger
-your run again.
-
-Compile and deploy your pipeline
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After defining the pipeline in Python, you must compile the pipeline to
-an intermediate representation before you can submit it to the Kubeflow
-Pipelines service. The intermediate representation is a workflow
-specification in the form of a YAML file compressed into a tar.gz
-file. You need the KFP SDK to compile your pipeline.
-
-Install KFP SDK
-^^^^^^^^^^^^^^^
-
-Run the following from the command line of your gateway node:
-
-Install the KFP SDK following the instructions in the \ `Kubeflow
-pipelines
-documentation <https://www.kubeflow.org/docs/pipelines/sdk/install-sdk/>`__.
-
-Verify that the KFP SDK is installed with the following command:
-
-::
-
-    pip show kfp
-
-Verify that ``dsl-compile`` has been installed correctly as follows:
-
-::
-
-    which dsl-compile
-
-Compile your pipeline
-^^^^^^^^^^^^^^^^^^^^^
-
-You have three options to interact with Kubeflow Pipelines: KFP UI, KFP
-CLI, or the KFP SDK. The following sections illustrate the workflow
-using the KFP UI and CLI.
-
-Complete the following from your gateway node to compile your pipeline.
-
-Modify your Python file with your S3 bucket name and IAM role ARN.
-
-Use the ``dsl-compile`` command from the command line to compile your
-pipeline as follows. Replace ``<path-to-python-file>`` with the path
-to your pipeline and ``<path-to-output>`` with the location where you
-want your tar.gz file to be.
-
-::
-
-    dsl-compile --py <path-to-python-file> --output <path-to-output>
-
-Upload and run the pipeline using the KFP CLI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Complete the following steps from the command line of your gateway node.
-KFP organizes runs of your pipeline as experiments. You have the option
-to specify an experiment name. If you do not specify one, the run will
-be listed under ‘Default’ experiment.
-
-Upload your pipeline as follows:
-
-::
-
-    kfp pipeline upload --pipeline-name <pipeline-name> <path-to-output-tar.gz>
-
-Your output should look like the following. Take note of the \ ``ID``.
-
-::
-
-    Pipeline 29c3ff21-49f5-4dfe-94f6-618c0e2420fe has been submitted
-
-    Pipeline Details
-    ------------------
-    ID           29c3ff21-49f5-4dfe-94f6-618c0e2420fe
-    Name         sm-pipeline
-    Description
-    Uploaded at  2020-04-30T20:22:39+00:00
-    ...
-    ...
-
-Create a run using the following command. The KFP CLI run command
-currently does not support specifying input parameters while creating
-the run. You need to update your parameters in the Python pipeline file
-before compiling. Replace ``<experiment-name>`` and ``<job-name>``
-with any names. Replace ``<pipeline-id>`` with the ID of your submitted
-pipeline.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --pipeline-id <pipeline-id>
-
-You can also directly submit a run using the compiled pipeline package
-created as the output of the ``dsl-compile`` command.
-
-::
-
-    kfp run submit --experiment-name <experiment-name> --run-name <job-name> --package-file <path-to-output>
-
-Your output should look like the following:
-
-::
-
-    Creating experiment aws.
-    Run 95084a2c-f18d-4b77-a9da-eba00bf01e63 is submitted
-    +--------------------------------------+--------+----------+---------------------------+
-    | run id                               | name   | status   | created at                |
-    +======================================+========+==========+===========================+
-    | 95084a2c-f18d-4b77-a9da-eba00bf01e63 | sm-job |          | 2020-04-30T20:36:41+00:00 |
-    +--------------------------------------+--------+----------+---------------------------+
-
-Navigate to the UI to check the progress of the job
-
-Upload and run the pipeline using the KFP UI
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
--  On the left panel, choose the **Pipelines** tab.
-
--  In the upper-right corner, choose ``+UploadPipeline``.
-
--  Enter the pipeline name and description.
-
--  Choose ``Upload a file`` and enter the path to the tar.gz file you
-   created using the CLI or with the Python SDK.
-
--  On the left panel, choose the **Pipelines** tab.
-
--  Find the pipeline you created.
-
--  Choose ``+CreateRun``.
-
--  Enter your input parameters.
-
--  Choose ``Run``.
-
-Running predictions
-~~~~~~~~~~~~~~~~~~~
-
-Once your classification pipeline is deployed, you can run
-classification predictions against the endpoint that was created by the
-Deploy component. Use the KFP UI to check the output artifacts
-for ``sagemaker-deploy-model-endpoint_name``. Download the .tgz
-file to extract the endpoint name or check the Amazon SageMaker console
-in the region you used.
-
-Configure permissions to run predictions
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-If you want to run predictions from your gateway node, skip this
-section.
-
-If you want to use any other machine to run predictions, assign
-the ``sagemaker:InvokeEndpoint`` permission to the IAM role or IAM
-user used by the client machine. This permission is used to run
-predictions.
-
-On your gateway node, run the following to create a policy file:
-
-::
-
-    cat <<EoF > ./sagemaker-invoke.json
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "sagemaker:InvokeEndpoint"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
-    EoF
-
-Attach the policy to the client node’s IAM role or IAM user.
-
-If your client machine has an IAM role attached, run the following.
-Replace ``<your-instance-IAM-role>`` with the name of the client
-node’s IAM role. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-role-policy --role-name <your-instance-IAM-role> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-If your client machine has IAM user credentials configured, run the
-following. Replace ``<your_IAM_user_name>`` with the name of the client
-node’s IAM user. Replace ``<path-to-sagemaker-invoke-json>`` with the
-path to the policy file you created.
-
-::
-
-    aws iam put-user-policy --user-name <your-IAM-user-name> --policy-name sagemaker-invoke-for-worker --policy-document file://<path-to-sagemaker-invoke-json>
-
-Run predictions
-^^^^^^^^^^^^^^^
-
-Create a Python file from your client machine
-named ``mnist-predictions.py`` with the following content . Replace
-the ``ENDPOINT_NAME`` and ``REGION`` variables. This script loads the
-MNIST dataset, then creates a CSV from those digits and sends it to the
-endpoint for prediction. It then outputs the results.
-
-::
-
-    import pickle, gzip, numpy, urllib.request, json
-    from urllib.parse import urlparse
-    import json
-    import io
-    import boto3
-
-    ENDPOINT_NAME='<endpoint-name>'
-    REGION = '<region>'
-
-    # Load the dataset
-    urllib.request.urlretrieve("http://deeplearning.net/data/mnist/mnist.pkl.gz", "mnist.pkl.gz")
-    with gzip.open('mnist.pkl.gz', 'rb') as f:
-        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
-
-    # Simple function to create a csv from our numpy array
-    def np2csv(arr):
-        csv = io.BytesIO()
-        numpy.savetxt(csv, arr, delimiter=',', fmt='%g')
-        return csv.getvalue().decode().rstrip()
-
-    runtime = boto3.Session(region_name=REGION).client('sagemaker-runtime')
-
-    payload = np2csv(train_set[0][30:31])
-
-    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
-                                       ContentType='text/csv',
-                                       Body=payload)
-    result = json.loads(response['Body'].read().decode())
-    print(result)
-
-Run the Python file as follows:
-
-::
-
-    python mnist-predictions.py
-
-View results and logs
-~~~~~~~~~~~~~~~~~~~~~
-
-When the pipeline is running, you can click on any component to check
-execution details, such as inputs and outputs. This will list the names
-of created resources.
-
-If the KFP request is successfully processed and an Amazon SageMaker job
-is created, the component logs in the KFP UI will provide a link to the
-job created in Amazon SageMaker. The CloudWatch logs will also be
-provided if the job is successfully created.
-
-If you run too many pipeline jobs on the same cluster, you may see an
-error message that indicates you do not have enough pods available. To
-fix this, log in to your gateway node and delete the pods created by the
-pipelines you are not using as follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Cleanup
-~~~~~~~
-
-When you’re finished with your pipeline, you need to cleanup your
-resources.
-
-From the KFP dashboard, terminate your pipeline runs if they do not exit
-properly by clicking ``Terminate``.
-
-If the ``Terminate`` option doesn’t work, log in to your gateway node
-and terminate all the pods created by your pipeline run manually as
-follows:
-
-::
-
-    kubectl get pods -n kubeflow
-    kubectl delete pods -n kubeflow <name-of-pipeline-pod>
-
-Using your AWS account, log in to the Amazon SageMaker service. Manually
-stop all training, batch transform, and HPO jobs. Delete models, data
-buckets and endpoints to avoid incurring any additional
-costs. Terminating the pipeline runs does not stop the jobs in Amazon
-SageMaker.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-28 12:17:07[0m
[92mHash: bb46c1caf98e81995c199985ed03471a83adadd4[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.60.1.post0
[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-28 12:17:07[0m
[92mHash: bb46c1caf98e81995c199985ed03471a83adadd4[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: prepare release v1.60.1.post0
[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-28 11:20:37[0m
[92mHash: 5b078f758a915c37618eee4d27d7bbefc8b86238[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: add v2 migration script to console_scripts in setup.py (#1530)

[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-28 11:20:37[0m
[92mHash: 5b078f758a915c37618eee4d27d7bbefc8b86238[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: change: add v2 migration script to console_scripts in setup.py (#1530)

[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-27 14:20:09[0m
[92mHash: af3b54ef77119c95d9f3ea737a6ee02147ac220b[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: update development version to v1.60.2.dev0
[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-27 14:20:09[0m
[92mHash: af3b54ef77119c95d9f3ea737a6ee02147ac220b[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: update development version to v1.60.2.dev0
[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-27 14:04:13[0m
[92mHash: 40f1d7586bcc396d61e7786ddb68df8c34053b91[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove estimator parameters for TF legacy mode (#1510)

[0m
@@ -1,898 +1,3 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-27 14:04:13[0m
[92mHash: 40f1d7586bcc396d61e7786ddb68df8c34053b91[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: breaking: remove estimator parameters for TF legacy mode (#1510)

[0m
@@ -0,0 +1,893 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-26 18:19:13[0m
[92mHash: 39494161f5964a415cca417ac5d3cdd9a65c25e9[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: refactor the navigation (#1525)

[0m
@@ -1,3 +1,898 @@
+#########################################
+Amazon SageMaker Operators for Kubernetes
+#########################################
+
+
+
+Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
+
+There is no additional charge to use these operators. You do incur charges
+for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
+
+
+.. contents::
+
+What is an operator?
+--------------------
+
+Kubernetes is built on top of what is called the controller pattern.
+This pattern allows applications and tools to listen to a central state
+manager (ETCD) and act when something happens. Examples of such
+applications
+include ``cloud-controller-manager`` and ``controller-manager``.
+The controller pattern allows you to create decoupled experiences and not
+have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
+
+Prerequisites
+~~~~~~~~~~~~~
+
+This guide assumes that you’ve
+completed the following prerequisites:
+
+-  Installed the following tools on the client machine used to access your k8s cluster:
+
+   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
+      Version 1.13 or later. Use a ``kubectl`` version that is within
+      one minor version of your Amazon Elastic Kubernetes Service
+      (Amazon EKS) cluster control plane. For example, a
+      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
+
+   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
+      later
+
+   -  `AWS
+      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
+      1.16.232 or later
+
+   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
+      3.0 or later
+
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+
+-  Have IAM permissions to create roles and attach policies to roles.
+
+-  Created a Kubernetes cluster to run the operators on. It should either be
+   Kubernetes version 1.13 or 1.14. For automated cluster
+   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   It takes 20 to 30 minutes to provision a cluster.
+
+Permissions overview
+~~~~~~~~~~~~~~~~~~~~
+
+The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
+in Amazon SageMaker from your Kubernetes cluster. Thus the operators
+will access Amazon SageMaker resources on your behalf. The
+IAM role that the operator assumes to interact with AWS resources differs
+from the credentials you use to access the Kubernetes cluster. The
+role also differs from the role that Amazon SageMaker assumes when running your machine learning
+jobs. The following image explains this design and flow.
+
+.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
+
+IAM role-based setup and operator deployment
+--------------------------------------------
+
+The following sections describe the steps to setup and deploy the
+operator.
+
+Cluster-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
+authenticate with the IAM service.
+
+Create an OpenID Connect Provider for Your Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+::
+
+    # Set the Region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get the OIDC ID
+^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create an IAM Role
+^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to the Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator
+^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator Using YAML
+''''''''''''''''''''''''''''''
+
+This is the simplest way to deploy your operators. The process is as
+follows:
+
+-  Download the installer script using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
+
+-  Edit the ``installer.yaml`` file to
+   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f installer.yaml
+
+Deploy the Operator Using Helm Charts
+'''''''''''''''''''''''''''''''''''''
+
+Use the provided Helm Chart to install
+the operator.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    kubectl create namespace sagemaker-k8s-operator-system
+    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
+
+
+.. warning::
+    If you decide to install the operator into a namespace other than the one specified above,
+    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
+
+After a moment, the chart will be installed with a randomly generated
+name. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n sagemaker-k8s-operator-system get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``sagemaker-k8s-operator-system``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Namespace-scoped deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
+
+This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
+
+
+
+
+Create an OpenID Connect Provider for Your EKS Cluster
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The following instruction will create and associate an OIDC provider
+with your EKS cluster.
+
+Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+variables as follows:
+
+.. code:: shell
+
+    # Set the region and cluster
+    export CLUSTER_NAME="<your cluster name>"
+    export AWS_REGION="<your region>"
+
+Use the following command to associate the OIDC provider with your
+cluster. For more information, see \ `Enabling IAM Roles for Service
+Accounts on your
+Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
+
+::
+
+    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
+        --region ${AWS_REGION} --approve
+
+Your output should look like the following:
+
+::
+
+    [_]  eksctl version 0.10.1
+    [_]  using region us-east-1
+    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
+
+Now that the cluster has an OIDC identity provider, you can create a
+role and give a Kubernetes ServiceAccount permission to assume the role.
+
+Get your OIDC ID
+^^^^^^^^^^^^^^^^
+
+To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
+using the following command:
+
+::
+
+    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
+        --query cluster.identity.oidc.issuer --output text
+
+The command will return a URL like the following:
+
+::
+
+    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
+The OIDC ID for your cluster will be different. You need this OIDC ID
+value to create a role.
+
+If your output is ``None``, it means that your client version is old.
+To work around this, run the following command:
+
+::
+
+    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
+
+The OIDC URL will be returned as follows:
+
+::
+
+    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
+
+Create your IAM Role
+^^^^^^^^^^^^^^^^^^^^
+
+Create a file named ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
+
+::
+
+    {
+      "Version": "2012-10-17",
+      "Statement": [
+        {
+          "Effect": "Allow",
+          "Principal": {
+            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
+          },
+          "Action": "sts:AssumeRoleWithWebIdentity",
+          "Condition": {
+            "StringEquals": {
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
+              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
+            }
+          }
+        }
+      ]
+    }
+
+Run the following command to create a role with the trust
+relationship defined in ``trust.json``. This role enables the
+Amazon EKS cluster to get and refresh credentials from IAM.
+
+::
+
+    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
+
+Your output should look like the following:
+
+::
+
+    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
+    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
+    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
+    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
+    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
+
+Take note of ``ROLE ARN``, you pass this value to your
+operator.
+
+Attach the AmazonSageMakerFullAccess Policy to your Role
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To give the role access to Amazon SageMaker, attach
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+If you want to limit permissions to the operator, you can create your
+own custom policy and attach it.
+
+To attach AmazonSageMakerFullAccess, run the following command:
+
+::
+
+    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
+
+The Kubernetes
+ServiceAccount ``sagemaker-k8s-operator-default`` should
+have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+install the operator.
+
+Deploy the Operator to Your Namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When deploying your operator, you can use either a YAML file or Helm
+charts.
+
+Deploy the Operator to Your Namespace Using YAML
+''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
+
+If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
+
+::
+
+    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+To install the operator onto the cluster:
+
+-  Download the operator installer YAML using the following command:
+
+   ::
+
+       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
+
+-  Update the installer YAML to place the resources into your specified namespace using the following command:
+
+   ::
+
+       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
+
+-  Edit the ``operator.yaml`` file to
+   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+
+-  Use the following command to deploy the cluster:
+
+   ::
+
+       kubectl apply -f operator.yaml
+
+Deploy the Operator to Your Namespace Using Helm Charts
+'''''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
+
+
+Clone the Helm installer directory using the following command:
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
+the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
+created.
+
+Install the Helm Chart using the following command:
+
+::
+
+    helm install crds crd_chart/
+
+
+Create the required namespace and install the operator using the following command:
+
+::
+
+    kubectl create namespace <namespace>
+    helm install --n <namespace> op operator_chart/
+
+
+After a moment, the chart will be installed with the
+name ``sagemaker-operator``. Verify that the installation succeeded by running the following
+command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+
+Verify the operator deployment to your namespace
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You should be able to see the Amazon SageMaker Custom Resource
+Definitions (CRDs) for each operator deployed to your cluster by running
+the following command:
+
+::
+
+    kubectl get crd | grep sagemaker
+
+Your output should look like the following:
+
+::
+
+    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
+    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
+    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
+    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
+    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
+
+Ensure that the operator pod is running successfully. Use the following
+command to list all pods:
+
+::
+
+    kubectl -n my-namespace get pods
+
+You should see a pod
+named ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace ``my-namespace``  as follows:
+
+::
+
+    NAME                                                         READY   STATUS    RESTARTS   AGE
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+
+
+
+Install the Amazon SageMaker logs \ ``kubectl`` plugin
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As part of the Amazon SageMaker Operators for Kubernetes, you can use
+the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+This enables Amazon SageMaker CloudWatch logs to be streamed
+with ``kubectl``. ``kubectl`` must be installed onto
+your `PATH <http://www.linfo.org/path_env_var.html>`__. The
+following commands place the binary in
+the ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your ``PATH``.
+
+::
+
+    export os="linux"
+
+    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
+    tar xvzf ${os}.amd64.tar.gz
+
+    # Move binaries to a directory in your homedir.
+    mkdir ~/sagemaker-k8s-bin
+    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
+
+    # This line will add the binaries to your PATH in your .bashrc.
+
+    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
+
+    # Source your .bashrc to update environment variables:
+    source ~/.bashrc
+
+Use the following command to verify that the ``kubectl`` plugin is
+installed correctly:
+
+::
+
+    kubectl smlogs
+
+If the ``kubectl`` plugin is installed correctly, your output should
+look like the following:
+
+::
+
+    View Amazon SageMaker logs via Kubernetes
+
+    Usage:
+      smlogs [command]
+
+    Aliases:
+      smlogs, SMLogs, Smlogs
+
+    Available Commands:
+      BatchTransformJob       View BatchTransformJob logs via Kubernetes
+      TrainingJob             View TrainingJob logs via Kubernetes
+      help                    Help about any command
+
+    Flags:
+      -h, --help   help for smlogs
+
+    Use "smlogs [command] --help" for more information about a command.
+
+
+Delete operators
+----------------
+
+Delete cluster-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Operators installed using YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to
+delete the operator from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+    # Delete the operator and its resources
+    kubectl delete -f /installer.yaml
+
+You should see output like the following:
+
+::
+
+    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+    $ kubectl delete -f raw-yaml/installer.yaml
+    namespace "sagemaker-k8s-operator-system" deleted
+    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
+    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
+    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
+    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
+    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
+    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
+    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
+    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
+    secrets "sagemaker-k8s-operator-abcde" deleted
+
+Operators installed using Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # get the helm charts
+    $ helm ls
+
+    # delete the charts
+    $ helm delete <chart name>
+
+
+
+Delete namespace-based operators
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+
+Operators installed with YAML
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To uninstall the operator from your cluster, make sure that all
+Amazon SageMaker resources have been deleted from the cluster. Failure
+to do so will cause the operator delete operation to hang. Once you have
+deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
+all jobs and delete the operator from the cluster:
+
+::
+
+    # Delete all Amazon SageMaker jobs from Kubernetes
+    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
+    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
+
+
+::
+
+    # Delete the operator using the same yaml file that was used to install the operator
+    kubectl delete -f operator.yaml
+
+    # Now delete the CRDs using the CRD installer yaml
+    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
+
+    # Now you can delete the namespace if you want
+    kubectl delete namespace <namespace>
+
+Operators installed with Helm Charts
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To delete the operator CRDs, first delete all the running jobs. Then
+delete the helm chart that was used to deploy the operators using the
+following commands:
+
+::
+
+    # Delete the operator
+    $ helm delete -n <namespace> op
+
+    # delete the crds
+    $ helm delete crds
+
+    # optionally delete the namespace
+    $ kubectl delete namespace <namespace>
+
+
+
+
+Troubleshooting
+---------------
+
+Debugging a Failed Job
+~~~~~~~~~~~~~~~~~~~~~~
+
+Check the job status by running:
+
+::
+
+    kubectl get <CRD Type> <job name>
+
+If the job was created in Amazon SageMaker, you can use the following
+command to see the ``STATUS`` and the ``SageMaker Job Name``:
+
+::
+
+    kubectl get <crd type> <job name>
+
+-  You can use ``smlogs`` to find the cause of the issue using the
+   following command:
+
+   ::
+
+       kubectl smlogs <crd type> <job name>
+
+-  You can also use the ``describe`` command to get more details about
+   the job using the following command.The output will have
+   an ``additional`` field that will have more information about the
+   status of the job.
+
+   ::
+
+       kubectl describe <crd type> <job name>
+
+If the job was not created in Amazon SageMaker, then use the logs of the
+operator’s pod to find the cause of the issue as follows:
+
+::
+
+    $ kubectl get pods -A | grep sagemaker
+    # Output:
+    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
+
+    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
+
+Deleting an Operator CRD
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+If deleting a job is stuck, check if the operator is running. If the
+operator is not running, then you will have to delete the finalizer
+using the following steps:
+
+-  In a new terminal, open the job in an editor using ``kubectl edit``
+   as follows:
+
+   ::
+
+       $ kubectl edit <crd type> <job name>
+
+       # for example for the batchtransformjob xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist
+
+-  Edit the job to delete the finalizer by removing the following two
+   lines from the file. Save the file and the job should immediately get
+   deleted/updated.
+
+   ::
+
+         finalizers:
+         - sagemaker-operator-finalizer
+
+Images and SMlogs in each Region
+--------------------------------
+
+The following table lists the available operator images and SMLogs in
+each region.
+
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
++=============+=============================================================================================+========================================================================================================================+
+| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
++-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
+
+
 Using Amazon Sagemaker Jobs
 ---------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-05-26 18:19:13[0m
[92mHash: 39494161f5964a415cca417ac5d3cdd9a65c25e9[0m
[92mFilepath: doc/kubernetes/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: refactor the navigation (#1525)

[0m
@@ -1,893 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include ``cloud-controller-manager`` and ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-IAM role-based setup and operator deployment
---------------------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Create an OpenID Connect Provider for Your Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create an IAM Role
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows:
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    kubectl create namespace sagemaker-k8s-operator-system
-    helm install --namespace sagemaker-k8s-operator-system sagemaker-operator rolebased/
-
-
-.. warning::
-    If you decide to install the operator into a namespace other than the one specified above,
-    you will need to adjust the namespace defined in the IAM role ``trust.json`` file to match.
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      sagemaker-k8s-operator-system   1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0m[0m[0mOIDCID
-
-Create your IAM Role
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator.
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts.
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
-
--  Use the following command to deploy the cluster:
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc.
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # get the helm charts
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-
-
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands:
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-
-
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output:
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated.
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-04-27 16:19:18[0m
[92mHash: db5a109d60b7e3a3dc94b84676c9ece433600dd7[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: infra: add doc8 check for documentation files (#1441)

[0m
@@ -48,7 +48,7 @@ completed the following prerequisites:
    -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
       3.0 or later
 
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__
+   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__ 
 
 -  Have IAM permissions to create roles and attach policies to roles.
 
@@ -140,7 +140,7 @@ The OIDC ID for your cluster will be different. You need this OIDC ID
 value to create a role.
 
 If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
+To work around this, run the following command: 
 
 ::
 
@@ -152,7 +152,7 @@ The OIDC URL will be returned as follows:
 
     OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93mD48675832CA65BD10A532F597[0m[0mOIDCID
 
-Create an IAM Role
+Create an IAM Role 
 ^^^^^^^^^^^^^^^^^^^
 
 Create a file named ``trust.json``  and insert the following trust
@@ -198,7 +198,7 @@ Your output should look like the following:
     PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
 
 Take note of ``ROLE ARN``, you pass this value to your
-operator.
+operator. 
 
 Attach the AmazonSageMakerFullAccess Policy to the Role
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -223,13 +223,13 @@ Deploy the Operator
 ^^^^^^^^^^^^^^^^^^^
 
 When deploying your operator, you can use either a YAML file or Helm
-charts.
+charts. 
 
 Deploy the Operator Using YAML
 ''''''''''''''''''''''''''''''
 
 This is the simplest way to deploy your operators. The process is as
-follows:
+follows: 
 
 -  Download the installer script using the following command:
 
@@ -239,9 +239,9 @@ follows:
 
 -  Edit the ``installer.yaml`` file to
    replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created. 
 
--  Use the following command to deploy the cluster:
+-  Use the following command to deploy the cluster:  
 
    ::
 
@@ -264,7 +264,7 @@ Navigate to the
 ``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
 the ``rolebased/values.yaml`` file, which includes high-level parameters for the
 Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
+created. 
 
 Install the Helm Chart using the following command:
 
@@ -298,7 +298,7 @@ Verify the operator deployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 You should be able to see the Amazon SageMaker Custom Resource
 Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
+the following command: 
 
 ::
 
@@ -330,8 +330,8 @@ namespace ``sagemaker-k8s-operator-system``  as follows:
 
     NAME                                                         READY   STATUS    RESTARTS   AGE
     sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-
-
+    
+​
 
 Namespace-scoped deployment
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -401,7 +401,7 @@ The OIDC ID for your cluster will be different. You need this OIDC ID
 value to create a role.
 
 If your output is ``None``, it means that your client version is old.
-To work around this, run the following command:
+To work around this, run the following command: 
 
 ::
 
@@ -413,7 +413,7 @@ The OIDC URL will be returned as follows:
 
     OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93mD48675832CA65BD10A532F597[0m[0mOIDCID
 
-Create your IAM Role
+Create your IAM Role 
 ^^^^^^^^^^^^^^^^^^^^
 
 Create a file named ``trust.json``  and insert the following trust
@@ -426,7 +426,7 @@ relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS ac
       "Statement": [
         {
           "Effect": "Allow",
-          "Principal": {
+          "Principal": {             
             "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
           },
           "Action": "sts:AssumeRoleWithWebIdentity",
@@ -459,7 +459,7 @@ Your output should look like the following:
     PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
 
 Take note of ``ROLE ARN``, you pass this value to your
-operator.
+operator. 
 
 Attach the AmazonSageMakerFullAccess Policy to your Role
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -484,7 +484,7 @@ Deploy the Operator to Your Namespace
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 When deploying your operator, you can use either a YAML file or Helm
-charts.
+charts. 
 
 Deploy the Operator to Your Namespace Using YAML
 ''''''''''''''''''''''''''''''''''''''''''''''''
@@ -496,7 +496,7 @@ If you have not already installed the CRDs into the cluster, apply the CRD insta
 ::
 
     kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
+    
 To install the operator onto the cluster:
 
 -  Download the operator installer YAML using the following command:
@@ -505,17 +505,16 @@ To install the operator onto the cluster:
 
        wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
 
--  Update the installer YAML to place the resources into your specified namespace using the following command:
-
-   ::
-
+- Update the installer YAML to place the resources into your specified namespace using the following command:
+    ::
+   
        sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
 
 -  Edit the ``operator.yaml`` file to
    place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created.
+   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created. 
 
--  Use the following command to deploy the cluster:
+-  Use the following command to deploy the cluster:  
 
    ::
 
@@ -537,7 +536,7 @@ Navigate to the
 ``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
 the ``rolebased/values.yaml`` file, which includes high-level parameters for the
 Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created.
+created. 
 
 Install the Helm Chart using the following command:
 
@@ -574,7 +573,7 @@ Verify the operator deployment to your namespace
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 You should be able to see the Amazon SageMaker Custom Resource
 Definitions (CRDs) for each operator deployed to your cluster by running
-the following command:
+the following command: 
 
 ::
 
@@ -607,7 +606,7 @@ namespace ``my-namespace``  as follows:
     NAME                                                         READY   STATUS    RESTARTS   AGE
     sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
 
-
+​
 
 Install the Amazon SageMaker logs \ ``kubectl`` plugin
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -632,7 +631,7 @@ that directory to your ``PATH``.
     mkdir ~/sagemaker-k8s-bin
     cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
 
-    # This line will add the binaries to your PATH in your .bashrc.
+    # This line will add the binaries to your PATH in your .bashrc. 
 
     echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
 
@@ -736,17 +735,17 @@ Operators installed using Helm Charts
 
 To delete the operator CRDs, first delete all the running jobs. Then
 delete the helm chart that was used to deploy the operators using the
-following commands:
+following commands: 
 
 ::
 
-    # get the helm charts
+    # get the helm charts 
     $ helm ls
 
     # delete the charts
     $ helm delete <chart name>
 
-
+​
 
 Delete namespace-based operators
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -772,7 +771,7 @@ all jobs and delete the operator from the cluster:
 
 ::
 
-    # Delete the operator using the same yaml file that was used to install the operator
+    # Delete the operator using the same yaml file that was used to install the operator 
     kubectl delete -f operator.yaml
 
     # Now delete the CRDs using the CRD installer yaml
@@ -786,7 +785,7 @@ Operators installed with Helm Charts
 
 To delete the operator CRDs, first delete all the running jobs. Then
 delete the helm chart that was used to deploy the operators using the
-following commands:
+following commands: 
 
 ::
 
@@ -799,7 +798,7 @@ following commands:
     # optionally delete the namespace
     $ kubectl delete namespace <namespace>
 
-
+​
 
 
 Troubleshooting
@@ -815,14 +814,14 @@ Check the job status by running:
     kubectl get <CRD Type> <job name>
 
 If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``:
+command to see the ``STATUS`` and the ``SageMaker Job Name``: 
 
 ::
 
     kubectl get <crd type> <job name>
 
 -  You can use ``smlogs`` to find the cause of the issue using the
-   following command:
+   following command: 
 
    ::
 
@@ -843,7 +842,7 @@ operator’s pod to find the cause of the issue as follows:
 ::
 
     $ kubectl get pods -A | grep sagemaker
-    # Output:
+    # Output: 
     sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
 
     $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
@@ -856,18 +855,18 @@ operator is not running, then you will have to delete the finalizer
 using the following steps:
 
 -  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows:
+   as follows: 
 
    ::
 
        $ kubectl edit <crd type> <job name>
 
        # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist
+       $ kubectl edit batchtransformjobs xgboost-mnist 
 
 -  Edit the job to delete the finalizer by removing the following two
    lines from the file. Save the file and the job should immediately get
-   deleted/updated.
+   deleted/updated. 
 
    ::
 
@@ -918,7 +917,7 @@ documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainin
 Create a TrainingJob Using a Simple YAML File
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Download the sample YAML file for training using the following command:
+Download the sample YAML file for training using the following command: 
 
 ::
 
@@ -926,7 +925,7 @@ Download the sample YAML file for training using the following command:
 
 Edit the ``xgboost-mnist-trainingjob.yaml`` file to replace the ``roleArn`` parameter with your ``<sagemaker-execution-role>``, and ``outputPath`` with your S3 bucket that the Amazon SageMaker
 execution role has write access to. The ``roleArn`` must have permissions so that Amazon SageMaker
-can access Amazon S3, Amazon CloudWatch, and other services on your
+can access Amazon S3, Amazon CloudWatch, and other services on your 
 behalf. For more information on creating an Amazon SageMaker
 ExecutionRole, see `Amazon SageMaker
 Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
@@ -940,9 +939,9 @@ following command:
 Create a TrainingJob Using a Helm Chart
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-You can use Helm Charts to run TrainingJobs.
+You can use Helm Charts to run TrainingJobs. 
 
-Clone the github repo to get the source using the following command:
+Clone the github repo to get the source using the following command: 
 
 ::
 
@@ -958,7 +957,7 @@ behalf. For more information on creating an Amazon SageMaker
 ExecutionRole, see `Amazon SageMaker
 Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
 
-Create the Training Job
+Create the Training Job 
 ''''''''''''''''''''''''
 
 With the roles and S3 buckets replaced with appropriate values
@@ -1033,12 +1032,12 @@ The output listing all jobs should look like the following:
 A training job continues to be listed after the job has completed or
 failed. You can remove a ``TrainingJob``  job from the list by
 following the Delete a Training Job steps. Jobs that have completed or
-stopped do not incur any charges for Amazon SageMaker resources.
+stopped do not incur any charges for Amazon SageMaker resources. 
 
 Training Job Status Values
 ''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values:
+The ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1155,7 +1154,7 @@ The output for your training job should look like the following:
 View Logs from Training Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Use the following command to see the logs from the ``kmeans-mnist``
+Use the following command to see the logs from the ``kmeans-mnist`` 
 training job:
 
 ::
@@ -1192,7 +1191,7 @@ command returns the following output:
 
 If the job is still in progress on Amazon SageMaker, the job will stop.
 You do not incur any charges for Amazon SageMaker resources after your
-job stops or completes.
+job stops or completes. 
 
 **Note**: Amazon SageMaker does not delete training jobs. Stopped jobs
 continue to show on the Amazon SageMaker console. The delete command
@@ -1212,7 +1211,7 @@ Create a HyperParameterTuningJob Using a Simple YAML File
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Download the sample YAML file for the hyperparameter tuning job using
-the following command:
+the following command: 
 
 ::
 
@@ -1234,7 +1233,7 @@ Create a HyperParameterTuningJob using a Helm Chart
 
 You can use Helm Charts to run HyperParameterTuningJobs.
 
-Clone the github repo to get the source using the following command:
+Clone the github repo to get the source using the following command: 
 
 ::
 
@@ -1246,8 +1245,8 @@ folder.
 
 Edit the ``values.yaml`` file to replace the ``roleArn`` parameter
 with your <sagemaker-execution-role>. For HyperparameterTuningJob to
-succeed, you must also change the ``s3InputPath``
-and ``s3OutputPath`` to values that correspond to your account.
+succeed, you must also change the ``s3InputPath`` 
+and ``s3OutputPath`` to values that correspond to your account. 
 
 Create the HPO Job
 ''''''''''''''''''
@@ -1287,7 +1286,7 @@ Your output should look like the following:
 
 ::
 
-    NAME                    NAMESPACE       REVISION        UPDATED
+    NAME                    NAMESPACE       REVISION        UPDATED  
     chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0                               STATUS          CHART                           APP VERSION
     chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
     rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
@@ -1313,7 +1312,7 @@ operator:
 
 ::
 
-    kubectl get hyperparametertuningjob
+    kubectl get hyperparametertuningjob 
 
 Your output will look like the following:
 
@@ -1323,15 +1322,15 @@ Your output will look like the following:
     xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
 
 A hyper parameter tuning job will continue to be listed after the job
-has completed or failed. You can remove a ``hyperparametertuningjob``
+has completed or failed. You can remove a ``hyperparametertuningjob`` 
 from the list by following the steps in Delete a Hyper Parameter Tuning
 Job. Jobs that have completed or stopped do not incur any charges for
-Amazon SageMaker resources.
+Amazon SageMaker resources. 
 
 Hyperparameter Tuning Job Status Values
 '''''''''''''''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values:
+The ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1358,7 +1357,7 @@ like ``COMPLETED`` and ``INPROGRESS``. These represent how many
 training jobs have completed and are in progress, respectively. For more
 information about how these are determined,
 see `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
-the Amazon SageMaker API documentation.
+the Amazon SageMaker API documentation. 
 
 Best Training Job
 '''''''''''''''''
@@ -1393,7 +1392,7 @@ Describe a Hyperparameter Tuning Job
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 You can obtain debugging details using the ``describe`` kubectl verb
-by running the following command.
+by running the following command. 
 
 ::
 
@@ -1518,7 +1517,7 @@ follows:
         Creation Time:  2019-10-17T01:16:14Z
         Final Hyper Parameter Tuning Job Objective Metric:
           Metric Name:        validation:error
-          Value:
+          Value:              
         Objective Status:     Succeeded
         Training End Time:    2019-10-17T01:20:24Z
         Training Job Arn:     arn:aws:sagemaker:us-east-2:123456789012:training-job/xgboosth[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m-009-4sample
@@ -1552,7 +1551,7 @@ Delete HyperParameterTuning jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Use the following command to stop a hyperparameter job in
-Amazon SageMaker.
+Amazon SageMaker. 
 
 ::
 
@@ -1563,9 +1562,9 @@ training jobs from your Kubernetes cluster, as well as stops them in
 Amazon SageMaker. Jobs that have stopped or completed do not incur any
 charges for Amazon SageMaker resources.  Amazon SageMaker does not
 delete hyperparameter tuning jobs. Stopped jobs continue to show on the
-Amazon SageMaker Console.
+Amazon SageMaker Console. 
 
-Your output should look like the following:
+Your output should look like the following:  
 
 ::
 
@@ -1587,7 +1586,7 @@ Create a BatchTransformJob Using a Simple YAML File
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Download the sample YAML file for the batch transform job using the
-following command:
+following command: 
 
 ::
 
@@ -1596,7 +1595,7 @@ following command:
 Edit the file ``xgboost-mnist-batchtransform.yaml`` to change
 necessary parameters to replace the ``inputdataconfig``  with your
 input data and ``s3OutputPath`` with your S3 buckets that the Amazon
-SageMaker execution role has write access to.
+SageMaker execution role has write access to.  
 
 Apply the YAML file using the following command:
 
@@ -1612,7 +1611,7 @@ You can use Helm Charts to run batch transform jobs.
 Get the Helm installer directory
 ''''''''''''''''''''''''''''''''
 
-Clone the github repo to get the source using the following command:
+Clone the github repo to get the source using the following command: 
 
 ::
 
@@ -1623,11 +1622,11 @@ Configure the Helm Chart
 
 Navigate to the
 ``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
-folder.
+folder. 
 
-Edit the ``values.yaml`` file to replace the ``inputdataconfig``
+Edit the ``values.yaml`` file to replace the ``inputdataconfig`` 
 with your input data and outputPath with your S3 buckets that the Amazon
-SageMaker execution role has write access to.
+SageMaker execution role has write access to. 
 
 Create a Batch Transform Job
 ''''''''''''''''''''''''''''
@@ -1684,7 +1683,7 @@ operator:
 
 ::
 
-     kubectl get batchtransformjob
+     kubectl get batchtransformjob 
 
 Your output should look like the following:
 
@@ -1694,15 +1693,15 @@ Your output should look like the following:
     xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93ma88fb19809b511eaac440aa8a[0mxgboost
 
 A batch transform job will continue to be listed after the job has
-completed or failed. You can remove a ``hyperparametertuningjob``
+completed or failed. You can remove a ``hyperparametertuningjob`` 
 from the list by following the Delete a Batch Transform Job steps. Jobs
 that have completed or stopped do not incur any charges for
-Amazon SageMaker resources.
+Amazon SageMaker resources. 
 
 Batch Transform Status Values
 '''''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values:
+The ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1726,7 +1725,7 @@ Describe a Batch Transform Job
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 You can obtain debugging details using the ``describe`` kubectl verb
-by running the following command.
+by running the following command. 
 
 ::
 
@@ -1775,7 +1774,7 @@ Your output should look like the following:
 View Logs from Batch Transform Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Use the following command to see the logs from the ``xgboost-mnist``
+Use the following command to see the logs from the ``xgboost-mnist`` 
 batch transform job:
 
 ::
@@ -1786,7 +1785,7 @@ Delete a Batch Transform Job
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Use the following command to stop a batch transform job in
-Amazon SageMaker.
+Amazon SageMaker. 
 
 ::
 
@@ -1805,7 +1804,7 @@ resources. Delete takes about 2 minutes to clean up the resources from
 Amazon SageMaker.
 
 **Note**: Amazon SageMaker does not delete batch transform jobs. Stopped
-jobs continue to show on the Amazon SageMaker console.
+jobs continue to show on the Amazon SageMaker console. 
 
 Real-time inference
 ~~~~~~~~~~~~~~~~~~~
@@ -1822,7 +1821,7 @@ Configure a HostingDeployment Resource
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Download the sample YAML file for the hosting deployment job using the
-following command:
+following command: 
 
 ::
 
@@ -1937,7 +1936,7 @@ Describe a Hostingdeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 You can obtain debugging details using the ``describe`` kubectl verb
-by running the following command.
+by running the following command. 
 
 ::
 
@@ -2073,7 +2072,7 @@ Update HostingDeployment
 Once a HostingDeployment has a status of ``InService``, it can be
 updated. It might take about 10 minutes for HostingDeployment to be in
 service. To verify that the status is ``InService``, use the following
-command:
+command: 
 
 ::
 
@@ -2138,7 +2137,7 @@ Delete the HostingDeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Use ``kubectl`` to delete a HostingDeployment with the following
-command:
+command: 
 
 ::
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-04-24 16:02:56[0m
[92mHash: ddd5800f4f6fdbc2b2c140b9d7e6938429e80d00[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: Add namespace-based setup for SageMaker Operators for Kubernetes (#1392)

[0m
@@ -19,7 +19,7 @@ Kubernetes is built on top of what is called the controller pattern.
 This pattern allows applications and tools to listen to a central state
 manager (ETCD) and act when something happens. Examples of such
 applications
-include ``cloud-controller-manager`` and ``controller-manager``.
+include \ ``cloud-controller-manager`` and \ ``controller-manager``.
 The controller pattern allows you to create decoupled experiences and not
 have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
 
@@ -32,10 +32,10 @@ completed the following prerequisites:
 -  Installed the following tools on the client machine used to access your k8s cluster:
 
    -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a ``kubectl`` version that is within
+      Version 1.13 or later. Use a \ ``kubectl`` version that is within
       one minor version of your Amazon Elastic Kubernetes Service
       (Amazon EKS) cluster control plane. For example, a
-      1.13 ``kubectl`` client works with Kubernetes 1.13 and 1.14
+      1.13 \ ``kubectl`` client works with Kubernetes 1.13 and 1.14
       clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
 
    -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
@@ -54,7 +54,7 @@ completed the following prerequisites:
 
 -  Created a Kubernetes cluster to run the operators on. It should either be
    Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
+   creation using \ ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
    It takes 20 to 30 minutes to provision a cluster.
 
 Permissions overview
@@ -70,14 +70,14 @@ jobs. The following image explains this design and flow.
 
 .. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
 
-IAM role-based setup and operator deployment
---------------------------------------------
+Setup and operator deployment
+-----------------------------
 
 The following sections describe the steps to setup and deploy the
 operator.
 
-Cluster-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~
+IAM role-based operator deployment
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
 authenticate with the IAM service.
@@ -88,7 +88,7 @@ Create an OpenID Connect Provider for Your Cluster
 The following instruction will create and associate an OIDC provider
 with your EKS cluster.
 
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
+Set the local ``CLUSTER_NAME`` and \ ``AWS_REGION`` environment
 variables as follows:
 
 ::
@@ -98,7 +98,7 @@ variables as follows:
     export AWS_REGION="<your region>"
 
 Use the following command to associate the OIDC provider with your
-cluster. For more information, see `Enabling IAM Roles for Service
+cluster. For more information, see \ `Enabling IAM Roles for Service
 Accounts on your
 Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
 
@@ -139,7 +139,7 @@ In this URL, the value [93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0mOIDCID is the OIDC ID.
 The OIDC ID for your cluster will be different. You need this OIDC ID
 value to create a role.
 
-If your output is ``None``, it means that your client version is old.
+If your output is \ ``None``, it means that your client version is old.
 To work around this, run the following command: 
 
 ::
@@ -155,8 +155,8 @@ The OIDC URL will be returned as follows:
 Create an IAM Role 
 ^^^^^^^^^^^^^^^^^^^
 
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, and ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
+Create a file named \ ``trust.json``  and insert the following trust
+relationship code block into it. Be sure to replace all \ ``<OIDC ID>``, \ ``<AWS account number>``, and \ ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
 
 ::
 
@@ -180,7 +180,7 @@ relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS ac
     }
 
 Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
+relationship defined in \ ``trust.json``. This role enables the
 Amazon EKS cluster to get and refresh credentials from IAM.
 
 ::
@@ -197,14 +197,14 @@ Your output should look like the following:
     STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
     PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
 
-Take note of ``ROLE ARN``, you pass this value to your
+Take note of \ ``ROLE ARN``, you pass this value to your
 operator. 
 
 Attach the AmazonSageMakerFullAccess Policy to the Role
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 To give the role access to Amazon SageMaker, attach
-the `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
+the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
 If you want to limit permissions to the operator, you can create your
 own custom policy and attach it.
 
@@ -215,8 +215,8 @@ To attach AmazonSageMakerFullAccess, run the following command:
     aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
 
 The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
+ServiceAccount \ ``sagemaker-k8s-operator-default`` should
+have \ ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
 install the operator.
 
 Deploy the Operator
@@ -237,8 +237,8 @@ follows:
 
        wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
 
--  Edit the ``installer.yaml`` file to
-   replace ``eks.amazonaws.com/role-arn``. Replace the ARN here with
+-  Edit the \ ``installer.yaml`` file to
+   replace \ ``eks.amazonaws.com/role-arn``. Replace the ARN here with
    the Amazon Resource Name (ARN) for the OIDC-based role you’ve created. 
 
 -  Use the following command to deploy the cluster:  
@@ -262,7 +262,7 @@ Clone the Helm installer directory using the following command:
 
 Navigate to the
 ``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
+the \ ``rolebased/values.yaml`` file, which includes high-level parameters for the
 Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
 created. 
 
@@ -295,7 +295,7 @@ Your output should look like the following:
 
 
 Verify the operator deployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 You should be able to see the Amazon SageMaker Custom Resource
 Definitions (CRDs) for each operator deployed to your cluster by running
 the following command: 
@@ -323,288 +323,13 @@ command to list all pods:
     kubectl -n sagemaker-k8s-operator-system get pods
 
 You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``sagemaker-k8s-operator-system``  as follows:
+named \ ``sagemaker-k8s-operator-controller-manager-*****`` in the
+namespace \ ``sagemaker-k8s-operator-system``  as follows:
 
 ::
 
     NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
-    
-​
-
-Namespace-scoped deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You have the option to install your operator within the scope of an individual Kubernetes namespace. In this mode, the controller will only monitor and reconcile resources with Amazon SageMaker if the resources are created within that namespace. This allows for finer grained control over which controller is managing which resources. This is useful for deploying to multiple AWS accounts or controlling which users have access to particular jobs.
-
-This guide outlines how to install an operator into a particular, predefined namespace. To deploy a controller into a second namespace, follow the guide from beginning to end and change out the namespace in each step.
-
-
-
-
-Create an OpenID Connect Provider for Your EKS Cluster
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The following instruction will create and associate an OIDC provider
-with your EKS cluster.
-
-Set the local ``CLUSTER_NAME`` and ``AWS_REGION`` environment
-variables as follows:
-
-.. code:: shell
-
-    # Set the region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get your OIDC ID
-^^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is ``None``, it means that your client version is old.
-To work around this, run the following command: 
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0m[0mOIDCID
-
-Create your IAM Role 
-^^^^^^^^^^^^^^^^^^^^
-
-Create a file named ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all ``<OIDC ID>``, ``<AWS account number>``, ``<EKS Cluster region>``, and ``<Namespace>`` placeholders with values corresponding to your cluster. For the purposes of this guide, ``my-namespace`` is used for the ``<Namespace>`` value.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {             
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:<Namespace>:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:my-namespace:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of ``ROLE ARN``, you pass this value to your
-operator. 
-
-Attach the AmazonSageMakerFullAccess Policy to your Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount ``sagemaker-k8s-operator-default`` should
-have ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator to Your Namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts. 
-
-Deploy the Operator to Your Namespace Using YAML
-''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts to deploying an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself.
-
-If you have not already installed the CRDs into the cluster, apply the CRD installer YAML using the following command:
-
-::
-
-    kubectl apply -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-    
-To install the operator onto the cluster:
-
--  Download the operator installer YAML using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/operator.yaml
-
-- Update the installer YAML to place the resources into your specified namespace using the following command:
-    ::
-   
-       sed -i -e 's/PLACEHOLDER-NAMESPACE/<YOUR NAMESPACE>/g' operator.yaml
-
--  Edit the ``operator.yaml`` file to
-   place resources into your ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the Amazon Resource Name (ARN) for the OIDC-based role you’ve created. 
-
--  Use the following command to deploy the cluster:  
-
-   ::
-
-       kubectl apply -f operator.yaml
-
-Deploy the Operator to Your Namespace Using Helm Charts
-'''''''''''''''''''''''''''''''''''''''''''''''''''''''
-
-There are two parts needed to deploy an operator within the scope of a namespace. The first is the set of CRDs that are installed at a cluster level. These resource definitions only need to be installed once per Kubernetes cluster. The second part is the operator permissions and deployment itself. When using helm charts you will have to first create the namespace using kubectl.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer/namespaced`` folder. Edit
-the ``rolebased/values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the role ARN here with the Amazon Resource Name (ARN) for the OIDC-based role you’ve
-created. 
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install crds crd_chart/
-
-
-Create the required namespace and install the operator using the following command:
-
-::
-
-    kubectl create namespace <namespace>
-    helm install --n <namespace> op operator_chart/
-
-
-After a moment, the chart will be installed with the
-name ``sagemaker-operator``. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE                       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    sagemaker-operator      my-namespace                    1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment to your namespace
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command: 
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n my-namespace get pods
-
-You should see a pod
-named ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace ``my-namespace``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc     2/2     Running   0          23s
+    sagemaker-k8s-operator-controller-manager-12345678-r8abc   2/2     Running   0          23s
 
 ​
 
@@ -612,13 +337,13 @@ Install the Amazon SageMaker logs \ ``kubectl`` plugin
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
+the \ ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
 This enables Amazon SageMaker CloudWatch logs to be streamed
-with ``kubectl``. ``kubectl`` must be installed onto
+with \ ``kubectl``. \ ``kubectl``\ must be installed onto
 your `PATH <http://www.linfo.org/path_env_var.html>`__. The
 following commands place the binary in
-the ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your ``PATH``.
+the \ ``sagemaker-k8s-bin`` directory in your home directory, and add
+that directory to your \ ``PATH``.
 
 ::
 
@@ -638,14 +363,14 @@ that directory to your ``PATH``.
     # Source your .bashrc to update environment variables:
     source ~/.bashrc
 
-Use the following command to verify that the ``kubectl`` plugin is
+Use the following command to verify that the \ ``kubectl`` plugin is
 installed correctly:
 
 ::
 
     kubectl smlogs
 
-If the ``kubectl`` plugin is installed correctly, your output should
+If the \ ``kubectl`` plugin is installed correctly, your output should
 look like the following:
 
 ::
@@ -669,19 +394,16 @@ look like the following:
     Use "smlogs [command] --help" for more information about a command.
 
 
-Delete operators
-----------------
-
-Delete cluster-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+Delete operators from the cluster 
+----------------------------------
 
 Operators installed using YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 To uninstall the operator from your cluster, make sure that all
 Amazon SageMaker resources have been deleted from the cluster. Failure
 to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to
+deleted all Amazon SageMaker jobs, use \ ``kubectl`` to
 delete the operator from the cluster. Run the following commands to stop
 all jobs and delete the operator from the cluster:
 
@@ -731,7 +453,7 @@ You should see output like the following:
     secrets "sagemaker-k8s-operator-abcde" deleted
 
 Operators installed using Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 To delete the operator CRDs, first delete all the running jobs. Then
 delete the helm chart that was used to deploy the operators using the
@@ -747,60 +469,6 @@ following commands:
 
 ​
 
-Delete namespace-based operators
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-
-Operators installed with YAML
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use ``kubectl`` to first delete the operator from the namespace and then the CRDs from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-
-::
-
-    # Delete the operator using the same yaml file that was used to install the operator 
-    kubectl delete -f operator.yaml
-
-    # Now delete the CRDs using the CRD installer yaml
-    kubectl delete -f https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/namespaced/crd.yaml
-
-    # Now you can delete the namespace if you want
-    kubectl delete namespace <namespace>
-
-Operators installed with Helm Charts
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands: 
-
-::
-
-    # Delete the operator
-    $ helm delete -n <namespace> op
-
-    # delete the crds
-    $ helm delete crds
-
-    # optionally delete the namespace
-    $ kubectl delete namespace <namespace>
-
-​
-
-
 Troubleshooting
 ---------------
 
@@ -814,22 +482,22 @@ Check the job status by running:
     kubectl get <CRD Type> <job name>
 
 If the job was created in Amazon SageMaker, you can use the following
-command to see the ``STATUS`` and the ``SageMaker Job Name``: 
+command to see the \ ``STATUS`` and the ``SageMaker Job Name``: 
 
 ::
 
     kubectl get <crd type> <job name>
 
--  You can use ``smlogs`` to find the cause of the issue using the
+-  You can use \ ``smlogs`` to find the cause of the issue using the
    following command: 
 
    ::
 
        kubectl smlogs <crd type> <job name>
 
--  You can also use the ``describe`` command to get more details about
+-  You can also use the \ ``describe`` command to get more details about
    the job using the following command.The output will have
-   an ``additional`` field that will have more information about the
+   an \ ``additional`` field that will have more information about the
    status of the job.
 
    ::
@@ -900,7 +568,7 @@ a YAML file or use the supplied Helm charts.
 
 All operator sample jobs in the following tutorials use sample data
 taken from a public MNIST dataset. In order to run these samples, download the dataset into your S3 bucket. You can find
-the dataset in `Download the MNIST
+the dataset in \ `Download the MNIST
 Dataset. <https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data-pull-data.html>`__
 
 .. contents::
@@ -911,7 +579,7 @@ TrainingJob operator
 Training job operators reconcile your specified training job spec to
 Amazon SageMaker by launching it for you in Amazon SageMaker. You can
 learn more about Amazon SageMaker training jobs in the Amazon
-SageMaker `CreateTrainingJob API
+SageMaker \ `CreateTrainingJob API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html>`__.
 
 Create a TrainingJob Using a Simple YAML File
@@ -948,20 +616,21 @@ Clone the github repo to get the source using the following command:
     git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
 
 
-Navigate to the ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
-and edit the ``values.yaml`` file to replace values
-like ``rolearn`` and ``outputpath`` with values that correspond to
+Navigate to the
+\ ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
+and edit the \ ``values.yaml`` file to replace values
+like \ ``rolearn`` and ``outputpath`` with values that correspond to
 your account. The RoleARN must have permissions so that Amazon SageMaker
 can access Amazon S3, Amazon CloudWatch, and other services on your
 behalf. For more information on creating an Amazon SageMaker
-ExecutionRole, see `Amazon SageMaker
+ExecutionRole, see \ `Amazon SageMaker
 Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
 
 Create the Training Job 
 ''''''''''''''''''''''''
 
 With the roles and S3 buckets replaced with appropriate values
-in ``values.yaml``, you can create a training job using the following
+in \ ``values.yaml``, you can create a training job using the following
 command:
 
 ::
@@ -998,9 +667,9 @@ Your output should look like the following:
     chart-12345678        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
     rolebased-12345678    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
 
-``helm install`` creates a ``TrainingJob`` k8s resource. The operator
+``helm install`` creates a \ ``TrainingJob`` k8s resource. The operator
 launches the actual training job in Amazon SageMaker and updates
-the ``TrainingJob`` k8s resource to reflect the status of the job in
+the \ ``TrainingJob`` k8s resource to reflect the status of the job in
 Amazon SageMaker. You incur charges for Amazon SageMaker resources used
 during the duration of your job. You do not incur any charges once your
 job completes or stops.
@@ -1009,7 +678,7 @@ job completes or stops.
 training job. You cannot edit any parameter and re-apply the
 file/config. Either change the metadata name or delete the existing job
 and create a new one. Similar to existing training job operators like
-TFJob in Kubeflow, ``update`` is not supported.
+TFJob in Kubeflow, \ ``update`` is not supported.
 
 List Training Jobs
 ^^^^^^^^^^^^^^^^^^
@@ -1030,14 +699,14 @@ The output listing all jobs should look like the following:
     xgboost-mnist-from-for-s3   InProgress   Starting           2019-11-20T23:42:35Z   xgboost-mnist-from-for-s3-exampl[93mef11eab94e0ed4671d5a8f[0m
 
 A training job continues to be listed after the job has completed or
-failed. You can remove a ``TrainingJob``  job from the list by
+failed. You can remove a \ ``TrainingJob``  job from the list by
 following the Delete a Training Job steps. Jobs that have completed or
 stopped do not incur any charges for Amazon SageMaker resources. 
 
 Training Job Status Values
 ''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values: 
+The \ ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1049,18 +718,18 @@ The ``STATUS`` field can be one of the following values:
 
 -  ``Stopping``
 
-These statuses come directly from the Amazon SageMaker official `API
+These statuses come directly from the Amazon SageMaker official \ `API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-TrainingJobStatus>`__.
 
 In addition to the official Amazon SageMaker status, it is possible
-for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
 means that the operator has not yet processed the job.
 
 Secondary Status Values
 '''''''''''''''''''''''
 
 The secondary statuses come directly from the Amazon SageMaker
-official `API
+official \ `API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-SecondaryStatus>`__.
 They contain more granular information about the status of the job.
 
@@ -1068,7 +737,7 @@ Describe a Training Job
 ^^^^^^^^^^^^^^^^^^^^^^^
 
 You can get more details about the training job by using
-the ``describe`` kubectl verb. This is typically used for debugging a
+the \ ``describe`` kubectl verb. This is typically used for debugging a
 problem or checking the parameters of a training job. To get information
 about your training job, use the following command:
 
@@ -1154,7 +823,7 @@ The output for your training job should look like the following:
 View Logs from Training Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Use the following command to see the logs from the ``kmeans-mnist`` 
+Use the following command to see the logs from the \ ``kmeans-mnist`` 
 training job:
 
 ::
@@ -1204,7 +873,7 @@ Hyperparameter tuning job operators reconcile your
 specified hyperparameter tuning job spec to Amazon SageMaker by
 launching it in Amazon SageMaker. You can learn more about Amazon
 SageMaker hyperparameter tuning jobs in the Amazon
-SageMaker `CreateHyperParameterTuningJob API
+SageMaker \ `CreateHyperParameterTuningJob API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateHyperParameterTuningJob.html>`__.
 
 Create a HyperParameterTuningJob Using a Simple YAML File
@@ -1217,10 +886,10 @@ the following command:
 
     wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-hpo.yaml
 
-Edit the ``xgboost-mnist-hpo.yaml`` file to replace
-the ``roleArn`` parameter with your <sagemaker-execution-role>. For
+Edit the \ ``xgboost-mnist-hpo.yaml`` file to replace
+the \ ``roleArn`` parameter with your <sagemaker-execution-role>. For
 HyperparameterTuningJob to succeed, you must also change
-the ``s3InputPath``  and ``s3OutputPath`` to values that correspond
+the \ ``s3InputPath``  and \ ``s3OutputPath`` to values that correspond
 to your account. Apply the updates YAML file using the following
 command:
 
@@ -1240,19 +909,20 @@ Clone the github repo to get the source using the following command:
     git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
 
 
-Navigate to the ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
+Navigate to the
+\ ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
 folder.
 
-Edit the ``values.yaml`` file to replace the ``roleArn`` parameter
+Edit the \ ``values.yaml`` file to replace the \ ``roleArn`` parameter
 with your <sagemaker-execution-role>. For HyperparameterTuningJob to
-succeed, you must also change the ``s3InputPath`` 
-and ``s3OutputPath`` to values that correspond to your account. 
+succeed, you must also change the \ ``s3InputPath`` 
+and \ ``s3OutputPath`` to values that correspond to your account. 
 
 Create the HPO Job
 ''''''''''''''''''
 
 With the roles and Amazon S3 paths replaced with appropriate values
-in ``values.yaml``, you can create a hyperparameter tuning job using
+in \ ``values.yaml``, you can create a hyperparameter tuning job using
 the following command:
 
 ::
@@ -1291,9 +961,9 @@ Your output should look like the following:
     chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
     rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
 
-``helm install`` creates a ``HyperParameterTuningJob`` k8s resource.
+``helm install`` creates a \ ``HyperParameterTuningJob`` k8s resource.
 The operator launches the actual hyperparameter optimization job in
-Amazon SageMaker and updates the ``HyperParameterTuningJob`` k8s
+Amazon SageMaker and updates the \ ``HyperParameterTuningJob`` k8s
 resource to reflect the status of the job in Amazon SageMaker. You incur
 charges for Amazon SageMaker resources used during the duration of your
 job. You do not incur any charges once your job completes or stops.
@@ -1302,7 +972,7 @@ job. You do not incur any charges once your job completes or stops.
 hyperparameter tuning job. You cannot edit any parameter and re-apply
 the file/config. You must either change the metadata name or delete the
 existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, ``update`` is not supported.
+operators like TFJob in Kubeflow, \ ``update`` is not supported.
 
 List Hyperparameter Tuning Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -1322,7 +992,7 @@ Your output will look like the following:
     xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93ma92f5e3cf07b11e9bf6c06d6[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
 
 A hyper parameter tuning job will continue to be listed after the job
-has completed or failed. You can remove a ``hyperparametertuningjob`` 
+has completed or failed. You can remove a \ ``hyperparametertuningjob`` 
 from the list by following the steps in Delete a Hyper Parameter Tuning
 Job. Jobs that have completed or stopped do not incur any charges for
 Amazon SageMaker resources. 
@@ -1330,7 +1000,7 @@ Amazon SageMaker resources.
 Hyperparameter Tuning Job Status Values
 '''''''''''''''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values: 
+The \ ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1346,23 +1016,23 @@ These statuses come directly from the Amazon SageMaker official `API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
 
 In addition to the official Amazon SageMaker status, it is possible
-for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
 means that the operator has not yet processed the job.
 
 Status Counters
 '''''''''''''''
 
 The output has several counters,
-like ``COMPLETED`` and ``INPROGRESS``. These represent how many
+like \ ``COMPLETED`` and ``INPROGRESS``. These represent how many
 training jobs have completed and are in progress, respectively. For more
 information about how these are determined,
-see `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
+see \ `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
 the Amazon SageMaker API documentation. 
 
 Best Training Job
 '''''''''''''''''
 
-This column contains the name of the ``TrainingJob`` that best
+This column contains the name of the \ ``TrainingJob`` that best
 optimized the selected metric.
 
 To see a summary of the tuned hyperparameters, run:
@@ -1371,7 +1041,7 @@ To see a summary of the tuned hyperparameters, run:
 
     kubectl describe hyperparametertuningjob xgboost-mnist-hpo
 
-To see detailed information about the ``TrainingJob``, run:
+To see detailed information about the \ ``TrainingJob``, run:
 
 ::
 
@@ -1391,7 +1061,7 @@ You can also track all 10 training jobs in k8s launched by
 Describe a Hyperparameter Tuning Job
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-You can obtain debugging details using the ``describe`` kubectl verb
+You can obtain debugging details using the \ ``describe`` kubectl verb
 by running the following command. 
 
 ::
@@ -1401,7 +1071,7 @@ by running the following command.
 In addition to information about the tuning job, the Amazon SageMaker
 Operator for Kubernetes also exposes the `best training
 job <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html#automatic-model-tuning-best-training-job>`__\  found
-by the hyperparameter tuning job in the ``describe`` output as
+by the hyperparameter tuning job in the \ ``describe`` output as
 follows:
 
 ::
@@ -1579,7 +1249,7 @@ BatchTransformJobs operator
 Batch transform job operators reconcile your specified batch transform
 job spec to Amazon SageMaker by launching it in Amazon SageMaker. You
 can learn more about Amazon SageMaker batch transform job in the Amazon
-SageMaker `CreateTransformJob API
+SageMaker \ `CreateTransformJob API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`__.
 
 Create a BatchTransformJob Using a Simple YAML File
@@ -1592,9 +1262,9 @@ following command:
 
     wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/samples/xgboost-mnist-batchtransform.yaml
 
-Edit the file ``xgboost-mnist-batchtransform.yaml`` to change
-necessary parameters to replace the ``inputdataconfig``  with your
-input data and ``s3OutputPath`` with your S3 buckets that the Amazon
+Edit the file \ ``xgboost-mnist-batchtransform.yaml`` to change
+necessary parameters to replace the \ ``inputdataconfig``  with your
+input data and \ ``s3OutputPath`` with your S3 buckets that the Amazon
 SageMaker execution role has write access to.  
 
 Apply the YAML file using the following command:
@@ -1624,7 +1294,7 @@ Navigate to the
 ``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
 folder. 
 
-Edit the ``values.yaml`` file to replace the ``inputdataconfig`` 
+Edit the \ ``values.yaml`` file to replace the \ ``inputdataconfig`` 
 with your input data and outputPath with your S3 buckets that the Amazon
 SageMaker execution role has write access to. 
 
@@ -1662,9 +1332,9 @@ following command:
     chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
     rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
 
-The previous command creates a ``BatchTransformJob`` k8s resource. The
+The previous command creates a \ ``BatchTransformJob`` k8s resource. The
 operator launches the actual transform job in Amazon SageMaker and
-updates the ``BatchTransformJob`` k8s resource to reflect the status
+updates the \ ``BatchTransformJob`` k8s resource to reflect the status
 of the job in Amazon SageMaker. You incur charges for Amazon SageMaker
 resources used during the duration of your job. You do not incur any
 charges once your job completes or stops.
@@ -1673,7 +1343,7 @@ charges once your job completes or stops.
 transform job. You cannot edit any parameter and re-apply the
 file/config. You must either change the metadata name or delete the
 existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, ``update`` is not supported.
+operators like TFJob in Kubeflow, \ ``update`` is not supported.
 
 List Batch Transform Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -1693,7 +1363,7 @@ Your output should look like the following:
     xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93ma88fb19809b511eaac440aa8a[0mxgboost
 
 A batch transform job will continue to be listed after the job has
-completed or failed. You can remove a ``hyperparametertuningjob`` 
+completed or failed. You can remove a \ ``hyperparametertuningjob`` 
 from the list by following the Delete a Batch Transform Job steps. Jobs
 that have completed or stopped do not incur any charges for
 Amazon SageMaker resources. 
@@ -1701,7 +1371,7 @@ Amazon SageMaker resources.
 Batch Transform Status Values
 '''''''''''''''''''''''''''''
 
-The ``STATUS`` field can be one of the following values: 
+The \ ``STATUS`` field can be one of the following values: 
 
 -  ``Completed``
 
@@ -1717,14 +1387,14 @@ These statuses come directly from the Amazon SageMaker official `API
 documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
 
 In addition to the official Amazon SageMaker status, it is possible
-for ``STATUS`` to be ``SynchronizingK8sJobWithSageMaker``. This
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
 means that the operator has not yet processed the job and will get to it
 soon.
 
 Describe a Batch Transform Job
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-You can obtain debugging details using the ``describe`` kubectl verb
+You can obtain debugging details using the \ ``describe`` kubectl verb
 by running the following command. 
 
 ::
@@ -1774,7 +1444,7 @@ Your output should look like the following:
 View Logs from Batch Transform Jobs
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Use the following command to see the logs from the ``xgboost-mnist`` 
+Use the following command to see the logs from the \ ``xgboost-mnist`` 
 batch transform job:
 
 ::
@@ -1814,7 +1484,7 @@ updating an existing endpoint. The hosting deployment operator
 reconciles your specified hosting deployment job spec to Amazon
 SageMaker by creating models, endpoint-configs and endpoints in Amazon
 SageMaker. You can learn more about Amazon SageMaker inference in the
-Amazon SageMaker `CreateEndpoint API
+Amazon SageMaker \ `CreateEndpoint API
 documentaiton <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__.
 
 Configure a HostingDeployment Resource
@@ -1846,8 +1516,8 @@ The ``xgboost-mnist-hostingdeployment.yaml`` file has the following components t
 Create a HostingDeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-To create a HostingDeployment, use ``kubectl`` to apply the
-file ``hosting.yaml`` with the following command:
+To create a HostingDeployment, use \ ``kubectl`` to apply the
+file \ ``hosting.yaml`` with the following command:
 
 ::
 
@@ -1887,7 +1557,7 @@ The status field can be one of several values:
 
 -  ``ReconcilingEndpoint``: The operator is creating, updating, or
    deleting endpoint resources. If the HostingDeployment remains in this
-   state, use ``kubectl describe`` to see the reason in the
+   state, use \ ``kubectl describe`` to see the reason in the
    ``Additional`` field.
 
 -  ``OutOfService``: Endpoint is not available to take incoming
@@ -1935,7 +1605,7 @@ The status field can be one of several values:
 Describe a Hostingdeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-You can obtain debugging details using the ``describe`` kubectl verb
+You can obtain debugging details using the \ ``describe`` kubectl verb
 by running the following command. 
 
 ::
@@ -2010,7 +1680,7 @@ The status field provides more information using the following fields:
 -  ``Endpoint Status``: The Status of the endpoint.
 
 -  ``Endpoint URL``: The HTTPS URL that can be used to access the
-   endpoint. For more information, see `Deploy a Model on Amazon
+   endpoint. For more information, see \ `Deploy a Model on Amazon
    SageMaker Hosting
    Services <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>`__.
 
@@ -2028,7 +1698,7 @@ The status field provides more information using the following fields:
 Invoking the Endpoint
 ^^^^^^^^^^^^^^^^^^^^^
 
-Once the endpoint status is ``InService``, you can invoke the endpoint
+Once the endpoint status is \ ``InService``, you can invoke the endpoint
 in two ways: using the AWS CLI, which does authentication and URL
 request signing, or using an HTTP client like curl. If you use your own
 client, you will need to do AWSv4 URL signing and authentication on your
@@ -2037,7 +1707,7 @@ own.
 To invoke the endpoint using the AWS CLI, run the following command.
 Make sure to replace the Region and endpoint-name with your endpoint’s
 Region and Amazon SageMaker endpoint name. This information can be
-obtained from the output of ``kubectl describe``.
+obtained from the output of \ ``kubectl describe``.
 
 ::
 
@@ -2049,8 +1719,8 @@ obtained from the output of ``kubectl describe``.
       >(cat) \
       --content-type text/csv > /dev/null
 
-For example, if your Region were ``us-east-2`` and your endpoint
-config name were ``host-xgboost-[93m[93mf56b6b280d7511ea824b129926e[0m[0mxample``,
+For example, if your Region were \ ``us-east-2`` and your endpoint
+config name were \ ``host-xgboost-[93m[93mf56b6b280d7511ea824b129926e[0m[0mxample``,
 then the following command would invoke the endpoint:
 
 ::
@@ -2063,15 +1733,15 @@ then the following command would invoke the endpoint:
       --content-type text/csv > /dev/null
     4.95847082138
 
-Here, ``4.95847082138`` is the prediction from the model for the mock
+Here, \ ``4.95847082138`` is the prediction from the model for the mock
 data.
 
 Update HostingDeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^
 
-Once a HostingDeployment has a status of ``InService``, it can be
+Once a HostingDeployment has a status of \ ``InService``, it can be
 updated. It might take about 10 minutes for HostingDeployment to be in
-service. To verify that the status is ``InService``, use the following
+service. To verify that the status is \ ``InService``, use the following
 command: 
 
 ::
@@ -2079,11 +1749,11 @@ command:
     kubectl get hostingdeployments
 
 The HostingDeployment can be updated before the status
-is ``InService``. The operator will wait until the Amazon SageMaker
-endpoint is ``InService`` before applying the update.
+is \ ``InService``. The operator will wait until the Amazon SageMaker
+endpoint is \ ``InService`` before applying the update.
 
-To apply an update, modify the ``hosting.yaml`` file. For example,
-change the ``initialInstanceCount`` field from 1 to 2 as follows:
+To apply an update, modify the \ ``hosting.yaml`` file. For example,
+change the \ ``initialInstanceCount`` field from 1 to 2 as follows:
 
 ::
 
@@ -2109,10 +1779,10 @@ change the ``initialInstanceCount`` field from 1 to 2 as follows:
               modelDataUrl: s3://my-bucket/inference/xgboost-mnist/model.tar.gz
               image: 123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
 
-Save the file, then use ``kubectl`` to apply your update as follows.
+Save the file, then use \ ``kubectl`` to apply your update as follows.
 You should see the status change
-from ``InService`` to ``ReconcilingEndpoint``,
-then ``Updating``.
+from \ ``InService`` to ``ReconcilingEndpoint``,
+then \ ``Updating``.
 
 ::
 
@@ -2129,14 +1799,14 @@ then ``Updating``.
 
 Amazon SageMaker deploys a new set of instances with your models,
 switches traffic to use the new instances, and drains the old instances.
-As soon as this process begins, the status becomes ``Updating``. After
-the update is complete, your endpoint becomes ``InService``. This
+As soon as this process begins, the status becomes \ ``Updating``. After
+the update is complete, your endpoint becomes \ ``InService``. This
 process takes approximately 10 minutes.
 
 Delete the HostingDeployment
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Use ``kubectl`` to delete a HostingDeployment with the following
+Use \ ``kubectl`` to delete a HostingDeployment with the following
 command: 
 
 ::

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-04-01 15:31:45[0m
[92mHash: 17133906b2cca8cece5db01cff9b8670dc0b837c[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: Merge Amazon Sagemaker Operators for Kubernetes and Kubernetes Jobs pages (#1334)

[0m
@@ -558,1274 +558,3 @@ each region.
 +-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
 | eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:v1``       | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/v1/linux.amd64.tar.gz       |
 +-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-
-
-Using Amazon Sagemaker Jobs
----------------------------
-
-To run a job using the Amazon Sagemaker Operators for Kubernetes, you can either apply
-a YAML file or use the supplied Helm charts.
-
-All operator sample jobs in the following tutorials use sample data
-taken from a public MNIST dataset. In order to run these samples, download the dataset into your S3 bucket. You can find
-the dataset in \ `Download the MNIST
-Dataset. <https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data-pull-data.html>`__
-
-.. contents::
-
-TrainingJob operator
-~~~~~~~~~~~~~~~~~~~~
-
-Training job operators reconcile your specified training job spec to
-Amazon SageMaker by launching it for you in Amazon SageMaker. You can
-learn more about Amazon SageMaker training jobs in the Amazon
-SageMaker \ `CreateTrainingJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html>`__.
-
-Create a TrainingJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for training using the following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-trainingjob.yaml
-
-Edit the ``xgboost-mnist-trainingjob.yaml`` file to replace the ``roleArn`` parameter with your ``<sagemaker-execution-role>``, and ``outputPath`` with your S3 bucket that the Amazon SageMaker
-execution role has write access to. The ``roleArn`` must have permissions so that Amazon SageMaker
-can access Amazon S3, Amazon CloudWatch, and other services on your 
-behalf. For more information on creating an Amazon SageMaker
-ExecutionRole, see `Amazon SageMaker
-Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
-Apply the YAML file using the
-following command:
-
-::
-
-    kubectl apply -f xgboost-mnist-trainingjob.yaml
-
-Create a TrainingJob Using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run TrainingJobs. 
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-
-Navigate to the
-\ ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
-and edit the \ ``values.yaml`` file to replace values
-like \ ``rolearn`` and ``outputpath`` with values that correspond to
-your account. The RoleARN must have permissions so that Amazon SageMaker
-can access Amazon S3, Amazon CloudWatch, and other services on your
-behalf. For more information on creating an Amazon SageMaker
-ExecutionRole, see \ `Amazon SageMaker
-Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
-
-Create the Training Job 
-''''''''''''''''''''''''
-
-With the roles and S3 buckets replaced with appropriate values
-in \ ``values.yaml``, you can create a training job using the following
-command:
-
-::
-
-    helm install . --generate-name
-
-Your output should look like the following:
-
-::
-
-    NAME: chart-12345678
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-trainingjob.
-
-Verify Your Training Helm Chart
-'''''''''''''''''''''''''''''''
-
-To verify that the Helm Chart was created successfully, run:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    chart-12345678        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-12345678    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-``helm install`` creates a \ ``TrainingJob`` k8s resource. The operator
-launches the actual training job in Amazon SageMaker and updates
-the \ ``TrainingJob`` k8s resource to reflect the status of the job in
-Amazon SageMaker. You incur charges for Amazon SageMaker resources used
-during the duration of your job. You do not incur any charges once your
-job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running
-training job. You cannot edit any parameter and re-apply the
-file/config. Either change the metadata name or delete the existing job
-and create a new one. Similar to existing training job operators like
-TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Training Jobs
-^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-    kubectl get TrainingJob
-
-The output listing all jobs should look like the following:
-
-::
-
-    kubectl get trainingjobs
-    NAME                        STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME
-    xgboost-mnist-from-for-s3   InProgress   Starting           2019-11-20T23:42:35Z   xgboost-mnist-from-for-s3-exampl[93mef11eab94e0ed4671d5a8f[0m
-
-A training job continues to be listed after the job has completed or
-failed. You can remove a \ ``TrainingJob``  job from the list by
-following the Delete a Training Job steps. Jobs that have completed or
-stopped do not incur any charges for Amazon SageMaker resources. 
-
-Training Job Status Values
-''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official \ `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-TrainingJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job.
-
-Secondary Status Values
-'''''''''''''''''''''''
-
-The secondary statuses come directly from the Amazon SageMaker
-official \ `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-SecondaryStatus>`__.
-They contain more granular information about the status of the job.
-
-Describe a Training Job
-^^^^^^^^^^^^^^^^^^^^^^^
-
-You can get more details about the training job by using
-the \ ``describe`` kubectl verb. This is typically used for debugging a
-problem or checking the parameters of a training job. To get information
-about your training job, use the following command:
-
-::
-
-    kubectl describe trainingjob xgboost-mnist-from-for-s3
-
-The output for your training job should look like the following:
-
-::
-
-    Name:         xgboost-mnist-from-for-s3
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  <none>
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         TrainingJob
-    Metadata:
-      Creation Timestamp:  2019-11-20T23:42:35Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  23119
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-from-for-s3
-      UID:               6d7uiui-0bef-11ea-b94e-0ed467example
-    Spec:
-      Algorithm Specification:
-        Training Image:       8256416981234.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
-        Training Input Mode:  File
-      Hyper Parameters:
-        Name:   eta
-        Value:  0.2
-        Name:   gamma
-        Value:  4
-        Name:   max_depth
-        Value:  5
-        Name:   min_child_weight
-        Value:  6
-        Name:   num_class
-        Value:  10
-        Name:   num_round
-        Value:  10
-        Name:   objective
-        Value:  multi:softmax
-        Name:   silent
-        Value:  0
-      Input Data Config:
-        Channel Name:      train
-        Compression Type:  None
-        Content Type:      text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Distribution Type:  FullyReplicated
-            S 3 Data Type:               S3Prefix
-            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
-        Channel Name:                    validation
-        Compression Type:                None
-        Content Type:                    text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Distribution Type:  FullyReplicated
-            S 3 Data Type:               S3Prefix
-            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
-      Output Data Config:
-        S 3 Output Path:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost/
-      Region:             us-east-2
-      Resource Config:
-        Instance Count:     1
-        Instance Type:      ml.m4.xlarge
-        Volume Size In GB:  5
-      Role Arn:             arn:aws:iam::12345678910:role/service-role/AmazonSageMaker-ExecutionRole
-      Stopping Condition:
-        Max Runtime In Seconds:  86400
-      Training Job Name:         xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0mxample
-    Status:
-      Cloud Watch Log URL:           https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=<example>;streamFilter=typeLogStreamPrefix
-      Last Check Time:               2019-11-20T23:44:29Z
-      Sage Maker Training Job Name:  xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94ee[0mxample
-      Secondary Status:              Downloading
-      Training Job Status:           InProgress
-    Events:                          <none>
-
-View Logs from Training Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to see the logs from the \ ``kmeans-mnist`` 
-training job:
-
-::
-
-    kubectl smlogs trainingjob xgboost-mnist-from-for-s3
-
-Your output will look similar to the following. The logs from instances
-are ordered chronologically.
-
-::
-
-    "xgboost-mnist-from-for-s3" has SageMaker TrainingJobName "xgboost-mnist-from-for-s3-123456789" in region "us-east-2", status "InProgress" and secondary status "Starting"
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC Arguments: train
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Running standalone xgboost training.
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 8586.0mb
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Determined delimiter of CSV input is ','
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [23:45:22] S3DistributionType set as FullyReplicated
-
-Delete Training Jobs
-^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a training job on Amazon SageMaker:
-
-::
-
-    kubectl delete trainingjob xgboost-mnist-from-for-s3
-
-This command removes the Amazon SageMaker training job from k8s. This
-command returns the following output:
-
-::
-
-    trainingjob.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-If the job is still in progress on Amazon SageMaker, the job will stop.
-You do not incur any charges for Amazon SageMaker resources after your
-job stops or completes. 
-
-**Note**: Amazon SageMaker does not delete training jobs. Stopped jobs
-continue to show on the Amazon SageMaker console. The delete command
-takes about 2 minutes to clean up the resources from Amazon SageMaker.
-
-HyperParameterTuningJobs operator
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Hyperparameter tuning job operators reconcile your
-specified hyperparameter tuning job spec to Amazon SageMaker by
-launching it in Amazon SageMaker. You can learn more about Amazon
-SageMaker hyperparameter tuning jobs in the Amazon
-SageMaker \ `CreateHyperParameterTuningJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateHyperParameterTuningJob.html>`__.
-
-Create a HyperParameterTuningJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the hyperparameter tuning job using
-the following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hpo.yaml
-
-Edit the \ ``xgboost-mnist-hpo.yaml`` file to replace
-the \ ``roleArn`` parameter with your <sagemaker-execution-role>. For
-HyperparameterTuningJob to succeed, you must also change
-the \ ``s3InputPath``  and \ ``s3OutputPath`` to values that correspond
-to your account. Apply the updates YAML file using the following
-command:
-
-::
-
-    kubectl apply -f xgboost-mnist-hpo.yaml
-
-Create a HyperParameterTuningJob using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run HyperParameterTuningJobs.
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-
-Navigate to the
-\ ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
-folder.
-
-Edit the \ ``values.yaml`` file to replace the \ ``roleArn`` parameter
-with your <sagemaker-execution-role>. For HyperparameterTuningJob to
-succeed, you must also change the \ ``s3InputPath`` 
-and \ ``s3OutputPath`` to values that correspond to your account. 
-
-Create the HPO Job
-''''''''''''''''''
-
-With the roles and Amazon S3 paths replaced with appropriate values
-in \ ``values.yaml``, you can create a hyperparameter tuning job using
-the following command:
-
-::
-
-    helm install . --generate-name
-
-Your output will look similar to the following:
-
-::
-
-    NAME: chart-1574292948
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-hyperparametertuningjob.
-
-Verify Chart Installation
-'''''''''''''''''''''''''
-
-To verify that the Helm Chart was created successfully, run the
-following command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE       REVISION        UPDATED  
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0                               STATUS          CHART                           APP VERSION
-    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-``helm install`` creates a \ ``HyperParameterTuningJob`` k8s resource.
-The operator launches the actual hyperparameter optimization job in
-Amazon SageMaker and updates the \ ``HyperParameterTuningJob`` k8s
-resource to reflect the status of the job in Amazon SageMaker. You incur
-charges for Amazon SageMaker resources used during the duration of your
-job. You do not incur any charges once your job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running
-hyperparameter tuning job. You cannot edit any parameter and re-apply
-the file/config. You must either change the metadata name or delete the
-existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Hyperparameter Tuning Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-    kubectl get hyperparametertuningjob 
-
-Your output will look like the following:
-
-::
-
-    NAME         STATUS      CREATION-TIME          COMPLETED   INPROGRESS   ERRORS   STOPPED   BEST-TRAINING-JOB                               SAGEMAKER-JOB-NAME
-    xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
-
-A hyper parameter tuning job will continue to be listed after the job
-has completed or failed. You can remove a \ ``hyperparametertuningjob`` 
-from the list by following the steps in Delete a Hyper Parameter Tuning
-Job. Jobs that have completed or stopped do not incur any charges for
-Amazon SageMaker resources. 
-
-Hyperparameter Tuning Job Status Values
-'''''''''''''''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job.
-
-Status Counters
-'''''''''''''''
-
-The output has several counters,
-like \ ``COMPLETED`` and ``INPROGRESS``. These represent how many
-training jobs have completed and are in progress, respectively. For more
-information about how these are determined,
-see \ `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
-the Amazon SageMaker API documentation. 
-
-Best Training Job
-'''''''''''''''''
-
-This column contains the name of the \ ``TrainingJob`` that best
-optimized the selected metric.
-
-To see a summary of the tuned hyperparameters, run:
-
-::
-
-    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
-
-To see detailed information about the \ ``TrainingJob``, run:
-
-::
-
-    kubectl describe trainingjobs <job name>
-
-
-Spawned Training Jobs
-'''''''''''''''''''''
-
-You can also track all 10 training jobs in k8s launched by
-``HyperparameterTuningJob`` by running the following command:
-
-::
-
-    kubectl get trainingjobs
-
-Describe a Hyperparameter Tuning Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
-
-In addition to information about the tuning job, the Amazon SageMaker
-Operator for Kubernetes also exposes the `best training
-job <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html#automatic-model-tuning-best-training-job>`__\  found
-by the hyperparameter tuning job in the \ ``describe`` output as
-follows:
-
-::
-
-    Name:         xgboost-mnist-hpo
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HyperparameterTuningJob","metadata":{"annotations":{},"name":"xgboost-mnist-hpo","namespace":...
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         HyperparameterTuningJob
-    Metadata:
-      Creation Timestamp:  2019-10-17T01:15:52Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  8167
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hyperparametertuningjobs/xgboost-mnist-hpo
-      UID:               a92f5e3c-f07b-11e9-bf6c-06d6f303uidu
-    Spec:
-      Hyper Parameter Tuning Job Config:
-        Hyper Parameter Tuning Job Objective:
-          Metric Name:  validation:error
-          Type:         Minimize
-        Parameter Ranges:
-          Integer Parameter Ranges:
-            Max Value:     20
-            Min Value:     10
-            Name:          num_round
-            Scaling Type:  Linear
-        Resource Limits:
-          Max Number Of Training Jobs:     10
-          Max Parallel Training Jobs:      10
-        Strategy:                          Bayesian
-        Training Job Early Stopping Type:  Off
-      Hyper Parameter Tuning Job Name:     xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
-      Region:                              us-east-2
-      Training Job Definition:
-        Algorithm Specification:
-          Training Image:       12345678910.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
-          Training Input Mode:  File
-        Input Data Config:
-          Channel Name:  train
-          Content Type:  text/csv
-          Data Source:
-            s3DataSource:
-              s3DataDistributionType:  FullyReplicated
-              s3DataType:              S3Prefix
-              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
-          Channel Name:                validation
-          Content Type:                text/csv
-          Data Source:
-            s3DataSource:
-              s3DataDistributionType:  FullyReplicated
-              s3DataType:              S3Prefix
-              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
-        Output Data Config:
-          s3OutputPath:  https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/xgboost
-        Resource Config:
-          Instance Count:     1
-          Instance Type:      ml.m4.xlarge
-          Volume Size In GB:  5
-        Role Arn:             arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-        Static Hyper Parameters:
-          Name:   base_score
-          Value:  0.5
-          Name:   booster
-          Value:  gbtree
-          Name:   csv_weights
-          Value:  0
-          Name:   dsplit
-          Value:  row
-          Name:   grow_policy
-          Value:  depthwise
-          Name:   lambda_bias
-          Value:  0.0
-          Name:   max_bin
-          Value:  256
-          Name:   max_leaves
-          Value:  0
-          Name:   normalize_type
-          Value:  tree
-          Name:   objective
-          Value:  reg:linear
-          Name:   one_drop
-          Value:  0
-          Name:   prob_buffer_row
-          Value:  1.0
-          Name:   process_type
-          Value:  default
-          Name:   rate_drop
-          Value:  0.0
-          Name:   refresh_leaf
-          Value:  1
-          Name:   sample_type
-          Value:  uniform
-          Name:   scale_pos_weight
-          Value:  1.0
-          Name:   silent
-          Value:  0
-          Name:   sketch_eps
-          Value:  0.03
-          Name:   skip_drop
-          Value:  0.0
-          Name:   tree_method
-          Value:  auto
-          Name:   tweedie_variance_power
-          Value:  1.5
-        Stopping Condition:
-          Max Runtime In Seconds:  86400
-    Status:
-      Best Training Job:
-        Creation Time:  2019-10-17T01:16:14Z
-        Final Hyper Parameter Tuning Job Objective Metric:
-          Metric Name:        validation:error
-          Value:              
-        Objective Status:     Succeeded
-        Training End Time:    2019-10-17T01:20:24Z
-        Training Job Arn:     arn:aws:sagemaker:us-east-2:123456789012:training-job/xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4sample
-        Training Job Name:    xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a3059
-        Training Job Status:  Completed
-        Training Start Time:  2019-10-17T01:18:35Z
-        Tuned Hyper Parameters:
-          Name:                                    num_round
-          Value:                                   18
-      Hyper Parameter Tuning Job Status:           Completed
-      Last Check Time:                             2019-10-17T01:21:01Z
-      Sage Maker Hyper Parameter Tuning Job Name:  xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
-      Training Job Status Counters:
-        Completed:            10
-        In Progress:          0
-        Non Retryable Error:  0
-        Retryable Error:      0
-        Stopped:              0
-        Total Error:          0
-    Events:                   <none>
-
-View Logs from HyperParameterTuning Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Hyperparameter tuning jobs do not have logs, but all training jobs
-launched by them do have logs. These logs can be accessed as if they
-were a normal training job. For more information, see View Logs from
-Training Jobs.
-
-Delete HyperParameterTuning jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a hyperparameter job in
-Amazon SageMaker. 
-
-::
-
-    kubectl delete hyperparametertuningjob xgboost-mnist-hpo
-
-This command removes the hyperparameter tuning job and associated
-training jobs from your Kubernetes cluster, as well as stops them in
-Amazon SageMaker. Jobs that have stopped or completed do not incur any
-charges for Amazon SageMaker resources.  Amazon SageMaker does not
-delete hyperparameter tuning jobs. Stopped jobs continue to show on the
-Amazon SageMaker Console. 
-
-Your output should look like the following:  
-
-::
-
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-**Note**:  The delete command takes about 2 minutes to clean up the
-resources from Amazon SageMaker.
-
-BatchTransformJobs operator
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Batch transform job operators reconcile your specified batch transform
-job spec to Amazon SageMaker by launching it in Amazon SageMaker. You
-can learn more about Amazon SageMaker batch transform job in the Amazon
-SageMaker \ `CreateTransformJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`__.
-
-Create a BatchTransformJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the batch transform job using the
-following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-batchtransform.yaml
-
-Edit the file \ ``xgboost-mnist-batchtransform.yaml`` to change
-necessary parameters to replace the \ ``inputdataconfig``  with your
-input data and \ ``s3OutputPath`` with your S3 buckets that the Amazon
-SageMaker execution role has write access to.  
-
-Apply the YAML file using the following command:
-
-::
-
-    kubectl apply -f xgboost-mnist-batchtransform.yaml
-
-Create a BatchTransformJob Using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run batch transform jobs.
-
-Get the Helm installer directory
-''''''''''''''''''''''''''''''''
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Configure the Helm Chart
-''''''''''''''''''''''''
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
-folder. 
-
-Edit the \ ``values.yaml`` file to replace the \ ``inputdataconfig`` 
-with your input data and outputPath with your S3 buckets that the Amazon
-SageMaker execution role has write access to. 
-
-Create a Batch Transform Job
-''''''''''''''''''''''''''''
-
-Use the following command to create a batch transform job:
-
-::
-
-    helm install . --generate-name
-
-Your output should look like the following:
-
-::
-
-    NAME: chart-1574292948
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-batch-transform-job.
-
-To verify that the Helm Chart was created successfully, run the
-following command:
-
-::
-
-    helm ls
-    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-batchtransformjob-0.1.0
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0
-    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-The previous command creates a \ ``BatchTransformJob`` k8s resource. The
-operator launches the actual transform job in Amazon SageMaker and
-updates the \ ``BatchTransformJob`` k8s resource to reflect the status
-of the job in Amazon SageMaker. You incur charges for Amazon SageMaker
-resources used during the duration of your job. You do not incur any
-charges once your job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running batch
-transform job. You cannot edit any parameter and re-apply the
-file/config. You must either change the metadata name or delete the
-existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Batch Transform Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-     kubectl get batchtransformjob 
-
-Your output should look like the following:
-
-::
-
-    NAME                                STATUS      CREATION-TIME          SAGEMAKER-JOB-NAME
-    xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0mxgboost
-
-A batch transform job will continue to be listed after the job has
-completed or failed. You can remove a \ ``hyperparametertuningjob`` 
-from the list by following the Delete a Batch Transform Job steps. Jobs
-that have completed or stopped do not incur any charges for
-Amazon SageMaker resources. 
-
-Batch Transform Status Values
-'''''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job and will get to it
-soon.
-
-Describe a Batch Transform Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe batchtransformjob xgboost-mnist-batch-transform
-
-Your output should look like the following:
-
-::
-
-    Name:         xgboost-mnist-batch-transform
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"BatchTransformJob","metadata":{"annotations":{},"name":"xgboost-mnist","namespace"...
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         BatchTransformJob
-    Metadata:
-      Creation Timestamp:  2019-11-18T03:44:00Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  21990924
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/batchtransformjobs/xgboost-mnist
-      UID:               a88fb198-09b5-11ea-ac44-0aa8a9UIDNUM
-    Spec:
-      Model Name:  TrainingJob-20190814SMJOb-IKEB
-      Region:      us-east-1
-      Transform Input:
-        Content Type:  text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Type:  S3Prefix
-            S 3 Uri:        s3://my-bucket/mnist_kmeans_example/input
-      Transform Job Name:   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0m9SMJOB
-      Transform Output:
-        S 3 Output Path:  s3://my-bucket/mnist_kmeans_example/output
-      Transform Resources:
-        Instance Count:  1
-        Instance Type:   ml.m4.xlarge
-    Status:
-      Last Check Time:                2019-11-19T22:50:40Z
-      Sage Maker Transform Job Name:  xgboost-mnist-[93ma88fb19809b511eaac440aa[0mSMJOB
-      Transform Job Status:           Completed
-    Events:                           <none>
-
-View Logs from Batch Transform Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to see the logs from the \ ``xgboost-mnist`` 
-batch transform job:
-
-::
-
-    kubectl smlogs batchtransformjob xgboost-mnist-batch-transform
-
-Delete a Batch Transform Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a batch transform job in
-Amazon SageMaker. 
-
-::
-
-    kubectl delete batchTransformJob xgboost-mnist-batch-transform
-
-Your output will look like the following:
-
-::
-
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-This command removes the batch transform job from your Kubernetes
-cluster, as well as stops them in Amazon SageMaker. Jobs that have
-stopped or completed do not incur any charges for Amazon SageMaker
-resources. Delete takes about 2 minutes to clean up the resources from
-Amazon SageMaker.
-
-**Note**: Amazon SageMaker does not delete batch transform jobs. Stopped
-jobs continue to show on the Amazon SageMaker console. 
-
-Real-time inference
-~~~~~~~~~~~~~~~~~~~
-
-HostingDeployments support creating and deleting an endpoint, as well as
-updating an existing endpoint. The hosting deployment operator
-reconciles your specified hosting deployment job spec to Amazon
-SageMaker by creating models, endpoint-configs and endpoints in Amazon
-SageMaker. You can learn more about Amazon SageMaker inference in the
-Amazon SageMaker \ `CreateEndpoint API
-documentaiton <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__.
-
-Configure a HostingDeployment Resource
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the hosting deployment job using the
-following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hostingdeployment.yaml
-
-The ``xgboost-mnist-hostingdeployment.yaml`` file has the following components that can be edited as required:
-
--  ProductionVariants. A production variant is a set of instances
-   serving a single model. Amazon SageMaker will load-balance between
-   all production variants according to set weights.
-
--  Models. A model is the containers and execution role ARN necessary to
-   serve a model. It requires at least a single container.
-
--  Containers. A container specifies the dataset and serving image. If
-   you are using your own custom algorithm instead of an algorithm
-   provided by Amazon SageMaker, the inference code must meet Amazon
-   SageMaker requirements. For more information, see `Using Your Own
-   Algorithms with Amazon
-   SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.
-
-Create a HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To create a HostingDeployment, use \ ``kubectl`` to apply the
-file \ ``hosting.yaml`` with the following command:
-
-::
-
-    kubectl apply -f hosting.yaml
-
-Amazon SageMaker create an endpoint with the specified
-configuration. You incur charges for Amazon SageMaker resources used
-during the lifetime of your endpoint. You do not incur any charges once
-your endpoint is deleted.
-
-The creation process will take approximately 10 minutes.
-
-List HostingDeployments
-^^^^^^^^^^^^^^^^^^^^^^^
-
-To verify that the HostingDeployment was created, use the following
-command:
-
-::
-
-    kubectl get hostingdeployments
-
-Your output should look like the following:
-
-::
-
-    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   Creating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0mSMLOGS
-
-HostingDeployment Status Values
-'''''''''''''''''''''''''''''''
-
-The status field can be one of several values:
-
--  ``SynchronizingK8sJobWithSageMaker``: The operator is preparing to
-   create the endpoint.
-
--  ``ReconcilingEndpoint``: The operator is creating, updating, or
-   deleting endpoint resources. If the HostingDeployment remains in this
-   state, use \ ``kubectl describe`` to see the reason in the
-   ``Additional`` field.
-
--  ``OutOfService``: Endpoint is not available to take incoming
-   requests.
-
--  ``Creating``:
-   `CreateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__
-   is executing.
-
--  ``Updating``:
-   `UpdateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html>`__
-   or
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   is executing.
-
--  ``SystemUpdating``: Endpoint is undergoing maintenance and cannot be
-   updated or deleted or re-scaled until it has completed. This
-   maintenance operation does not change any customer-specified values
-   such as VPC config, KMS encryption, model, instance type, or instance
-   count.
-
--  ``RollingBack``: Endpoint fails to scale up or down or change its
-   variant weight and is in the process of rolling back to its previous
-   configuration. Once the rollback completes, endpoint returns to an
-   ``InService`` status. This transitional status only applies to an
-   endpoint that has autoscaling enabled and is undergoing variant
-   weight or capacity changes as part of an
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   call or when the
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   operation is called explicitly.
-
--  ``InService``: Endpoint is available to process incoming requests.
-
--  ``Deleting``:
-   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
-   is executing.
-
--  ``Failed``: Endpoint could not be created, updated, or re-scaled. Use
-   `DescribeEndpoint:FailureReason <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeEndpoint.html#SageMaker-DescribeEndpoint-response-FailureReason>`__
-   for information about the failure.
-   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
-   is the only operation that can be performed on a failed endpoint.
-
-Describe a Hostingdeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe hostingdeployment
-
-Your output should look like the following:
-
-::
-
-    Name:         host-xgboost
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HostingDeployment","metadata":{"annotations":{},"name":"host-xgboost","namespace":"def..."
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         HostingDeployment
-    Metadata:
-      Creation Timestamp:  2019-11-22T19:40:00Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        1
-      Resource Version:  4258134
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hostingdeployments/host-xgboost
-      UID:               def0e83e-0d5f-11ea-aa45-0a3507uiduid
-    Spec:
-      Containers:
-        Container Hostname:  xgboost
-        Image:               123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
-        Model Data URL:      s3://my-bucket/inference/xgboost-mnist/model.tar.gz
-      Models:
-        Containers:
-          xgboost
-        Execution Role Arn:  arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-        Name:                xgboost-model
-        Primary Container:   xgboost
-      Production Variants:
-        Initial Instance Count:  1
-        Instance Type:           ml.c5.large
-        Model Name:              xgboost-model
-        Variant Name:            all-traffic
-      Region:                    us-east-2
-    Status:
-      Creation Time:         2019-11-22T19:40:04Z
-      Endpoint Arn:          arn:aws:sagemaker:us-east-2:123456789012:endpoint/host-xgboost-def0e83e0d5f11eaaaexample
-      Endpoint Config Name:  host-xgboost-1-def0e83e0d5f11e-[93me08f6c510d5f11eaaa450ae[0mxample
-      Endpoint Name:         host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350733ba06
-      Endpoint Status:       Creating
-      Endpoint URL:          https://runtime.sagemaker.us-east-2.amazonaws.com/endpoints/host-xgboost-def0e83e0d5f11eaaaexample/invocations
-      Last Check Time:       2019-11-22T19:43:57Z
-      Last Modified Time:    2019-11-22T19:40:04Z
-      Model Names:
-        Name:   xgboost-model
-        Value:  xgboost-model-1-def0e83e0d5f11-[93mdf5cc9fd0d5f11eaaa450ae[0mxample
-    Events:     <none>
-
-The status field provides more information using the following fields:
-
--  ``Additional``: Additional information about the status of the
-   hosting deployment. This field is optional and only gets populated in
-   case of error.
-
--  ``Creation Time``: When the endpoint was created in Amazon SageMaker.
-
--  ``Endpoint ARN``: The Amazon SageMaker endpoint ARN.
-
--  ``Endpoint Config Name``: The Amazon SageMaker name of the endpoint
-   configuration.
-
--  ``Endpoint Name``: The Amazon SageMaker name of the endpoint.
-
--  ``Endpoint Status``: The Status of the endpoint.
-
--  ``Endpoint URL``: The HTTPS URL that can be used to access the
-   endpoint. For more information, see \ `Deploy a Model on Amazon
-   SageMaker Hosting
-   Services <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>`__.
-
--  ``FailureReason``: If a create, update, or delete command fails, the
-   cause will be shown here.
-
--  ``Last Check Time``: The last time the operator checked the status of
-   the endpoint.
-
--  ``Last Modified Time``: The last time the endpoint was modified.
-
--  ``Model Names``: A key-value pair of HostingDeployment model names to
-   Amazon SageMaker model names.
-
-Invoking the Endpoint
-^^^^^^^^^^^^^^^^^^^^^
-
-Once the endpoint status is \ ``InService``, you can invoke the endpoint
-in two ways: using the AWS CLI, which does authentication and URL
-request signing, or using an HTTP client like curl. If you use your own
-client, you will need to do AWSv4 URL signing and authentication on your
-own.
-
-To invoke the endpoint using the AWS CLI, run the following command.
-Make sure to replace the Region and endpoint-name with your endpoint’s
-Region and Amazon SageMaker endpoint name. This information can be
-obtained from the output of \ ``kubectl describe``.
-
-::
-
-    # Invoke the endpoint with mock input data.
-    aws sagemaker-runtime invoke-endpoint \
-      --region us-east-2 \
-      --endpoint-name <endpoint name> \
-      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
-      >(cat) \
-      --content-type text/csv > /dev/null
-
-For example, if your Region were \ ``us-east-2`` and your endpoint
-config name were \ ``host-xgboost-[93mf56b6b280d7511ea824b129926e[0mxample``,
-then the following command would invoke the endpoint:
-
-::
-
-    aws sagemaker-runtime invoke-endpoint \
-      --region us-east-2 \
-      --endpoint-name host-xgboost-[93mf56b6b280d7511ea824b1299e[0mxample \
-      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
-      >(cat) \
-      --content-type text/csv > /dev/null
-    4.95847082138
-
-Here, \ ``4.95847082138`` is the prediction from the model for the mock
-data.
-
-Update HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^
-
-Once a HostingDeployment has a status of \ ``InService``, it can be
-updated. It might take about 10 minutes for HostingDeployment to be in
-service. To verify that the status is \ ``InService``, use the following
-command: 
-
-::
-
-    kubectl get hostingdeployments
-
-The HostingDeployment can be updated before the status
-is \ ``InService``. The operator will wait until the Amazon SageMaker
-endpoint is \ ``InService`` before applying the update.
-
-To apply an update, modify the \ ``hosting.yaml`` file. For example,
-change the \ ``initialInstanceCount`` field from 1 to 2 as follows:
-
-::
-
-    apiVersion: sagemaker.aws.amazon.com/v1
-    kind: HostingDeployment
-    metadata:
-      name: host-xgboost
-    spec:
-        region: us-east-2
-        productionVariants:
-            - variantName: all-traffic
-              modelName: xgboost-model
-              initialInstanceCount: 2
-              instanceType: ml.c5.large
-        models:
-            - name: xgboost-model
-              executionRoleArn: arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-              primaryContainer: xgboost
-              containers:
-                - xgboost
-        containers:
-            - containerHostname: xgboost
-              modelDataUrl: s3://my-bucket/inference/xgboost-mnist/model.tar.gz
-              image: 123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
-
-Save the file, then use \ ``kubectl`` to apply your update as follows.
-You should see the status change
-from \ ``InService`` to ``ReconcilingEndpoint``,
-then \ ``Updating``.
-
-::
-
-    $ kubectl apply -f hosting.yaml
-    hostingdeployment.sagemaker.aws.amazon.com/host-xgboost configured
-
-    $ kubectl get hostingdeployments
-    NAME           STATUS                SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   ReconcilingEndpoint   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350abcdef
-
-    $ kubectl get hostingdeployments
-    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   Updating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m3507abcdef
-
-Amazon SageMaker deploys a new set of instances with your models,
-switches traffic to use the new instances, and drains the old instances.
-As soon as this process begins, the status becomes \ ``Updating``. After
-the update is complete, your endpoint becomes \ ``InService``. This
-process takes approximately 10 minutes.
-
-Delete the HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use \ ``kubectl`` to delete a HostingDeployment with the following
-command: 
-
-::
-
-    kubectl delete hostingdeployments host-xgboost
-
-Your output should look like the following:
-
-::
-
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-To verify that the hosting deployment has been deleted, use the
-following command:
-
-::
-
-    kubectl get hostingdeployments
-    No resources found.
-
-Endpoints that have been deleted do not incur any charges for
-Amazon SageMaker resources.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-04-01 15:31:45[0m
[92mHash: 17133906b2cca8cece5db01cff9b8670dc0b837c[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: documentation: Merge Amazon Sagemaker Operators for Kubernetes and Kubernetes Jobs pages (#1334)

[0m
@@ -0,0 +1,1398 @@
+Using Amazon Sagemaker Jobs
+---------------------------
+
+To run a job using the Amazon Sagemaker Operators for Kubernetes, you can either apply
+a YAML file or use the supplied Helm charts.
+
+All operator sample jobs in the following tutorials use sample data
+taken from a public MNIST dataset. In order to run these samples, download the dataset into your S3 bucket. You can find
+the dataset in \ `Download the MNIST
+Dataset. <https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data-pull-data.html>`__
+
+.. contents::
+
+TrainingJob operator
+~~~~~~~~~~~~~~~~~~~~
+
+Training job operators reconcile your specified training job spec to
+Amazon SageMaker by launching it for you in Amazon SageMaker. You can
+learn more about Amazon SageMaker training jobs in the Amazon
+SageMaker \ `CreateTrainingJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html>`__.
+
+Create a TrainingJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for training using the following command: 
+
+::
+
+    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-trainingjob.yaml
+
+Edit the ``xgboost-mnist-trainingjob.yaml`` file to replace the ``roleArn`` parameter with your ``<sagemaker-execution-role>``, and ``outputPath`` with your S3 bucket that the Amazon SageMaker
+execution role has write access to. The ``roleArn`` must have permissions so that Amazon SageMaker
+can access Amazon S3, Amazon CloudWatch, and other services on your 
+behalf. For more information on creating an Amazon SageMaker
+ExecutionRole, see `Amazon SageMaker
+Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
+Apply the YAML file using the
+following command:
+
+::
+
+    kubectl apply -f xgboost-mnist-trainingjob.yaml
+
+Create a TrainingJob Using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run TrainingJobs. 
+
+Clone the github repo to get the source using the following command: 
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+
+Navigate to the
+\ ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
+and edit the \ ``values.yaml`` file to replace values
+like \ ``rolearn`` and ``outputpath`` with values that correspond to
+your account. The RoleARN must have permissions so that Amazon SageMaker
+can access Amazon S3, Amazon CloudWatch, and other services on your
+behalf. For more information on creating an Amazon SageMaker
+ExecutionRole, see \ `Amazon SageMaker
+Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
+
+Create the Training Job 
+''''''''''''''''''''''''
+
+With the roles and S3 buckets replaced with appropriate values
+in \ ``values.yaml``, you can create a training job using the following
+command:
+
+::
+
+    helm install . --generate-name
+
+Your output should look like the following:
+
+::
+
+    NAME: chart-12345678
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-trainingjob.
+
+Verify Your Training Helm Chart
+'''''''''''''''''''''''''''''''
+
+To verify that the Helm Chart was created successfully, run:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    chart-12345678        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-12345678    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+``helm install`` creates a \ ``TrainingJob`` k8s resource. The operator
+launches the actual training job in Amazon SageMaker and updates
+the \ ``TrainingJob`` k8s resource to reflect the status of the job in
+Amazon SageMaker. You incur charges for Amazon SageMaker resources used
+during the duration of your job. You do not incur any charges once your
+job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running
+training job. You cannot edit any parameter and re-apply the
+file/config. Either change the metadata name or delete the existing job
+and create a new one. Similar to existing training job operators like
+TFJob in Kubeflow, \ ``update`` is not supported.
+
+List Training Jobs
+^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+    kubectl get TrainingJob
+
+The output listing all jobs should look like the following:
+
+::
+
+    kubectl get trainingjobs
+    NAME                        STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME
+    xgboost-mnist-from-for-s3   InProgress   Starting           2019-11-20T23:42:35Z   xgboost-mnist-from-for-s3-exampl[93mef11eab94e0ed4671d5a8f[0m
+
+A training job continues to be listed after the job has completed or
+failed. You can remove a \ ``TrainingJob``  job from the list by
+following the Delete a Training Job steps. Jobs that have completed or
+stopped do not incur any charges for Amazon SageMaker resources. 
+
+Training Job Status Values
+''''''''''''''''''''''''''
+
+The \ ``STATUS`` field can be one of the following values: 
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official \ `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-TrainingJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job.
+
+Secondary Status Values
+'''''''''''''''''''''''
+
+The secondary statuses come directly from the Amazon SageMaker
+official \ `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-SecondaryStatus>`__.
+They contain more granular information about the status of the job.
+
+Describe a Training Job
+^^^^^^^^^^^^^^^^^^^^^^^
+
+You can get more details about the training job by using
+the \ ``describe`` kubectl verb. This is typically used for debugging a
+problem or checking the parameters of a training job. To get information
+about your training job, use the following command:
+
+::
+
+    kubectl describe trainingjob xgboost-mnist-from-for-s3
+
+The output for your training job should look like the following:
+
+::
+
+    Name:         xgboost-mnist-from-for-s3
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  <none>
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         TrainingJob
+    Metadata:
+      Creation Timestamp:  2019-11-20T23:42:35Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  23119
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-from-for-s3
+      UID:               6d7uiui-0bef-11ea-b94e-0ed467example
+    Spec:
+      Algorithm Specification:
+        Training Image:       8256416981234.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
+        Training Input Mode:  File
+      Hyper Parameters:
+        Name:   eta
+        Value:  0.2
+        Name:   gamma
+        Value:  4
+        Name:   max_depth
+        Value:  5
+        Name:   min_child_weight
+        Value:  6
+        Name:   num_class
+        Value:  10
+        Name:   num_round
+        Value:  10
+        Name:   objective
+        Value:  multi:softmax
+        Name:   silent
+        Value:  0
+      Input Data Config:
+        Channel Name:      train
+        Compression Type:  None
+        Content Type:      text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Distribution Type:  FullyReplicated
+            S 3 Data Type:               S3Prefix
+            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
+        Channel Name:                    validation
+        Compression Type:                None
+        Content Type:                    text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Distribution Type:  FullyReplicated
+            S 3 Data Type:               S3Prefix
+            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
+      Output Data Config:
+        S 3 Output Path:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost/
+      Region:             us-east-2
+      Resource Config:
+        Instance Count:     1
+        Instance Type:      ml.m4.xlarge
+        Volume Size In GB:  5
+      Role Arn:             arn:aws:iam::12345678910:role/service-role/AmazonSageMaker-ExecutionRole
+      Stopping Condition:
+        Max Runtime In Seconds:  86400
+      Training Job Name:         xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0mxample
+    Status:
+      Cloud Watch Log URL:           https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=<example>;streamFilter=typeLogStreamPrefix
+      Last Check Time:               2019-11-20T23:44:29Z
+      Sage Maker Training Job Name:  xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94ee[0mxample
+      Secondary Status:              Downloading
+      Training Job Status:           InProgress
+    Events:                          <none>
+
+View Logs from Training Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to see the logs from the \ ``kmeans-mnist`` 
+training job:
+
+::
+
+    kubectl smlogs trainingjob xgboost-mnist-from-for-s3
+
+Your output will look similar to the following. The logs from instances
+are ordered chronologically.
+
+::
+
+    "xgboost-mnist-from-for-s3" has SageMaker TrainingJobName "xgboost-mnist-from-for-s3-123456789" in region "us-east-2", status "InProgress" and secondary status "Starting"
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC Arguments: train
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Running standalone xgboost training.
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 8586.0mb
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Determined delimiter of CSV input is ','
+    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [23:45:22] S3DistributionType set as FullyReplicated
+
+Delete Training Jobs
+^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a training job on Amazon SageMaker:
+
+::
+
+    kubectl delete trainingjob xgboost-mnist-from-for-s3
+
+This command removes the Amazon SageMaker training job from k8s. This
+command returns the following output:
+
+::
+
+    trainingjob.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
+
+If the job is still in progress on Amazon SageMaker, the job will stop.
+You do not incur any charges for Amazon SageMaker resources after your
+job stops or completes. 
+
+**Note**: Amazon SageMaker does not delete training jobs. Stopped jobs
+continue to show on the Amazon SageMaker console. The delete command
+takes about 2 minutes to clean up the resources from Amazon SageMaker.
+
+SageMaker Debugger Jobs
+^^^^^^^^^^^^^^^^^^^^^^^
+
+When creating a SageMaker training job, you have an option to run 
+asynchronous debugger jobs for your model. It gives you full visibility 
+into a training job by using a hook to capture tensors that define 
+the state of the training process at each instance in its lifecycle. 
+It also provides the capability of defining 'rules' to
+analyze the captured tensors. See `SageMaker Debugger Introduction <https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html>`__ and `How Debugger Works <https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-how-it-works.html>`__ for details.
+
+You can get more details on debug job by using the ``describe`` kubectl verb.
+The output of describing a training job will now have a new field ``Debug Rule Evaluation Statuses:``
+
+::
+
+    kubectl describe trainingjobs xgboost-mnist-debugger
+
+    Name:         xgboost-mnist-debugger
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"TrainingJob","metadata":{"annotations":{},"name":"xgboost-mnist-debugger","namespace":...
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         TrainingJob
+    Metadata:
+      Creation Timestamp:  2020-03-18T05:58:59Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  2939388
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-debugger
+      UID:               8fe3799e-68dd-11ea-8423-1260529a8dc9
+    Spec:
+      Algorithm Specification:
+        Training Image:       246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3
+        Training Input Mode:  File
+      Debug Hook Config:
+        Collection Configurations:
+          Collection Name:  feature_importance
+          Collection Parameters:
+            Name:           save_interval
+            Value:          5
+          Collection Name:  losses
+          Collection Parameters:
+            Name:           save_interval"
+            Value:          500
+          Collection Name:  average_shap
+          Collection Parameters:
+            Name:           save_interval
+            Value:          5
+          Collection Name:  metrics
+          Collection Parameters:
+            Name:      save_interval
+            Value:     5
+        s3OutputPath:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/
+      Debug Rule Configurations:
+        Rule Configuration Name:  LossNotDecreasing
+        Rule Evaluator Image:     895741380848.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rules:latest
+        Rule Parameters:
+          Name:   collection_names
+          Value:  metrics
+          Name:   num_steps
+          Value:  10
+          Name:   rule_to_invoke
+          Value:  LossNotDecreasing
+      Hyper Parameters:
+        Name:   max_depth
+        Value:  5
+        Name:   eta
+        Value:  0.2
+        Name:   gamma
+        Value:  4
+        Name:   min_child_weight
+        Value:  6
+        Name:   silent
+        Value:  0
+        Name:   objective
+        Value:  reg:squarederror
+        Name:   subsample
+        Value:  0.7
+        Name:   num_round
+        Value:  51
+      Input Data Config:
+        Channel Name:      train
+        Compression Type:  None
+        Content Type:      libsvm
+        Data Source:
+          s3DataSource:
+            s3DataDistributionType:  FullyReplicated
+            s3DataType:              S3Prefix
+            s3Uri:                   s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/train
+        Channel Name:                validation
+        Compression Type:            None
+        Content Type:                libsvm
+        Data Source:
+          s3DataSource:
+            s3DataDistributionType:  FullyReplicated
+            s3DataType:              S3Prefix
+            s3Uri:                   s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/validation
+      Output Data Config:
+        s3OutputPath:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/
+      Region:          us-west-2
+      Resource Config:
+        Instance Count:     1
+        Instance Type:      ml.m4.xlarge
+        Volume Size In GB:  5
+      Role Arn:             arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole
+      Stopping Condition:
+        Max Runtime In Seconds:  86400
+      Tags:
+        Key:              tagKey
+        Value:            tagValue
+      Training Job Name:  xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m
+    Status:
+      Cloud Watch Log URL:  https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m;streamFilter=typeLogStreamPrefix
+      Debug Rule Evaluation Statuses:
+        Last Modified Time:          2020-03-18T06:03:48Z
+        Rule Configuration Name:     LossNotDecreasing
+        Rule Evaluation Job Arn:     arn:aws:sagemaker:us-west-2:1234567890:processing-job/xgboost-mnist-debugger-8fe-lossnotdecreasing-a7d0eaf2
+        Rule Evaluation Status:      NoIssuesFound
+      Model Path:                    s3://my-bucket/sagemaker/xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m/output/model.tar.gz
+      Sage Maker Training Job Name:  xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m
+      Secondary Status:              Completed
+      Training Job Status:           Completed
+    Events:                          <none>
+
+See `SageMaker Debugger Examples <https://github.com/aws/amazon-sagemaker-operator-for-k8s/tree/master/samples>`__ for more examples of debugger jobs.
+
+
+HyperParameterTuningJobs operator
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Hyperparameter tuning job operators reconcile your
+specified hyperparameter tuning job spec to Amazon SageMaker by
+launching it in Amazon SageMaker. You can learn more about Amazon
+SageMaker hyperparameter tuning jobs in the Amazon
+SageMaker \ `CreateHyperParameterTuningJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateHyperParameterTuningJob.html>`__.
+
+Create a HyperParameterTuningJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the hyperparameter tuning job using
+the following command: 
+
+::
+
+    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hpo.yaml
+
+Edit the \ ``xgboost-mnist-hpo.yaml`` file to replace
+the \ ``roleArn`` parameter with your <sagemaker-execution-role>. For
+HyperparameterTuningJob to succeed, you must also change
+the \ ``s3InputPath``  and \ ``s3OutputPath`` to values that correspond
+to your account. Apply the updates YAML file using the following
+command:
+
+::
+
+    kubectl apply -f xgboost-mnist-hpo.yaml
+
+Create a HyperParameterTuningJob using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run HyperParameterTuningJobs.
+
+Clone the github repo to get the source using the following command: 
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+
+Navigate to the
+\ ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
+folder.
+
+Edit the \ ``values.yaml`` file to replace the \ ``roleArn`` parameter
+with your <sagemaker-execution-role>. For HyperparameterTuningJob to
+succeed, you must also change the \ ``s3InputPath`` 
+and \ ``s3OutputPath`` to values that correspond to your account. 
+
+Create the HPO Job
+''''''''''''''''''
+
+With the roles and Amazon S3 paths replaced with appropriate values
+in \ ``values.yaml``, you can create a hyperparameter tuning job using
+the following command:
+
+::
+
+    helm install . --generate-name
+
+Your output will look similar to the following:
+
+::
+
+    NAME: chart-1574292948
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-hyperparametertuningjob.
+
+Verify Chart Installation
+'''''''''''''''''''''''''
+
+To verify that the Helm Chart was created successfully, run the
+following command:
+
+::
+
+    helm ls
+
+Your output should look like the following:
+
+::
+
+    NAME                    NAMESPACE       REVISION        UPDATED  
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0                               STATUS          CHART                           APP VERSION
+    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+``helm install`` creates a \ ``HyperParameterTuningJob`` k8s resource.
+The operator launches the actual hyperparameter optimization job in
+Amazon SageMaker and updates the \ ``HyperParameterTuningJob`` k8s
+resource to reflect the status of the job in Amazon SageMaker. You incur
+charges for Amazon SageMaker resources used during the duration of your
+job. You do not incur any charges once your job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running
+hyperparameter tuning job. You cannot edit any parameter and re-apply
+the file/config. You must either change the metadata name or delete the
+existing job and create a new one. Similar to existing training job
+operators like TFJob in Kubeflow, \ ``update`` is not supported.
+
+List Hyperparameter Tuning Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+    kubectl get hyperparametertuningjob 
+
+Your output will look like the following:
+
+::
+
+    NAME         STATUS      CREATION-TIME          COMPLETED   INPROGRESS   ERRORS   STOPPED   BEST-TRAINING-JOB                               SAGEMAKER-JOB-NAME
+    xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
+
+A hyper parameter tuning job will continue to be listed after the job
+has completed or failed. You can remove a \ ``hyperparametertuningjob`` 
+from the list by following the steps in Delete a Hyper Parameter Tuning
+Job. Jobs that have completed or stopped do not incur any charges for
+Amazon SageMaker resources. 
+
+Hyperparameter Tuning Job Status Values
+'''''''''''''''''''''''''''''''''''''''
+
+The \ ``STATUS`` field can be one of the following values: 
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job.
+
+Status Counters
+'''''''''''''''
+
+The output has several counters,
+like \ ``COMPLETED`` and ``INPROGRESS``. These represent how many
+training jobs have completed and are in progress, respectively. For more
+information about how these are determined,
+see \ `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
+the Amazon SageMaker API documentation. 
+
+Best Training Job
+'''''''''''''''''
+
+This column contains the name of the \ ``TrainingJob`` that best
+optimized the selected metric.
+
+To see a summary of the tuned hyperparameters, run:
+
+::
+
+    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
+
+To see detailed information about the \ ``TrainingJob``, run:
+
+::
+
+    kubectl describe trainingjobs <job name>
+
+
+Spawned Training Jobs
+'''''''''''''''''''''
+
+You can also track all 10 training jobs in k8s launched by
+``HyperparameterTuningJob`` by running the following command:
+
+::
+
+    kubectl get trainingjobs
+
+Describe a Hyperparameter Tuning Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the \ ``describe`` kubectl verb
+by running the following command. 
+
+::
+
+    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
+
+In addition to information about the tuning job, the Amazon SageMaker
+Operator for Kubernetes also exposes the `best training
+job <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html#automatic-model-tuning-best-training-job>`__\  found
+by the hyperparameter tuning job in the \ ``describe`` output as
+follows:
+
+::
+
+    Name:         xgboost-mnist-hpo
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HyperparameterTuningJob","metadata":{"annotations":{},"name":"xgboost-mnist-hpo","namespace":...
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         HyperparameterTuningJob
+    Metadata:
+      Creation Timestamp:  2019-10-17T01:15:52Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  8167
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hyperparametertuningjobs/xgboost-mnist-hpo
+      UID:               a92f5e3c-f07b-11e9-bf6c-06d6f303uidu
+    Spec:
+      Hyper Parameter Tuning Job Config:
+        Hyper Parameter Tuning Job Objective:
+          Metric Name:  validation:error
+          Type:         Minimize
+        Parameter Ranges:
+          Integer Parameter Ranges:
+            Max Value:     20
+            Min Value:     10
+            Name:          num_round
+            Scaling Type:  Linear
+        Resource Limits:
+          Max Number Of Training Jobs:     10
+          Max Parallel Training Jobs:      10
+        Strategy:                          Bayesian
+        Training Job Early Stopping Type:  Off
+      Hyper Parameter Tuning Job Name:     xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
+      Region:                              us-east-2
+      Training Job Definition:
+        Algorithm Specification:
+          Training Image:       12345678910.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
+          Training Input Mode:  File
+        Input Data Config:
+          Channel Name:  train
+          Content Type:  text/csv
+          Data Source:
+            s3DataSource:
+              s3DataDistributionType:  FullyReplicated
+              s3DataType:              S3Prefix
+              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
+          Channel Name:                validation
+          Content Type:                text/csv
+          Data Source:
+            s3DataSource:
+              s3DataDistributionType:  FullyReplicated
+              s3DataType:              S3Prefix
+              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
+        Output Data Config:
+          s3OutputPath:  https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/xgboost
+        Resource Config:
+          Instance Count:     1
+          Instance Type:      ml.m4.xlarge
+          Volume Size In GB:  5
+        Role Arn:             arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+        Static Hyper Parameters:
+          Name:   base_score
+          Value:  0.5
+          Name:   booster
+          Value:  gbtree
+          Name:   csv_weights
+          Value:  0
+          Name:   dsplit
+          Value:  row
+          Name:   grow_policy
+          Value:  depthwise
+          Name:   lambda_bias
+          Value:  0.0
+          Name:   max_bin
+          Value:  256
+          Name:   max_leaves
+          Value:  0
+          Name:   normalize_type
+          Value:  tree
+          Name:   objective
+          Value:  reg:linear
+          Name:   one_drop
+          Value:  0
+          Name:   prob_buffer_row
+          Value:  1.0
+          Name:   process_type
+          Value:  default
+          Name:   rate_drop
+          Value:  0.0
+          Name:   refresh_leaf
+          Value:  1
+          Name:   sample_type
+          Value:  uniform
+          Name:   scale_pos_weight
+          Value:  1.0
+          Name:   silent
+          Value:  0
+          Name:   sketch_eps
+          Value:  0.03
+          Name:   skip_drop
+          Value:  0.0
+          Name:   tree_method
+          Value:  auto
+          Name:   tweedie_variance_power
+          Value:  1.5
+        Stopping Condition:
+          Max Runtime In Seconds:  86400
+    Status:
+      Best Training Job:
+        Creation Time:  2019-10-17T01:16:14Z
+        Final Hyper Parameter Tuning Job Objective Metric:
+          Metric Name:        validation:error
+          Value:              
+        Objective Status:     Succeeded
+        Training End Time:    2019-10-17T01:20:24Z
+        Training Job Arn:     arn:aws:sagemaker:us-east-2:123456789012:training-job/xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4sample
+        Training Job Name:    xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a3059
+        Training Job Status:  Completed
+        Training Start Time:  2019-10-17T01:18:35Z
+        Tuned Hyper Parameters:
+          Name:                                    num_round
+          Value:                                   18
+      Hyper Parameter Tuning Job Status:           Completed
+      Last Check Time:                             2019-10-17T01:21:01Z
+      Sage Maker Hyper Parameter Tuning Job Name:  xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
+      Training Job Status Counters:
+        Completed:            10
+        In Progress:          0
+        Non Retryable Error:  0
+        Retryable Error:      0
+        Stopped:              0
+        Total Error:          0
+    Events:                   <none>
+
+View Logs from HyperParameterTuning Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Hyperparameter tuning jobs do not have logs, but all training jobs
+launched by them do have logs. These logs can be accessed as if they
+were a normal training job. For more information, see View Logs from
+Training Jobs.
+
+Delete HyperParameterTuning jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a hyperparameter job in
+Amazon SageMaker. 
+
+::
+
+    kubectl delete hyperparametertuningjob xgboost-mnist-hpo
+
+This command removes the hyperparameter tuning job and associated
+training jobs from your Kubernetes cluster, as well as stops them in
+Amazon SageMaker. Jobs that have stopped or completed do not incur any
+charges for Amazon SageMaker resources.  Amazon SageMaker does not
+delete hyperparameter tuning jobs. Stopped jobs continue to show on the
+Amazon SageMaker Console. 
+
+Your output should look like the following:  
+
+::
+
+    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
+
+**Note**:  The delete command takes about 2 minutes to clean up the
+resources from Amazon SageMaker.
+
+BatchTransformJobs operator
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Batch transform job operators reconcile your specified batch transform
+job spec to Amazon SageMaker by launching it in Amazon SageMaker. You
+can learn more about Amazon SageMaker batch transform job in the Amazon
+SageMaker \ `CreateTransformJob API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`__.
+
+Create a BatchTransformJob Using a Simple YAML File
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the batch transform job using the
+following command: 
+
+::
+
+    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-batchtransform.yaml
+
+Edit the file \ ``xgboost-mnist-batchtransform.yaml`` to change
+necessary parameters to replace the \ ``inputdataconfig``  with your
+input data and \ ``s3OutputPath`` with your S3 buckets that the Amazon
+SageMaker execution role has write access to.  
+
+Apply the YAML file using the following command:
+
+::
+
+    kubectl apply -f xgboost-mnist-batchtransform.yaml
+
+Create a BatchTransformJob Using a Helm Chart
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can use Helm Charts to run batch transform jobs.
+
+Get the Helm installer directory
+''''''''''''''''''''''''''''''''
+
+Clone the github repo to get the source using the following command: 
+
+::
+
+    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
+
+Configure the Helm Chart
+''''''''''''''''''''''''
+
+Navigate to the
+``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
+folder. 
+
+Edit the \ ``values.yaml`` file to replace the \ ``inputdataconfig`` 
+with your input data and outputPath with your S3 buckets that the Amazon
+SageMaker execution role has write access to. 
+
+Create a Batch Transform Job
+''''''''''''''''''''''''''''
+
+Use the following command to create a batch transform job:
+
+::
+
+    helm install . --generate-name
+
+Your output should look like the following:
+
+::
+
+    NAME: chart-1574292948
+    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
+    NAMESPACE: default
+    STATUS: deployed
+    REVISION: 1
+    TEST SUITE: None
+    NOTES:
+    Thanks for installing the sagemaker-k8s-batch-transform-job.
+
+To verify that the Helm Chart was created successfully, run the
+following command:
+
+::
+
+    helm ls
+    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-batchtransformjob-0.1.0
+    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0
+    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
+    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
+
+The previous command creates a \ ``BatchTransformJob`` k8s resource. The
+operator launches the actual transform job in Amazon SageMaker and
+updates the \ ``BatchTransformJob`` k8s resource to reflect the status
+of the job in Amazon SageMaker. You incur charges for Amazon SageMaker
+resources used during the duration of your job. You do not incur any
+charges once your job completes or stops.
+
+**Note**: Amazon SageMaker does not allow you to update a running batch
+transform job. You cannot edit any parameter and re-apply the
+file/config. You must either change the metadata name or delete the
+existing job and create a new one. Similar to existing training job
+operators like TFJob in Kubeflow, \ ``update`` is not supported.
+
+List Batch Transform Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to list all jobs created using the k8s
+operator:
+
+::
+
+     kubectl get batchtransformjob 
+
+Your output should look like the following:
+
+::
+
+    NAME                                STATUS      CREATION-TIME          SAGEMAKER-JOB-NAME
+    xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0mxgboost
+
+A batch transform job will continue to be listed after the job has
+completed or failed. You can remove a \ ``hyperparametertuningjob`` 
+from the list by following the Delete a Batch Transform Job steps. Jobs
+that have completed or stopped do not incur any charges for
+Amazon SageMaker resources. 
+
+Batch Transform Status Values
+'''''''''''''''''''''''''''''
+
+The \ ``STATUS`` field can be one of the following values: 
+
+-  ``Completed``
+
+-  ``InProgress``
+
+-  ``Failed``
+
+-  ``Stopped``
+
+-  ``Stopping``
+
+These statuses come directly from the Amazon SageMaker official `API
+documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
+
+In addition to the official Amazon SageMaker status, it is possible
+for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
+means that the operator has not yet processed the job and will get to it
+soon.
+
+Describe a Batch Transform Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the \ ``describe`` kubectl verb
+by running the following command. 
+
+::
+
+    kubectl describe batchtransformjob xgboost-mnist-batch-transform
+
+Your output should look like the following:
+
+::
+
+    Name:         xgboost-mnist-batch-transform
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"BatchTransformJob","metadata":{"annotations":{},"name":"xgboost-mnist","namespace"...
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         BatchTransformJob
+    Metadata:
+      Creation Timestamp:  2019-11-18T03:44:00Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        2
+      Resource Version:  21990924
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/batchtransformjobs/xgboost-mnist
+      UID:               a88fb198-09b5-11ea-ac44-0aa8a9UIDNUM
+    Spec:
+      Model Name:  TrainingJob-20190814SMJOb-IKEB
+      Region:      us-east-1
+      Transform Input:
+        Content Type:  text/csv
+        Data Source:
+          S 3 Data Source:
+            S 3 Data Type:  S3Prefix
+            S 3 Uri:        s3://my-bucket/mnist_kmeans_example/input
+      Transform Job Name:   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0m9SMJOB
+      Transform Output:
+        S 3 Output Path:  s3://my-bucket/mnist_kmeans_example/output
+      Transform Resources:
+        Instance Count:  1
+        Instance Type:   ml.m4.xlarge
+    Status:
+      Last Check Time:                2019-11-19T22:50:40Z
+      Sage Maker Transform Job Name:  xgboost-mnist-[93ma88fb19809b511eaac440aa[0mSMJOB
+      Transform Job Status:           Completed
+    Events:                           <none>
+
+View Logs from Batch Transform Jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to see the logs from the \ ``xgboost-mnist`` 
+batch transform job:
+
+::
+
+    kubectl smlogs batchtransformjob xgboost-mnist-batch-transform
+
+Delete a Batch Transform Job
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use the following command to stop a batch transform job in
+Amazon SageMaker. 
+
+::
+
+    kubectl delete batchTransformJob xgboost-mnist-batch-transform
+
+Your output will look like the following:
+
+::
+
+    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
+
+This command removes the batch transform job from your Kubernetes
+cluster, as well as stops them in Amazon SageMaker. Jobs that have
+stopped or completed do not incur any charges for Amazon SageMaker
+resources. Delete takes about 2 minutes to clean up the resources from
+Amazon SageMaker.
+
+**Note**: Amazon SageMaker does not delete batch transform jobs. Stopped
+jobs continue to show on the Amazon SageMaker console. 
+
+Real-time inference
+~~~~~~~~~~~~~~~~~~~
+
+HostingDeployments support creating and deleting an endpoint, as well as
+updating an existing endpoint. The hosting deployment operator
+reconciles your specified hosting deployment job spec to Amazon
+SageMaker by creating models, endpoint-configs and endpoints in Amazon
+SageMaker. You can learn more about Amazon SageMaker inference in the
+Amazon SageMaker \ `CreateEndpoint API
+documentaiton <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__.
+
+Configure a HostingDeployment Resource
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Download the sample YAML file for the hosting deployment job using the
+following command: 
+
+::
+
+    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hostingdeployment.yaml
+
+The ``xgboost-mnist-hostingdeployment.yaml`` file has the following components that can be edited as required:
+
+-  ProductionVariants. A production variant is a set of instances
+   serving a single model. Amazon SageMaker will load-balance between
+   all production variants according to set weights.
+
+-  Models. A model is the containers and execution role ARN necessary to
+   serve a model. It requires at least a single container.
+
+-  Containers. A container specifies the dataset and serving image. If
+   you are using your own custom algorithm instead of an algorithm
+   provided by Amazon SageMaker, the inference code must meet Amazon
+   SageMaker requirements. For more information, see `Using Your Own
+   Algorithms with Amazon
+   SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.
+
+Create a HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+To create a HostingDeployment, use \ ``kubectl`` to apply the
+file \ ``hosting.yaml`` with the following command:
+
+::
+
+    kubectl apply -f hosting.yaml
+
+Amazon SageMaker create an endpoint with the specified
+configuration. You incur charges for Amazon SageMaker resources used
+during the lifetime of your endpoint. You do not incur any charges once
+your endpoint is deleted.
+
+The creation process will take approximately 10 minutes.
+
+List HostingDeployments
+^^^^^^^^^^^^^^^^^^^^^^^
+
+To verify that the HostingDeployment was created, use the following
+command:
+
+::
+
+    kubectl get hostingdeployments
+
+Your output should look like the following:
+
+::
+
+    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   Creating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0mSMLOGS
+
+HostingDeployment Status Values
+'''''''''''''''''''''''''''''''
+
+The status field can be one of several values:
+
+-  ``SynchronizingK8sJobWithSageMaker``: The operator is preparing to
+   create the endpoint.
+
+-  ``ReconcilingEndpoint``: The operator is creating, updating, or
+   deleting endpoint resources. If the HostingDeployment remains in this
+   state, use \ ``kubectl describe`` to see the reason in the
+   ``Additional`` field.
+
+-  ``OutOfService``: Endpoint is not available to take incoming
+   requests.
+
+-  ``Creating``:
+   `CreateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__
+   is executing.
+
+-  ``Updating``:
+   `UpdateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html>`__
+   or
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   is executing.
+
+-  ``SystemUpdating``: Endpoint is undergoing maintenance and cannot be
+   updated or deleted or re-scaled until it has completed. This
+   maintenance operation does not change any customer-specified values
+   such as VPC config, KMS encryption, model, instance type, or instance
+   count.
+
+-  ``RollingBack``: Endpoint fails to scale up or down or change its
+   variant weight and is in the process of rolling back to its previous
+   configuration. Once the rollback completes, endpoint returns to an
+   ``InService`` status. This transitional status only applies to an
+   endpoint that has autoscaling enabled and is undergoing variant
+   weight or capacity changes as part of an
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   call or when the
+   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
+   operation is called explicitly.
+
+-  ``InService``: Endpoint is available to process incoming requests.
+
+-  ``Deleting``:
+   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
+   is executing.
+
+-  ``Failed``: Endpoint could not be created, updated, or re-scaled. Use
+   `DescribeEndpoint:FailureReason <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeEndpoint.html#SageMaker-DescribeEndpoint-response-FailureReason>`__
+   for information about the failure.
+   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
+   is the only operation that can be performed on a failed endpoint.
+
+Describe a Hostingdeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can obtain debugging details using the \ ``describe`` kubectl verb
+by running the following command. 
+
+::
+
+    kubectl describe hostingdeployment
+
+Your output should look like the following:
+
+::
+
+    Name:         host-xgboost
+    Namespace:    default
+    Labels:       <none>
+    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
+                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HostingDeployment","metadata":{"annotations":{},"name":"host-xgboost","namespace":"def..."
+    API Version:  sagemaker.aws.amazon.com/v1
+    Kind:         HostingDeployment
+    Metadata:
+      Creation Timestamp:  2019-11-22T19:40:00Z
+      Finalizers:
+        sagemaker-operator-finalizer
+      Generation:        1
+      Resource Version:  4258134
+      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hostingdeployments/host-xgboost
+      UID:               def0e83e-0d5f-11ea-aa45-0a3507uiduid
+    Spec:
+      Containers:
+        Container Hostname:  xgboost
+        Image:               123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
+        Model Data URL:      s3://my-bucket/inference/xgboost-mnist/model.tar.gz
+      Models:
+        Containers:
+          xgboost
+        Execution Role Arn:  arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+        Name:                xgboost-model
+        Primary Container:   xgboost
+      Production Variants:
+        Initial Instance Count:  1
+        Instance Type:           ml.c5.large
+        Model Name:              xgboost-model
+        Variant Name:            all-traffic
+      Region:                    us-east-2
+    Status:
+      Creation Time:         2019-11-22T19:40:04Z
+      Endpoint Arn:          arn:aws:sagemaker:us-east-2:123456789012:endpoint/host-xgboost-def0e83e0d5f11eaaaexample
+      Endpoint Config Name:  host-xgboost-1-def0e83e0d5f11e-[93me08f6c510d5f11eaaa450ae[0mxample
+      Endpoint Name:         host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350733ba06
+      Endpoint Status:       Creating
+      Endpoint URL:          https://runtime.sagemaker.us-east-2.amazonaws.com/endpoints/host-xgboost-def0e83e0d5f11eaaaexample/invocations
+      Last Check Time:       2019-11-22T19:43:57Z
+      Last Modified Time:    2019-11-22T19:40:04Z
+      Model Names:
+        Name:   xgboost-model
+        Value:  xgboost-model-1-def0e83e0d5f11-[93mdf5cc9fd0d5f11eaaa450ae[0mxample
+    Events:     <none>
+
+The status field provides more information using the following fields:
+
+-  ``Additional``: Additional information about the status of the
+   hosting deployment. This field is optional and only gets populated in
+   case of error.
+
+-  ``Creation Time``: When the endpoint was created in Amazon SageMaker.
+
+-  ``Endpoint ARN``: The Amazon SageMaker endpoint ARN.
+
+-  ``Endpoint Config Name``: The Amazon SageMaker name of the endpoint
+   configuration.
+
+-  ``Endpoint Name``: The Amazon SageMaker name of the endpoint.
+
+-  ``Endpoint Status``: The Status of the endpoint.
+
+-  ``Endpoint URL``: The HTTPS URL that can be used to access the
+   endpoint. For more information, see \ `Deploy a Model on Amazon
+   SageMaker Hosting
+   Services <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>`__.
+
+-  ``FailureReason``: If a create, update, or delete command fails, the
+   cause will be shown here.
+
+-  ``Last Check Time``: The last time the operator checked the status of
+   the endpoint.
+
+-  ``Last Modified Time``: The last time the endpoint was modified.
+
+-  ``Model Names``: A key-value pair of HostingDeployment model names to
+   Amazon SageMaker model names.
+
+Invoking the Endpoint
+^^^^^^^^^^^^^^^^^^^^^
+
+Once the endpoint status is \ ``InService``, you can invoke the endpoint
+in two ways: using the AWS CLI, which does authentication and URL
+request signing, or using an HTTP client like curl. If you use your own
+client, you will need to do AWSv4 URL signing and authentication on your
+own.
+
+To invoke the endpoint using the AWS CLI, run the following command.
+Make sure to replace the Region and endpoint-name with your endpoint’s
+Region and Amazon SageMaker endpoint name. This information can be
+obtained from the output of \ ``kubectl describe``.
+
+::
+
+    # Invoke the endpoint with mock input data.
+    aws sagemaker-runtime invoke-endpoint \
+      --region us-east-2 \
+      --endpoint-name <endpoint name> \
+      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
+      >(cat) \
+      --content-type text/csv > /dev/null
+
+For example, if your Region were \ ``us-east-2`` and your endpoint
+config name were \ ``host-xgboost-[93mf56b6b280d7511ea824b129926e[0mxample``,
+then the following command would invoke the endpoint:
+
+::
+
+    aws sagemaker-runtime invoke-endpoint \
+      --region us-east-2 \
+      --endpoint-name host-xgboost-[93mf56b6b280d7511ea824b1299e[0mxample \
+      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
+      >(cat) \
+      --content-type text/csv > /dev/null
+    4.95847082138
+
+Here, \ ``4.95847082138`` is the prediction from the model for the mock
+data.
+
+Update HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^
+
+Once a HostingDeployment has a status of \ ``InService``, it can be
+updated. It might take about 10 minutes for HostingDeployment to be in
+service. To verify that the status is \ ``InService``, use the following
+command: 
+
+::
+
+    kubectl get hostingdeployments
+
+The HostingDeployment can be updated before the status
+is \ ``InService``. The operator will wait until the Amazon SageMaker
+endpoint is \ ``InService`` before applying the update.
+
+To apply an update, modify the \ ``hosting.yaml`` file. For example,
+change the \ ``initialInstanceCount`` field from 1 to 2 as follows:
+
+::
+
+    apiVersion: sagemaker.aws.amazon.com/v1
+    kind: HostingDeployment
+    metadata:
+      name: host-xgboost
+    spec:
+        region: us-east-2
+        productionVariants:
+            - variantName: all-traffic
+              modelName: xgboost-model
+              initialInstanceCount: 2
+              instanceType: ml.c5.large
+        models:
+            - name: xgboost-model
+              executionRoleArn: arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
+              primaryContainer: xgboost
+              containers:
+                - xgboost
+        containers:
+            - containerHostname: xgboost
+              modelDataUrl: s3://my-bucket/inference/xgboost-mnist/model.tar.gz
+              image: 123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
+
+Save the file, then use \ ``kubectl`` to apply your update as follows.
+You should see the status change
+from \ ``InService`` to ``ReconcilingEndpoint``,
+then \ ``Updating``.
+
+::
+
+    $ kubectl apply -f hosting.yaml
+    hostingdeployment.sagemaker.aws.amazon.com/host-xgboost configured
+
+    $ kubectl get hostingdeployments
+    NAME           STATUS                SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   ReconcilingEndpoint   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350abcdef
+
+    $ kubectl get hostingdeployments
+    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
+    host-xgboost   Updating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m3507abcdef
+
+Amazon SageMaker deploys a new set of instances with your models,
+switches traffic to use the new instances, and drains the old instances.
+As soon as this process begins, the status becomes \ ``Updating``. After
+the update is complete, your endpoint becomes \ ``InService``. This
+process takes approximately 10 minutes.
+
+Delete the HostingDeployment
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Use \ ``kubectl`` to delete a HostingDeployment with the following
+command: 
+
+::
+
+    kubectl delete hostingdeployments host-xgboost
+
+Your output should look like the following:
+
+::
+
+    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
+
+To verify that the hosting deployment has been deleted, use the
+following command:
+
+::
+
+    kubectl get hostingdeployments
+    No resources found.
+
+Endpoints that have been deleted do not incur any charges for
+Amazon SageMaker resources.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-03-25 11:01:57[0m
[92mHash: 60eb0bca7d3cf15a8b5919d271c33a38fadee7aa[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: Add docs for debugger job support in operator (#1367)

[0m
@@ -304,135 +304,6 @@ job stops or completes.
 continue to show on the Amazon SageMaker console. The delete command
 takes about 2 minutes to clean up the resources from Amazon SageMaker.
 
-SageMaker Debugger Jobs
-^^^^^^^^^^^^^^^^^^^^^^^
-
-When creating a SageMaker training job, you have an option to run 
-asynchronous debugger jobs for your model. It gives you full visibility 
-into a training job by using a hook to capture tensors that define 
-the state of the training process at each instance in its lifecycle. 
-It also provides the capability of defining 'rules' to
-analyze the captured tensors. See `SageMaker Debugger Introduction <https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html>`__ and `How Debugger Works <https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-how-it-works.html>`__ for details.
-
-You can get more details on debug job by using the ``describe`` kubectl verb.
-The output of describing a training job will now have a new field ``Debug Rule Evaluation Statuses:``
-
-::
-
-    kubectl describe trainingjobs xgboost-mnist-debugger
-
-    Name:         xgboost-mnist-debugger
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"TrainingJob","metadata":{"annotations":{},"name":"xgboost-mnist-debugger","namespace":...
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         TrainingJob
-    Metadata:
-      Creation Timestamp:  2020-03-18T05:58:59Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  2939388
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-debugger
-      UID:               8fe3799e-68dd-11ea-8423-1260529a8dc9
-    Spec:
-      Algorithm Specification:
-        Training Image:       246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3
-        Training Input Mode:  File
-      Debug Hook Config:
-        Collection Configurations:
-          Collection Name:  feature_importance
-          Collection Parameters:
-            Name:           save_interval
-            Value:          5
-          Collection Name:  losses
-          Collection Parameters:
-            Name:           save_interval"
-            Value:          500
-          Collection Name:  average_shap
-          Collection Parameters:
-            Name:           save_interval
-            Value:          5
-          Collection Name:  metrics
-          Collection Parameters:
-            Name:      save_interval
-            Value:     5
-        s3OutputPath:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/
-      Debug Rule Configurations:
-        Rule Configuration Name:  LossNotDecreasing
-        Rule Evaluator Image:     895741380848.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rules:latest
-        Rule Parameters:
-          Name:   collection_names
-          Value:  metrics
-          Name:   num_steps
-          Value:  10
-          Name:   rule_to_invoke
-          Value:  LossNotDecreasing
-      Hyper Parameters:
-        Name:   max_depth
-        Value:  5
-        Name:   eta
-        Value:  0.2
-        Name:   gamma
-        Value:  4
-        Name:   min_child_weight
-        Value:  6
-        Name:   silent
-        Value:  0
-        Name:   objective
-        Value:  reg:squarederror
-        Name:   subsample
-        Value:  0.7
-        Name:   num_round
-        Value:  51
-      Input Data Config:
-        Channel Name:      train
-        Compression Type:  None
-        Content Type:      libsvm
-        Data Source:
-          s3DataSource:
-            s3DataDistributionType:  FullyReplicated
-            s3DataType:              S3Prefix
-            s3Uri:                   s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/train
-        Channel Name:                validation
-        Compression Type:            None
-        Content Type:                libsvm
-        Data Source:
-          s3DataSource:
-            s3DataDistributionType:  FullyReplicated
-            s3DataType:              S3Prefix
-            s3Uri:                   s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/validation
-      Output Data Config:
-        s3OutputPath:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost-debugger/
-      Region:          us-west-2
-      Resource Config:
-        Instance Count:     1
-        Instance Type:      ml.m4.xlarge
-        Volume Size In GB:  5
-      Role Arn:             arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole
-      Stopping Condition:
-        Max Runtime In Seconds:  86400
-      Tags:
-        Key:              tagKey
-        Value:            tagValue
-      Training Job Name:  xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m
-    Status:
-      Cloud Watch Log URL:  https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m;streamFilter=typeLogStreamPrefix
-      Debug Rule Evaluation Statuses:
-        Last Modified Time:          2020-03-18T06:03:48Z
-        Rule Configuration Name:     LossNotDecreasing
-        Rule Evaluation Job Arn:     arn:aws:sagemaker:us-west-2:1234567890:processing-job/xgboost-mnist-debugger-8fe-lossnotdecreasing-a7d0eaf2
-        Rule Evaluation Status:      NoIssuesFound
-      Model Path:                    s3://my-bucket/sagemaker/xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m/output/model.tar.gz
-      Sage Maker Training Job Name:  xgboost-mnist-debugger-[93m[93m[93m[93m8fe3799e68dd11ea84231260529a8dc9[0m[0m[0m[0m
-      Secondary Status:              Completed
-      Training Job Status:           Completed
-    Events:                          <none>
-
-See `SageMaker Debugger Examples <https://github.com/aws/amazon-sagemaker-operator-for-k8s/tree/master/samples>`__ for more examples of debugger jobs.
-
-
 HyperParameterTuningJobs operator
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-03-02 16:02:10[0m
[92mHash: 60023d033d802276e6fa9e8a3d4c58b232560701[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/master[0m
[92mCommit: infra: programmatically determine partition based on region (#1316)


[0m
@@ -24,7 +24,6 @@ import tests.integ
 from sagemaker import AlgorithmEstimator, ModelPackage
 from sagemaker.tuner import IntegerParameter, HyperparameterTuner
 from sagemaker.utils import sagemaker_timestamp
-from sagemaker.utils import _aws_partition
 from tests.integ import DATA_DIR
 from tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name
 from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
@@ -40,12 +39,12 @@ from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
 # Both are written by Amazon and are free to subscribe.
 
 ALGORITHM_ARN = (
-    "arn:{partition}:sagemaker:{region}:{account}:algorithm/scikit-decision-trees-"
+    "arn:aws:sagemaker:%s:%s:algorithm/scikit-decision-trees-"
     "15423055-[93m57b73412d2e93e9239e4e16f83298b8f[0m"
 )
 
 MODEL_PACKAGE_ARN = (
-    "arn:{partition}:sagemaker:{region}:{account}:model-package/scikit-iris-detector-"
+    "arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-"
     "154230595-[93m8f00905c1f927a512b73ea29dd09ae30[0m"
 )
 
@@ -64,9 +63,7 @@ def test_marketplace_estimator(sagemaker_session, cpu_instance_type):
         data_path = os.path.join(DATA_DIR, "marketplace", "training")
         region = sagemaker_session.boto_region_name
         account = REGION_ACCOUNT_MAP[region]
-        algorithm_arn = ALGORITHM_ARN.format(
-            partition=_aws_partition(region), region=region, account=account
-        )
+        algorithm_arn = ALGORITHM_ARN % (region, account)
 
         algo = AlgorithmEstimator(
             algorithm_arn=algorithm_arn,
@@ -106,9 +103,7 @@ def test_marketplace_attach(sagemaker_session, cpu_instance_type):
         data_path = os.path.join(DATA_DIR, "marketplace", "training")
         region = sagemaker_session.boto_region_name
         account = REGION_ACCOUNT_MAP[region]
-        algorithm_arn = ALGORITHM_ARN.format(
-            partition=_aws_partition(region), region=region, account=account
-        )
+        algorithm_arn = ALGORITHM_ARN % (region, account)
 
         mktplace = AlgorithmEstimator(
             algorithm_arn=algorithm_arn,
@@ -160,9 +155,7 @@ def test_marketplace_attach(sagemaker_session, cpu_instance_type):
 def test_marketplace_model(sagemaker_session, cpu_instance_type):
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
-    model_package_arn = MODEL_PACKAGE_ARN.format(
-        partition=_aws_partition(region), region=region, account=account
-    )
+    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
 
     def predict_wrapper(endpoint, session):
         return sagemaker.RealTimePredictor(
@@ -199,9 +192,7 @@ def test_marketplace_tuning_job(sagemaker_session, cpu_instance_type):
     data_path = os.path.join(DATA_DIR, "marketplace", "training")
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
-    algorithm_arn = ALGORITHM_ARN.format(
-        partition=_aws_partition(region), region=region, account=account
-    )
+    algorithm_arn = ALGORITHM_ARN % (region, account)
 
     mktplace = AlgorithmEstimator(
         algorithm_arn=algorithm_arn,
@@ -242,9 +233,7 @@ def test_marketplace_transform_job(sagemaker_session, cpu_instance_type):
     data_path = os.path.join(DATA_DIR, "marketplace", "training")
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
-    algorithm_arn = ALGORITHM_ARN.format(
-        partition=_aws_partition(region), region=region, account=account
-    )
+    algorithm_arn = ALGORITHM_ARN % (region, account)
 
     algo = AlgorithmEstimator(
         algorithm_arn=algorithm_arn,
@@ -290,9 +279,7 @@ def test_marketplace_transform_job_from_model_package(sagemaker_session, cpu_ins
 
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
-    model_package_arn = MODEL_PACKAGE_ARN.format(
-        partition=_aws_partition(region), region=region, account=account
-    )
+    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
 
     model = ModelPackage(
         role="SageMakerRole",

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-02-11 12:41:17[0m
[92mHash: 20f435302363df6c243ba61ea184083121b82e93[0m
[92mFilepath: CONTRIBUTING.md[0m
[92mBranch: origin/master[0m
[92mCommit: doc: add documentation guidelines to CONTRIBUTING.md (#1279)

* doc: add doc guidelines to CONTRIBUTING.md

* fix headings

* remove doc-building section from main README

Co-authored-by: Aaron Markham <markhama@amazon.com>
[0m
@@ -9,25 +9,22 @@ information to effectively respond to your bug report or contribution.
 
 ## Table of Contents
 
-* [Report Bugs/Feature Requests](#report-bugsfeature-requests)
-* [Contribute via Pull Requests (PRs)](#contribute-via-pull-requests-prs)
-  * [Set up Your Development Environment *[Optional, but Recommended]*](#set-up-your-development-environment-optional-but-recommended)
-  * [Pull Down the Code](#pull-down-the-code)
-  * [Run the Unit Tests](#run-the-unit-tests)
-  * [Run the Integration Tests](#run-the-integration-tests)
-  * [Make and Test Your Change](#make-and-test-your-change)
-  * [Commit Your Change](#commit-your-change)
-  * [Send a Pull Request](#send-a-pull-request)
-* [Documentation Guidelines](#documentation-guidelines)
-  * [Overviews](#overviews)
-  * [API References (docstrings)](#api-references-docstrings)
-  * [Build and Test Documentation](#build-and-test-documentation)
-* [Find Contributions to Work On](#find-contributions-to-work-on)
+* [Table of Contents](#table-of-contents)
+* [Reporting Bugs/Feature Requests](#reporting-bugsfeature-requests)
+* [Contributing via Pull Requests (PRs)](#contributing-via-pull-requests-prs)
+  * [Setting up Your Development Environment *[Optional, but Recommended]*](#setting-up-your-development-environment-optional-but-recommended)  
+  * [Pulling Down the Code](#pulling-down-the-code)
+  * [Running the Unit Tests](#running-the-unit-tests)
+  * [Running the Integration Tests](#running-the-integration-tests)
+  * [Making and Testing Your Change](#making-and-testing-your-change)
+  * [Committing Your Change](#committing-your-change)
+  * [Sending a Pull Request](#sending-a-pull-request)
+* [Finding Contributions to Work On](#finding-contributions-to-work-on)
 * [Code of Conduct](#code-of-conduct)
 * [Security Issue Notifications](#security-issue-notifications)
 * [Licensing](#licensing)
 
-## Report Bugs/Feature Requests
+## Reporting Bugs/Feature Requests
 
 We welcome you to use the GitHub issue tracker to report bugs or suggest features.
 
@@ -40,7 +37,7 @@ reported the issue. Please try to include as much information as you can. Detail
 * A description of your environment or deployment.
 
 
-## Contribute via Pull Requests (PRs)
+## Contributing via Pull Requests (PRs)
 
 Contributions via pull requests are much appreciated.
 
@@ -51,19 +48,19 @@ Before sending us a pull request, please ensure that:
 * You open an issue to discuss any significant work - we would hate for your time to be wasted.
 
 
-### Set up Your Development Environment *[Optional, but Recommended]*
+### Setting up Your Development Environment *[Optional, but Recommended]*
 
 1. Set up the Cloud9 environment:
    1. Instance type: You'll need at least 4 GB of RAM to avoid running into memory issues. We recommend at least a t3.medium to run the unit tests. A larger host will reduce the chance of encountering resource limits.
    1. Follow the instructions at [Creating a Cloud9 EC2 Environment](https://docs.aws.amazon.com/cloud9/latest/user-guide/create-environment.html#create-environment-main) to set up a Cloud9 EC2 environment.
 1. Expand the storage of the EC2 instance from 10GB to 20GB:
-   1. Because you'll need a minimum of 11GB of disk storage on the EC2 instance to run the repository's unit tests, you'll need to expand your EC2 volume size. We recommend at least 20GB. A larger volume will reduce the chance of encountering resource limits.
+   1. Because you'll need a minimum of 11GB of disk storage on the EC2 instance to run the repository's unit tests, you'll need to expand your EC2 volume size. We recommend at least 20GB. A larger volume will reduce the chance of encountering resource limits. 
    1. Follow the instructions at [Modifying an EBS Volume Using Elastic Volumes (Console)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html#modify-ebs-volume) to increase the EBS volume size associated with the newly created EC2 instance.
    1. Wait 5-10min for the new EBS volume increase to finalize.
    1. Allow EC2 to claim the additional space by stopping and then starting your EC2 host.
 
 
-### Pull Down the Code
+### Pulling Down the Code
 
 1. If you do not already have one, create a GitHub account by following the prompts at [Join Github](https://github.com/join).
 1. Create a fork of this repository on GitHub. You should end up with a fork at `https://github.com/<username>/sagemaker-python-sdk`.
@@ -71,33 +68,33 @@ Before sending us a pull request, please ensure that:
 1. Clone your fork of the repository: `git clone https://github.com/<username>/sagemaker-python-sdk` where `<username>` is your github username.
 
 
-### Run the Unit Tests
+### Running the Unit Tests
 
 1. Install tox using `pip install tox`
 1. Install coverage using `pip install .[test]`
 1. cd into the sagemaker-python-sdk folder: `cd sagemaker-python-sdk` or `cd /environment/sagemaker-python-sdk`
 1. Run the following tox command and verify that all code checks and unit tests pass: `tox tests/unit`
 
-You can also run a single test with the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`
+You can also run a single test with the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`  
   * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
   * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/unit/test_estimator.py::test_sagemaker_model_s3_uri_invalid ; unset IGNORE_COVERAGE`
 
 
-### Run the Integration Tests
+### Running the Integration Tests
 
-Our CI system runs integration tests (the ones in the `tests/integ` directory), in parallel, for every Pull Request.
-You should only worry about manually running any new integration tests that you write, or integration tests that test an area of code that you've modified.
+Our CI system runs integration tests (the ones in the `tests/integ` directory), in parallel, for every Pull Request.  
+You should only worry about manually running any new integration tests that you write, or integration tests that test an area of code that you've modified.  
 
 1. Follow the instructions at [Set Up the AWS Command Line Interface (AWS CLI)](https://docs.aws.amazon.com/polly/latest/dg/setup-aws-cli.html).
 1. To run a test, specify the test file and method you want to run per the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`
    * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
    * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/integ/test_tf_script_mode.py::test_mnist ; unset IGNORE_COVERAGE`
 
-If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m/tests/integ/test_tfs.py#L37) and
+If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m/tests/integ/test_tfs.py#L37) and 
 [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.
 
 
-### Make and Test Your Change
+### Making and Testing Your Change
 
 1. Create a new git branch:
      ```shell
@@ -108,12 +105,11 @@ If you are writing or modifying a test that creates a SageMaker job (training, t
       1. Prove that your code works correctly.
       1. Guard against future breaking changes to lower the maintenance cost.
    1. Please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.
-1. Run all the unit tests as per [Run the Unit Tests](#run-the-unit-tests), and verify that all checks and tests pass.
-   1. Note that this also runs tools that may be necessary for the automated build to pass (ex: code reformatting by 'black').
-1. If your changes include documentation changes, please see the [Documentation Guidelines](#documentation-guidelines).
+1. Run all the unit tests as per [Running the Unit Tests](#running-the-unit-tests), and verify that all checks and tests pass.
+   1. Note that this also runs tools that may be necessary for the automated build to pass (ex: code reformatting by 'black').  
 
 
-### Commit Your Change
+### Committing Your Change
 
 We use commit messages to update the project version number and generate changelog entries, so it's important for them to follow the right format. Valid commit messages include a prefix, separated from the rest of the message by a colon and a space. Here are a few examples:
 
@@ -139,107 +135,17 @@ Some of the prefixes allow abbreviation ; `break`, `feat`, `depr`, and `doc` are
 For the rest of the message, use imperative style and keep things concise but informative. See [How to Write a Git Commit Message](https://chris.beams.io/posts/git-commit/) for guidance.
 
 
-### Send a Pull Request
+### Sending a Pull Request
 
 GitHub provides additional document on [Creating a Pull Request](https://help.github.com/articles/creating-a-pull-request/).
 
 Please remember to:
-* Use commit messages (and PR titles) that follow the guidelines under [Commit Your Change](#commit-your-change).
+* Use commit messages (and PR titles) that follow the guidelines under [Committing Your Change](#committing-your-change).
 * Send us a pull request, answering any default questions in the pull request interface.
 * Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.
 
 
-## Documentation Guidelines
-
-We use reStructuredText (RST) for most of our documentation. For a quick primer on the syntax,
-see [the Sphinx documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html).
-
-In this repository, we have two main categories of documentation: overviews and API references.
-"How to" tutorials are housed in the [Amazon SageMaker Examples repository](https://github.com/awslabs/amazon-sagemaker-examples).
-Overviews and API references are discussed in more detail below.
-
-Here are some general guidelines to follow when writing either kind of documentation:
-* Use present tense.
-  * 👍 "The estimator fits a model."
-  * 👎 "The estimator will fit a model."
-* When referring to an AWS product, use its full name in the first invocation.
-  (This applies only to prose; use what makes sense when it comes to writing code, etc.)
-  * 👍 "Amazon S3"
-  * 👎 "s3"
-* Provide links to other ReadTheDocs pages, AWS documentation, etc. when helpful.
-  Try to not duplicate documentation when you can reference it instead.
-  * Use meaningful text in a link.
-    * 👍 You can learn more about [hyperparameter tuning with SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) in the SageMaker docs.
-    * 👎 Read more about it [here](#).
-
-
-### Overviews
-
-This section refers to documentation that discusses a specific topic or feature to
-help the reader deepen their understanding, and may include short snippets of how to do specific tasks.
-Examples include "[Amazon SageMaker Debugger](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html)"
-and "[Use MXNet with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html)."
-
-The goal of these documents is to explain basic usage.
-This includes the general purpose of the topic or feature,
-and common ways to use the SageMaker Python SDK in that context.
-
-This type of documentation should not be a step-by-step tutorial.
-That is better suited for the [example notebooks](https://github.com/awslabs/amazon-sagemaker-examples).
-Instead, keep the content focused on the unique aspects of the feature.
-For example, if one is writing specifically about deploying models,
-there is no need to also include instructions on how to train a model first.
-In this case, consider linking to existing documentation about training models and any other prerequisites.
-
-Lastly, in addition to the general guidelines listed above:
-* Use the imperative mood for headings.
-  * 👍 "Prepare a Training Script"
-  * 👎 "Preparing a Training Script"
-* Don’t refer to features as "new" - they might be at the time of writing, but they won’t always be!
-
-### API References (docstrings)
-
-The API references are generated from docstrings.
-A docstring is the comment in the source code that describes a module, class, function, or variable.
-
-```python
-def foo():
-    """This comment is a docstring for the function foo."""
-```
-
-We use [Google-style docstrings](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
-There should be a docstring for every public module, class, and function.
-For functions, make sure your docstring covers all of the arguments, exceptions, and any other relevant information.
-When possible, link to classes and functions, e.g. use ":class:~\`sagemaker.session.Session\`" over just "Session."
-
-If a parameter of a function has a default value, please note what the default is.
-If that default value is `None`, it can also be helpful to explain what happens when the parameter is `None`.
-If `**kwargs` is part of the function signature, link to the parent class(es) or method(s) so that the reader knows where to find the available parameters.
-
-For an example file with docstrings, see [the `processing` module](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/processing.py).
-
-To have a class's docstrings included in the API reference, it needs to be included in one of the files in the `doc/` folder.
-For example, see the [Processing API reference](https://github.com/aws/sagemaker-python-sdk/blob/master/doc/processing.rst).
-
-
-### Build and Test Documentation
-
-To build the Sphinx docs, run the following command in the `doc/` directory:
-
-```shell
-make html
-```
-
-You can then find the generated HTML files in `doc/_build/html/`.
-
-To check both the README and API documentation for build errors, you can run the following:
-
-```shell
-tox -e twine,sphinx
-```
-
-
-## Find Contributions to Work On
+## Finding Contributions to Work On
 
 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any ['help wanted'](https://github.com/aws/sagemaker-python-sdk/labels/help%20wanted) issues is a great place to start.
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-01-28 11:08:51[0m
[92mHash: b4d1db77406858bdd679aed3ec648fd7b6e7b2bc[0m
[92mFilepath: doc/using_tf.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: update links to TF notebook examples to link to script mode examples. (#1268)

Co-authored-by: Karim Nakad <karimnakad@gmail.com>
[0m
@@ -91,7 +91,7 @@ is good practice.
 Note that SageMaker doesn't support argparse actions.
 For example, if you want to use a boolean hyperparameter, specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when you create the TensorFlow estimator.
 
-For a complete example of a TensorFlow training script, see `mnist.py <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/mnist.py>`__.
+For a complete example of a TensorFlow training script, see `mnist.py <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/mnist.py>`__.
 
 
 Adapting your local TensorFlow script
@@ -360,7 +360,7 @@ Training with MKL-DNN disabled
 SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
 
 In certain cases you might be able to get a better performance by disabling this optimization
-(for example when using small models).
+(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
 
 You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-01-27 18:28:31[0m
[92mHash: f7cd477d3afd6a507d0a49cc452a3de62db84292[0m
[92mFilepath: doc/using_mxnet.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: clean up headings, verb tenses, names, etc. in MXNet overview (#1263)

[0m
@@ -1,6 +1,6 @@
-#######################################
-Use MXNet with the SageMaker Python SDK
-#######################################
+#########################################
+Using MXNet with the SageMaker Python SDK
+#########################################
 
 With the SageMaker Python SDK, you can train and host MXNet models on Amazon SageMaker.
 
@@ -36,16 +36,16 @@ Prepare an MXNet Training Script
 
 For versions 1.3 and higher
 ---------------------------
-Your MXNet training script must be compatible with Python 2.7 or 3.6.
+Your MXNet training script must be a Python 2.7 or 3.6 compatible source file.
 
-The training script is very similar to a training script you might run outside of Amazon SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
 
 * ``SM_MODEL_DIR``: A string that represents the path where the training job writes the model artifacts to.
-  After training, artifacts in this directory are uploaded to Amazon S3 for model hosting.
+  After training, artifacts in this directory are uploaded to S3 for model hosting.
 * ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
 * ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
   For example, if you specify two input channels in the MXNet estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
-* ``SM_HPS``: A JSON dump of the hyperparameters preserving JSON types (boolean, integer, etc.)
+* ``SM_HPS``: A json dump of the hyperparameters preserving json types (boolean, integer, etc.)
 
 For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
 
@@ -80,10 +80,10 @@ For example, a training script might start with the following:
 
         # ... load from args.train and args.test, train a model, write model to args.model_dir.
 
-Because Amazon SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
-so that Amazon SageMaker does not inadvertently run your training code at the wrong point in execution.
+Because the SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
+so that SageMaker does not inadvertently run your training code at the wrong point in execution.
 
-Note that Amazon SageMaker doesn't support argparse actions.
+Note that SageMaker doesn't support argparse actions.
 If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your MXNet estimator.
 
 For more on training environment variables, please visit `SageMaker Containers <https://github.com/aws/sagemaker-containers>`_.
@@ -91,18 +91,15 @@ For more on training environment variables, please visit `SageMaker Containers <
 For versions 1.2 and lower
 --------------------------
 
-Your MXNet training script must be compatible with Python 2.7 or 3.5.
-The script must contain a function named ``train``, which Amazon SageMaker invokes to run training.
-You can include other functions as well, but it must contain a ``train`` function.
+Your MXNet training script must be a Python 2.7 or 3.6 compatible source file. The MXNet training script must contain a function ``train``, which SageMaker invokes to run training. You can include other functions as well, but it must contain a ``train`` function.
 
-When you run your script on Amazon SageMaker via the ``MXNet`` estimator, Amazon SageMaker injects information about the training environment into your training function via Python keyword arguments.
-You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
+When you run your script on SageMaker via the ``MXNet`` Estimator, SageMaker injects information about the training environment into your training function via Python keyword arguments. You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
 
 -  ``hyperparameters (dict[string,string])``: The hyperparameters passed
-   to an Amazon SageMaker TrainingJob that runs your MXNet training script. You
+   to SageMaker TrainingJob that runs your MXNet training script. You
    can use this to pass hyperparameters to your training script.
--  ``input_data_config (dict[string,dict])``: The Amazon SageMaker TrainingJob
-   InputDataConfig object, that's set when the Amazon SageMaker TrainingJob is
+-  ``input_data_config (dict[string,dict])``: The SageMaker TrainingJob
+   InputDataConfig object, that's set when the SageMaker TrainingJob is
    created. This is discussed in more detail below.
 -  ``channel_input_dirs (dict[string,string])``: A collection of
    directories containing training data. When you run training, you can
@@ -110,14 +107,14 @@ You can choose to take advantage of these by including them as keyword arguments
    Depending on your problem, some common channel ideas are: "train",
    "test", "evaluation" or "images',"labels".
 -  ``output_data_dir (str)``: A directory where your training script can
-   write data that is moved to Amazon S3 after training is complete.
+   write data that will be moved to S3 after training is complete.
 -  ``num_gpus (int)``: The number of GPU devices available on your
    training instance.
 -  ``num_cpus (int)``: The number of CPU devices available on your training instance.
 -  ``hosts (list[str])``: The list of host names running in the
-   Amazon SageMaker Training Job cluster.
+   SageMaker Training Job cluster.
 -  ``current_host (str)``: The name of the host executing the script.
-   When you use Amazon SageMaker for MXNet training, the script is run on each
+   When you use SageMaker for MXNet training, the script is run on each
    host in the cluster.
 
 A training script that takes advantage of all arguments would have the following definition:
@@ -125,40 +122,34 @@ A training script that takes advantage of all arguments would have the following
 .. code:: python
 
     def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,
-              num_gpus, num_cpus, hosts, current_host)
+              num_gpus, num_cpus, hosts, current_host):
+        pass
 
-You don't have to use all the arguments.
-Arguments you don't care about can be ignored by including ``**kwargs``.
+You don't have to use all the arguments, arguments you don't care about can be ignored by including ``**kwargs``.
 
 .. code:: python
 
-    # Only work with hyperparameters and num_gpus, and ignore all other hyperparameters
-    def train(hyperparameters, num_gpus, **kwargs)
+    # Only work with hyperparameters and num_gpus, ignore all other hyperparameters
+    def train(hyperparameters, num_gpus, **kwargs):
+        pass
 
-.. note::
-    **Writing a training script that imports correctly:**
-    When Amazon SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module.
-    Consequently, you should not include any statements that won't execute successfully in Amazon SageMaker when your module is imported.
-    For example, don't attempt to open any local files in top-level statements in your training script.
+**Note: Writing a training script that imports correctly:**
+When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
 
-If you want to run your training script locally by using the Python interpreter, use a ``___name__ == '__main__'`` guard.
+If you want to run your training script locally by using the Python interpreter,  use a ``___name__ == '__main__'`` guard.
 For more information, see https://stackoverflow.com/questions/419163/what-does-if-name-main-do.
 
 Save the Model
-^^^^^^^^^^^^^^
+--------------
 
-Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script.
-If your script includes a ``save`` function, Amazon SageMaker invokes it with the return value of ``train``.
-Model saving is a two-step process.
-First, return the model you want to save from ``train``.
-Then, define your model-serialization logic in ``save``.
+Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script. If your script includes a ``save`` function, SageMaker will invoke it with the return-value of ``train``. Model saving is a two-step process, firstly you return the model you want to save from
+``train``, then you define your model-serialization logic in ``save``.
 
-Amazon SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects.
-If your training script does not define a ``save`` function, then the default ``save`` function is invoked on the return value of your ``train`` function.
+SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects. If your training script does not define a ``save`` function, then the default ``save`` function will be invoked on the return-value of your ``train`` function.
 
 The default serialization system generates three files:
 
--  ``model-shapes.json``: A JSON list, containing a serialization of the
+-  ``model-shapes.json``: A json list, containing a serialization of the
    ``Module`` ``data_shapes`` property. Each object in the list contains
    the serialization of one ``DataShape`` in the returned ``Module``.
    Each object has a ``name`` property, containing the ``DataShape``
@@ -175,7 +166,7 @@ The default serialization system generates three files:
 -  ``model-symbol.json``: The MXNet ``Module`` ``Symbol`` serialization,
    produced by invoking ``save`` on the ``symbol`` property of the
    ``Module`` being saved.
--  ``modle.params``: The MXNet ``Module`` parameters, produced by
+-  ``modle.params``: The MXNet ``Module`` parameters. Produced by
    invoking ``save_params`` on the ``Module`` being saved.
 
 You can provide your own save function. This is useful if you are not working with the ``Module`` API or you need special processing.
@@ -184,23 +175,25 @@ To provide your own save function, define a ``save`` function in your training s
 
 .. code:: python
 
-    def save(model, model_dir)
+    def save(model, model_dir):
+        pass
 
 The function should take two arguments:
 
--  ``model``: This is the object that is returned from your ``train`` function.
-   You may return an object of any type from ``train``;
-   you do not have to return ``Module`` or ``Gluon`` API specific objects.
-   If your ``train`` function does not return an object, ``model`` is set to ``None``.
--  ``model_dir``: This is the string path on the Amazon SageMaker training host where you save your model.
-   Files created in this directory are accessible in Amazon S3 after your Amazon SageMaker Training Job completes.
+-  ``model``: This is the object that was returned from your ``train``
+   function. If your ``train`` function does not return an object, it
+   will be ``None``. You are free to return an object of any type from
+   ``train``, you do not have to return ``Module`` or ``Gluon`` API
+   specific objects.
+-  ``model_dir``: This is the string path on the SageMaker training host
+   where you save your model. Files created in this directory will be
+   accessible in S3 after your SageMaker Training Job completes.
 
-After your ``train`` function completes, Amazon SageMaker invokes ``save`` with the object returned from ``train``.
+After your ``train`` function completes, SageMaker will invoke ``save`` with the object returned from ``train``.
 
-.. note::
-    **How to save Gluon models with Amazon SageMaker:**
-    If your train function returns a Gluon API ``net`` object as its model, you need to write your own ``save`` function and serialize the ``net`` parameters.
-    Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
+**Note: How to save Gluon models with SageMaker**
+
+If your train function returns a Gluon API ``net`` object as its model, you'll need to write your own ``save`` function. You will want to serialize the ``net`` parameters. Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
 
 Save a Checkpoint
 -----------------
@@ -233,12 +226,12 @@ To save MXNet model checkpoints, do the following in your training script:
 For a complete example of an MXNet training script that impelements checkpointing, see https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_cifar10/cifar10.py.
 
 
-Update your MXNet training script
----------------------------------
+Updating your MXNet training script
+-----------------------------------
 
 The structure for training scripts changed with MXNet version 1.3.
 The ``train`` function is no longer be required; instead the training script must be able to be run as a standalone script.
-In this way, the training script is similar to a training script you might run outside of Amazon SageMaker.
+In this way, the training script is similar to a training script you might run outside of SageMaker.
 
 There are a few steps needed to make a training script with the old format compatible with the new format.
 
@@ -249,12 +242,12 @@ The code executed from your main guard needs to:
 2. Initiate training
 3. Save the model
 
-Hyperparameters are passed as command-line arguments to your training script.
-In addition, the container defines the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
+Hyperparameters will be passed as command-line arguments to your training script.
+In addition, the container will define the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
 You can find the full list of available environment variables in the `SageMaker Containers README <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
 
 We recommend using `an argument parser <https://docs.python.org/3.5/howto/argparse.html>`__ for this part.
-Using the ``argparse`` library as an example, the code looks something like this:
+Using the ``argparse`` library as an example, the code would look something like this:
 
 .. code:: python
 
@@ -287,8 +280,8 @@ This can be as simple as just calling the ``train`` and ``save`` methods used in
         model = train(args.batch_size, args.epochs, args.learning_rate, args.train, args.test)
         save(args.model_dir, model)
 
-Note that saving the model is no longer be done by default; this must be done by the training script.
-If you were previously relying on the default save method, you can import one from the container:
+Note that saving the model will no longer be done by default; this must be done by the training script.
+If you were previously relying on the default save method, you can now import one from the container:
 
 .. code:: python
 
@@ -299,7 +292,7 @@ If you were previously relying on the default save method, you can import one fr
 
         save(args.model_dir, model)
 
-Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must set ``distributions`` to the following dictionary when creating an MXNet estimator:
+Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must now set ``distributions`` to the following dictionary when creating an MXNet estimator:
 
 .. code:: python
 
@@ -310,10 +303,10 @@ Lastly, if you were relying on the container launching a parameter server for us
                       distributions={'parameter_server': {'enabled': True}})
 
 
-Use third-party libraries
--------------------------
+Using third-party libraries
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-When running your training script on Amazon SageMaker, it has access to some pre-installed third-party libraries, including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``.
+When running your training script on SageMaker, it will have access to some pre-installed third-party libraries including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``.
 For more information on the runtime environment, including specific package versions, see `SageMaker MXNet Containers <#sagemaker-mxnet-containers>`__.
 
 If there are other packages you want to use with your script, you can include a ``requirements.txt`` file in the same directory as your training script to install other dependencies at runtime.
@@ -323,7 +316,7 @@ You must specify this folder in ``source_dir`` argument when creating an MXNet e
 The function of installing packages using ``requirements.txt`` is supported for all MXNet versions during training.
 When serving an MXNet model, support for this function varies with MXNet versions.
 For MXNet 1.6.0 or newer, ``requirements.txt`` must be under folder ``code``.
-The SageMaker MXNet Estimator automatically saves ``code`` in ``model.tar.gz`` after training (assuming you set up your script and ``requirements.txt`` correctly as stipulated in the previous paragraph).
+The SageMaker MXNet Estimator will automatically save ``code`` in ``model.tar.gz`` after training (assuming you set up your script and ``requirements.txt`` correctly as stipulated in the previous paragraph).
 In the case of bringing your own trained model for deployment, you must save ``requirements.txt`` under folder ``code`` in ``model.tar.gz`` yourself or specify it through ``dependencies``.
 For MXNet 1.4.1, ``requirements.txt`` is not supported for inference.
 For MXNet 0.12.1-1.3.0, ``requirements.txt`` must be in ``source_dir``.
@@ -335,8 +328,8 @@ For information about the format of a ``requirements.txt`` file, see `Requiremen
 Create an Estimator
 ===================
 
-You run MXNet training scripts on Amazon SageMaker by creating an ``MXNet`` estimator.
-When you call ``fit`` on an ``MXNet`` estimator, Amazon SageMaker starts a training job using your script as training code.
+You run MXNet training scripts on SageMaker by creating an ``MXNet`` estimator.
+When you call ``fit`` on an ``MXNet`` estimator, SageMaker starts a training job using your script as training code.
 The following code sample shows how you train a custom MXNet script "train.py".
 
 .. code:: python
@@ -350,8 +343,41 @@ The following code sample shows how you train a custom MXNet script "train.py".
                                              'learning-rate': 0.1})
     mxnet_estimator.fit('s3://my_bucket/my_training_data/')
 
-For more information about the sagemaker.mxnet.MXNet estimator, see `SageMaker MXNet Classes`_.
+For more information about the sagemaker.mxnet.MXNet estimator, see `sagemaker.mxnet.MXNet Class`_.
+
+
+
+Call the fit Method
+===================
 
+You start your training script by calling ``fit`` on an ``MXNet`` Estimator. ``fit`` takes both required and optional arguments.
+
+fit Required argument
+---------------------
+
+-  ``inputs``: This can take one of the following forms: A string
+   S3 URI, for example ``s3://my-bucket/my-training-data``. In this
+   case, the S3 objects rooted at the ``my-training-data`` prefix will
+   be available in the default ``training`` channel. A dict from
+   string channel names to S3 URIs. In this case, the objects rooted at
+   each S3 prefix will available as files in each channel directory.
+
+For example:
+
+.. code:: python
+
+    {'train':'s3://my-bucket/my-training-data',
+     'eval':'s3://my-bucket/my-evaluation-data'}
+
+.. optional-arguments-1:
+
+fit Optional arguments
+----------------------
+
+-  ``wait``: Defaults to True, whether to block and wait for the
+   training script to complete before returning.
+-  ``logs``: Defaults to True, whether to show logs produced by training
+   job in the Python session. Only meaningful when wait is True.
 
 Distributed training
 ====================
@@ -363,29 +389,23 @@ If you want to use parameter servers for distributed training, set the following
     distributions={'parameter_server': {'enabled': True}}
 
 Then, when writing a distributed training script, use an MXNet kvstore to store and share model parameters.
-During training, Amazon SageMaker automatically starts an MXNet kvstore server and scheduler processes on hosts in your training job cluster.
+During training, SageMaker automatically starts an MXNet kvstore server and scheduler processes on hosts in your training job cluster.
 Your script runs as an MXNet worker task, with one server process on each host in your cluster.
 One host is selected arbitrarily to run the scheduler process.
 
 To learn more about writing distributed MXNet programs, please see `Distributed Training <https://mxnet.incubator.apache.org/versions/master/faq/distributed_training.html>`__ in the MXNet docs.
 
-
-Call the fit Method
-===================
-
-Start your training script by calling ``fit`` on an ``MXNet`` Estimator.
-``fit`` takes both required and optional arguments.
-For what arguments can be passed into ``fit``, see the `API reference <https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework>`_.
-
 *******************
 Deploy MXNet models
 *******************
 
-Once you have a trained MXNet model, you can host it in Amazon SageMaker by creating an Amazon SageMaker Endpoint.
-The endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script.
-This model can be one you trained in Amazon SageMaker or a pretrained one from somewhere else.
+After an MXNet Estimator has been fit, you can host the newly created model in SageMaker.
+
+After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to create a SageMaker Endpoint. The Endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script, which was run when you called ``fit``. This was the model object you returned from ``train`` and saved with either a custom save function or the default save function.
 
-If you use the ``MXNet`` estimator to train the model, you can call ``deploy`` to create an Amazon SageMaker Endpoint:
+``deploy`` returns a ``Predictor`` object, which you can use to do inference on the Endpoint hosting your MXNet model. Each ``Predictor`` provides a ``predict`` method which can do inference with numpy arrays or Python lists. Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
+
+``predict`` returns the result of inference against your model. By default, the inference result is either a Python list or dictionary.
 
 .. code:: python
 
@@ -393,35 +413,16 @@ If you use the ``MXNet`` estimator to train the model, you can call ``deploy`` t
     mxnet_estimator = MXNet('train.py',
                             train_instance_type='ml.p2.xlarge',
                             train_instance_count=1,
-                            py_version='py3',
-                            framework_version='1.6.0')
+                            framework_version='1.2.1')
     mxnet_estimator.fit('s3://my_bucket/my_training_data/')
 
-    # Deploy my estimator to an Amazon SageMaker Endpoint and get a Predictor
+    # Deploy my estimator to a SageMaker Endpoint and get a Predictor
     predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
                                        initial_instance_count=1)
 
-If using a pretrained model, create an ``MXNetModel`` object, and then call ``deploy`` to create the Amazon SageMaker Endpoint:
-
-.. code:: python
-
-    mxnet_model = MXNetModel(model_data='s3://my_bucket/pretrained_model/model.tar.gz',
-                             role=role,
-                             entry_point='inference.py',
-                             py_version='py3',
-                             framework_version='1.6.0')
-    predictor = mxnet_model.deploy(instance_type='ml.m4.xlarge',
-                                   initial_instance_count=1)
-
-In both cases, ``deploy`` returns a ``Predictor`` object, which you can use to do inference on the endpoint hosting your MXNet model.
+You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
 
-Each ``Predictor`` provides a ``predict`` method, which can do inference with numpy arrays or Python lists.
-Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
-``predict`` returns the result of inference against your model.
-By default, the inference result is either a Python list or dictionary.
-
-MXNet on Amazon SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.
-In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
 
 .. code:: python
 
@@ -432,87 +433,71 @@ In order to attach an Elastic Inference accelerator to your endpoint provide the
 The SageMaker MXNet Model Server
 ================================
 
-The MXNet endpoint you create with ``deploy`` runs a SageMaker MXNet model server.
-The model server loads the model provided and performs inference on the model in response to SageMaker ``InvokeEndpoint`` API calls.
+The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model server. The model server loads the model that was saved by your training script and performs inference on the model in response to SageMaker InvokeEndpoint API calls.
+
+You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
 
-You can configure two components of the model server: model loading and model serving.
-Model loading is the process of deserializing your saved model back into an MXNet model.
-Serving is the process of translating ``InvokeEndpoint`` requests to inference calls on the loaded model.
-These are configured by defining functions in the Python source file you pass to the ``MXNet`` or ``MXNetModel`` constructor.
+As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
 
 Load a Model
 ------------
 
-Before a model can be served, it must be loaded.
-The model server loads your model by invoking the ``model_fn`` function in your inference script.
-If you don't provide a ``model_fn`` function, the model server uses a default ``model_fn`` function.
-The default function works with MXNet Module model objects saved via the default ``save`` function.
+Before a model can be served, it must be loaded. The SageMaker model server loads your model by invoking a ``model_fn`` function on your training script. If you don't provide a ``model_fn`` function, SageMaker will use a default ``model_fn`` function. The default function works with MXNet Module model objects, saved via the default ``save`` function.
 
-If you wrote your own save logic, then you may need to write a custom ``model_fn`` function.
-The ``model_fn`` function must have the following signature:
+If you wrote a custom ``save`` function then you may need to write a custom ``model_fn`` function. If your save function serializes ``Module`` objects under the same format as the default ``save`` function, then you won't need to write a custom model_fn function. If you do write a ``model_fn`` function must have the following signature:
 
 .. code:: python
 
     def model_fn(model_dir)
 
-Amazon SageMaker injects the directory where your model files and sub-directories have been mounted.
-Your model function should return a model object that can be used for model serving.
+SageMaker will inject the directory where your model files and sub-directories, saved by ``save``, have been mounted. Your model function should return a model object that can be used for model serving. SageMaker provides automated serving functions that work with Gluon API ``net`` objects and Module API ``Module`` objects. If you return either of these types of objects, then you will be able to use the default serving request handling functions.
 
-The following code snippet shows an example custom ``model_fn`` implementation.
-This returns an MXNet Gluon net model for resnet-34 inference.
-It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
+The following code-snippet shows an example custom ``model_fn`` implementation. This loads returns an MXNet Gluon net model for resnet-34 inference. It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
 
 .. code:: python
 
     def model_fn(model_dir):
-        """Load the Gluon model. Called when the hosting service starts.
-
-        Args:
-            model_dir (str): The directory where model files are stored.
-
-        Returns:
-            mxnet.gluon.nn.Block: a Gluon network (for this example)
+        """
+        Load the gluon model. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
         """
         net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
         net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
         return net
 
-MXNet on Amazon SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.
-In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
 
 Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
 
 .. code:: python
 
     def model_fn(model_dir):
-        """Load the Gluon model. Called when the hosting service starts.
-
-        Args:
-            model_dir (str): The directory where model files are stored.
-
-        Returns:
-            mxnet.gluon.nn.Block: a Gluon network (for this example)
+        """
+        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
         """
         net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
         net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
         return net
 
-The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ loads and serve your model through Elastic Inference, if applicable, within the Amazon SageMaker MXNet containers.
+The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
 
 For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
 
 Serve an MXNet Model
 --------------------
 
-After the MXNet model server loads your model by calling either the default ``model_fn`` or the implementation in your script, it serves your model.
+After the SageMaker model server loads your model by calling either the default ``model_fn`` or the implementation in your script, SageMaker serves your model.
 Model serving is the process of responding to inference requests received by SageMaker ``InvokeEndpoint`` API calls.
 Defining how to handle these requests can be done in one of two ways:
 
 - using ``input_fn``, ``predict_fn``, and ``output_fn``, some of which may be your own implementations
 - writing your own ``transform_fn`` for handling input processing, prediction, and output processing
 
-Use ``input_fn``, ``predict_fn``, and ``output_fn``
----------------------------------------------------
+Using ``input_fn``, ``predict_fn``, and ``output_fn``
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 The SageMaker MXNet model server breaks request handling into three steps:
 
@@ -523,7 +508,7 @@ The SageMaker MXNet model server breaks request handling into three steps:
 Just like with ``model_fn``, you configure these steps by defining functions in your Python source file.
 
 Each step has its own Python function, which takes in information about the request and the return value from the previous function in the chain.
-Inside the MXNet model server, the process looks like:
+Inside the SageMaker MXNet model server, the process looks like:
 
 .. code:: python
 
@@ -545,7 +530,7 @@ The above code sample shows the three function definitions that correlate to the
 -  ``output_fn``: Takes the result of prediction and serializes this
    according to the response content type.
 
-The MXNet model server provides default implementations of these functions.
+The SageMaker MXNet model server provides default implementations of these functions.
 These work with both Gluon API and Module API model objects.
 The following content types are supported:
 
@@ -553,27 +538,26 @@ The following content types are supported:
 - Module API: 'application/json', 'application/x-npy', 'text-csv'
 
 You can also provide your own implementations for these functions in your training script.
-If you omit any definition, the MXNet model server uses its default implementation for that function.
+If you omit any definition then the SageMaker MXNet model server will use its default implementation for that function.
 
 If you rely solely on the SageMaker MXNet model server defaults, you get the following functionality:
 
--  Prediction on MXNet Gluon API ``net`` and Module API ``Module`` objects.
+-  Prediction on MXNet Gluon API ``net`` and Module API ``Module``
+   objects.
 -  Deserialization from CSV and JSON to NDArrayIters.
 -  Serialization of NDArrayIters to CSV or JSON.
 
-In the following sections, we describe the default implementations of ``input_fn``, ``predict_fn``, and ``output_fn``.
-We describe the input arguments and expected return types of each, so you can define your own implementations.
+In the following sections we describe the default implementations of input_fn, predict_fn, and output_fn. We describe the input arguments and expected return types of each, so you can define your own implementations.
 
 Process Model Input
-^^^^^^^^^^^^^^^^^^^
+-------------------
 
-When an ``InvokeEndpoint`` operation is made against an endpoint running an MXNet model server, the model server receives two pieces of information:
+When an InvokeEndpoint operation is made against an Endpoint running a SageMaker MXNet model server, the model server receives two pieces of information:
 
--  The request's content type, e.g. 'application/json'
+-  The request's content type, for example "application/json"
 -  The request data body as a byte array
 
-The MXNet model server invokes ``input_fn``, passing in this information.
-If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
+The SageMaker MXNet model server will invoke ``input_fn``, passing in this information. If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
 
 .. code:: python
 
@@ -581,9 +565,9 @@ If you define an ``input_fn`` function definition, it should return an object th
 
 Where ``request_body`` is a byte buffer and ``request_content_type`` is the content type of the request.
 
-The MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
+The SageMaker MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
 
-Default JSON deserialization requires ``request_body`` contain a single JSON list. Sending multiple JSON objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
+Default JSON deserialization requires ``request_body`` contain a single json list. Sending multiple json objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
 
 Default CSV deserialization requires ``request_body`` contain one or more lines of CSV numerical data. The data is loaded into a two-dimensional array, where each line break defines the boundaries of the first dimension. This two-dimensional array is then re-shaped to be compatible with the shape expected by the model object. Specifically, the first dimension is kept unchanged, but the second dimension is reshaped to be consistent with the shape of all dimensions in the model, following the first dimension.
 
@@ -606,11 +590,10 @@ If you provide your own implementation of input_fn, you should abide by the ``in
             # if the content type is not supported.
             pass
 
-Predict from a Deployed Model
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Getting Predictions from a Deployed Model
+=========================================
 
-After the inference request has been deserialized by ``input_fn``, the MXNet model server invokes ``predict_fn``.
-As with the other functions, you can define your own ``predict_fn`` or use the model server's default.
+After the inference request has been deserialized by ``input_fn``, the SageMaker MXNet model server invokes ``predict_fn``. As with ``input_fn``, you can define your own ``predict_fn`` or use the SageMaker Mxnet default.
 
 The ``predict_fn`` function has the following signature:
 
@@ -631,18 +614,19 @@ The default implementation performs inference with the input
 If you implement your own prediction function, you should take care to ensure that:
 
 -  The first argument is expected to be the return value from input_fn.
-   If you use the default input_fn, this is an ``NDArrayIter``.
+   If you use the default input_fn, this will be an ``NDArrayIter``.
 -  The second argument is the loaded model. If you use the default
-   ``model_fn`` implementation, this is an MXNet Module object.
-   Otherwise, it is the return value of your ``model_fn`` implementation.
+   ``model_fn`` implementation, this will be an MXNet Module object.
+   Otherwise, it will be the return value of your ``model_fn``
+   implementation.
 -  The return value should be of the correct type to be passed as the
    first argument to ``output_fn``. If you use the default
    ``output_fn``, this should be an ``NDArrayIter``.
 
-Process Model Output
-^^^^^^^^^^^^^^^^^^^^
+Processing Model Output
+-----------------------
 
-After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return value from ``predict_fn`` and the ``InvokeEndpoint`` requested response content type.
+After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return value from ``predict_fn`` and the InvokeEndpoint requested response content type.
 
 The ``output_fn`` has the following signature:
 
@@ -655,11 +639,10 @@ The function should return an array of bytes serialized to the expected content
 
 The default implementation expects ``prediction`` to be an ``NDArray`` and can serialize the result to either JSON or CSV. It accepts response content types of "application/json" and "text/csv".
 
-Use ``transform_fn``
---------------------
+Using ``transform_fn``
+^^^^^^^^^^^^^^^^^^^^^^
 
-If you would rather not structure your code around the three methods described above, you can instead define your own ``transform_fn`` to handle inference requests.
-An error is thrown if a ``transform_fn`` is present in conjunction with any ``input_fn``, ``predict_fn``, and/or ``output_fn``.
+If you would rather not structure your code around the three methods described above, you can instead define your own ``transform_fn`` to handle inference requests. An error will be thrown if a ``transform_fn`` is present in conjunction with any ``input_fn``, ``predict_fn``, and/or ``output_fn``.
 ``transform_fn`` has the following signature:
 
 .. code:: python
@@ -674,21 +657,20 @@ The return object should be one of the following:
 For versions 1.4 and higher:
 
 - a tuple with two items: the response data and ``accept_type`` (the content type of the response data), or
-- the response data: (the content type of the response is set to either the accept header in the initial request or default to "application/json")
+- the response data: (the content type of the response will be set to either the accept header in the initial request or default to "application/json")
 
 For versions 1.3 and lower:
 
 - a tuple with two items: the response data and ``accept_type`` (the content type of the response data), or
 - a Flask response object: http://flask.pocoo.org/docs/1.0/api/#response-objects
 
-For an example inference script using this structure, see the `mxnet_gluon_sentiment <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_sentiment/sentiment.py#L344-L387>`__ notebook.
+You can find examples of hosting scripts using this structure in the example notebooks, such as the `mxnet_gluon_sentiment <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_sentiment/sentiment.py#L344-L387>`__ notebook.
 
-***********************************************
-Work with Existing Model Data and Training Jobs
-***********************************************
+Working with existing model data and training jobs
+==================================================
 
 Attach to Existing Training Jobs
-================================
+--------------------------------
 
 You can attach an MXNet Estimator to an existing training job using the
 ``attach`` method.
@@ -698,15 +680,21 @@ You can attach an MXNet Estimator to an existing training job using the
     my_training_job_name = 'MyAwesomeMXNetTrainingJob'
     mxnet_estimator = MXNet.attach(my_training_job_name)
 
-After attaching, if the training job's status is "Complete", it can be ``deploy``\ ed to create an Amazon SageMaker Endpoint and return a ``Predictor``.
-If the training job is in progress, ``attach`` blocks and displays log messages from the training job until the training job completes.
+After attaching, if the training job is in a Complete status, it can be
+``deploy``\ ed to create a SageMaker Endpoint and return a
+``Predictor``. If the training job is in progress, attach will block and display log messages from the training job, until the training job completes.
 
-For information about arguments that ``attach`` accepts, see `the function's API reference <https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework.attach>`_.
+The ``attach`` method accepts the following arguments:
+
+-  ``training_job_name (str):`` The name of the training job to attach
+   to.
+-  ``sagemaker_session (sagemaker.Session or None):`` The Session used
+   to interact with SageMaker
 
 Deploy Endpoints from Model Data
-================================
+--------------------------------
 
-As well as attaching to existing training jobs, you can deploy models directly from model data in Amazon S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
+As well as attaching to existing training jobs, you can deploy models directly from model data in S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
 
 .. code:: python
 
@@ -714,20 +702,46 @@ As well as attaching to existing training jobs, you can deploy models directly f
 
     predictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
 
-For information about arguments that the ``MXNetModel`` constructor accepts, see `the class's API reference <https://sagemaker.readthedocs.io/en/stable/sagemaker.mxnet.html#sagemaker.mxnet.model.MXNetModel>`_.
-
-Your model data must be a .tar.gz file in Amazon S3. Amazon SageMaker Training Job model data is saved to .tar.gz files in Amazon S3, however if you have local data you want to deploy, you can prepare the data yourself.
-
-Assuming you have a local directory containing your model data named "my_model" you can tar and gzip compress the file and upload to Amazon S3 using the following commands:
+The MXNetModel constructor takes the following arguments:
+
+-  ``model_data (str):`` An S3 location of a SageMaker model data
+   .tar.gz file
+-  ``image (str):`` A Docker image URI
+-  ``role (str):`` An IAM role name or Arn for SageMaker to access AWS
+   resources on your behalf.
+-  ``predictor_cls (callable[string,sagemaker.Session]):`` A function to
+   call to create a predictor. If not None, ``deploy`` will return the
+   result of invoking this function on the created endpoint name
+-  ``env (dict[string,string]):`` Environment variables to run with
+   ``image`` when hosted in SageMaker.
+-  ``name (str):`` The model name. If None, a default model name will be
+   selected on each ``deploy.``
+-  ``entry_point (str):`` Path (absolute or relative) to the Python file
+   which should be executed as the entry point to model hosting.
+-  ``source_dir (str):`` Optional. Path (absolute or relative) to a
+   directory with any other training source code dependencies including
+   the entry point file. Structure within this directory will be
+   preserved when training on SageMaker.
+-  ``container_log_level (int):`` Log level to use within the container.
+   Valid values are defined in the Python logging module.
+-  ``code_location (str):`` Optional. Name of the S3 bucket where your
+   custom code will be uploaded to. If not specified, will use the
+   SageMaker default bucket created by sagemaker.Session.
+-  ``sagemaker_session (sagemaker.Session):`` The SageMaker Session
+   object, used for SageMaker interaction
+
+Your model data must be a .tar.gz file in S3. SageMaker Training Job model data is saved to .tar.gz files in S3, however if you have local data you want to deploy, you can prepare the data yourself.
+
+Assuming you have a local directory containg your model data named "my_model" you can tar and gzip compress the file and upload to S3 using the following commands:
 
 ::
 
     tar -czf model.tar.gz my_model
     aws s3 cp model.tar.gz s3://my-bucket/my-path/model.tar.gz
 
-This uploads the contents of my_model to a gzip-compressed tar file to Amazon S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
+This uploads the contents of my_model to a gzip compressed tar file to S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
 
-To run this command, you need the AWS CLI tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
+To run this command, you'll need the aws cli tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
 
 ********
 Examples
@@ -737,13 +751,100 @@ Amazon provides several example Jupyter notebooks that demonstrate end-to-end tr
 
 https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk
 
-These are also available in Amazon SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
-
-***********************
-SageMaker MXNet Classes
-***********************
+These are also available in SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
+
+***************************
+sagemaker.mxnet.MXNet Class
+***************************
+
+The following are the most commonly used ``MXNet`` constructor arguments.
+
+Required arguments
+==================
+
+The following are required arguments to the ``MXNet`` constructor. When you create an MXNet object, you must include these in the constructor, either positionally or as keyword arguments.
+
+-  ``entry_point`` Path (absolute or relative) to the Python file which
+   should be executed as the entry point to training.
+-  ``role`` An AWS IAM role (either name or full ARN). The Amazon
+   SageMaker training jobs and APIs that create Amazon SageMaker
+   endpoints use this role to access training data and model artifacts.
+   After the endpoint is created, the inference code might use the IAM
+   role, if accessing AWS resource.
+-  ``train_instance_count`` Number of Amazon EC2 instances to use for
+   training.
+-  ``train_instance_type`` Type of EC2 instance to use for training, for
+   example, 'ml.c4.xlarge'.
+
+Optional arguments
+==================
+
+The following are optional arguments. When you create an ``MXNet`` object, you can specify these as keyword arguments.
+
+-  ``source_dir`` Path (absolute or relative) to a directory with any
+   other training source code dependencies including the entry point
+   file. Structure within this directory will be preserved when training
+   on SageMaker.
+-  ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+   any additional libraries that will be exported to the container (default: ``[]``).
+   The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+   If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+   instead. For example, the following call
+
+   >>> MXNet(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+   results in the following inside the container:
+
+   .. code::
+
+       opt/ml/code
+         ├── train.py
+         ├── common
+         └── virtual-env
+
+-  ``hyperparameters`` Hyperparameters that will be used for training.
+   Will be made accessible as a dict[str, str] to the training code on
+   SageMaker. For convenience, accepts other types besides str, but
+   str() will be called on keys and values to convert them before
+   training.
+-  ``py_version`` Python version you want to use for executing your
+   model training code. Valid values: 'py2' and 'py3'.
+-  ``train_volume_size`` Size in GB of the EBS volume to use for storing
+   input data during training. Must be large enough to store training
+   data if input_mode='File' is used (which is the default).
+-  ``train_max_run`` Timeout in seconds for training, after which Amazon
+   SageMaker terminates the job regardless of its current status.
+-  ``input_mode`` The input mode that the algorithm supports. Valid
+   modes: 'File' - Amazon SageMaker copies the training dataset from the
+   S3 location to a directory in the Docker container. 'Pipe' - Amazon
+   SageMaker streams data directly from S3 to the container via a Unix
+   named pipe.
+-  ``output_path`` Location where you want the training result (model artifacts and optional output files) saved.
+   This should be an S3 location unless you're using Local Mode, which also supports local output paths.
+   If not specified, results are stored to a default S3 bucket.
+-  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+   output with.
+-  ``job_name`` Name to assign for the training job that the fit()
+   method launches. If not specified, the estimator generates a default
+   job name, based on the training image name and current timestamp
+-  ``image_name`` An alternative docker image to use for training and
+   serving.  If specified, the estimator will use this image for training and
+   hosting, instead of selecting the appropriate SageMaker official image based on
+   framework_version and py_version. Refer to: `SageMaker MXNet Docker Containers
+   <#sagemaker-mxnet-docker-containers>`_ for details on what the Official images support
+   and where to find the source code to build your custom image.
+-  ``distributions`` For versions 1.3 and above only.
+   Specifies information for how to run distributed training.
+   To launch a parameter server during training, set this argument to:
+
+.. code::
+
+    {
+      'parameter_server': {
+        'enabled': True
+      }
+    }
 
-For information about the different MXNet-related classes in the SageMaker Python SDK, see https://sagemaker.readthedocs.io/en/stable/sagemaker.mxnet.html.
 
 **************************
 SageMaker MXNet Containers

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2020-01-15 15:43:18[0m
[92mHash: 82849f19b19985afb34ffa87e9fbcacfaa98e206[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: change: remove remaining instances of python-dateutil pin (#1235)

[0m
@@ -97,6 +97,8 @@ changedir = doc
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
 install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
+# https://github.com/boto/botocore/issues/1872
 deps =
     Pygments==2.2.0
     setuptools<40
@@ -111,7 +113,9 @@ deps =
 # pip install requirements.txt is separate as RTD does it in separate steps
 # having the requirements.txt installed in deps above results in Double Requirement exception
 # https://github.com/pypa/pip/issues/988
+# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
+    pip install python-dateutil==2.8.0
     pip install --exists-action=w -r requirements.txt
     sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-03 21:53:41[0m
[92mHash: f1bed5e1070d25b1f0d5cffcaee88b5cef540bdc[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: Import smdebug_rulesconfig from PyPI (#303)

- Remove `smdebug_rulesconfig` module from the `src/sagemaker` directory.
- Remove `pip install` from wheel in `tox.ini`.
- Install `smdebug-rulesconfig` in `setup.py`, pinned to version 0.1.2.
[0m
@@ -59,8 +59,10 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
+# TODO-reinvent-2019: Remove pip install tests/data/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl once package is in PyPI
 commands =
     pip install python-dateutil==2.8.0
+    pip install src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     coverage run --source sagemaker -m pytest {posargs}
     {env:IGNORE_COVERAGE:} coverage report --fail-under=84 --omit */tensorflow/tensorflow_serving/*
 extras = test
@@ -102,6 +104,7 @@ install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m/readthedocs/doc_builder/python_environments.py#L280
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
+# TODO-reinvent-2019: Remove pip install ../tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
 deps =
     Pygments==2.2.0
     setuptools<40
@@ -119,6 +122,7 @@ deps =
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
     pip install python-dateutil==2.8.0
+    pip install ../src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     pip install --exists-action=w -r requirements.txt
     sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-03 21:53:41[0m
[92mHash: 129460dc0fa7386395efb0d52fc8d8d2d391ebd4[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: fix: consume smdebug_ruleconfig .whl for ITs (#295)

[0m
@@ -59,10 +59,8 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
-# TODO-reinvent-2019: Remove pip install tests/data/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl once package is in PyPI
 commands =
     pip install python-dateutil==2.8.0
-    pip install src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     coverage run --source sagemaker -m pytest {posargs}
     {env:IGNORE_COVERAGE:} coverage report --fail-under=84 --omit */tensorflow/tensorflow_serving/*
 extras = test
@@ -104,7 +102,6 @@ install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m/readthedocs/doc_builder/python_environments.py#L280
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
-# TODO-reinvent-2019: Remove pip install ../tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
 deps =
     Pygments==2.2.0
     setuptools<40
@@ -122,7 +119,6 @@ deps =
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
     pip install python-dateutil==2.8.0
-    pip install ../src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     pip install --exists-action=w -r requirements.txt
     sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-03 21:53:41[0m
[92mHash: 8a992e6893d6c594cccc346faf92ed5b7a1996c1[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: fix: incorporate smdebug_ruleconfigs pkg until availability in PyPI (#291)

[0m
@@ -59,8 +59,10 @@ passenv =
 # Can be used to specify which tests to run, e.g.: tox -- -s
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
+# TODO-reinvent-2019: Remove pip install tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
 commands =
     pip install python-dateutil==2.8.0
+    pip install src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     coverage run --source sagemaker -m pytest {posargs}
     {env:IGNORE_COVERAGE:} coverage report --fail-under=84 --omit */tensorflow/tensorflow_serving/*
 extras = test
@@ -102,6 +104,7 @@ install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m/readthedocs/doc_builder/python_environments.py#L280
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
 # https://github.com/boto/botocore/issues/1872
+# TODO-reinvent-2019: Remove pip install ../tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
 deps =
     Pygments==2.2.0
     setuptools<40
@@ -119,6 +122,7 @@ deps =
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
     pip install python-dateutil==2.8.0
+    pip install ../src/sagemaker/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl
     pip install --exists-action=w -r requirements.txt
     sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-03 21:53:41[0m
[92mHash: a8b462fcecb63eb0f18ec76ea35c33b1dccef72a[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: Add support for SageMaker Debugger [WIP] (#247)

[0m
@@ -57,12 +57,9 @@ passenv =
 
 # {posargs} can be passed in by additional arguments specified when invoking tox.
 # Can be used to specify which tests to run, e.g.: tox -- -s
-# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
-# https://github.com/boto/botocore/issues/1872
-# TODO-reinvent-2019: Remove pip install tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
+# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
     pip install python-dateutil==2.8.0
-    pip install tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl
     coverage run --source sagemaker -m pytest {posargs}
     {env:IGNORE_COVERAGE:} coverage report --fail-under=90 --omit */tensorflow/tensorflow_serving/*
 extras = test
@@ -102,14 +99,13 @@ changedir = doc
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
 install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version:
-# https://github.com/boto/botocore/issues/1872
-# TODO-reinvent-2019: Remove pip install ../tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl once package is in PyPI
+# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 deps =
     Pygments==2.2.0
     setuptools<40
     docutils==0.13.1
     mock==1.0.1
+#    pillow==2.6.1
     alabaster>=0.7,<0.8,!=0.7.5
     commonmark==0.5.4
     recommonmark==0.4.0
@@ -122,7 +118,6 @@ deps =
 # TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 commands =
     pip install python-dateutil==2.8.0
-    pip install ../tests/data/smdebug_rulesconfig-0.4a0-py2.py3-none-any.whl
     pip install --exists-action=w -r requirements.txt
     sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-03 21:53:41[0m
[92mHash: ac311c5155ae6ba10f0b571d32af443e98240798[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: Processing Jobs Python SDK support (#225)

This change adds support for Amazon SageMaker Processing jobs. New classes include Processor, ScriptModeProcessor, SKLearnProcessor, SparkMLJavaProcessor, SparkMLPythonProcessor, ProcessingJob, FileInput, FileOutput, S3Uploader, and S3Downloader.[0m
@@ -99,13 +99,12 @@ changedir = doc
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
 install_command = python -m pip install --upgrade -I {packages}
 # Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-# TODO: Remove pip install python-dateutil==2.8.0 once botocore no longer requires a pinned version
 deps =
     Pygments==2.2.0
     setuptools<40
     docutils==0.13.1
     mock==1.0.1
-#    pillow==2.6.1
+    pillow==2.6.1
     alabaster>=0.7,<0.8,!=0.7.5
     commonmark==0.5.4
     recommonmark==0.4.0

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-01 23:13:55[0m
[92mHash: eae7b1bc574f2893e76f62b2d0d64698e558e452[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Documentation for Amazon Sagemaker Operators (#1141)

* added Amazon SageMaker Operators for Kubernetes content

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* split Amazon SageMaker Operators for Kubernetes in two

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Modifications to Kubernetes re:Invent Documentation branch (#288)

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

Modified placeholder values for consistency

* Update amazon_sagemaker_operators_for_kubernetes.rst

Removed reference to 1.12

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Remove creds-based installation in amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Adding Amazon SageMaker Operators for Kubernetes permissions diagram.

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update README.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update index.rst
[0m
@@ -1,556 +0,0 @@
-#########################################
-Amazon SageMaker Operators for Kubernetes
-#########################################
-
-
-
-Amazon SageMaker Operators for Kubernetes make it easier for developers and data scientists using Kubernetes to train, tune, and deploy machine learning (ML) models in Amazon SageMaker. You can install these SageMaker Operators on your Kubernetes cluster in Amazon Elastic Kubernetes Service (EKS) to create SageMaker jobs natively using the Kubernetes API and command-line Kubernetes tools such as ‘kubectl’. This guide shows you how to set up the operators. The guide also explains how to use the operators to run model training, hyperparameter tuning, and inference (real-time and batch).
-
-There is no additional charge to use these operators. You do incur charges
-for any Amazon SageMaker resources that you use through these operators. The procedures and guidelines here assume you are familiar with Kubernetes and its basic commands.
-
-
-.. contents::
-
-What is an operator?
---------------------
-
-Kubernetes is built on top of what is called the controller pattern.
-This pattern allows applications and tools to listen to a central state
-manager (ETCD) and act when something happens. Examples of such
-applications
-include \ ``cloud-controller-manager`` and \ ``controller-manager``.
-The controller pattern allows you to create decoupled experiences and not
-have to worry about how other components are integrated. To add new capabilities to Kubernetes, developers can extend the Kubernetes API by creating a custom resource that contains their application-specific or domain-specific logic and components. Operators in Kubernetes allow users to natively invoke these custom resources and automate associated workflows.
-
-Prerequisites
-~~~~~~~~~~~~~
-
-This guide assumes that you’ve
-completed the following prerequisites:
-
--  Installed the following tools on the client machine used to access your k8s cluster:
-
-   -  `kubectl <https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html>`__
-      Version 1.13 or later. Use a \ ``kubectl`` version that is within
-      one minor version of your Amazon Elastic Kubernetes Service
-      (Amazon EKS) cluster control plane. For example, a
-      1.13 \ ``kubectl`` client works with Kubernetes 1.13 and 1.14
-      clusters. OpenID Connect (OIDC) is not supported in versions earlier than 1.13.
-
-   -  `eksctl <https://github.com/weaveworks/eksctl>`__ Version 0.7.0 or
-      later
-
-   -  `AWS
-      CLI <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>`__ Version
-      1.16.232 or later
-
-   -  (optional) `Helm <https://helm.sh/docs/intro/install/>`__ Version
-      3.0 or later
-
-   -  `aws-iam-authenticator <https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html>`__ 
-
--  Have IAM permissions to create roles and attach policies to roles.
-
--  Created a Kubernetes cluster to run the operators on. It should either be
-   Kubernetes version 1.13 or 1.14. For automated cluster
-   creation using \ ``eksctl``, see `Getting Started with eksctl <https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>`__.
-   It takes 20 to 30 minutes to provision a cluster.
-
-Permissions overview
-~~~~~~~~~~~~~~~~~~~~
-
-The Amazon SageMaker Operators for Kubernetes allow you to manage jobs
-in Amazon SageMaker from your Kubernetes cluster. Thus the operators
-will access Amazon SageMaker resources on your behalf. The
-IAM role that the operator assumes to interact with AWS resources differs
-from the credentials you use to access the Kubernetes cluster. The
-role also differs from the role that Amazon SageMaker assumes when running your machine learning
-jobs. The following image explains this design and flow.
-
-.. image:: ./amazon_sagemaker_operators_for_kubernetes_authentication.png
-
-Setup and operator deployment
------------------------------
-
-The following sections describe the steps to setup and deploy the
-operator.
-
-IAM role-based operator deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before you can deploy your operator using an IAM role, associate an OpenID Connect (OIDC) provider with your role to
-authenticate with the IAM service.
-
-Associate an OpenID Connect Provider to Your Instance
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Create an OIDC identity provider for your cluster. If your
-cluster is managed by EKS, then your cluster will already have an OIDC
-attached to it. 
-
-Set the local ``CLUSTER_NAME`` and \ ``AWS_REGION`` environment
-variables as follows:
-
-::
-
-    # Set the Region and cluster
-    export CLUSTER_NAME="<your cluster name>"
-    export AWS_REGION="<your region>"
-
-Use the following command to associate the OIDC provider with your
-cluster. For more information, see \ `Enabling IAM Roles for Service
-Accounts on your
-Cluster. <https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>`__
-
-::
-
-    eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} \
-        --region ${AWS_REGION} --approve
-
-Your output should look like the following:
-
-::
-
-    [_]  eksctl version 0.10.1
-    [_]  using region us-east-1
-    [_]  IAM OpenID Connect provider is associated with cluster "my-cluster" in "us-east-1"
-
-Now that the cluster has an OIDC identity provider, you can create a
-role and give a Kubernetes ServiceAccount permission to assume the role.
-
-Get the OIDC ID
-^^^^^^^^^^^^^^^
-
-To set up the ServiceAccount, first obtain the OpenID Connect issuer URL
-using the following command:
-
-::
-
-    aws eks describe-cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} \
-        --query cluster.identity.oidc.issuer --output text
-
-The command will return a URL like the following:
-
-::
-
-    https://oidc.eks.${AWS_REGION}.amazonaws.com/id/[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0mOIDCID
-
-In this URL, the value [93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0mOIDCID is the OIDC ID.
-The OIDC ID for your cluster will be different. You need this OIDC ID
-value to create a role.
-
-If your output is \ ``None``, it means that your client version is old.
-To work around this, run the following command: 
-
-::
-
-    aws eks describe-cluster --query cluster --name ${CLUSTER_NAME} --output text | grep OIDC
-
-The OIDC URL will be returned as follows:
-
-::
-
-    OIDC https://oidc.eks.us-east-1.amazonaws.com/id/[93m[93m[93mD48675832CA65BD10A532F597[0m[0m[0mOIDCID
-
-Create an IAM Role 
-^^^^^^^^^^^^^^^^^^^
-
-Create a file named \ ``trust.json``  and insert the following trust
-relationship code block into it. Be sure to replace all \ ``<OIDC ID>``, \ ``<AWS account number>``, and \ ``<EKS Cluster region>`` placeholders with values corresponding to your cluster.
-
-::
-
-    {
-      "Version": "2012-10-17",
-      "Statement": [
-        {
-          "Effect": "Allow",
-          "Principal": {
-            "Federated": "arn:aws:iam::<AWS account number>:oidc-provider/oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>"
-          },
-          "Action": "sts:AssumeRoleWithWebIdentity",
-          "Condition": {
-            "StringEquals": {
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:aud": "sts.amazonaws.com",
-              "oidc.eks.<EKS Cluster region>.amazonaws.com/id/<OIDC ID>:sub": "system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default"
-            }
-          }
-        }
-      ]
-    }
-
-Run the following command to create a role with the trust
-relationship defined in \ ``trust.json``. This role enables the
-Amazon EKS cluster to get and refresh credentials from IAM.
-
-::
-
-    aws iam create-role --role-name <role name> --assume-role-policy-document file://trust.json --output=text
-
-Your output should look like the following:
-
-::
-
-    ROLE    arn:aws:iam::123456789012:role/my-role 2019-11-22T21:46:10Z    /       ABCDEFSFODNN7EXAMPLE   my-role
-    ASSUMEROLEPOLICYDOCUMENT        2012-10-17
-    STATEMENT       sts:AssumeRoleWithWebIdentity   Allow
-    STRINGEQUALS    sts.amazonaws.com       system:serviceaccount:sagemaker-k8s-operator-system:sagemaker-k8s-operator-default
-    PRINCIPAL       arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/
-
-Take note of \ ``ROLE ARN``, you pass this value to your
-operator. 
-
-Attach the AmazonSageMakerFullAccess Policy to the Role
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To give the role access to Amazon SageMaker, attach
-the \ `AmazonSageMakerFullAccess <https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess>`__ policy.
-If you want to limit permissions to the operator, you can create your
-own custom policy and attach it.
-
-To attach AmazonSageMakerFullAccess, run the following command:
-
-::
-
-    aws iam attach-role-policy --role-name <role name>  --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
-
-The Kubernetes
-ServiceAccount \ ``sagemaker-k8s-operator-default`` should
-have \ ``AmazonSageMakerFullAccess`` permissions. Confirm this when you
-install the operator.
-
-Deploy the Operator
-^^^^^^^^^^^^^^^^^^^
-
-When deploying your operator, you can use either a YAML file or Helm
-charts. 
-
-Deploy the Operator Using YAML
-''''''''''''''''''''''''''''''
-
-This is the simplest way to deploy your operators. The process is as
-follows: 
-
--  Download the installer script using the following command:
-
-   ::
-
-       wget https://raw.githubusercontent.com/aws/amazon-sagemaker-operator-for-k8s/master/release/rolebased/installer.yaml
-
--  Edit the \ ``installer.yaml`` file to
-   replace \ ``eks.amazonaws.com/role-arn``. Replace the ARN here with
-   the ARN for the OIDC-based role you’ve created. 
-
--  Use the following command to deploy the cluster:  
-
-   ::
-
-       kubectl apply -f installer.yaml
-
-Deploy the Operator Using Helm Charts
-'''''''''''''''''''''''''''''''''''''
-
-Use the provided Helm Chart to install
-the operator.
-
-
-Clone the Helm installer directory using the following command:
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/installer`` folder. Edit
-the \ ``values.yaml`` file, which includes high-level parameters for the
-Chart. Replace the ARN here with the ARN for the OIDC-based role you’ve
-created. 
-
-Install the Helm Chart using the following command:
-
-::
-
-    helm install rolebased/ --generate-name
-
-
-After a moment, the chart will be installed with a randomly generated
-name. Verify that the installation succeeded by running the following
-command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    rolebased-1234567    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-
-Verify the operator deployment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-You should be able to see the Amazon SageMaker Custom Resource
-Definitions (CRDs) for each operator deployed to your cluster by running
-the following command: 
-
-::
-
-    kubectl get crd | grep sagemaker
-
-Your output should look like the following:
-
-::
-
-    batchtransformjobs.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    endpointconfigs.sagemaker.aws.amazon.com            2019-11-20T17:12:34Z
-    hostingdeployments.sagemaker.aws.amazon.com         2019-11-20T17:12:34Z
-    hyperparametertuningjobs.sagemaker.aws.amazon.com   2019-11-20T17:12:34Z
-    models.sagemaker.aws.amazon.com                     2019-11-20T17:12:34Z
-    trainingjobs.sagemaker.aws.amazon.com               2019-11-20T17:12:34Z
-
-Ensure that the operator pod is running successfully. Use the following
-command to list all pods:
-
-::
-
-    kubectl -n sagemaker-k8s-operator-system get pods
-
-You should see a pod
-named \ ``sagemaker-k8s-operator-controller-manager-*****`` in the
-namespace \ ``sagemaker-k8s-operator-system``  as follows:
-
-::
-
-    NAME                                                         READY   STATUS    RESTARTS   AGE
-    sagemaker-k8s-operator-controller-manager-12345678-r8abc   2/2     Running   0          23s
-
-​
-
-Install the Amazon SageMaker logs \ ``kubectl`` plugin
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As part of the Amazon SageMaker Operators for Kubernetes, you can use
-the \ ``smlogs`` `plugin <https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>`__ for ``kubectl`` .
-This enables Amazon SageMaker CloudWatch logs to be streamed
-with \ ``kubectl``. \ ``kubectl``\ must be installed onto
-your `PATH <http://www.linfo.org/path_env_var.html>`__. The
-following commands place the binary in
-the \ ``sagemaker-k8s-bin`` directory in your home directory, and add
-that directory to your \ ``PATH``.
-
-::
-
-    export os="linux"
-
-    wget https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/latest/${os}.amd64.tar.gz
-    tar xvzf ${os}.amd64.tar.gz
-
-    # Move binaries to a directory in your homedir.
-    mkdir ~/sagemaker-k8s-bin
-    cp ./kubectl-smlogs.${os}.amd64/kubectl-smlogs ~/sagemaker-k8s-bin/.
-
-    # This line will add the binaries to your PATH in your .bashrc. 
-
-    echo 'export PATH=$PATH:~/sagemaker-k8s-bin' >> ~/.bashrc
-
-    # Source your .bashrc to update environment variables:
-    source ~/.bashrc
-
-Use the following command to verify that the \ ``kubectl`` plugin is
-installed correctly:
-
-::
-
-    kubectl smlogs
-
-If the \ ``kubectl`` plugin is installed correctly, your output should
-look like the following:
-
-::
-
-    View Amazon SageMaker logs via Kubernetes
-
-    Usage:
-      smlogs [command]
-
-    Aliases:
-      smlogs, SMLogs, Smlogs
-
-    Available Commands:
-      BatchTransformJob       View BatchTransformJob logs via Kubernetes
-      TrainingJob             View TrainingJob logs via Kubernetes
-      help                    Help about any command
-
-    Flags:
-      -h, --help   help for smlogs
-
-    Use "smlogs [command] --help" for more information about a command.
-
-
-Delete operators from the cluster 
-----------------------------------
-
-Operators installed using YAML
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-To uninstall the operator from your cluster, make sure that all
-Amazon SageMaker resources have been deleted from the cluster. Failure
-to do so will cause the operator delete operation to hang. Once you have
-deleted all Amazon SageMaker jobs, use \ ``kubectl`` to
-delete the operator from the cluster. Run the following commands to stop
-all jobs and delete the operator from the cluster:
-
-::
-
-    # Delete all Amazon SageMaker jobs from Kubernetes
-    kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-
-    # Delete the operator and its resources
-    kubectl delete -f /installer.yaml
-
-You should see output like the following:
-
-::
-
-    $ kubectl delete --all --all-namespaces trainingjobs.sagemaker.aws.amazon.com
-    trainingjobs.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-    $ kubectl delete --all --all-namespaces hyperparametertuningjob.sagemaker.aws.amazon.com
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-    $ kubectl delete --all --all-namespaces batchtransformjob.sagemaker.aws.amazon.com
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-    $ kubectl delete --all --all-namespaces hostingdeployment.sagemaker.aws.amazon.com
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-    $ kubectl delete -f raw-yaml/installer.yaml
-    namespace "sagemaker-k8s-operator-system" deleted
-    customresourcedefinition.apiextensions.k8s.io "batchtransformjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "endpointconfigs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hostingdeployments.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "hyperparametertuningjobs.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "models.sagemaker.aws.amazon.com" deleted
-    customresourcedefinition.apiextensions.k8s.io "trainingjobs.sagemaker.aws.amazon.com" deleted
-    role.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-role" deleted
-    clusterrole.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-role" deleted
-    rolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-leader-election-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-manager-rolebinding" deleted
-    clusterrolebinding.rbac.authorization.k8s.io "sagemaker-k8s-operator-proxy-rolebinding" deleted
-    service "sagemaker-k8s-operator-controller-manager-metrics-service" deleted
-    deployment.apps "sagemaker-k8s-operator-controller-manager" deleted
-    secrets "sagemaker-k8s-operator-abcde" deleted
-
-Operators installed using Helm Charts
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-To delete the operator CRDs, first delete all the running jobs. Then
-delete the helm chart that was used to deploy the operators using the
-following commands: 
-
-::
-
-    # get the helm charts 
-    $ helm ls
-
-    # delete the charts
-    $ helm delete <chart name>
-
-​
-
-Troubleshooting
----------------
-
-Debugging a Failed Job
-~~~~~~~~~~~~~~~~~~~~~~
-
-Check the job status by running:
-
-::
-
-    kubectl get <CRD Type> <job name>
-
-If the job was created in Amazon SageMaker, you can use the following
-command to see the \ ``STATUS`` and the ``SageMaker Job Name``: 
-
-::
-
-    kubectl get <crd type> <job name>
-
--  You can use \ ``smlogs`` to find the cause of the issue using the
-   following command: 
-
-   ::
-
-       kubectl smlogs <crd type> <job name>
-
--  You can also use the \ ``describe`` command to get more details about
-   the job using the following command.The output will have
-   an \ ``additional`` field that will have more information about the
-   status of the job.
-
-   ::
-
-       kubectl describe <crd type> <job name>
-
-If the job was not created in Amazon SageMaker, then use the logs of the
-operator’s pod to find the cause of the issue as follows:
-
-::
-
-    $ kubectl get pods -A | grep sagemaker
-    # Output: 
-    sagemaker-k8s-operator-system   sagemaker-k8s-operator-controller-manager-5cd7df4d74-wh22z   2/2     Running   0          3h33m
-
-    $ kubectl logs -p <pod name> -c manager -n sagemaker-k8s-operator-system
-
-Deleting an Operator CRD
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If deleting a job is stuck, check if the operator is running. If the
-operator is not running, then you will have to delete the finalizer
-using the following steps:
-
--  In a new terminal, open the job in an editor using ``kubectl edit``
-   as follows: 
-
-   ::
-
-       $ kubectl edit <crd type> <job name>
-
-       # for example for the batchtransformjob xgboost-mnist
-       $ kubectl edit batchtransformjobs xgboost-mnist 
-
--  Edit the job to delete the finalizer by removing the following two
-   lines from the file. Save the file and the job should immediately get
-   deleted/updated. 
-
-   ::
-
-         finalizers:
-         - sagemaker-operator-finalizer
-
-Images and SMlogs in each Region
---------------------------------
-
-The following table lists the available operator images and SMLogs in
-each region.
-
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| Region      | Controller Image                                                                            | Linux SMLogs                                                                                                           |
-+=============+=============================================================================================+========================================================================================================================+
-| us-east-1   | ``957583890962.dkr.ecr.us-east-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:latest``   | https://amazon-sagemaker-operator-for-k8s-us-east-1.s3.amazonaws.com/kubectl-smlogs-plugin/latest/linux.amd64.tar.gz   |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-east-2   | ``922499468684.dkr.ecr.us-east-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:latest``   | https://amazon-sagemaker-operator-for-k8s-us-east-2.s3.amazonaws.com/kubectl-smlogs-plugin/latest/linux.amd64.tar.gz   |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| us-west-2   | ``640106867763.dkr.ecr.us-west-2.amazonaws.com/amazon-sagemaker-operator-for-k8s:latest``   | https://amazon-sagemaker-operator-for-k8s-us-west-2.s3.amazonaws.com/kubectl-smlogs-plugin/latest/linux.amd64.tar.gz   |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+
-| eu-west-1   | ``613661167059.dkr.ecr.eu-west-1.amazonaws.com/amazon-sagemaker-operator-for-k8s:latest``   | https://amazon-sagemaker-operator-for-k8s-eu-west-1.s3.amazonaws.com/kubectl-smlogs-plugin/latest/linux.amd64.tar.gz   |
-+-------------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-12-01 23:13:55[0m
[92mHash: eae7b1bc574f2893e76f62b2d0d64698e558e452[0m
[92mFilepath: doc/amazon_sagemaker_operators_for_kubernetes_jobs.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Documentation for Amazon Sagemaker Operators (#1141)

* added Amazon SageMaker Operators for Kubernetes content

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* split Amazon SageMaker Operators for Kubernetes in two

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Modifications to Kubernetes re:Invent Documentation branch (#288)

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

Modified placeholder values for consistency

* Update amazon_sagemaker_operators_for_kubernetes.rst

Removed reference to 1.12

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Remove creds-based installation in amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Adding Amazon SageMaker Operators for Kubernetes permissions diagram.

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update README.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update amazon_sagemaker_operators_for_kubernetes.rst

* Update amazon_sagemaker_operators_for_kubernetes_jobs.rst

* Update index.rst
[0m
@@ -1,1264 +0,0 @@
-Using Amazon Sagemaker Jobs
----------------------------
-
-To run a job using the Amazon Sagemaker Operators for Kubernetes, you can either apply
-a YAML file or use the supplied Helm charts.
-
-All operator sample jobs in the following tutorials use sample data
-taken from a public MNIST dataset. In order to run these samples, download the dataset into your S3 bucket. You can find
-the dataset in \ `Download the MNIST
-Dataset. <https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data-pull-data.html>`__
-
-.. contents::
-
-TrainingJob operator
-~~~~~~~~~~~~~~~~~~~~
-
-Training job operators reconcile your specified training job spec to
-Amazon SageMaker by launching it for you in Amazon SageMaker. You can
-learn more about Amazon SageMaker training jobs in the Amazon
-SageMaker \ `CreateTrainingJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html>`__.
-
-Create a TrainingJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for training using the following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-trainingjob.yaml
-
-Edit the \ ``xgboost-mnist-trainingjob.yaml`` file to replace the ``roleArn`` parameter with your \ ``<sagemaker-execution-role>``, and \ ``outputPath`` with your S3 bucket that the Amazon SageMaker
-execution role has write access to. Apply the YAML file using the
-following command:
-
-::
-
-    kubectl apply -f xgboost-mnist-trainingjob.yaml
-
-Create a TrainingJob Using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run TrainingJobs. 
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-
-Navigate to the
-\ ``amazon-sagemaker-operator-for-k8s/hack/charts/training-jobs/`` folder
-and edit the \ ``values.yaml`` file to replace values
-like \ ``rolearn`` and ``outputpath`` with values that correspond to
-your account. The RoleARN must have permissions so that Amazon SageMaker
-can access Amazon S3, Amazon CloudWatch, and other services on your
-behalf. For more information on creating an Amazon SageMaker
-ExecutionRole, see \ `Amazon SageMaker
-Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms>`__.
-
-Create the Training Job 
-''''''''''''''''''''''''
-
-With the roles and S3 buckets replaced with appropriate values
-in \ ``values.yaml``, you can create a training job using the following
-command:
-
-::
-
-    helm install . --generate-name
-
-Your output should look like the following:
-
-::
-
-    NAME: chart-12345678
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-trainingjob.
-
-Verify Your Training Helm Chart
-'''''''''''''''''''''''''''''''
-
-To verify that the Helm Chart was created successfully, run:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    chart-12345678        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-12345678    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-``helm install`` creates a \ ``TrainingJob`` k8s resource. The operator
-launches the actual training job in Amazon SageMaker and updates
-the \ ``TrainingJob`` k8s resource to reflect the status of the job in
-Amazon SageMaker. You incur charges for Amazon SageMaker resources used
-during the duration of your job. You do not incur any charges once your
-job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running
-training job. You cannot edit any parameter and re-apply the
-file/config. Either change the metadata name or delete the existing job
-and create a new one. Similar to existing training job operators like
-TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Training Jobs
-^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-    kubectl get TrainingJob
-
-The output listing all jobs should look like the following:
-
-::
-
-    kubectl get trainingjobs
-    NAME                        STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME
-    xgboost-mnist-from-for-s3   InProgress   Starting           2019-11-20T23:42:35Z   xgboost-mnist-from-for-s3-exampl[93mef11eab94e0ed4671d5a8f[0m
-
-A training job continues to be listed after the job has completed or
-failed. You can remove a \ ``TrainingJob``  job from the list by
-following the Delete a Training Job steps. Jobs that have completed or
-stopped do not incur any charges for Amazon SageMaker resources. 
-
-Training Job Status Values
-''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official \ `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-TrainingJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job.
-
-Secondary Status Values
-'''''''''''''''''''''''
-
-The secondary statuses come directly from the Amazon SageMaker
-official \ `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html#SageMaker-DescribeTrainingJob-response-SecondaryStatus>`__.
-They contain more granular information about the status of the job.
-
-Describe a Training Job
-^^^^^^^^^^^^^^^^^^^^^^^
-
-You can get more details about the training job by using
-the \ ``describe`` kubectl verb. This is typically used for debugging a
-problem or checking the parameters of a training job. To get information
-about your training job, use the following command:
-
-::
-
-    kubectl describe trainingjob xgboost-mnist-from-for-s3
-
-The output for your training job should look like the following:
-
-::
-
-    Name:         xgboost-mnist-from-for-s3
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  <none>
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         TrainingJob
-    Metadata:
-      Creation Timestamp:  2019-11-20T23:42:35Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  23119
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/trainingjobs/xgboost-mnist-from-for-s3
-      UID:               6d7uiui-0bef-11ea-b94e-0ed467example
-    Spec:
-      Algorithm Specification:
-        Training Image:       8256416981234.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
-        Training Input Mode:  File
-      Hyper Parameters:
-        Name:   eta
-        Value:  0.2
-        Name:   gamma
-        Value:  4
-        Name:   max_depth
-        Value:  5
-        Name:   min_child_weight
-        Value:  6
-        Name:   num_class
-        Value:  10
-        Name:   num_round
-        Value:  10
-        Name:   objective
-        Value:  multi:softmax
-        Name:   silent
-        Value:  0
-      Input Data Config:
-        Channel Name:      train
-        Compression Type:  None
-        Content Type:      text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Distribution Type:  FullyReplicated
-            S 3 Data Type:               S3Prefix
-            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
-        Channel Name:                    validation
-        Compression Type:                None
-        Content Type:                    text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Distribution Type:  FullyReplicated
-            S 3 Data Type:               S3Prefix
-            S 3 Uri:                     https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
-      Output Data Config:
-        S 3 Output Path:  s3://my-bucket/sagemaker/xgboost-mnist/xgboost/
-      Region:             us-east-2
-      Resource Config:
-        Instance Count:     1
-        Instance Type:      ml.m4.xlarge
-        Volume Size In GB:  5
-      Role Arn:             arn:aws:iam::12345678910:role/service-role/AmazonSageMaker-ExecutionRole
-      Stopping Condition:
-        Max Runtime In Seconds:  86400
-      Training Job Name:         xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0mxample
-    Status:
-      Cloud Watch Log URL:           https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=<example>;streamFilter=typeLogStreamPrefix
-      Last Check Time:               2019-11-20T23:44:29Z
-      Sage Maker Training Job Name:  xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94ee[0mxample
-      Secondary Status:              Downloading
-      Training Job Status:           InProgress
-    Events:                          <none>
-
-View Logs from Training Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to see the logs from the \ ``kmeans-mnist`` 
-training job:
-
-::
-
-    kubectl smlogs trainingjob xgboost-mnist-from-for-s3
-
-Your output will look similar to the following. The logs from instances
-are ordered chronologically.
-
-::
-
-    "xgboost-mnist-from-for-s3" has SageMaker TrainingJobName "xgboost-mnist-from-for-s3-123456789" in region "us-east-2", status "InProgress" and secondary status "Starting"
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC Arguments: train
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Running standalone xgboost training.
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 8586.0mb
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [2019-11-20:23:45:22:INFO] Determined delimiter of CSV input is ','
-    xgboost-mnist-from-for-s3-[93m6d7fa0af0bef11eab94e0e[0md46example/algo-1-1574293123 2019-11-20 23:45:24.7 +0000 UTC [23:45:22] S3DistributionType set as FullyReplicated
-
-Delete Training Jobs
-^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a training job on Amazon SageMaker:
-
-::
-
-    kubectl delete trainingjob xgboost-mnist-from-for-s3
-
-This command removes the Amazon SageMaker training job from k8s. This
-command returns the following output:
-
-::
-
-    trainingjob.sagemaker.aws.amazon.com "xgboost-mnist-from-for-s3" deleted
-
-If the job is still in progress on Amazon SageMaker, the job will stop.
-You do not incur any charges for Amazon SageMaker resources after your
-job stops or completes. 
-
-**Note**: Amazon SageMaker does not delete training jobs. Stopped jobs
-continue to show on the Amazon SageMaker console. The delete command
-takes about 2 minutes to clean up the resources from Amazon SageMaker.
-
-HyperParameterTuningJobs operator
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Hyperparameter tuning job operators reconcile your
-specified hyperparameter tuning job spec to Amazon SageMaker by
-launching it in Amazon SageMaker. You can learn more about Amazon
-SageMaker hyperparameter tuning jobs in the Amazon
-SageMaker \ `CreateHyperParameterTuningJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateHyperParameterTuningJob.html>`__.
-
-Create a HyperParameterTuningJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the hyperparameter tuning job using
-the following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hpo.yaml
-
-Edit the \ ``xgboost-mnist-hpo.yaml`` file to replace
-the \ ``roleArn`` parameter with your <sagemaker-execution-role>. For
-HyperparameterTuningJob to succeed, you must also change
-the \ ``s3InputPath``  and \ ``s3OutputPath`` to values that correspond
-to your account. Apply the updates YAML file using the following
-command:
-
-::
-
-    kubectl apply -f xgboost-mnist-hpo.yaml
-
-Create a HyperParameterTuningJob using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run HyperParameterTuningJobs.
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-
-Navigate to the
-\ ``amazon-sagemaker-operator-for-k8s/hack/charts/hyperparameter-tuning-jobs/``
-folder.
-
-Edit the \ ``values.yaml`` file to replace the \ ``roleArn`` parameter
-with your <sagemaker-execution-role>. For HyperparameterTuningJob to
-succeed, you must also change the \ ``s3InputPath`` 
-and \ ``s3OutputPath`` to values that correspond to your account. 
-
-Create the HPO Job
-''''''''''''''''''
-
-With the roles and Amazon S3 paths replaced with appropriate values
-in \ ``values.yaml``, you can create a hyperparameter tuning job using
-the following command:
-
-::
-
-    helm install . --generate-name
-
-Your output will look similar to the following:
-
-::
-
-    NAME: chart-1574292948
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-hyperparametertuningjob.
-
-Verify Chart Installation
-'''''''''''''''''''''''''
-
-To verify that the Helm Chart was created successfully, run the
-following command:
-
-::
-
-    helm ls
-
-Your output should look like the following:
-
-::
-
-    NAME                    NAMESPACE       REVISION        UPDATED  
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0                               STATUS          CHART                           APP VERSION
-    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-``helm install`` creates a \ ``HyperParameterTuningJob`` k8s resource.
-The operator launches the actual hyperparameter optimization job in
-Amazon SageMaker and updates the \ ``HyperParameterTuningJob`` k8s
-resource to reflect the status of the job in Amazon SageMaker. You incur
-charges for Amazon SageMaker resources used during the duration of your
-job. You do not incur any charges once your job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running
-hyperparameter tuning job. You cannot edit any parameter and re-apply
-the file/config. You must either change the metadata name or delete the
-existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Hyperparameter Tuning Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-    kubectl get hyperparametertuningjob 
-
-Your output will look like the following:
-
-::
-
-    NAME         STATUS      CREATION-TIME          COMPLETED   INPROGRESS   ERRORS   STOPPED   BEST-TRAINING-JOB                               SAGEMAKER-JOB-NAME
-    xgboost-mnist-hpo   Completed   2019-10-17T01:15:52Z   10          0            0        0         xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a123   xgboosth[93ma92f5e3cf07b11e9bf6c123[0m
-
-A hyper parameter tuning job will continue to be listed after the job
-has completed or failed. You can remove a \ ``hyperparametertuningjob`` 
-from the list by following the steps in Delete a Hyper Parameter Tuning
-Job. Jobs that have completed or stopped do not incur any charges for
-Amazon SageMaker resources. 
-
-Hyperparameter Tuning Job Status Values
-'''''''''''''''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job.
-
-Status Counters
-'''''''''''''''
-
-The output has several counters,
-like \ ``COMPLETED`` and ``INPROGRESS``. These represent how many
-training jobs have completed and are in progress, respectively. For more
-information about how these are determined,
-see \ `TrainingJobStatusCounters <https://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobStatusCounters.html>`__ in
-the Amazon SageMaker API documentation. 
-
-Best Training Job
-'''''''''''''''''
-
-This column contains the name of the \ ``TrainingJob`` that best
-optimized the selected metric.
-
-To see a summary of the tuned hyperparameters, run:
-
-::
-
-    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
-
-To see detailed information about the \ ``TrainingJob``, run:
-
-::
-
-    kubectl describe trainingjobs <job name>
-
-
-Spawned Training Jobs
-'''''''''''''''''''''
-
-You can also track all 10 training jobs in k8s launched by
-``HyperparameterTuningJob`` by running the following command:
-
-::
-
-    kubectl get trainingjobs
-
-Describe a Hyperparameter Tuning Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe hyperparametertuningjob xgboost-mnist-hpo
-
-In addition to information about the tuning job, the Amazon SageMaker
-Operator for Kubernetes also exposes the `best training
-job <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html#automatic-model-tuning-best-training-job>`__\  found
-by the hyperparameter tuning job in the \ ``describe`` output as
-follows:
-
-::
-
-    Name:         xgboost-mnist-hpo
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HyperparameterTuningJob","metadata":{"annotations":{},"name":"xgboost-mnist-hpo","namespace":...
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         HyperparameterTuningJob
-    Metadata:
-      Creation Timestamp:  2019-10-17T01:15:52Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  8167
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hyperparametertuningjobs/xgboost-mnist-hpo
-      UID:               a92f5e3c-f07b-11e9-bf6c-06d6f303uidu
-    Spec:
-      Hyper Parameter Tuning Job Config:
-        Hyper Parameter Tuning Job Objective:
-          Metric Name:  validation:error
-          Type:         Minimize
-        Parameter Ranges:
-          Integer Parameter Ranges:
-            Max Value:     20
-            Min Value:     10
-            Name:          num_round
-            Scaling Type:  Linear
-        Resource Limits:
-          Max Number Of Training Jobs:     10
-          Max Parallel Training Jobs:      10
-        Strategy:                          Bayesian
-        Training Job Early Stopping Type:  Off
-      Hyper Parameter Tuning Job Name:     xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
-      Region:                              us-east-2
-      Training Job Definition:
-        Algorithm Specification:
-          Training Image:       12345678910.dkr.ecr.us-east-2.amazonaws.com/xgboost:1
-          Training Input Mode:  File
-        Input Data Config:
-          Channel Name:  train
-          Content Type:  text/csv
-          Data Source:
-            s3DataSource:
-              s3DataDistributionType:  FullyReplicated
-              s3DataType:              S3Prefix
-              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/train/
-          Channel Name:                validation
-          Content Type:                text/csv
-          Data Source:
-            s3DataSource:
-              s3DataDistributionType:  FullyReplicated
-              s3DataType:              S3Prefix
-              s3Uri:                   https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/validation/
-        Output Data Config:
-          s3OutputPath:  https://s3-us-east-2.amazonaws.com/my-bucket/sagemaker/xgboost-mnist/xgboost
-        Resource Config:
-          Instance Count:     1
-          Instance Type:      ml.m4.xlarge
-          Volume Size In GB:  5
-        Role Arn:             arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-        Static Hyper Parameters:
-          Name:   base_score
-          Value:  0.5
-          Name:   booster
-          Value:  gbtree
-          Name:   csv_weights
-          Value:  0
-          Name:   dsplit
-          Value:  row
-          Name:   grow_policy
-          Value:  depthwise
-          Name:   lambda_bias
-          Value:  0.0
-          Name:   max_bin
-          Value:  256
-          Name:   max_leaves
-          Value:  0
-          Name:   normalize_type
-          Value:  tree
-          Name:   objective
-          Value:  reg:linear
-          Name:   one_drop
-          Value:  0
-          Name:   prob_buffer_row
-          Value:  1.0
-          Name:   process_type
-          Value:  default
-          Name:   rate_drop
-          Value:  0.0
-          Name:   refresh_leaf
-          Value:  1
-          Name:   sample_type
-          Value:  uniform
-          Name:   scale_pos_weight
-          Value:  1.0
-          Name:   silent
-          Value:  0
-          Name:   sketch_eps
-          Value:  0.03
-          Name:   skip_drop
-          Value:  0.0
-          Name:   tree_method
-          Value:  auto
-          Name:   tweedie_variance_power
-          Value:  1.5
-        Stopping Condition:
-          Max Runtime In Seconds:  86400
-    Status:
-      Best Training Job:
-        Creation Time:  2019-10-17T01:16:14Z
-        Final Hyper Parameter Tuning Job Objective Metric:
-          Metric Name:        validation:error
-          Value:              
-        Objective Status:     Succeeded
-        Training End Time:    2019-10-17T01:20:24Z
-        Training Job Arn:     arn:aws:sagemaker:us-east-2:123456789012:training-job/xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4sample
-        Training Job Name:    xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m-009-4c7a3059
-        Training Job Status:  Completed
-        Training Start Time:  2019-10-17T01:18:35Z
-        Tuned Hyper Parameters:
-          Name:                                    num_round
-          Value:                                   18
-      Hyper Parameter Tuning Job Status:           Completed
-      Last Check Time:                             2019-10-17T01:21:01Z
-      Sage Maker Hyper Parameter Tuning Job Name:  xgboosth[93m[93m[93m[93m[93ma92f5e3cf07b11e9bf6c06d6[0m[0m[0m[0m[0m
-      Training Job Status Counters:
-        Completed:            10
-        In Progress:          0
-        Non Retryable Error:  0
-        Retryable Error:      0
-        Stopped:              0
-        Total Error:          0
-    Events:                   <none>
-
-View Logs from HyperParameterTuning Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Hyperparameter tuning jobs do not have logs, but all training jobs
-launched by them do have logs. These logs can be accessed as if they
-were a normal training job. For more information, see View Logs from
-Training Jobs.
-
-Delete HyperParameterTuning jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a hyperparameter job in
-Amazon SageMaker. 
-
-::
-
-    kubectl delete hyperparametertuningjob xgboost-mnist-hpo
-
-This command removes the hyperparameter tuning job and associated
-training jobs from your Kubernetes cluster, as well as stops them in
-Amazon SageMaker. Jobs that have stopped or completed do not incur any
-charges for Amazon SageMaker resources.  Amazon SageMaker does not
-delete hyperparameter tuning jobs. Stopped jobs continue to show on the
-Amazon SageMaker Console. 
-
-Your output should look like the following:  
-
-::
-
-    hyperparametertuningjob.sagemaker.aws.amazon.com "xgboost-mnist-hpo" deleted
-
-**Note**:  The delete command takes about 2 minutes to clean up the
-resources from Amazon SageMaker.
-
-BatchTransformJobs operator
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Batch transform job operators reconcile your specified batch transform
-job spec to Amazon SageMaker by launching it in Amazon SageMaker. You
-can learn more about Amazon SageMaker batch transform job in the Amazon
-SageMaker \ `CreateTransformJob API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`__.
-
-Create a BatchTransformJob Using a Simple YAML File
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the batch transform job using the
-following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-batchtransform.yaml
-
-Edit the file \ ``xgboost-mnist-batchtransform.yaml`` to change
-necessary parameters to replace the \ ``inputdataconfig``  with your
-input data and \ ``s3OutputPath`` with your S3 buckets that the Amazon
-SageMaker execution role has write access to.  
-
-Apply the YAML file using the following command:
-
-::
-
-    kubectl apply -f xgboost-mnist-batchtransform.yaml
-
-Create a BatchTransformJob Using a Helm Chart
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can use Helm Charts to run batch transform jobs.
-
-Get the Helm installer directory
-''''''''''''''''''''''''''''''''
-
-Clone the github repo to get the source using the following command: 
-
-::
-
-    git clone https://github.com/aws/amazon-sagemaker-operator-for-k8s.git
-
-Configure the Helm Chart
-''''''''''''''''''''''''
-
-Navigate to the
-``amazon-sagemaker-operator-for-k8s/hack/charts/batch-transform-jobs/``
-folder. 
-
-Edit the \ ``values.yaml`` file to replace the \ ``inputdataconfig`` 
-with your input data and outputPath with your S3 buckets that the Amazon
-SageMaker execution role has write access to. 
-
-Create a Batch Transform Job
-''''''''''''''''''''''''''''
-
-Use the following command to create a batch transform job:
-
-::
-
-    helm install . --generate-name
-
-Your output should look like the following:
-
-::
-
-    NAME: chart-1574292948
-    LAST DEPLOYED: Wed Nov 20 23:35:49 2019
-    NAMESPACE: default
-    STATUS: deployed
-    REVISION: 1
-    TEST SUITE: None
-    NOTES:
-    Thanks for installing the sagemaker-k8s-batch-transform-job.
-
-To verify that the Helm Chart was created successfully, run the
-following command:
-
-::
-
-    helm ls
-    NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-batchtransformjob-0.1.0
-    chart-1474292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-hyperparametertuningjob-0.1.0
-    chart-1574292948        default         1               2019-11-20 23:35:49.9136092 +0000 UTC   deployed        sagemaker-k8s-trainingjob-0.1.0
-    rolebased-1574291698    default         1               2019-11-20 23:14:59.6777082 +0000 UTC   deployed        sagemaker-k8s-operator-0.1.0
-
-The previous command creates a \ ``BatchTransformJob`` k8s resource. The
-operator launches the actual transform job in Amazon SageMaker and
-updates the \ ``BatchTransformJob`` k8s resource to reflect the status
-of the job in Amazon SageMaker. You incur charges for Amazon SageMaker
-resources used during the duration of your job. You do not incur any
-charges once your job completes or stops.
-
-**Note**: Amazon SageMaker does not allow you to update a running batch
-transform job. You cannot edit any parameter and re-apply the
-file/config. You must either change the metadata name or delete the
-existing job and create a new one. Similar to existing training job
-operators like TFJob in Kubeflow, \ ``update`` is not supported.
-
-List Batch Transform Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to list all jobs created using the k8s
-operator:
-
-::
-
-     kubectl get batchtransformjob 
-
-Your output should look like the following:
-
-::
-
-    NAME                                STATUS      CREATION-TIME          SAGEMAKER-JOB-NAME
-    xgboost-mnist-batch-transform       Completed   2019-11-18T03:44:00Z   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0mxgboost
-
-A batch transform job will continue to be listed after the job has
-completed or failed. You can remove a \ ``hyperparametertuningjob`` 
-from the list by following the Delete a Batch Transform Job steps. Jobs
-that have completed or stopped do not incur any charges for
-Amazon SageMaker resources. 
-
-Batch Transform Status Values
-'''''''''''''''''''''''''''''
-
-The \ ``STATUS`` field can be one of the following values: 
-
--  ``Completed``
-
--  ``InProgress``
-
--  ``Failed``
-
--  ``Stopped``
-
--  ``Stopping``
-
-These statuses come directly from the Amazon SageMaker official `API
-documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeHyperParameterTuningJob.html#SageMaker-DescribeHyperParameterTuningJob-response-HyperParameterTuningJobStatus>`__.
-
-In addition to the official Amazon SageMaker status, it is possible
-for \ ``STATUS`` to be \ ``SynchronizingK8sJobWithSageMaker``. This
-means that the operator has not yet processed the job and will get to it
-soon.
-
-Describe a Batch Transform Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe batchtransformjob xgboost-mnist-batch-transform
-
-Your output should look like the following:
-
-::
-
-    Name:         xgboost-mnist-batch-transform
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"BatchTransformJob","metadata":{"annotations":{},"name":"xgboost-mnist","namespace"...
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         BatchTransformJob
-    Metadata:
-      Creation Timestamp:  2019-11-18T03:44:00Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        2
-      Resource Version:  21990924
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/batchtransformjobs/xgboost-mnist
-      UID:               a88fb198-09b5-11ea-ac44-0aa8a9UIDNUM
-    Spec:
-      Model Name:  TrainingJob-20190814SMJOb-IKEB
-      Region:      us-east-1
-      Transform Input:
-        Content Type:  text/csv
-        Data Source:
-          S 3 Data Source:
-            S 3 Data Type:  S3Prefix
-            S 3 Uri:        s3://my-bucket/mnist_kmeans_example/input
-      Transform Job Name:   xgboost-mnist-[93m[93ma88fb19809b511eaac440aa[0m8a[0m9SMJOB
-      Transform Output:
-        S 3 Output Path:  s3://my-bucket/mnist_kmeans_example/output
-      Transform Resources:
-        Instance Count:  1
-        Instance Type:   ml.m4.xlarge
-    Status:
-      Last Check Time:                2019-11-19T22:50:40Z
-      Sage Maker Transform Job Name:  xgboost-mnist-[93ma88fb19809b511eaac440aa[0mSMJOB
-      Transform Job Status:           Completed
-    Events:                           <none>
-
-View Logs from Batch Transform Jobs
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to see the logs from the \ ``xgboost-mnist`` 
-batch transform job:
-
-::
-
-    kubectl smlogs batchtransformjob xgboost-mnist-batch-transform
-
-Delete a Batch Transform Job
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use the following command to stop a batch transform job in
-Amazon SageMaker. 
-
-::
-
-    kubectl delete batchTransformJob xgboost-mnist-batch-transform
-
-Your output will look like the following:
-
-::
-
-    batchtransformjob.sagemaker.aws.amazon.com "xgboost-mnist" deleted
-
-This command removes the batch transform job from your Kubernetes
-cluster, as well as stops them in Amazon SageMaker. Jobs that have
-stopped or completed do not incur any charges for Amazon SageMaker
-resources. Delete takes about 2 minutes to clean up the resources from
-Amazon SageMaker.
-
-**Note**: Amazon SageMaker does not delete batch transform jobs. Stopped
-jobs continue to show on the Amazon SageMaker console. 
-
-Real-time inference
-~~~~~~~~~~~~~~~~~~~
-
-HostingDeployments support creating and deleting an endpoint, as well as
-updating an existing endpoint. The hosting deployment operator
-reconciles your specified hosting deployment job spec to Amazon
-SageMaker by creating models, endpoint-configs and endpoints in Amazon
-SageMaker. You can learn more about Amazon SageMaker inference in the
-Amazon SageMaker \ `CreateEndpoint API
-documentaiton <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__.
-
-Configure a HostingDeployment Resource
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Download the sample YAML file for the hosting deployment job using the
-following command: 
-
-::
-
-    wget https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/samples/xgboost-mnist-hostingdeployment.yaml
-
-The ``xgboost-mnist-hostingdeployment.yaml`` file has the following components that can be edited as required:
-
--  ProductionVariants. A production variant is a set of instances
-   serving a single model. Amazon SageMaker will load-balance between
-   all production variants according to set weights.
-
--  Models. A model is the containers and execution role ARN necessary to
-   serve a model. It requires at least a single container.
-
--  Containers. A container specifies the dataset and serving image. If
-   you are using your own custom algorithm instead of an algorithm
-   provided by Amazon SageMaker, the inference code must meet Amazon
-   SageMaker requirements. For more information, see `Using Your Own
-   Algorithms with Amazon
-   SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.
-
-Create a HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-To create a HostingDeployment, use \ ``kubectl`` to apply the
-file \ ``hosting.yaml`` with the following command:
-
-::
-
-    kubectl apply -f hosting.yaml
-
-Amazon SageMaker create an endpoint with the specified
-configuration. You incur charges for Amazon SageMaker resources used
-during the lifetime of your endpoint. You do not incur any charges once
-your endpoint is deleted.
-
-The creation process will take approximately 10 minutes.
-
-List HostingDeployments
-^^^^^^^^^^^^^^^^^^^^^^^
-
-To verify that the HostingDeployment was created, use the following
-command:
-
-::
-
-    kubectl get hostingdeployments
-
-Your output should look like the following:
-
-::
-
-    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   Creating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0mSMLOGS
-
-HostingDeployment Status Values
-'''''''''''''''''''''''''''''''
-
-The status field can be one of several values:
-
--  ``SynchronizingK8sJobWithSageMaker``: The operator is preparing to
-   create the endpoint.
-
--  ``ReconcilingEndpoint``: The operator is creating, updating, or
-   deleting endpoint resources. If the HostingDeployment remains in this
-   state, use \ ``kubectl describe`` to see the reason in the
-   ``Additional`` field.
-
--  ``OutOfService``: Endpoint is not available to take incoming
-   requests.
-
--  ``Creating``:
-   `CreateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__
-   is executing.
-
--  ``Updating``:
-   `UpdateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html>`__
-   or
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   is executing.
-
--  ``SystemUpdating``: Endpoint is undergoing maintenance and cannot be
-   updated or deleted or re-scaled until it has completed. This
-   maintenance operation does not change any customer-specified values
-   such as VPC config, KMS encryption, model, instance type, or instance
-   count.
-
--  ``RollingBack``: Endpoint fails to scale up or down or change its
-   variant weight and is in the process of rolling back to its previous
-   configuration. Once the rollback completes, endpoint returns to an
-   ``InService`` status. This transitional status only applies to an
-   endpoint that has autoscaling enabled and is undergoing variant
-   weight or capacity changes as part of an
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   call or when the
-   `UpdateEndpointWeightsAndCapacities <https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html>`__
-   operation is called explicitly.
-
--  ``InService``: Endpoint is available to process incoming requests.
-
--  ``Deleting``:
-   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
-   is executing.
-
--  ``Failed``: Endpoint could not be created, updated, or re-scaled. Use
-   `DescribeEndpoint:FailureReason <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeEndpoint.html#SageMaker-DescribeEndpoint-response-FailureReason>`__
-   for information about the failure.
-   `DeleteEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html>`__
-   is the only operation that can be performed on a failed endpoint.
-
-Describe a Hostingdeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-You can obtain debugging details using the \ ``describe`` kubectl verb
-by running the following command. 
-
-::
-
-    kubectl describe hostingdeployment
-
-Your output should look like the following:
-
-::
-
-    Name:         host-xgboost
-    Namespace:    default
-    Labels:       <none>
-    Annotations:  kubectl.kubernetes.io/last-applied-configuration:
-                    {"apiVersion":"sagemaker.aws.amazon.com/v1","kind":"HostingDeployment","metadata":{"annotations":{},"name":"host-xgboost","namespace":"def..."
-    API Version:  sagemaker.aws.amazon.com/v1
-    Kind:         HostingDeployment
-    Metadata:
-      Creation Timestamp:  2019-11-22T19:40:00Z
-      Finalizers:
-        sagemaker-operator-finalizer
-      Generation:        1
-      Resource Version:  4258134
-      Self Link:         /apis/sagemaker.aws.amazon.com/v1/namespaces/default/hostingdeployments/host-xgboost
-      UID:               def0e83e-0d5f-11ea-aa45-0a3507uiduid
-    Spec:
-      Containers:
-        Container Hostname:  xgboost
-        Image:               123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
-        Model Data URL:      s3://my-bucket/inference/xgboost-mnist/model.tar.gz
-      Models:
-        Containers:
-          xgboost
-        Execution Role Arn:  arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-        Name:                xgboost-model
-        Primary Container:   xgboost
-      Production Variants:
-        Initial Instance Count:  1
-        Instance Type:           ml.c5.large
-        Model Name:              xgboost-model
-        Variant Name:            all-traffic
-      Region:                    us-east-2
-    Status:
-      Creation Time:         2019-11-22T19:40:04Z
-      Endpoint Arn:          arn:aws:sagemaker:us-east-2:123456789012:endpoint/host-xgboost-def0e83e0d5f11eaaaexample
-      Endpoint Config Name:  host-xgboost-1-def0e83e0d5f11e-[93me08f6c510d5f11eaaa450ae[0mxample
-      Endpoint Name:         host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350733ba06
-      Endpoint Status:       Creating
-      Endpoint URL:          https://runtime.sagemaker.us-east-2.amazonaws.com/endpoints/host-xgboost-def0e83e0d5f11eaaaexample/invocations
-      Last Check Time:       2019-11-22T19:43:57Z
-      Last Modified Time:    2019-11-22T19:40:04Z
-      Model Names:
-        Name:   xgboost-model
-        Value:  xgboost-model-1-def0e83e0d5f11-[93mdf5cc9fd0d5f11eaaa450ae[0mxample
-    Events:     <none>
-
-The status field provides more information using the following fields:
-
--  ``Additional``: Additional information about the status of the
-   hosting deployment. This field is optional and only gets populated in
-   case of error.
-
--  ``Creation Time``: When the endpoint was created in Amazon SageMaker.
-
--  ``Endpoint ARN``: The Amazon SageMaker endpoint ARN.
-
--  ``Endpoint Config Name``: The Amazon SageMaker name of the endpoint
-   configuration.
-
--  ``Endpoint Name``: The Amazon SageMaker name of the endpoint.
-
--  ``Endpoint Status``: The Status of the endpoint.
-
--  ``Endpoint URL``: The HTTPS URL that can be used to access the
-   endpoint. For more information, see \ `Deploy a Model on Amazon
-   SageMaker Hosting
-   Services <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>`__.
-
--  ``FailureReason``: If a create, update, or delete command fails, the
-   cause will be shown here.
-
--  ``Last Check Time``: The last time the operator checked the status of
-   the endpoint.
-
--  ``Last Modified Time``: The last time the endpoint was modified.
-
--  ``Model Names``: A key-value pair of HostingDeployment model names to
-   Amazon SageMaker model names.
-
-Invoking the Endpoint
-^^^^^^^^^^^^^^^^^^^^^
-
-Once the endpoint status is \ ``InService``, you can invoke the endpoint
-in two ways: using the AWS CLI, which does authentication and URL
-request signing, or using an HTTP client like curl. If you use your own
-client, you will need to do AWSv4 URL signing and authentication on your
-own.
-
-To invoke the endpoint using the AWS CLI, run the following command.
-Make sure to replace the Region and endpoint-name with your endpoint’s
-Region and Amazon SageMaker endpoint name. This information can be
-obtained from the output of \ ``kubectl describe``.
-
-::
-
-    # Invoke the endpoint with mock input data.
-    aws sagemaker-runtime invoke-endpoint \
-      --region us-east-2 \
-      --endpoint-name <endpoint name> \
-      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
-      >(cat) \
-      --content-type text/csv > /dev/null
-
-For example, if your Region were \ ``us-east-2`` and your endpoint
-config name were \ ``host-xgboost-[93mf56b6b280d7511ea824b129926e[0mxample``,
-then the following command would invoke the endpoint:
-
-::
-
-    aws sagemaker-runtime invoke-endpoint \
-      --region us-east-2 \
-      --endpoint-name host-xgboost-[93mf56b6b280d7511ea824b1299e[0mxample \
-      --body $(seq 784 | xargs echo | sed 's/ /,/g') \
-      >(cat) \
-      --content-type text/csv > /dev/null
-    4.95847082138
-
-Here, \ ``4.95847082138`` is the prediction from the model for the mock
-data.
-
-Update HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^
-
-Once a HostingDeployment has a status of \ ``InService``, it can be
-updated. It might take about 10 minutes for HostingDeployment to be in
-service. To verify that the status is \ ``InService``, use the following
-command: 
-
-::
-
-    kubectl get hostingdeployments
-
-The HostingDeployment can be updated before the status
-is \ ``InService``. The operator will wait until the Amazon SageMaker
-endpoint is \ ``InService`` before applying the update.
-
-To apply an update, modify the \ ``hosting.yaml`` file. For example,
-change the \ ``initialInstanceCount`` field from 1 to 2 as follows:
-
-::
-
-    apiVersion: sagemaker.aws.amazon.com/v1
-    kind: HostingDeployment
-    metadata:
-      name: host-xgboost
-    spec:
-        region: us-east-2
-        productionVariants:
-            - variantName: all-traffic
-              modelName: xgboost-model
-              initialInstanceCount: 2
-              instanceType: ml.c5.large
-        models:
-            - name: xgboost-model
-              executionRoleArn: arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole
-              primaryContainer: xgboost
-              containers:
-                - xgboost
-        containers:
-            - containerHostname: xgboost
-              modelDataUrl: s3://my-bucket/inference/xgboost-mnist/model.tar.gz
-              image: 123456789012.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
-
-Save the file, then use \ ``kubectl`` to apply your update as follows.
-You should see the status change
-from \ ``InService`` to ``ReconcilingEndpoint``,
-then \ ``Updating``.
-
-::
-
-    $ kubectl apply -f hosting.yaml
-    hostingdeployment.sagemaker.aws.amazon.com/host-xgboost configured
-
-    $ kubectl get hostingdeployments
-    NAME           STATUS                SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   ReconcilingEndpoint   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m350abcdef
-
-    $ kubectl get hostingdeployments
-    NAME           STATUS     SAGEMAKER-ENDPOINT-NAME
-    host-xgboost   Updating   host-xgboost-[93mdef0e83e0d5f11eaaa450a[0m3507abcdef
-
-Amazon SageMaker deploys a new set of instances with your models,
-switches traffic to use the new instances, and drains the old instances.
-As soon as this process begins, the status becomes \ ``Updating``. After
-the update is complete, your endpoint becomes \ ``InService``. This
-process takes approximately 10 minutes.
-
-Delete the HostingDeployment
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Use \ ``kubectl`` to delete a HostingDeployment with the following
-command: 
-
-::
-
-    kubectl delete hostingdeployments host-xgboost
-
-Your output should look like the following:
-
-::
-
-    hostingdeployment.sagemaker.aws.amazon.com "host-xgboost" deleted
-
-To verify that the hosting deployment has been deleted, use the
-following command:
-
-::
-
-    kubectl get hostingdeployments
-    No resources found.
-
-Endpoints that have been deleted do not incur any charges for
-Amazon SageMaker resources.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-10-01 11:33:57[0m
[92mHash: 3ce2d9584d60e4cbb4163087a27ee165f01820ef[0m
[92mFilepath: tests/unit/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: Revert "fix issue-987 error by adding instance_type in endpoint_name (#1058)" (#1070)

[0m
@@ -26,7 +26,6 @@ from mock import MagicMock, Mock, patch
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
 ENTRY_POINT = "blah.py"
-INSTANCE_TYPE = "p2.xlarge"
 ROLE = "some-role"
 
 DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
@@ -40,6 +39,7 @@ ACCELERATOR_TYPE = "ml.eia.medium"
 IMAGE_NAME = "fakeimage"
 REGION = "us-west-2"
 MODEL_NAME = "{}-{}".format(MODEL_IMAGE, TIMESTAMP)
+ENDPOINT_NAME = "{}-{}".format(MODEL_NAME, INSTANCE_TYPE.replace(".", "-"))
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
@@ -210,7 +210,7 @@ def test_deploy(sagemaker_session, tmpdir):
     model = DummyFrameworkModel(sagemaker_session, source_dir=str(tmpdir))
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        MODEL_NAME,
+        ENDPOINT_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -255,7 +255,7 @@ def test_deploy_tags(sagemaker_session, tmpdir):
     tags = [{"ModelName": "TestModel"}]
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1, tags=tags)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        MODEL_NAME,
+        ENDPOINT_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -280,7 +280,7 @@ def test_deploy_accelerator_type(tfo, time, sagemaker_session):
         instance_type=INSTANCE_TYPE, initial_instance_count=1, accelerator_type=ACCELERATOR_TYPE
     )
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        MODEL_NAME,
+        ENDPOINT_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -305,7 +305,7 @@ def test_deploy_kms_key(tfo, time, sagemaker_session):
     model = DummyFrameworkModel(sagemaker_session)
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1, kms_key=key)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        MODEL_NAME,
+        ENDPOINT_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -350,7 +350,7 @@ def test_deploy_update_endpoint(sagemaker_session, tmpdir):
         accelerator_type=ACCELERATOR_TYPE,
     )
     sagemaker_session.create_endpoint_config.assert_called_with(
-        name=model.name,
+        name=endpoint_name,
         model_name=model.name,
         initial_instance_count=INSTANCE_COUNT,
         instance_type=INSTANCE_TYPE,
@@ -359,7 +359,7 @@ def test_deploy_update_endpoint(sagemaker_session, tmpdir):
         kms_key=None,
     )
     config_name = sagemaker_session.create_endpoint_config(
-        name=model.name,
+        name=endpoint_name,
         model_name=model.name,
         initial_instance_count=INSTANCE_COUNT,
         instance_type=INSTANCE_TYPE,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-09-30 18:10:43[0m
[92mHash: a907597880cbaaa5520f38256bba4cbc63d4295c[0m
[92mFilepath: tests/unit/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix issue-987 error by adding instance_type in endpoint_name (#1058)

[0m
@@ -26,6 +26,7 @@ from mock import MagicMock, Mock, patch
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
 ENTRY_POINT = "blah.py"
+INSTANCE_TYPE = "p2.xlarge"
 ROLE = "some-role"
 
 DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
@@ -39,7 +40,6 @@ ACCELERATOR_TYPE = "ml.eia.medium"
 IMAGE_NAME = "fakeimage"
 REGION = "us-west-2"
 MODEL_NAME = "{}-{}".format(MODEL_IMAGE, TIMESTAMP)
-ENDPOINT_NAME = "{}-{}".format(MODEL_NAME, INSTANCE_TYPE.replace(".", "-"))
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
@@ -210,7 +210,7 @@ def test_deploy(sagemaker_session, tmpdir):
     model = DummyFrameworkModel(sagemaker_session, source_dir=str(tmpdir))
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        ENDPOINT_NAME,
+        MODEL_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -255,7 +255,7 @@ def test_deploy_tags(sagemaker_session, tmpdir):
     tags = [{"ModelName": "TestModel"}]
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1, tags=tags)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        ENDPOINT_NAME,
+        MODEL_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -280,7 +280,7 @@ def test_deploy_accelerator_type(tfo, time, sagemaker_session):
         instance_type=INSTANCE_TYPE, initial_instance_count=1, accelerator_type=ACCELERATOR_TYPE
     )
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        ENDPOINT_NAME,
+        MODEL_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -305,7 +305,7 @@ def test_deploy_kms_key(tfo, time, sagemaker_session):
     model = DummyFrameworkModel(sagemaker_session)
     model.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=1, kms_key=key)
     sagemaker_session.endpoint_from_production_variants.assert_called_with(
-        ENDPOINT_NAME,
+        MODEL_NAME,
         [
             {
                 "InitialVariantWeight": 1,
@@ -350,7 +350,7 @@ def test_deploy_update_endpoint(sagemaker_session, tmpdir):
         accelerator_type=ACCELERATOR_TYPE,
     )
     sagemaker_session.create_endpoint_config.assert_called_with(
-        name=endpoint_name,
+        name=model.name,
         model_name=model.name,
         initial_instance_count=INSTANCE_COUNT,
         instance_type=INSTANCE_TYPE,
@@ -359,7 +359,7 @@ def test_deploy_update_endpoint(sagemaker_session, tmpdir):
         kms_key=None,
     )
     config_name = sagemaker_session.create_endpoint_config(
-        name=endpoint_name,
+        name=model.name,
         model_name=model.name,
         initial_instance_count=INSTANCE_COUNT,
         instance_type=INSTANCE_TYPE,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-09-09 16:45:01[0m
[92mHash: 697732bfb1310b6c4407603b0f2bfb34eaaea018[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: clean up git support integ tests (#1032)

[0m
@@ -1,4 +1,4 @@
-# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
+# Copyright 2017-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License"). You
 # may not use this file except in compliance with the License. A copy of
@@ -24,6 +24,7 @@ from sagemaker.mxnet.estimator import MXNet
 from sagemaker.pytorch.defaults import PYTORCH_VERSION
 from sagemaker.pytorch.estimator import PyTorch
 from sagemaker.sklearn.estimator import SKLearn
+from sagemaker.mxnet.model import MXNetModel
 from sagemaker.sklearn.model import SKLearnModel
 from tests.integ import DATA_DIR, PYTHON_VERSION
 
@@ -32,27 +33,29 @@ MNIST_FOLDER_NAME = "MNIST"
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-
 PRIVATE_GIT_REPO = "https://github.com/git-support-test/test-git.git"
 PRIVATE_BRANCH = "master"
 PRIVATE_COMMIT = "[93ma46d6f9add3532ca3e4e231e4108b6bad15b7373[0m"
-
 PRIVATE_GIT_REPO_2FA = "https://github.com/git-support-test-2fa/test-git.git"
 PRIVATE_GIT_REPO_2FA_SSH = "git@github.com:git-support-test-2fa/test-git.git"
 PRIVATE_BRANCH_2FA = "master"
 PRIVATE_COMMIT_2FA = "[93m52381dee030eb332a7e42d9992878d7261eb21d4[0m"
-
 CODECOMMIT_REPO = (
     "https://git-codecommit.us-west-2.amazonaws.com/v1/repos/sagemaker-python-sdk-git-testing-repo/"
 )
 CODECOMMIT_BRANCH = "master"
 
+# Since personal access tokens will delete themselves if they are committed to GitHub repos,
+# we cannot hard code them here, but have to encrypt instead
+ENCRYPTED_PRIVATE_REPO_TOKEN = "e-4_1-1dc_71-f0e_[93mf7b54a0f3b7db2757163da7b5e8c3[0m"
+PRIVATE_REPO_TOKEN = ENCRYPTED_PRIVATE_REPO_TOKEN.replace("-", "").replace("_", "")
+
 # endpoint tests all use the same port, so we use this lock to prevent concurrent execution
 LOCK_PATH = os.path.join(tempfile.gettempdir(), "sagemaker_test_git_lock")
 
 
 @pytest.mark.local_mode
-def test_github(sagemaker_local_session):
+def test_git_support_with_pytorch(sagemaker_local_session):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "pytorch_mnist")
     git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
@@ -82,7 +85,7 @@ def test_github(sagemaker_local_session):
 
 @pytest.mark.local_mode
 @pytest.mark.skip("needs a secure authentication approach")
-def test_private_github(sagemaker_local_session):
+def test_git_support_with_mxnet(sagemaker_local_session):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")
     git_config = {
@@ -91,7 +94,7 @@ def test_private_github(sagemaker_local_session):
         "commit": PRIVATE_COMMIT,
         "2FA_enabled": False,
         "username": "git-support-test",
-        "password": "",  # TODO: find a secure approach
+        "password": "",  # TODO: find a more secure approach
     }
     source_dir = "mxnet"
     dependencies = ["foo/bar.py"]
@@ -123,8 +126,21 @@ def test_private_github(sagemaker_local_session):
     with lock.lock(LOCK_PATH):
         try:
             serving_script_path = "mnist_hosting_with_custom_handlers.py"
-            predictor = mx.deploy(1, "local", entry_point=serving_script_path)
-
+            client = sagemaker_local_session.sagemaker_client
+            desc = client.describe_training_job(TrainingJobName=mx.latest_training_job.name)
+            model_data = desc["ModelArtifacts"]["S3ModelArtifacts"]
+            model = MXNetModel(
+                model_data,
+                "SageMakerRole",
+                entry_point=serving_script_path,
+                source_dir=source_dir,
+                dependencies=dependencies,
+                py_version=PYTHON_VERSION,
+                sagemaker_session=sagemaker_local_session,
+                framework_version=MXNet.LATEST_VERSION,
+                git_config=git_config,
+            )
+            predictor = model.deploy(initial_instance_count=1, instance_type="local")
             data = numpy.zeros(shape=(1, 1, 28, 28))
             result = predictor.predict(data)
             assert result is not None
@@ -132,9 +148,9 @@ def test_private_github(sagemaker_local_session):
             predictor.delete_endpoint()
 
 
+@pytest.mark.skipif(PYTHON_VERSION != "py3", reason="Scikit-learn image supports only python 3.")
 @pytest.mark.local_mode
-@pytest.mark.skip("needs a secure authentication approach")
-def test_private_github_with_2fa(sagemaker_local_session, sklearn_full_version):
+def test_git_support_with_sklearn(sagemaker_local_session, sklearn_full_version):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "sklearn_mnist")
     git_config = {
@@ -142,15 +158,14 @@ def test_private_github_with_2fa(sagemaker_local_session, sklearn_full_version):
         "branch": PRIVATE_BRANCH_2FA,
         "commit": PRIVATE_COMMIT_2FA,
         "2FA_enabled": True,
-        "token": "",  # TODO: find a secure approach
+        "token": PRIVATE_REPO_TOKEN,
     }
     source_dir = "sklearn"
-
     sklearn = SKLearn(
         entry_point=script_path,
         role="SageMakerRole",
         source_dir=source_dir,
-        py_version="py3",  # Scikit-learn supports only Python 3
+        py_version=PYTHON_VERSION,
         train_instance_count=1,
         train_instance_type="local",
         sagemaker_session=sagemaker_local_session,
@@ -187,7 +202,9 @@ def test_private_github_with_2fa(sagemaker_local_session, sklearn_full_version):
 
 
 @pytest.mark.local_mode
-def test_github_with_ssh_passphrase_not_configured(sagemaker_local_session, sklearn_full_version):
+def test_git_support_with_sklearn_ssh_passphrase_not_configured(
+    sagemaker_local_session, sklearn_full_version
+):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "sklearn_mnist")
     git_config = {
@@ -196,12 +213,11 @@ def test_github_with_ssh_passphrase_not_configured(sagemaker_local_session, skle
         "commit": PRIVATE_COMMIT_2FA,
     }
     source_dir = "sklearn"
-
     sklearn = SKLearn(
         entry_point=script_path,
         role="SageMakerRole",
         source_dir=source_dir,
-        py_version="py3",  # Scikit-learn supports only Python 3
+        py_version=PYTHON_VERSION,
         train_instance_count=1,
         train_instance_type="local",
         sagemaker_session=sagemaker_local_session,
@@ -211,7 +227,6 @@ def test_github_with_ssh_passphrase_not_configured(sagemaker_local_session, skle
     )
     train_input = "file://" + os.path.join(data_path, "train")
     test_input = "file://" + os.path.join(data_path, "test")
-
     with pytest.raises(subprocess.CalledProcessError) as error:
         sklearn.fit({"train": train_input, "test": test_input})
     assert "returned non-zero exit status" in str(error)
@@ -219,7 +234,7 @@ def test_github_with_ssh_passphrase_not_configured(sagemaker_local_session, skle
 
 @pytest.mark.local_mode
 @pytest.mark.skip("needs a secure authentication approach")
-def test_codecommit(sagemaker_local_session):
+def test_git_support_codecommit_with_mxnet(sagemaker_local_session):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")
     git_config = {
@@ -257,7 +272,21 @@ def test_codecommit(sagemaker_local_session):
 
     with lock.lock(LOCK_PATH):
         try:
-            predictor = mx.deploy(1, "local")
+            client = sagemaker_local_session.sagemaker_client
+            desc = client.describe_training_job(TrainingJobName=mx.latest_training_job.name)
+            model_data = desc["ModelArtifacts"]["S3ModelArtifacts"]
+            model = MXNetModel(
+                model_data,
+                "SageMakerRole",
+                entry_point=script_path,
+                source_dir=source_dir,
+                dependencies=dependencies,
+                py_version=PYTHON_VERSION,
+                sagemaker_session=sagemaker_local_session,
+                framework_version=MXNet.LATEST_VERSION,
+                git_config=git_config,
+            )
+            predictor = model.deploy(1, "local")
 
             data = numpy.zeros(shape=(1, 1, 28, 28))
             result = predictor.predict(data)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-09-06 18:07:30[0m
[92mHash: 1316b6361ee6ab227b01e7e6fed020c3d74756c3[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: remove hardcoded creds from integ test (#1029)

[0m
@@ -233,7 +233,6 @@ def test_git_support_with_sklearn_ssh_passphrase_not_configured(
 
 
 @pytest.mark.local_mode
-@pytest.mark.skip("needs a secure authentication approach")
 def test_git_support_codecommit_with_mxnet(sagemaker_local_session):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")
@@ -241,7 +240,7 @@ def test_git_support_codecommit_with_mxnet(sagemaker_local_session):
         "repo": CODECOMMIT_REPO,
         "branch": CODECOMMIT_BRANCH,
         "username": "GitTest-at-142577830533",
-        "password": "",  # TODO: assume a role to get temporary credentials
+        "password": "[93m22LcZpWMtjpDG3fbOuHPooIoKoRxF36rQj7zdUvXooA=[0m",
     }
     source_dir = "mxnet"
     dependencies = ["foo/bar.py"]

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-08-16 18:07:11[0m
[92mHash: cdd56e6f2744fdbcedb069627d45712f4efd5eb4[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: eliminate dependency on mnist dataset website (#986)

This commit will modify the folder structure to avoid needing to
download the datasets and relying on the dataset website from being
available.
This caused a canary to fail.

The dependency has existed since Pytorch 1.1 upgrade, as the directory
structure was modified in https://github.com/pytorch/vision/pull/601
[0m
@@ -28,8 +28,6 @@ from sagemaker.mxnet.model import MXNetModel
 from sagemaker.sklearn.model import SKLearnModel
 from tests.integ import DATA_DIR, PYTHON_VERSION
 
-MNIST_FOLDER_NAME = "MNIST"
-
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
@@ -71,7 +69,7 @@ def test_git_support_with_pytorch(sagemaker_local_session):
         git_config=git_config,
     )
 
-    pytorch.fit({"training": "file://" + os.path.join(data_path, "training", MNIST_FOLDER_NAME)})
+    pytorch.fit({"training": "file://" + os.path.join(data_path, "training")})
 
     with lock.lock(LOCK_PATH):
         try:

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-08-08 17:53:40[0m
[92mHash: c38829e51252772f1d7aa37430a8adbb0e8349bd[0m
[92mFilepath: CONTRIBUTING.md[0m
[92mBranch: origin/master[0m
[92mCommit: rework CONTRIBUTING.md to include a development workflow (#973)

* doc: rework CONTRIBUTING.md to include a development workflow

This commit covers adding a development workflow to ease
contributions.

This commit also includes:
* Refactoring/formatting.
* Adding a Table of Contents.
* Capitalization changes.
* Consistency changes.
* Updating PULL_REQUEST_TEMPLATE to reflect changes.
[0m
@@ -7,108 +7,68 @@ Please read through this document before submitting any issues or pull requests
 information to effectively respond to your bug report or contribution.
 
 
-## Table of Contents
-
-* [Table of Contents](#table-of-contents)
-* [Reporting Bugs/Feature Requests](#reporting-bugsfeature-requests)
-* [Contributing via Pull Requests (PRs)](#contributing-via-pull-requests-prs)
-  * [Setting up Your Development Environment *[Optional, but Recommended]*](#setting-up-your-development-environment-optional-but-recommended)  
-  * [Pulling Down the Code](#pulling-down-the-code)
-  * [Running the Unit Tests](#running-the-unit-tests)
-  * [Running the Integration Tests](#running-the-integration-tests)
-  * [Making and Testing Your Change](#making-and-testing-your-change)
-  * [Committing Your Change](#committing-your-change)
-  * [Sending a Pull Request](#sending-a-pull-request)
-* [Finding Contributions to Work On](#finding-contributions-to-work-on)
-* [Code of Conduct](#code-of-conduct)
-* [Security Issue Notifications](#security-issue-notifications)
-* [Licensing](#licensing)
-
 ## Reporting Bugs/Feature Requests
 
 We welcome you to use the GitHub issue tracker to report bugs or suggest features.
 
-When filing an issue, please check [existing open](https://github.com/aws/sagemaker-python-sdk/issues) and [recently closed](https://github.com/aws/sagemaker-python-sdk/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aclosed%20) issues to make sure somebody else hasn't already
+When filing an issue, please check [existing open](https://github.com/aws/sagemaker-python-sdk/issues), or [recently closed](https://github.com/aws/sagemaker-python-sdk/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aclosed%20), issues to make sure somebody else hasn't already
 reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:
 
-* A reproducible test case or series of steps.
-* The version of our code being used.
-* Any modifications you've made relevant to the bug.
-* A description of your environment or deployment.
-
-
-## Contributing via Pull Requests (PRs)
-
-Contributions via pull requests are much appreciated.
+* A reproducible test case or series of steps
+* The version of our code being used
+* Any modifications you've made relevant to the bug
+* A description of your environment or deployment
+
+## Setting up your development environment [optional, but recommended]
+
+* Set up the Cloud9 environment:
+  * Instance type: You'll need at least 4 GB of RAM to avoid running into memory issues. We recommend at least a t3.medium to run the unit tests. Larger hosts will reduce the chance of encountering resource limits.
+  * Follow the instructions at [Creating a Cloud9 EC2 Environment](https://docs.aws.amazon.com/cloud9/latest/user-guide/create-environment.html#create-environment-main) to set up a Cloud9 EC2 environment
+* Expand the storage of the EC2 instance from 10GB to 20GB
+  * Because you'll need a minimum of 11GB of disk storage on the EC2 instance to run the repository's unit tests, you'll need to expand your EC2 volume size. We recommend at least 20GB. A larger volume will reduce the chance of encountering resource limits. 
+  * Follow the instructions at [Modifying an EBS Volume Using Elastic Volumes (Console)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html#modify-ebs-volume) to increase the EBS volume size associated with the newly created EC2 instance.
+  * Wait 5-10min for the new EBS volume increase to take effect.
+  * Allow EC2 to claim the additional space by stopping and then starting your EC2 host.
+* Create a fork of this repository on GitHub. You should end up with a fork at `https://github.com/<username>/sagemaker-python-sdk`
+  * Follow the instructions at [Fork a repo](https://help.github.com/en/articles/fork-a-repo) to fork a GitHub repository.
+* In the Cloud9 UI, pull down this repository by clicking on "Clone from Github" or running the following command in the Cloud9 terminal: `git clone https://github.com/<username>/sagemaker-python-sdk` where <username> is your github username.
+* Install tox using `pip install tox`
+* Install coverage using `pip install .[test]`
+* cd into the sagemaker-python-sdk folder: `cd sagemaker-python-sdk` or `cd /environment/sagemaker-python-sdk`
+* Run the following tox command and verify that all unit tests pass: `tox tests/unit`
+
+## Contributing via Pull Requests
+Contributions via pull requests are much appreciated. 
+
+You can use the following commands to setup your developing and testing environment after you fork sagemaker-python-sdk repository:
+
+```bash
+git clone git@github.com:<your-github-username>/sagemaker-python-sdk.git
+cd sagemaker-python-sdk
+pip install -U .
+pip install -U .[test]
+```
 
 Before sending us a pull request, please ensure that:
 
-* You are working against the latest source on the *master* branch.
-* You check the existing open and recently merged pull requests to make sure someone else hasn't already addressed the problem.
-* You open an issue to discuss any significant work - we would hate for your time to be wasted.
-
-
-### Setting up Your Development Environment *[Optional, but Recommended]*
-
-1. Set up the Cloud9 environment:
-   1. Instance type: You'll need at least 4 GB of RAM to avoid running into memory issues. We recommend at least a t3.medium to run the unit tests. A larger host will reduce the chance of encountering resource limits.
-   1. Follow the instructions at [Creating a Cloud9 EC2 Environment](https://docs.aws.amazon.com/cloud9/latest/user-guide/create-environment.html#create-environment-main) to set up a Cloud9 EC2 environment.
-1. Expand the storage of the EC2 instance from 10GB to 20GB:
-   1. Because you'll need a minimum of 11GB of disk storage on the EC2 instance to run the repository's unit tests, you'll need to expand your EC2 volume size. We recommend at least 20GB. A larger volume will reduce the chance of encountering resource limits. 
-   1. Follow the instructions at [Modifying an EBS Volume Using Elastic Volumes (Console)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html#modify-ebs-volume) to increase the EBS volume size associated with the newly created EC2 instance.
-   1. Wait 5-10min for the new EBS volume increase to finalize.
-   1. Allow EC2 to claim the additional space by stopping and then starting your EC2 host.
-
-
-### Pulling Down the Code
-
-1. If you do not already have one, create a GitHub account by following the prompts at [Join Github](https://github.com/join).
-1. Create a fork of this repository on GitHub. You should end up with a fork at `https://github.com/<username>/sagemaker-python-sdk`.
-   1. Follow the instructions at [Fork a Repo](https://help.github.com/en/articles/fork-a-repo) to fork a GitHub repository.
-1. Clone your fork of the repository: `git clone https://github.com/<username>/sagemaker-python-sdk` where `<username>` is your github username.
-
-
-### Running the Unit Tests
-
-1. Install tox using `pip install tox`
-1. Install coverage using `pip install .[test]`
-1. cd into the sagemaker-python-sdk folder: `cd sagemaker-python-sdk` or `cd /environment/sagemaker-python-sdk`
-1. Run the following tox command and verify that all code checks and unit tests pass: `tox tests/unit`
-
-You can also run a single test with the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`  
-  * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
-  * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/unit/test_estimator.py::test_sagemaker_model_s3_uri_invalid ; unset IGNORE_COVERAGE`
+1. You are working against the latest source on the *master* branch.
+2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.
+3. You open an issue to discuss any significant work - we would hate for your time to be wasted.
 
+To send us a pull request, please:
 
-### Running the Integration Tests
+1. Fork the repository.
+2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.
+3. Include unit tests when you contribute new features or make bug fixes, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost.
+4. Ensure local tests pass.
+5. Use commit messages (and PR titles) that follow [these guidelines](#commit-message-guidelines).
+6. Send us a pull request, answering any default questions in the pull request interface.
+7. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.
 
-Our CI system runs integration tests (the ones in the `tests/integ` directory), in parallel, for every Pull Request.  
-You should only worry about manually running any new integration tests that you write, or integration tests that test an area of code that you've modified.  
+GitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and
+[creating a pull request](https://help.github.com/articles/creating-a-pull-request/).
 
-1. Follow the instructions at [Set Up the AWS Command Line Interface (AWS CLI)](https://docs.aws.amazon.com/polly/latest/dg/setup-aws-cli.html).
-1. To run a test, specify the test file and method you want to run per the following command: `tox -e py36 -- -s -vv <path_to_file><file_name>::<test_function_name>`
-   * Note that the coverage test will fail if you only run a single test, so make sure to surround the command with `export IGNORE_COVERAGE=-` and `unset IGNORE_COVERAGE`
-   * Example: `export IGNORE_COVERAGE=- ; tox -e py36 -- -s -vv tests/integ/test_tf_script_mode.py::test_mnist ; unset IGNORE_COVERAGE`
-
-If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m[0m/tests/integ/test_tfs.py#L37) and 
-[here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.
-
-
-### Making and Testing Your Change
-
-1. Create a new git branch:
-     ```shell
-     git checkout -b my-fix-branch master
-     ```
-1. Make your changes, **including unit tests** and, if appropriate, integration tests.
-   1. Include unit tests when you contribute new features or make bug fixes, as they help to:
-      1. Prove that your code works correctly.
-      1. Guard against future breaking changes to lower the maintenance cost.
-   1. Please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.
-1. Run all the unit tests as per [Running the Unit Tests](#running-the-unit-tests), and verify that all checks and tests pass.
-
-
-### Committing Your Change
+### Commit message guidelines
 
 We use commit messages to update the project version number and generate changelog entries, so it's important for them to follow the right format. Valid commit messages include a prefix, separated from the rest of the message by a colon and a space. Here are a few examples:
 
@@ -121,7 +81,7 @@ documentation: add MXNet documentation
 Valid prefixes are listed in the table below.
 
 | Prefix          | Use for...                                                                                     |
-|----------------:|:-----------------------------------------------------------------------------------------------|
+|-----------------|------------------------------------------------------------------------------------------------|
 | `breaking`      | Incompatible API changes.                                                                      |
 | `deprecation`   | Deprecating an existing API or feature, or removing something that was previously deprecated.  |
 | `feature`       | Adding a new feature.                                                                          |
@@ -129,35 +89,26 @@ Valid prefixes are listed in the table below.
 | `change`        | Any other code change.                                                                         |
 | `documentation` | Documentation changes.                                                                         |
 
-Some of the prefixes allow abbreviation ; `break`, `feat`, `depr`, and `doc` are all valid. If you omit a prefix, the commit will be treated as a `change`.
+Some of the prefixes allow abbreviation -- `break`, `feat`, `depr`, and `doc` are all valid. If you omit a prefix, the commit will be treated as a `change`.
 
 For the rest of the message, use imperative style and keep things concise but informative. See [How to Write a Git Commit Message](https://chris.beams.io/posts/git-commit/) for guidance.
 
+### Integration tests
 
-### Sending a Pull Request
-
-GitHub provides additional document on [Creating a Pull Request](https://help.github.com/articles/creating-a-pull-request/).
-
-Please remember to:
-* Use commit messages (and PR titles) that follow the guidelines under [Committing Your Change](#committing-your-change).
-* Send us a pull request, answering any default questions in the pull request interface.
-* Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.
-
-
-## Finding Contributions to Work On
+Our CI system runs integration tests (the ones in the `tests/integ` directory) in parallel. If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or an endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m[0m/tests/integ/test_tfs.py#L37) and 
+[here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.
 
+## Finding contributions to work on
 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any ['help wanted'](https://github.com/aws/sagemaker-python-sdk/labels/help%20wanted) issues is a great place to start.
 
 
 ## Code of Conduct
-
 This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).
 For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact
 opensource-codeofconduct@amazon.com with any additional questions or comments.
 
 
-## Security Issue Notifications
-
+## Security issue notifications
 If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.
 
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-19 18:34:42[0m
[92mHash: af4b3e3f271d886f1125fab6f6c78c8b4d71c7b3[0m
[92mFilepath: doc/overview.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: update using_tensorflow topic (#946)

[0m
@@ -826,6 +826,59 @@ A new training job channel, named ``code``, will be added with that S3 URI.  Bef
 
 Once the training job begins, the training container will look at the offline input ``code`` channel to install dependencies and run the entry script. This isolates the training container, so no inbound or outbound network calls can be made.
 
+*********
+BYO Model
+*********
+
+You can also create an endpoint from an existing model rather than training one.
+That is, you can bring your own model:
+
+First, package the files for the trained model into a ``.tar.gz`` file, and upload the archive to S3.
+
+Next, create a ``Model`` object that corresponds to the framework that you are using: `MXNetModel <https://sagemaker.readthedocs.io/en/stable/sagemaker.mxnet.html#mxnet-model>`__ or `TensorFlowModel <https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html#tensorflow-model>`__.
+
+Example code using ``MXNetModel``:
+
+.. code:: python
+
+   from sagemaker.mxnet.model import MXNetModel
+
+   sagemaker_model = MXNetModel(model_data='s3://path/to/model.tar.gz',
+                                role='arn:aws:iam::accid:sagemaker-role',
+                                entry_point='entry_point.py')
+
+After that, invoke the ``deploy()`` method on the ``Model``:
+
+.. code:: python
+
+   predictor = sagemaker_model.deploy(initial_instance_count=1,
+                                      instance_type='ml.m4.xlarge')
+
+This returns a predictor the same way an ``Estimator`` does when ``deploy()`` is called. You can now get inferences just like with any other model deployed on Amazon SageMaker.
+
+Git support is also available when you bring your own model, through which you can use inference scripts stored in your
+Git repositories. The process is similar to using Git support for training jobs. You can simply provide ``git_config``
+when create the ``Model`` object, and let ``entry_point``, ``source_dir`` and ``dependencies`` (if needed) be relative
+paths inside the Git repository:
+
+.. code:: python
+
+    git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
+                  'branch': 'branch1',
+                  'commit': '[93m4893e528afa4a790331e1b5286954f073b0f14a2[0m'}
+
+    sagemaker_model = MXNetModel(model_data='s3://path/to/model.tar.gz',
+                                role='arn:aws:iam::accid:sagemaker-role',
+                                entry_point='inference.py',
+                                source_dir='mxnet',
+                                git_config=git_config)
+
+A full example is available in the `Amazon SageMaker examples repository <https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/mxnet_mnist_byom>`__.
+
+You can also find this notebook in the **Advanced Functionality** section of the **SageMaker Examples** section in a notebook instance.
+For information about using sample notebooks in a SageMaker notebook instance, see `Use Example Notebooks <https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-nbexamples.html>`__
+in the AWS documentation.
+
 *******************
 Inference Pipelines
 *******************

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-19 18:34:42[0m
[92mHash: af4b3e3f271d886f1125fab6f6c78c8b4d71c7b3[0m
[92mFilepath: doc/using_tf.rst[0m
[92mBranch: origin/master[0m
[92mCommit: doc: update using_tensorflow topic (#946)

[0m
@@ -1,985 +1,501 @@
-##############################################
-Using TensorFlow with the SageMaker Python SDK
-##############################################
-
-TensorFlow SageMaker Estimators allow you to run your own TensorFlow
-training algorithms on SageMaker Learner, and to host your own TensorFlow
-models on SageMaker Hosting.
-
-For general information about using the SageMaker Python SDK, see :ref:`overview:Using the SageMaker Python SDK`.
-
-.. warning::
-    We have added a new format of your TensorFlow training script with TensorFlow version 1.11.
-    This new way gives the user script more flexibility.
-    This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions.
-    In addition we are adding Python 3 support with Script Mode.
-    The last supported version of Legacy Mode will be TensorFlow 1.12.
-    Script Mode is available with TensorFlow version 1.11 and newer.
-    Make sure you refer to the correct version of this README when you prepare your script.
-    You can find the Legacy Mode README `here <https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#tensorflow-sagemaker-estimators-and-models>`_.
-
-.. contents::
-
-Supported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``.
-
-
-*****************************
-Train a Model with TensorFlow
-*****************************
-
-To train a TensorFlow model by using the SageMaker Python SDK:
-
-.. |create tf estimator| replace:: Create a ``sagemaker.tensorflow.TensorFlow estimator``
-.. _create tf estimator: #create-an-estimator
-
-.. |call fit| replace:: Call the estimator's ``fit`` method
-.. _call fit: #call-the-fit-method
-
-1. `Prepare a training script <#prepare-a-script-mode-training-script>`_
-2. |create tf estimator|_
-3. |call fit|_
-
-Prepare a Script Mode Training Script
-======================================
-
-Your TensorFlow training script must be a Python 2.7- or 3.6-compatible source file.
-
-The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
-
-* ``SM_MODEL_DIR``: A string that represents the local path where the training job writes the model artifacts to.
-  After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the ``model_dir``
-  argument passed in your training script, which is an S3 location. ``SM_MODEL_DIR`` is always set to ``/opt/ml/model``.
-* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
-* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write output artifacts to.
-  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
-  These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.
-* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
-  For example, if you specify two input channels in the TensorFlow estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
-
-For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`_.
-
-A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``SM_CHANNEL_TRAIN`` so that it can be deployed for inference later.
-Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
-For example, a training script might start with the following:
-
-.. code:: python
-
-    import argparse
-    import os
-
-    if __name__ =='__main__':
-
-        parser = argparse.ArgumentParser()
-
-        # hyperparameters sent by the client are passed as command-line arguments to the script.
-        parser.add_argument('--epochs', type=int, default=10)
-        parser.add_argument('--batch_size', type=int, default=100)
-        parser.add_argument('--learning_rate', type=float, default=0.1)
-
-        # input data and model directories
-        parser.add_argument('--model_dir', type=str)
-        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
-        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
-
-        args, _ = parser.parse_known_args()
-
-        # ... load from args.train and args.test, train a model, write model to args.model_dir.
-
-Because the SageMaker imports your training script, putting your training launching code in a main guard (``if __name__=='__main__':``)
-is good practice.
-
-Note that SageMaker doesn't support argparse actions.
-For example, if you want to use a boolean hyperparameter, specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when you create the TensorFlow estimator.
-
-For a complete example of a TensorFlow training script, see `mnist.py <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/mnist.py>`__.
-
-   
-Adapting your local TensorFlow script
--------------------------------------
-
-If you have a TensorFlow training script that runs outside of SageMaker, do the following to adapt the script to run in SageMaker:
-
-1. Make sure your script can handle ``--model_dir`` as an additional command line argument. If you did not specify a
-location when you created the TensorFlow estimator, an S3 location under the default training job bucket is used.
-Distributed training with parameter servers requires you to use the ``tf.estimator.train_and_evaluate`` API and
-to provide an S3 location as the model directory during training. Here is an example:
-
-.. code:: python
-
-    estimator = tf.estimator.Estimator(model_fn=my_model_fn, model_dir=args.model_dir)
-    ...
-    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
-    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
-    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)
-
-2. Load input data from the input channels. The input channels are defined when ``fit`` is called. For example:
-
-.. code:: python
-
-    estimator.fit({'train':'s3://my-bucket/my-training-data',
-                  'eval':'s3://my-bucket/my-evaluation-data'})
-
-In your training script the channels will be stored in environment variables ``SM_CHANNEL_TRAIN`` and
-``SM_CHANNEL_EVAL``. You can add them to your argument parsing logic like this:
-
-.. code:: python
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
-    parser.add_argument('--eval', type=str, default=os.environ.get('SM_CHANNEL_EVAL'))
-
-3. Export your final model to path stored in environment variable ``SM_MODEL_DIR`` which should always be
-   ``/opt/ml/model``. At end of training SageMaker will upload the model file under ``/opt/ml/model`` to
-   ``output_path``.
-
-
-Create an Estimator
-===================
-
-After you create your training script, create an instance of the :class:`sagemaker.tensorflow.TensorFlow` estimator.
-
-To use Script Mode, set at least one of these args
-
-- ``py_version='py3'``
-- ``script_mode=True``
-
-When using Script Mode, your training script needs to accept the following args:
-
-- ``model_dir``
-
-The following args are not permitted when using Script Mode:
-
-- ``checkpoint_path``
-- ``training_steps``
-- ``evaluation_steps``
-- ``requirements_file``
-
-.. code:: python
-
-  from sagemaker.tensorflow import TensorFlow
-
-  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                            train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                            framework_version='1.12', py_version='py3')
-  tf_estimator.fit('s3://bucket/path/to/training/data')
-
-Where the S3 url is a path to your training data within Amazon S3.
-The constructor keyword arguments define how SageMaker runs your training script.
-
-For more information about the sagemaker.tensorflow.TensorFlow estimator, see `sagemaker.tensorflow.TensorFlow Class`_.
-
-Call the fit Method
-===================
-
-You start your training script by calling the ``fit`` method on a ``TensorFlow`` estimator. ``fit`` takes
-both required and optional arguments.
-
-Required arguments
-------------------
-
-- ``inputs``: The S3 location(s) of datasets to be used for training. This can take one of two forms:
-
-  - ``str``: An S3 URI, for example ``s3://my-bucket/my-training-data``, which indicates the dataset's location.
-  - ``dict[str, str]``: A dictionary mapping channel names to S3 locations, for example ``{'train': 's3://my-bucket/my-training-data/train', 'test': 's3://my-bucket/my-training-data/test'}``
-  - ``sagemaker.session.s3_input``: channel configuration for S3 data sources that can provide additional information as well as the path to the training dataset. See `the API docs <https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input>`_ for full details.
-
-Optional arguments
-------------------
-
-- ``wait (bool)``: Defaults to True, whether to block and wait for the
-  training script to complete before returning.
-  If set to False, it will return immediately, and can later be attached to.
-- ``logs (bool)``: Defaults to True, whether to show logs produced by training
-  job in the Python session. Only meaningful when wait is True.
-- ``run_tensorboard_locally (bool)``: Defaults to False. If set to True a Tensorboard command will be printed out.
-- ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
-  based on the training image name and current timestamp.
-
-What happens when fit is called
--------------------------------
-
-Calling ``fit`` starts a SageMaker training job. The training job will execute the following.
-
-- Starts ``train_instance_count`` EC2 instances of the type ``train_instance_type``.
-- On each instance, it will do the following steps:
-
-  - starts a Docker container optimized for TensorFlow.
-  - downloads the dataset.
-  - setup up training related environment varialbes
-  - setup up distributed training environment if configured to use parameter server
-  - starts asynchronous training
-
-If the ``wait=False`` flag is passed to ``fit``, then it returns immediately. The training job continues running
-asynchronously. Later, a Tensorflow estimator can be obtained by attaching to the existing training job.
-If the training job is not finished, it starts showing the standard output of training and wait until it completes.
-After attaching, the estimator can be deployed as usual.
-
-.. code:: python
-
-    tf_estimator.fit(your_input_data, wait=False)
-    training_job_name = tf_estimator.latest_training_job.name
-
-    # after some time, or in a separate Python notebook, we can attach to it again.
-
-    tf_estimator = TensorFlow.attach(training_job_name=training_job_name)
-
-Distributed Training
-====================
-
-To run your training job with multiple instances in a distributed fashion, set ``train_instance_count``
-to a number larger than 1. We support two different types of distributed training, parameter server and Horovod.
-The ``distributions`` parameter is used to configure which distributed training strategy to use.
-
-Training with parameter servers
--------------------------------
-
-If you specify parameter_server as the value of the distributions parameter, the container launches a parameter server
-thread on each instance in the training cluster, and then executes your training code. You can find more information on
-TensorFlow distributed training at `TensorFlow docs <https://www.tensorflow.org/deploy/distributed>`__.
-To enable parameter server training:
-
-.. code:: python
-
-  from sagemaker.tensorflow import TensorFlow
-
-  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                            train_instance_count=2, train_instance_type='ml.p2.xlarge',
-                            framework_version='1.11', py_version='py3',
-                            distributions={'parameter_server': {'enabled': True}})
-  tf_estimator.fit('s3://bucket/path/to/training/data')
-
-Training with Horovod
----------------------
-
-Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version ``1.12`` or newer.
-You can find more details at `Horovod README <https://github.com/uber/horovod>`__.
-
-The container sets up the MPI environment and executes the ``mpirun`` command enabling you to run any Horovod
-training script with Script Mode.
-
-Training with ``MPI`` is configured by specifying following fields in ``distributions``:
-
-- ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.
-- ``processes_per_host (int)``: Number of processes MPI should launch on each host. Note, this should not be
-  greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu
-  training.
-- ``custom_mpi_options (str)``:  Any `mpirun` flag(s) can be passed in this field that will be added to the `mpirun`
-  command executed by SageMaker to launch distributed horovod training.
-
-
-In the below example we create an estimator to launch Horovod distributed training with 2 processes on one host:
-
-.. code:: python
-
-    from sagemaker.tensorflow import TensorFlow
-
-    tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                              framework_version='1.12', py_version='py3',
-                              distributions={
-                                  'mpi': {
-                                      'enabled': True,
-                                      'processes_per_host': 2,
-                                      'custom_mpi_options': '--NCCL_DEBUG INFO'
-                                  }
-                              })
-    tf_estimator.fit('s3://bucket/path/to/training/data')
-
-
-Training with Pipe Mode using PipeModeDataset
-=============================================
-
-Amazon SageMaker allows users to create training jobs using Pipe input mode.
-With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first.
-This means that your training jobs start sooner, finish quicker, and need less disk space.
-
-SageMaker TensorFlow provides an implementation of ``tf.data.Dataset`` that makes it easy to take advantage of Pipe
-input mode in SageMaker. You can replace your ``tf.data.Dataset`` with a ``sagemaker_tensorflow.PipeModeDataset`` to
-read TFRecords as they are streamed to your training instances.
-
-In your ``entry_point`` script, you can use ``PipeModeDataset`` like a ``Dataset``. In this example, we create a
-``PipeModeDataset`` to read TFRecords from the 'training' channel:
-
-
-.. code:: python
-
-    from sagemaker_tensorflow import PipeModeDataset
-
-    features = {
-        'data': tf.FixedLenFeature([], tf.string),
-        'labels': tf.FixedLenFeature([], tf.int64),
-    }
-
-    def parse(record):
-        parsed = tf.parse_single_example(record, features)
-        return ({
-            'data': tf.decode_raw(parsed['data'], tf.float64)
-        }, parsed['labels'])
-
-    def train_input_fn(training_dir, hyperparameters):
-        ds = PipeModeDataset(channel='training', record_format='TFRecord')
-        ds = ds.repeat(20)
-        ds = ds.prefetch(10)
-        ds = ds.map(parse, num_parallel_calls=10)
-        ds = ds.batch(64)
-        return ds
-
-
-To run training job with Pipe input mode, pass in ``input_mode='Pipe'`` to your TensorFlow Estimator:
-
-
-.. code:: python
-
-    from sagemaker.tensorflow import TensorFlow
-
-    tf_estimator = TensorFlow(entry_point='tf-train-with-pipemodedataset.py', role='SageMakerRole',
-                              training_steps=10000, evaluation_steps=100,
-                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                              framework_version='1.10.0', input_mode='Pipe')
-
-    tf_estimator.fit('s3://bucket/path/to/training/data')
-
-
-If your TFRecords are compressed, you can train on Gzipped TF Records by passing in ``compression='Gzip'`` to the call to
-``fit()``, and SageMaker will automatically unzip the records as data is streamed to your training instances:
-
-.. code:: python
-
-    from sagemaker.session import s3_input
-
-    train_s3_input = s3_input('s3://bucket/path/to/training/data', compression='Gzip')
-    tf_estimator.fit(train_s3_input)
-
-
-You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
-
-
-Training with MKL-DNN disabled
-==============================
-
-SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
-
-In certain cases you might be able to get a better performance by disabling this optimization
-(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93m[93md88d1c19861fb7733941969f5a68821d9da2982e[0m[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
-
-You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
-
-.. code:: python
-
-    import os
-
-    os.environ['TF_DISABLE_MKL'] = '1'
-    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
-
-********************************
-Deploy TensorFlow Serving models
-********************************
-
-After a TensorFlow estimator has been fit, it saves a TensorFlow SavedModel in
-the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
-estimator to create a SageMaker Endpoint, or you can call ``transformer`` to create a ``Transformer`` that you can use to run a batch transform job.
-
-Your model will be deployed to a TensorFlow Serving-based server. The server provides a super-set of the
-`TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_.
-
-
-Deploy to a SageMaker Endpoint
-==============================
-
-Deploying from an Estimator
----------------------------
-
-After a TensorFlow estimator has been fit, it saves a TensorFlow
-`SavedModel <https://www.tensorflow.org/guide/saved_model>`_ bundle in
-the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
-estimator object to create a SageMaker Endpoint:
-
-.. code:: python
-
-  from sagemaker.tensorflow import TensorFlow
-
-  estimator = TensorFlow(entry_point='tf-train.py', ..., train_instance_count=1,
-                         train_instance_type='ml.c4.xlarge', framework_version='1.11')
-
-  estimator.fit(inputs)
-
-  predictor = estimator.deploy(initial_instance_count=1,
-                               instance_type='ml.c5.xlarge',
-                               endpoint_type='tensorflow-serving')
-
-
-The code block above deploys a SageMaker Endpoint with one instance of the type 'ml.c5.xlarge'.
-
-What happens when deploy is called
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Calling ``deploy`` starts the process of creating a SageMaker Endpoint. This process includes the following steps.
-
-- Starts ``initial_instance_count`` EC2 instances of the type ``instance_type``.
-- On each instance, it will do the following steps:
-
-  - start a Docker container optimized for TensorFlow Serving, see `SageMaker TensorFlow Serving containers <https://github.com/aws/sagemaker-tensorflow-serving-container>`_.
-  - start a `TensorFlow Serving` process configured to run your model.
-  - start an HTTP server that provides access to TensorFlow Server through the SageMaker InvokeEndpoint API.
-
-
-When the ``deploy`` call finishes, the created SageMaker Endpoint is ready for prediction requests. The
-`Making predictions against a SageMaker Endpoint`_ section will explain how to make prediction requests
-against the Endpoint.
-
-Deploying directly from model artifacts
----------------------------------------
-
-If you already have existing model artifacts in S3, you can skip training and deploy them directly to an endpoint:
-
-.. code:: python
-
-  from sagemaker.tensorflow.serving import Model
-
-  model = Model(model_data='s3://mybucket/model.tar.gz', role='MySageMakerRole')
-
-  predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')
-
-Python-based TensorFlow serving on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to accelerator_type to your deploy call.
-
-.. code:: python
-
-    from sagemaker.tensorflow.serving import Model
-
-    model = Model(model_data='s3://mybucket/model.tar.gz', role='MySageMakerRole')
-
-    predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge', accelerator_type='ml.eia1.medium')
-
-Making predictions against a SageMaker Endpoint
------------------------------------------------
-
-Once you have the ``Predictor`` instance returned by ``model.deploy(...)`` or ``estimator.deploy(...)``, you
-can send prediction requests to your Endpoint.
-
-The following code shows how to make a prediction request:
-
-.. code:: python
-
-  input = {
-    'instances': [1.0, 2.0, 5.0]
-  }
-  result = predictor.predict(input)
-
-The result object will contain a Python dict like this:
-
-.. code:: python
-
-  {
-    'predictions': [3.5, 4.0, 5.5]
-  }
-
-The formats of the input and the output data correspond directly to the request and response formats
-of the ``Predict`` method in the `TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_.
-
-If your SavedModel includes the right ``signature_def``, you can also make Classify or Regress requests:
-
-.. code:: python
-
-  # input matches the Classify and Regress API
-  input = {
-    'signature_name': 'tensorflow/serving/regress',
-    'examples': [{'x': 1.0}, {'x': 2.0}]
-  }
-
-  result = predictor.regress(input)  # or predictor.classify(...)
-
-  # result contains:
-  {
-    'results': [3.5, 4.0]
-  }
-
-You can include multiple ``instances`` in your predict request (or multiple ``examples`` in
-classify/regress requests) to get multiple prediction results in one request to your Endpoint:
-
-.. code:: python
-
-  input = {
-    'instances': [
-      [1.0, 2.0, 5.0],
-      [1.0, 2.0, 5.0],
-      [1.0, 2.0, 5.0]
-    ]
-  }
-  result = predictor.predict(input)
-
-  # result contains:
-  {
-    'predictions': [
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5]
-    ]
-  }
-
-If your application allows request grouping like this, it is **much** more efficient than making separate requests.
-
-See `Deploying to TensorFlow Serving Endpoints <https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst>` to learn how to deploy your model and make inference requests.
-
-Run a Batch Transform Job
-=========================
-
-Batch transform allows you to get inferences for an entire dataset that is stored in an S3 bucket.
-
-For general information about using batch transform with the SageMaker Python SDK, see :ref:`overview:SageMaker Batch Transform`.
-For information about SageMaker batch transform, see `Get Inferences for an Entire Dataset with Batch Transform <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html>` in the AWS documentation.
-
-To run a batch transform job, you first create a ``Transformer`` object, and then call that object's ``transform`` method.
-
-Create a Transformer Object
----------------------------
-
-If you used an estimator to train your model, you can call the ``transformer`` method of the estimator to create a ``Transformer`` object.
-
-For example:
-
-.. code:: python
-
-  bucket = myBucket # The name of the S3 bucket where the results are stored
-  prefix = 'batch-results' # The folder in the S3 bucket where the results are stored
-
-  batch_output = 's3://{}/{}/results'.format(bucket, prefix) # The location to store the results
-
-  tf_transformer = tf_estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge, output_path=batch_output)
-
-To use a model trained outside of SageMaker, you can package the model as a SageMaker model, and call the ``transformer`` method of the SageMaker model.
-
-For example:
-
-.. code:: python
-
-  bucket = myBucket # The name of the S3 bucket where the results are stored
-  prefix = 'batch-results' # The folder in the S3 bucket where the results are stored
-
-  batch_output = 's3://{}/{}/results'.format(bucket, prefix) # The location to store the results
-
-  tf_transformer = tensorflow_serving_model.transformer(instance_count=1, instance_type='ml.m4.xlarge, output_path=batch_output)
-
-For information about how to package a model as a SageMaker model, see :ref:`overview:BYO Model`.
-When you call the ``tranformer`` method, you specify the type and number of instances to use for the batch transform job, and the location where the results are stored in S3.
-
-
-
-Call transform
---------------
-
-After you create a ``Transformer`` object, you call that object's ``transform`` method to start a batch transform job.
-For example:
-
-.. code:: python
-
-  batch_input = 's3://{}/{}/test/examples'.format(bucket, prefix) # The location of the input dataset
-
-  tf_transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')
-
-In the example, the content type is CSV, and each line in the dataset is treated as a record to get a predition for.
-
-Batch Transform Supported Data Formats
---------------------------------------
-
-When you call the ``tranform`` method to start a batch transform job,
-you specify the data format by providing a MIME type as the value for the ``content_type`` parameter.
-
-The following content formats are supported without custom intput and output handling:
-
-* CSV - specify ``text/csv`` as the value of the ``content_type`` parameter.
-* JSON - specify ``application/json`` as the value of the ``content_type`` parameter.
-* JSON lines - specify ``application/jsonlines`` as the value of the ``content_type`` parameter.
-
-For detailed information about how TensorFlow Serving formats these data types for input and output, see :ref:`using_tf:TensorFlow Serving Input and Output`.
-
-You can also accept any custom data format by writing input and output functions, and include them in the ``inference.py`` file in your model.
-For information, see :ref:`using_tf:Create Python Scripts for Custom Input and Output Formats`. 
-
-
-TensorFlow Serving Input and Output
-===================================
-
-The following sections describe the data formats that TensorFlow Serving endpoints and batch transform jobs accept,
-and how to write input and output functions to input and output custom data formats.
-
-Supported Formats
------------------
-
-SageMaker's TensforFlow Serving endpoints can also accept some additional input formats that are not part of the
-TensorFlow REST API, including a simplified json format, line-delimited json objects ("jsons" or "jsonlines"), and
-CSV data.
-
-Simplified JSON Input
-^^^^^^^^^^^^^^^^^^^^^
-
-The Endpoint will accept simplified JSON input that doesn't match the TensorFlow REST API's Predict request format.
-When the Endpoint receives data like this, it will attempt to transform it into a valid
-Predict request, using a few simple rules:
-
-- python value, dict, or one-dimensional arrays are treated as the input value in a single 'instance' Predict request.
-- multidimensional arrays are treated as a multiple values in a multi-instance Predict request.
-
-Combined with the client-side ``Predictor`` object's JSON serialization, this allows you to make simple
-requests like this:
-
-.. code:: python
-
-  input = [
-    [1.0, 2.0, 5.0],
-    [1.0, 2.0, 5.0]
-  ]
-  result = predictor.predict(input)
-
-  # result contains:
-  {
-    'predictions': [
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5]
-    ]
-  }
-
-Or this:
-
-.. code:: python
-
-  # 'x' must match name of input tensor in your SavedModel graph
-  # for models with multiple named inputs, just include all the keys in the input dict
-  input = {
-    'x': [1.0, 2.0, 5.0]
-  }
-
-  # result contains:
-  {
-    'predictions': [
-      [3.5, 4.0, 5.5]
-    ]
-  }
-
-
-Line-delimited JSON
-^^^^^^^^^^^^^^^^^^^
-
-The Endpoint will accept line-delimited JSON objects (also known as "jsons" or "jsonlines" data).
-The Endpoint treats each line as a separate instance in a multi-instance Predict request. To use
-this feature from your python code, you need to create a ``Predictor`` instance that does not
-try to serialize your input to JSON:
-
-.. code:: python
-
-  # create a Predictor without JSON serialization
-
-  predictor = Predictor('endpoint-name', serializer=None, content_type='application/jsonlines')
-
-  input = '''{'x': [1.0, 2.0, 5.0]}
-  {'x': [1.0, 2.0, 5.0]}
-  {'x': [1.0, 2.0, 5.0]}'''
-
-  result = predictor.predict(input)
-
-  # result contains:
-  {
-    'predictions': [
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5]
-    ]
-  }
-
-This feature is especially useful if you are reading data from a file containing jsonlines data.
-
-**CSV (comma-separated values)**
-
-The Endpoint will accept CSV data. Each line is treated as a separate instance. This is a
-compact format for representing multiple instances of 1-d array data. To use this feature
-from your python code, you need to create a ``Predictor`` instance that can serialize
-your input data to CSV format:
-
-.. code:: python
-
-  # create a Predictor with JSON serialization
-
-  predictor = Predictor('endpoint-name', serializer=sagemaker.predictor.csv_serializer)
-
-  # CSV-formatted string input
-  input = '1.0,2.0,5.0\n1.0,2.0,5.0\n1.0,2.0,5.0'
-
-  result = predictor.predict(input)
-
-  # result contains:
-  {
-    'predictions': [
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5],
-      [3.5, 4.0, 5.5]
-    ]
-  }
-
-You can also use python arrays or numpy arrays as input and let the `csv_serializer` object
-convert them to CSV, but the client-size CSV conversion is more sophisticated than the
-CSV parsing on the Endpoint, so if you encounter conversion problems, try using one of the
-JSON options instead.
-
-
-Create Python Scripts for Custom Input and Output Formats
----------------------------------------------------------
-
-You can add your customized Python code to process your input and output data:
-
-.. code::
-
-    from sagemaker.tensorflow.serving import Model
-
-    model = Model(entry_point='inference.py',
-                  model_data='s3://mybucket/model.tar.gz',
-                  role='MySageMakerRole')
-
-How to implement the pre- and/or post-processing handler(s)
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Your entry point file should implement either a pair of ``input_handler``
-   and ``output_handler`` functions or a single ``handler`` function.
-   Note that if ``handler`` function is implemented, ``input_handler``
-   and ``output_handler`` are ignored.
-
-To implement pre- and/or post-processing handler(s), use the Context
-object that the Python service creates. The Context object is a namedtuple with the following attributes:
-
--  ``model_name (string)``: the name of the model to use for
-   inference. For example, 'half-plus-three'
-
--  ``model_version (string)``: version of the model. For example, '5'
-
--  ``method (string)``: inference method. For example, 'predict',
-   'classify' or 'regress', for more information on methods, please see
-   `Classify and Regress
-   API <https://www.tensorflow.org/tfx/serving/api_rest#classify_and_regress_api>`__
-   and `Predict
-   API <https://www.tensorflow.org/tfx/serving/api_rest#predict_api>`__
-
--  ``rest_uri (string)``: the TFS REST uri generated by the Python
-   service. For example,
-   'http://localhost:8501/v1/models/half_plus_three:predict'
-
--  ``grpc_uri (string)``: the GRPC port number generated by the Python
-   service. For example, '9000'
-
--  ``custom_attributes (string)``: content of
-   'X-Amzn-SageMaker-Custom-Attributes' header from the original
-   request. For example,
-   'tfs-model-name=half*plus*\ three,tfs-method=predict'
-
--  ``request_content_type (string)``: the original request content type,
-   defaulted to 'application/json' if not provided
-
--  ``accept_header (string)``: the original request accept type,
-   defaulted to 'application/json' if not provided
-
--  ``content_length (int)``: content length of the original request
-
-The following code example implements ``input_handler`` and
-``output_handler``. By providing these, the Python service posts the
-request to the TFS REST URI with the data pre-processed by ``input_handler``
-and passes the response to ``output_handler`` for post-processing.
-
-.. code::
-
-   import json
-
-   def input_handler(data, context):
-       """ Pre-process request input before it is sent to TensorFlow Serving REST API
-       Args:
-           data (obj): the request data, in format of dict or string
-           context (Context): an object containing request and configuration details
-       Returns:
-           (dict): a JSON-serializable dict that contains request body and headers
-       """
-       if context.request_content_type == 'application/json':
-           # pass through json (assumes it's correctly formed)
-           d = data.read().decode('utf-8')
-           return d if len(d) else ''
-
-       if context.request_content_type == 'text/csv':
-           # very simple csv handler
-           return json.dumps({
-               'instances': [float(x) for x in data.read().decode('utf-8').split(',')]
-           })
-
-       raise ValueError('{{"error": "unsupported content type {}"}}'.format(
-           context.request_content_type or "unknown"))
-
-
-   def output_handler(data, context):
-       """Post-process TensorFlow Serving output before it is returned to the client.
-       Args:
-           data (obj): the TensorFlow serving response
-           context (Context): an object containing request and configuration details
-       Returns:
-           (bytes, string): data to return to client, response content type
-       """
-       if data.status_code != 200:
-           raise ValueError(data.content.decode('utf-8'))
-
-       response_content_type = context.accept_header
-       prediction = data.content
-       return prediction, response_content_type
-
-You might want to have complete control over the request.
-For example, you might want to make a TFS request (REST or GRPC) to the first model,
-inspect the results, and then make a request to a second model. In this case, implement
-the ``handler`` method instead of the ``input_handler`` and ``output_handler`` methods, as demonstrated
-in the following code:
-
-.. code::
-
-   import json
-   import requests
-
-
-   def handler(data, context):
-       """Handle request.
-       Args:
-           data (obj): the request data
-           context (Context): an object containing request and configuration details
-       Returns:
-           (bytes, string): data to return to client, (optional) response content type
-       """
-       processed_input = _process_input(data, context)
-       response = requests.post(context.rest_uri, data=processed_input)
-       return _process_output(response, context)
-
-
-   def _process_input(data, context):
-       if context.request_content_type == 'application/json':
-           # pass through json (assumes it's correctly formed)
-           d = data.read().decode('utf-8')
-           return d if len(d) else ''
-
-       if context.request_content_type == 'text/csv':
-           # very simple csv handler
-           return json.dumps({
-               'instances': [float(x) for x in data.read().decode('utf-8').split(',')]
-           })
-
-       raise ValueError('{{"error": "unsupported content type {}"}}'.format(
-           context.request_content_type or "unknown"))
-
-
-   def _process_output(data, context):
-       if data.status_code != 200:
-           raise ValueError(data.content.decode('utf-8'))
-
-       response_content_type = context.accept_header
-       prediction = data.content
-       return prediction, response_content_type
-
-You can also bring in external dependencies to help with your data
-processing. There are 2 ways to do this:
-
-1. If you included ``requirements.txt`` in your ``source_dir`` or in
-   your dependencies, the container installs the Python dependencies at runtime using ``pip install -r``:
-
-.. code::
-
-    from sagemaker.tensorflow.serving import Model
-
-    model = Model(entry_point='inference.py',
-                  dependencies=['requirements.txt'],
-                  model_data='s3://mybucket/model.tar.gz',
-                  role='MySageMakerRole')
-
-
-2. If you are working in a network-isolation situation or if you don't
-   want to install dependencies at runtime every time your endpoint starts or a batch
-   transform job runs, you might want to put
-   pre-downloaded dependencies under a ``lib`` directory and this
-   directory as dependency. The container adds the modules to the Python
-   path. Note that if both ``lib`` and ``requirements.txt``
-   are present in the model archive, the ``requirements.txt`` is ignored:
-
-.. code::
-
-    from sagemaker.tensorflow.serving import Model
-
-    model = Model(entry_point='inference.py',
-                  dependencies=['/path/to/folder/named/lib'],
-                  model_data='s3://mybucket/model.tar.gz',
-                  role='MySageMakerRole')
-
-
-*************************************
-sagemaker.tensorflow.TensorFlow Class
-*************************************
-
-The following are the most commonly used ``TensorFlow`` constructor arguments.
-
-Required:
-
-- ``entry_point (str)`` Path (absolute or relative) to the Python file which
-  should be executed as the entry point to training.
-- ``role (str)`` An AWS IAM role (either name or full ARN). The Amazon
-  SageMaker training jobs and APIs that create Amazon SageMaker
-  endpoints use this role to access training data and model artifacts.
-  After the endpoint is created, the inference code might use the IAM
-  role, if accessing AWS resource.
-- ``train_instance_count (int)`` Number of Amazon EC2 instances to use for
-  training.
-- ``train_instance_type (str)`` Type of EC2 instance to use for training, for
-  example, 'ml.c4.xlarge'.
-
-Optional:
-
-- ``source_dir (str)`` Path (absolute or relative) to a directory with any
-  other training source code dependencies including the entry point
-  file. Structure within this directory will be preserved when training
-  on SageMaker.
-- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
-  any additional libraries that will be exported to the container (default: ``[]``).
-  The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
-  If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
-  instead. Example:
-
-  The following call
-
-  >>> TensorFlow(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
-
-  results in the following inside the container:
-
-  >>> opt/ml/code
-  >>>     ├── train.py
-  >>>     ├── common
-  >>>     └── virtual-env
-
-- ``hyperparameters (dict[str, ANY])`` Hyperparameters that will be used for training.
-  Will be made accessible as command line arguments.
-- ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
-  input data during training. Must be large enough to the store training
-  data.
-- ``train_max_run (int)`` Timeout in seconds for training, after which Amazon
-  SageMaker terminates the job regardless of its current status.
-- ``output_path (str)`` S3 location where you want the training result (model
-  artifacts and optional output files) saved. If not specified, results
-  are stored to a default bucket. If the bucket with the specific name
-  does not exist, the estimator creates the bucket during the ``fit``
-  method execution.
-- ``output_kms_key`` Optional KMS key ID to optionally encrypt training
-  output with.
-- ``base_job_name`` Name to assign for the training job that the ``fit``
-  method launches. If not specified, the estimator generates a default
-  job name, based on the training image name and current timestamp.
-- ``image_name`` An alternative docker image to use for training and
-  serving.  If specified, the estimator will use this image for training and
-  hosting, instead of selecting the appropriate SageMaker official image based on
-  ``framework_version`` and ``py_version``. Refer to: `SageMaker TensorFlow Docker containers <https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#sagemaker-tensorflow-docker-containers>`_ for details on what the official images support
-  and where to find the source code to build your custom image.
-- ``script_mode (bool)`` Whether to use Script Mode or not. Script mode is the only available training mode in Python 3,
-  setting ``py_version`` to ``py3`` automatically sets ``script_mode`` to True.
-- ``model_dir (str)`` Location where model data, checkpoint data, and TensorBoard checkpoints should be saved during training.
-  If not specified a S3 location will be generated under the training job's default bucket. And ``model_dir`` will be
-  passed in your training script as one of the command line arguments.
-- ``distributions (dict)`` Configure your distribution strategy with this argument.
-
-**************************************
-SageMaker TensorFlow Docker containers
-**************************************
-
-For information about SageMaker TensorFlow Docker containers and their dependencies, see `SageMaker TensorFlow Docker containers <https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#sagemaker-tensorflow-docker-containers>`_.
+==============================================
+Using TensorFlow with the SageMaker Python SDK
+==============================================
+
+TensorFlow SageMaker Estimators allow you to run your own TensorFlow
+training algorithms on SageMaker Learner, and to host your own TensorFlow
+models on SageMaker Hosting.
+
+**Note:** This topic describes how to use script mode for TensorFlow versions 1.11 and later.
+For Documentation of the previous Legacy Mode versions, see:
+
+* `1.4.1 <https://github.com/aws/sagemaker-python-sdk/tree/v1.0.0#tensorflow-sagemaker-estimators>`_
+* `1.5.0 <https://github.com/aws/sagemaker-python-sdk/tree/v1.1.0#tensorflow-sagemaker-estimators>`_
+* `1.6.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.7.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.8.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.9.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.9.2/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.10.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.10.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+
+.. warning::
+    We have added a new format of your TensorFlow training script with TensorFlow version 1.11.
+    This new way gives the user script more flexibility.
+    This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions.
+    In addition we are adding Python 3 support with Script Mode.
+    Last supported version of Legacy Mode will be TensorFlow 1.12.
+    Script Mode is available with TensorFlow version 1.11 and newer.
+    Make sure you refer to the correct version of this README when you prepare your script.
+    You can find the Legacy Mode README `here <https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#tensorflow-sagemaker-estimators-and-models>`_.
+
+.. contents::
+
+Supported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``.
+
+Training with TensorFlow
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Training TensorFlow models using ``sagemaker.tensorflow.TensorFlow`` is a two-step process.
+First, you prepare your training script, then second, you run it on
+SageMaker Learner via the ``sagemaker.tensorflow.TensorFlow`` estimator.
+
+Preparing a Script Mode training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your TensorFlow training script must be a Python 2.7- or 3.6-compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the local path where the training job can write the model artifacts to.
+  After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the ``model_dir``
+  argument passed in your training script which is a S3 location. ``SM_MODEL_DIR`` is always set to ``/opt/ml/model``.
+* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
+* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write output artifacts to.
+  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
+  These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the TensorFlow estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+
+For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``SM_CHANNEL_TRAIN`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch_size', type=int, default=100)
+        parser.add_argument('--learning_rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model_dir', type=str)
+        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, putting your training launching code in a main guard (``if __name__=='__main__':``)
+is good practice.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your TensorFlow estimator.
+
+Adapting your local TensorFlow script
+'''''''''''''''''''''''''''''''''''''
+
+If you have a TensorFlow training script that runs outside of SageMaker please follow the directions here:
+
+1. Make sure your script can handle ``--model_dir`` as an additional command line argument. If you did not specify a
+location when the TensorFlow estimator is constructed a S3 location under the default training job bucket will be passed
+in here. Distributed training with parameter servers requires you use the ``tf.estimator.train_and_evaluate`` API and
+a S3 location is needed as the model directory during training. Here is an example:
+
+.. code:: python
+
+    estimator = tf.estimator.Estimator(model_fn=my_model_fn, model_dir=args.model_dir)
+    ...
+    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
+    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
+    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)
+
+2. Load input data from the input channels. The input channels are defined when ``fit`` is called. For example:
+
+.. code:: python
+
+    estimator.fit({'train':'s3://my-bucket/my-training-data',
+                  'eval':'s3://my-bucket/my-evaluation-data'})
+
+In your training script the channels will be stored in environment variables ``SM_CHANNEL_TRAIN`` and
+``SM_CHANNEL_EVAL``. You can add them to your argument parsing logic like this:
+
+.. code:: python
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+    parser.add_argument('--eval', type=str, default=os.environ.get('SM_CHANNEL_EVAL'))
+
+3. Export your final model to path stored in environment variable ``SM_MODEL_DIR`` which should always be
+   ``/opt/ml/model``. At end of training SageMaker will upload the model file under ``/opt/ml/model`` to
+   ``output_path``.
+
+
+Training with TensorFlow estimator
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Calling fit
+'''''''''''
+
+To use Script Mode, set at least one of these args
+
+- ``py_version='py3'``
+- ``script_mode=True``
+
+Please note that when using Script Mode, your training script need to accept the following args:
+
+- ``model_dir``
+
+Please note that the following args are not permitted when using Script Mode:
+
+- ``checkpoint_path``
+- ``training_steps``
+- ``evaluation_steps``
+- ``requirements_file``
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.12', py_version='py3')
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The
+constructor keyword arguments define how SageMaker runs your training
+script which we discussed earlier.
+
+You start your training script by calling ``fit`` on a ``TensorFlow`` estimator. ``fit`` takes
+both required and optional arguments.
+
+Required argument
+"""""""""""""""""
+
+- ``inputs``: The S3 location(s) of datasets to be used for training. This can take one of two forms:
+
+  - ``str``: An S3 URI, for example ``s3://my-bucket/my-training-data``, which indicates the dataset's location.
+  - ``dict[str, str]``: A dictionary mapping channel names to S3 locations, for example ``{'train': 's3://my-bucket/my-training-data/train', 'test': 's3://my-bucket/my-training-data/test'}``
+  - ``sagemaker.session.s3_input``: channel configuration for S3 data sources that can provide additional information as well as the path to the training dataset. See `the API docs <https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input>`_ for full details.
+
+Optional arguments
+""""""""""""""""""
+
+- ``wait (bool)``: Defaults to True, whether to block and wait for the
+  training script to complete before returning.
+  If set to False, it will return immediately, and can later be attached to.
+- ``logs (bool)``: Defaults to True, whether to show logs produced by training
+  job in the Python session. Only meaningful when wait is True.
+- ``run_tensorboard_locally (bool)``: Defaults to False. If set to True a Tensorboard command will be printed out.
+- ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
+  based on the training image name and current timestamp.
+
+What happens when fit is called
+"""""""""""""""""""""""""""""""
+
+Calling ``fit`` starts a SageMaker training job. The training job will execute the following.
+
+- Starts ``train_instance_count`` EC2 instances of the type ``train_instance_type``.
+- On each instance, it will do the following steps:
+
+  - starts a Docker container optimized for TensorFlow.
+  - downloads the dataset.
+  - setup up training related environment varialbes
+  - setup up distributed training environment if configured to use parameter server
+  - starts asynchronous training
+
+If the ``wait=False`` flag is passed to ``fit``, then it will return immediately. The training job will continue running
+asynchronously. At a later time, a Tensorflow Estimator can be obtained by attaching to the existing training job. If
+the training job is not finished it will start showing the standard output of training and wait until it completes.
+After attaching, the estimator can be deployed as usual.
+
+.. code:: python
+
+    tf_estimator.fit(your_input_data, wait=False)
+    training_job_name = tf_estimator.latest_training_job.name
+
+    # after some time, or in a separate Python notebook, we can attach to it again.
+
+    tf_estimator = TensorFlow.attach(training_job_name=training_job_name)
+
+Distributed Training
+''''''''''''''''''''
+
+To run your training job with multiple instances in a distributed fashion, set ``train_instance_count``
+to a number larger than 1. We support two different types of distributed training, parameter server and Horovod.
+The ``distributions`` parameter is used to configure which distributed training strategy to use.
+
+Training with parameter servers
+"""""""""""""""""""""""""""""""
+
+If you specify parameter_server as the value of the distributions parameter, the container launches a parameter server
+thread on each instance in the training cluster, and then executes your training code. You can find more information on
+TensorFlow distributed training at `TensorFlow docs <https://www.tensorflow.org/deploy/distributed>`__.
+To enable parameter server training:
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=2, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.11', py_version='py3',
+                            distributions={'parameter_server': {'enabled': True}})
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Training with Horovod
+"""""""""""""""""""""
+
+Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version ``1.12`` or newer.
+You can find more details at `Horovod README <https://github.com/uber/horovod>`__.
+
+The container sets up the MPI environment and executes the ``mpirun`` command enabling you to run any Horovod
+training script with Script Mode.
+
+Training with ``MPI`` is configured by specifying following fields in ``distributions``:
+
+- ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.
+- ``processes_per_host (int)``: Number of processes MPI should launch on each host. Note, this should not be
+  greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu
+  training.
+- ``custom_mpi_options (str)``:  Any `mpirun` flag(s) can be passed in this field that will be added to the `mpirun`
+  command executed by SageMaker to launch distributed horovod training.
+
+
+In the below example we create an estimator to launch Horovod distributed training with 2 processes on one host:
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.12', py_version='py3',
+                              distributions={
+                                  'mpi': {
+                                      'enabled': True,
+                                      'processes_per_host': 2,
+                                      'custom_mpi_options': '--NCCL_DEBUG INFO'
+                                  }
+                              })
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+sagemaker.tensorflow.TensorFlow class
+'''''''''''''''''''''''''''''''''''''
+
+The ``TensorFlow`` constructor takes both required and optional arguments.
+
+Required:
+
+- ``entry_point (str)`` Path (absolute or relative) to the Python file which
+  should be executed as the entry point to training.
+- ``role (str)`` An AWS IAM role (either name or full ARN). The Amazon
+  SageMaker training jobs and APIs that create Amazon SageMaker
+  endpoints use this role to access training data and model artifacts.
+  After the endpoint is created, the inference code might use the IAM
+  role, if accessing AWS resource.
+- ``train_instance_count (int)`` Number of Amazon EC2 instances to use for
+  training.
+- ``train_instance_type (str)`` Type of EC2 instance to use for training, for
+  example, 'ml.c4.xlarge'.
+
+Optional:
+
+- ``source_dir (str)`` Path (absolute or relative) to a directory with any
+  other training source code dependencies including the entry point
+  file. Structure within this directory will be preserved when training
+  on SageMaker.
+- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+  any additional libraries that will be exported to the container (default: ``[]``).
+  The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+  If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+  instead. Example:
+
+  The following call
+
+  >>> TensorFlow(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+  results in the following inside the container:
+
+  >>> opt/ml/code
+  >>>     ├── train.py
+  >>>     ├── common
+  >>>     └── virtual-env
+
+- ``hyperparameters (dict[str, ANY])`` Hyperparameters that will be used for training.
+  Will be made accessible as command line arguments.
+- ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
+  input data during training. Must be large enough to the store training
+  data.
+- ``train_max_run (int)`` Timeout in seconds for training, after which Amazon
+  SageMaker terminates the job regardless of its current status.
+- ``output_path (str)`` S3 location where you want the training result (model
+  artifacts and optional output files) saved. If not specified, results
+  are stored to a default bucket. If the bucket with the specific name
+  does not exist, the estimator creates the bucket during the ``fit``
+  method execution.
+- ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+  output with.
+- ``base_job_name`` Name to assign for the training job that the ``fit``
+  method launches. If not specified, the estimator generates a default
+  job name, based on the training image name and current timestamp.
+- ``image_name`` An alternative docker image to use for training and
+  serving.  If specified, the estimator will use this image for training and
+  hosting, instead of selecting the appropriate SageMaker official image based on
+  ``framework_version`` and ``py_version``. Refer to: `SageMaker TensorFlow Docker Containers
+  <#sagemaker-tensorflow-docker-containers>`_ for details on what the official images support
+  and where to find the source code to build your custom image.
+- ``script_mode (bool)`` Whether to use Script Mode or not. Script mode is the only available training mode in Python 3,
+  setting ``py_version`` to ``py3`` automatically sets ``script_mode`` to True.
+- ``model_dir (str)`` Location where model data, checkpoint data, and TensorBoard checkpoints should be saved during training.
+  If not specified a S3 location will be generated under the training job's default bucket. And ``model_dir`` will be
+  passed in your training script as one of the command line arguments.
+- ``distributions (dict)`` Configure your distribution strategy with this argument.
+
+Training with Pipe Mode using PipeModeDataset
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Amazon SageMaker allows users to create training jobs using Pipe input mode.
+With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first.
+This means that your training jobs start sooner, finish quicker, and need less disk space.
+
+SageMaker TensorFlow provides an implementation of ``tf.data.Dataset`` that makes it easy to take advantage of Pipe
+input mode in SageMaker. You can replace your ``tf.data.Dataset`` with a ``sagemaker_tensorflow.PipeModeDataset`` to
+read TFRecords as they are streamed to your training instances.
+
+In your ``entry_point`` script, you can use ``PipeModeDataset`` like a ``Dataset``. In this example, we create a
+``PipeModeDataset`` to read TFRecords from the 'training' channel:
+
+
+.. code:: python
+
+    from sagemaker_tensorflow import PipeModeDataset
+
+    features = {
+        'data': tf.FixedLenFeature([], tf.string),
+        'labels': tf.FixedLenFeature([], tf.int64),
+    }
+
+    def parse(record):
+        parsed = tf.parse_single_example(record, features)
+        return ({
+            'data': tf.decode_raw(parsed['data'], tf.float64)
+        }, parsed['labels'])
+
+    def train_input_fn(training_dir, hyperparameters):
+        ds = PipeModeDataset(channel='training', record_format='TFRecord')
+        ds = ds.repeat(20)
+        ds = ds.prefetch(10)
+        ds = ds.map(parse, num_parallel_calls=10)
+        ds = ds.batch(64)
+        return ds
+
+
+To run training job with Pipe input mode, pass in ``input_mode='Pipe'`` to your TensorFlow Estimator:
+
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train-with-pipemodedataset.py', role='SageMakerRole',
+                              training_steps=10000, evaluation_steps=100,
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.10.0', input_mode='Pipe')
+
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+
+If your TFRecords are compressed, you can train on Gzipped TF Records by passing in ``compression='Gzip'`` to the call to
+``fit()``, and SageMaker will automatically unzip the records as data is streamed to your training instances:
+
+.. code:: python
+
+    from sagemaker.session import s3_input
+
+    train_s3_input = s3_input('s3://bucket/path/to/training/data', compression='Gzip')
+    tf_estimator.fit(train_s3_input)
+
+
+You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
+
+
+Training with MKL-DNN disabled
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
+
+In certain cases you might be able to get a better performance by disabling this optimization
+(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93m[93md88d1c19861fb7733941969f5a68821d9da2982e[0m[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
+
+You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
+
+.. code:: python
+
+    import os
+
+    os.environ['TF_DISABLE_MKL'] = '1'
+    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
+
+
+Deploying TensorFlow Serving models
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After a TensorFlow estimator has been fit, it saves a TensorFlow SavedModel in
+the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
+estimator to create a SageMaker Endpoint.
+
+Your model will be deployed to a TensorFlow Serving-based server. The server provides a super-set of the
+`TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_.
+
+See `Deploying to TensorFlow Serving Endpoints <https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst>`_ to learn how to deploy your model and make inference requests.
+
+
+SageMaker TensorFlow Docker containers
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The containers include the following Python packages:
+
++--------------------------------+---------------+-------------------+
+| Dependencies                   | Script Mode   | Legacy Mode       |
++--------------------------------+---------------+-------------------+
+| boto3                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| botocore                       | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| CUDA (GPU image only)          | 9.0           | 9.0               |
++--------------------------------+---------------+-------------------+
+| numpy                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| Pillow                         | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| scipy                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| sklean                         | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| h5py                           | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| pip                            | 18.1          | 18.1              |
++--------------------------------+---------------+-------------------+
+| curl                           | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| tensorflow                     | 1.12.0        | 1.12.0            |
++--------------------------------+---------------+-------------------+
+| tensorflow-serving-api         | 1.12.0        | None              |
++--------------------------------+---------------+-------------------+
+| sagemaker-containers           | >=2.3.5       | >=2.3.5           |
++--------------------------------+---------------+-------------------+
+| sagemaker-tensorflow-container | 1.0           | 1.0               |
++--------------------------------+---------------+-------------------+
+| Python                         | 2.7 or 3.6    | 2.7               |
++--------------------------------+---------------+-------------------+
+
+Legacy Mode TensorFlow Docker images support Python 2.7. Script Mode TensorFlow Docker images support both Python 2.7
+and Python 3.6. The Docker images extend Ubuntu 16.04.
+
+You can select version of TensorFlow by passing a ``framework_version`` keyword arg to the TensorFlow Estimator constructor. Currently supported versions are listed in the table above. You can also set ``framework_version`` to only specify major and minor version, e.g ``'1.6'``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.6.0.
+Alternatively, you can build your own image by following the instructions in the SageMaker TensorFlow containers
+repository, and passing ``image_name`` to the TensorFlow Estimator constructor.
+
+For more information on the contents of the images, see the SageMaker TensorFlow containers repositories here:
+
+- training: https://github.com/aws/sagemaker-tensorflow-container
+- serving: https://github.com/aws/sagemaker-tensorflow-serving-container

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-18 18:16:06[0m
[92mHash: 356283e8c599d9b5acaa1bfff4c32fe20d4ad9c1[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: format and add missing docstring placeholders (#945)

This commit will format all existing docstring to follow Google
style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html
This commit will also add docstring placeholders to any class or method
previously missing it.

An ideal approach would be to take the time to include meaningful
docstrings in every file. However, since that is not a task that will
be prioritized, I've declared docstring bankruptcy on this package, in
order to enforce docstring on all future code changes to this package.[0m
@@ -10,7 +10,6 @@
 # distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
-"""Placeholder docstring"""
 from __future__ import print_function, absolute_import
 
 import json
@@ -58,8 +57,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html
 
     Subclasses must define a way to determine what image to use for training,
-    what hyperparameters to use, and how to create an appropriate predictor
-    instance.
+    what hyperparameters to use, and how to create an appropriate predictor instance.
     """
 
     def __init__(
@@ -86,77 +84,55 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         """Initialize an ``EstimatorBase`` instance.
 
         Args:
-            role (str): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role, if it needs to access an AWS resource.
-            train_instance_count (int): Number of Amazon EC2 instances to use
-                for training.
-            train_instance_type (str): Type of EC2 instance to use for training,
-                for example, 'ml.c4.xlarge'.
-            train_volume_size (int): Size in GB of the EBS volume to use for
-                storing input data during training (default: 30). Must be large
-                enough to store training data if File Mode is used (which is the
-                default).
-            train_volume_kms_key (str): Optional. KMS key ID for encrypting EBS
-                volume attached to the training instance (default: None).
-            train_max_run (int): Timeout in seconds for training (default: 24 *
-                60 * 60). After this amount of time Amazon SageMaker terminates
-                the job regardless of its current status.
-            input_mode (str): The input mode that the algorithm supports
-                (default: 'File'). Valid modes: 'File' - Amazon SageMaker copies
-                the training dataset from the S3 location to a local directory.
-                'Pipe' - Amazon SageMaker streams data directly from S3 to the
-                container via a Unix-named pipe. This argument can be overriden
-                on a per-channel basis using
-                ``sagemaker.session.s3_input.input_mode``.
-            output_path (str): S3 location for saving the training result (model
-                artifacts and output files). If not specified, results are
-                stored to a default bucket. If the bucket with the specific name
+            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs
+                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.
+                After the endpoint is created, the inference code might use the IAM role,
+                if it needs to access an AWS resource.
+            train_instance_count (int): Number of Amazon EC2 instances to use for training.
+            train_instance_type (str): Type of EC2 instance to use for training, for example, 'ml.c4.xlarge'.
+            train_volume_size (int): Size in GB of the EBS volume to use for storing input data
+                during training (default: 30). Must be large enough to store training data if File Mode is used
+                (which is the default).
+            train_volume_kms_key (str): Optional. KMS key ID for encrypting EBS volume attached to the
+                training instance (default: None).
+            train_max_run (int): Timeout in seconds for training (default: 24 * 60 * 60).
+                After this amount of time Amazon SageMaker terminates the job regardless of its current status.
+            input_mode (str): The input mode that the algorithm supports (default: 'File'). Valid modes:
+                'File' - Amazon SageMaker copies the training dataset from the S3 location to a local directory.
+                'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe.
+                This argument can be overriden on a per-channel basis using ``sagemaker.session.s3_input.input_mode``.
+            output_path (str): S3 location for saving the training result (model artifacts and output files).
+                If not specified, results are stored to a default bucket. If the bucket with the specific name
                 does not exist, the estimator creates the bucket during the
                 :meth:`~sagemaker.estimator.EstimatorBase.fit` method execution.
-            output_kms_key (str): Optional. KMS key ID for encrypting the
-                training output (default: None).
-            base_job_name (str): Prefix for training job name when the
-                :meth:`~sagemaker.estimator.EstimatorBase.fit` method launches.
-                If not specified, the estimator generates a default job name,
-                based on the training image name and current timestamp.
-            sagemaker_session (sagemaker.session.Session): Session object which
-                manages interactions with Amazon SageMaker APIs and any other
-                AWS services needed. If not specified, the estimator creates one
+            output_kms_key (str): Optional. KMS key ID for encrypting the training output (default: None).
+            base_job_name (str): Prefix for training job name when the :meth:`~sagemaker.estimator.EstimatorBase.fit`
+                method launches. If not specified, the estimator generates a default job name, based on
+                the training image name and current timestamp.
+            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with
+                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one
                 using the default AWS configuration chain.
-            tags (list[dict]): List of tags for labeling a training job. For
-                more, see
+            tags (list[dict]): List of tags for labeling a training job. For more, see
                 https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
-            subnets (list[str]): List of subnet ids. If not specified training
-                job will be created without VPC config.
-            security_group_ids (list[str]): List of security group ids. If not
-                specified training job will be created without VPC config.
-            model_uri (str): URI where a pre-trained model is stored, either
-                locally or in S3 (default: None). If specified, the estimator
-                will create a channel pointing to the model so the training job
-                can download it. This model can be a 'model.tar.gz' from a
-                previous training job, or other artifacts coming from a
+            subnets (list[str]): List of subnet ids. If not specified training job will be created without VPC config.
+            security_group_ids (list[str]): List of security group ids. If not specified training job will be created
+                without VPC config.
+            model_uri (str): URI where a pre-trained model is stored, either locally or in S3 (default: None). If
+                specified, the estimator will create a channel pointing to the model so the training job can download
+                it. This model can be a 'model.tar.gz' from a previous training job, or other artifacts coming from a
                 different source.
 
-                In local mode, this should point to the path in which the model
-                is located and not the file itself, as local Docker containers
-                will try to mount the URI as a volume.
-
-                More information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
-            model_channel_name (str): Name of the channel where 'model_uri' will
-                be downloaded (default: 'model').
-            metric_definitions (list[dict]): A list of dictionaries that defines
-                the metric(s) used to evaluate the training jobs. Each
-                dictionary contains two keys: 'Name' for the name of the metric,
-                and 'Regex' for the regular expression used to extract the
-                metric from the logs. This should be defined only for jobs that
-                don't use an Amazon algorithm.
-            encrypt_inter_container_traffic (bool): Specifies whether traffic
-                between training containers is encrypted for the training job
-                (default: ``False``).
+                In local mode, this should point to the path in which the model is located and not the file itself, as
+                local Docker containers will try to mount the URI as a volume.
+
+                More information: https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
+            model_channel_name (str): Name of the channel where 'model_uri' will be downloaded (default: 'model').
+            metric_definitions (list[dict]): A list of dictionaries that defines the metric(s) used to evaluate the
+                training jobs. Each dictionary contains two keys: 'Name' for the name of the metric, and 'Regex' for
+                the regular expression used to extract the metric from the logs. This should be defined only
+                for jobs that don't use an Amazon algorithm.
+            encrypt_inter_container_traffic (bool): Specifies whether traffic between training containers is encrypted
+                for the training job (default: ``False``).
         """
         self.role = role
         self.train_instance_count = train_instance_count
@@ -204,9 +180,8 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     def train_image(self):
         """Return the Docker image to use for training.
 
-        The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does
-        the model training, calls this method to find the image to use for model
-        training.
+        The  :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does the model training, calls this method to
+        find the image to use for model training.
 
         Returns:
             str: The URI of the Docker image.
@@ -216,8 +191,8 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
-        The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which
-        trains the model, calls this method to find the hyperparameters.
+        The  :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which trains the model, calls this method to
+        find the hyperparameters.
 
         Returns:
             dict[str, str]: The hyperparameters.
@@ -235,9 +210,8 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         """Set any values in the estimator that need to be set before training.
 
         Args:
-            job_name (str): Name of the training job to be created. If not
-                specified, one is generated, using the base name given to the
-                constructor if applicable.
+            * job_name (str): Name of the training job to be created. If not specified, one is generated,
+                using the base name given to the constructor if applicable.
         """
         if job_name is not None:
             self._current_job_name = job_name
@@ -264,37 +238,30 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     def fit(self, inputs=None, wait=True, logs=True, job_name=None):
         """Train a model using the input training dataset.
 
-        The API calls the Amazon SageMaker CreateTrainingJob API to start
-        model training. The API uses configuration you provided to create the
-        estimator and the specified input training data to send the
-        CreatingTrainingJob request to Amazon SageMaker.
+        The API calls the Amazon SageMaker CreateTrainingJob API to start model training.
+        The API uses configuration you provided to create the estimator and the
+        specified input training data to send the CreatingTrainingJob request to Amazon SageMaker.
 
-        This is a synchronous operation. After the model training
-        successfully completes, you can call the ``deploy()`` method to host the
-        model using the Amazon SageMaker hosting services.
+        This is a synchronous operation. After the model training successfully completes,
+        you can call the ``deploy()`` method to host the model using the Amazon SageMaker hosting services.
 
         Args:
-            inputs (str or dict or sagemaker.session.s3_input): Information
-                about the training data. This can be one of three types:
+            inputs (str or dict or sagemaker.session.s3_input): Information about the training data.
+                This can be one of three types:
 
                 * (str) the S3 location where training data is saved.
 
                 * (dict[str, str] or dict[str, sagemaker.session.s3_input]) If using multiple channels for
-                      training data, you can specify a dict mapping channel
-                      names to strings or :func:`~sagemaker.session.s3_input`
-                      objects.
-
+                    training data, you can specify a dict mapping channel names
+                    to strings or :func:`~sagemaker.session.s3_input` objects.
                 * (sagemaker.session.s3_input) - channel configuration for S3 data sources that can provide
-                      additional information as well as the path to the training
-                      dataset. See :func:`sagemaker.session.s3_input` for full
-                      details.
-            wait (bool): Whether the call should wait until the job completes
-                (default: True).
-            logs (bool): Whether to show the logs produced by the job. Only
-                meaningful when wait is True (default: True).
-            job_name (str): Training job name. If not specified, the estimator
-                generates a default job name, based on the training image name
-                and current timestamp.
+                    additional information as well as the path to the training dataset.
+                    See :func:`sagemaker.session.s3_input` for full details.
+            wait (bool): Whether the call should wait until the job completes (default: True).
+            logs (bool): Whether to show the logs produced by the job.
+                Only meaningful when wait is True (default: True).
+            job_name (str): Training job name. If not specified, the estimator generates a default job name,
+                based on the training image name and current timestamp.
         """
         self._prepare_for_training(job_name=job_name)
 
@@ -303,7 +270,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
             self.latest_training_job.wait(logs=logs)
 
     def _compilation_job_name(self):
-        """Placeholder docstring"""
         base_name = self.base_job_name or base_name_from_image(self.train_image())
         return name_from_base("compilation-" + base_name)
 
@@ -321,33 +287,25 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         """Compile a Neo model using the input model.
 
         Args:
-            target_instance_family (str): Identifies the device that you want to
-                run your model after compilation, for example: ml_c5. Allowed
-                strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2,
-                ml_p2, ml_p3, deeplens, rasp3b
-            input_shape (dict): Specifies the name and shape of the expected
-                inputs for your trained model in json dictionary form, for
-                example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28],
-                'var2':[1,1,28,28]}
+            target_instance_family (str): Identifies the device that you want to run your model after compilation, for
+                example: ml_c5. Allowed strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2, ml_p2, ml_p3,
+                deeplens, rasp3b
+            input_shape (dict): Specifies the name and shape of the expected inputs for your trained model in json
+                dictionary form, for example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28], 'var2':[1,1,28,28]}
             output_path (str): Specifies where to store the compiled model
-            framework (str): The framework that is used to train the original
-                model. Allowed values: 'mxnet', 'tensorflow', 'pytorch', 'onnx',
-                'xgboost'
+            framework (str): The framework that is used to train the original model. Allowed values: 'mxnet',
+                'tensorflow', 'pytorch', 'onnx', 'xgboost'
             framework_version (str): The version of the framework
-            compile_max_run (int): Timeout in seconds for compilation (default:
-                3 * 60). After this amount of time Amazon SageMaker Neo
-                terminates the compilation job regardless of its current status.
-            tags (list[dict]): List of tags for labeling a compilation job. For
-                more, see
+            compile_max_run (int): Timeout in seconds for compilation (default: 3 * 60).
+                After this amount of time Amazon SageMaker Neo terminates the compilation job regardless of its
+                current status.
+            tags (list[dict]): List of tags for labeling a compilation job. For more, see
                 https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
-            **kwargs: Passed to invocation of ``create_model()``.
-                Implementations may customize ``create_model()`` to accept
-                ``**kwargs`` to customize model creation during deploy. For
-                more, see the implementation docs.
-
+            **kwargs: Passed to invocation of ``create_model()``. Implementations may customize
+                ``create_model()`` to accept ``**kwargs`` to customize model creation during deploy.
+                For more, see the implementation docs.
         Returns:
-            sagemaker.model.Model: A SageMaker ``Model`` object. See
-            :func:`~sagemaker.model.Model` for full details.
+            sagemaker.model.Model: A SageMaker ``Model`` object. See :func:`~sagemaker.model.Model` for full details.
         """
         if target_instance_family not in NEO_ALLOWED_TARGET_INSTANCE_FAMILY:
             raise ValueError(
@@ -381,16 +339,21 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     def attach(cls, training_job_name, sagemaker_session=None, model_channel_name="model"):
         """Attach to an existing training job.
 
-        Create an Estimator bound to an existing training job, each subclass
-        is responsible to implement
-        ``_prepare_init_params_from_job_description()`` as this method delegates
-        the actual conversion of a training job description to the arguments
-        that the class constructor expects. After attaching, if the training job
-        has a Complete status, it can be ``deploy()`` ed to create a SageMaker
-        Endpoint and return a ``Predictor``.
+        Create an Estimator bound to an existing training job, each subclass is responsible to implement
+        ``_prepare_init_params_from_job_description()`` as this method delegates the actual conversion of a training
+        job description to the arguments that the class constructor expects. After attaching, if the training job has a
+        Complete status, it can be ``deploy()`` ed to create a SageMaker Endpoint and return a ``Predictor``.
+
+        If the training job is in progress, attach will block and display log messages
+        from the training job, until the training job completes.
 
-        If the training job is in progress, attach will block and display log
-        messages from the training job, until the training job completes.
+        Args:
+            training_job_name (str): The name of the training job to attach to.
+            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with
+                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one
+                using the default AWS configuration chain.
+            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded (default:
+                'model'). If no channel with the same name exists in the training job, this option will be ignored.
 
         Examples:
             >>> my_estimator.fit(wait=False)
@@ -399,20 +362,8 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
             >>> attached_estimator = Estimator.attach(training_job_name)
             >>> attached_estimator.deploy()
 
-        Args:
-            training_job_name (str): The name of the training job to attach to.
-            sagemaker_session (sagemaker.session.Session): Session object which
-                manages interactions with Amazon SageMaker APIs and any other
-                AWS services needed. If not specified, the estimator creates one
-                using the default AWS configuration chain.
-            model_channel_name (str): Name of the channel where pre-trained
-                model data will be downloaded (default: 'model'). If no channel
-                with the same name exists in the training job, this option will
-                be ignored.
-
         Returns:
-            Instance of the calling ``Estimator`` Class with the attached
-            training job.
+            Instance of the calling ``Estimator`` Class with the attached training job.
         """
         sagemaker_session = sagemaker_session or Session()
 
@@ -445,52 +396,40 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         model_name=None,
         **kwargs
     ):
-        """Deploy the trained model to an Amazon SageMaker endpoint and return a
-        ``sagemaker.RealTimePredictor`` object.
+        """Deploy the trained model to an Amazon SageMaker endpoint and return a ``sagemaker.RealTimePredictor`` object.
 
         More information:
         http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html
 
         Args:
-            initial_instance_count (int): Minimum number of EC2 instances to
-                deploy to an endpoint for prediction.
-            instance_type (str): Type of EC2 instance to deploy to an endpoint
-                for prediction, for example, 'ml.c4.xlarge'.
-            accelerator_type (str): Type of Elastic Inference accelerator to
-                attach to an endpoint for model loading and inference, for
-                example, 'ml.eia1.medium'. If not specified, no Elastic
-                Inference accelerator will be attached to the endpoint. For more
-                information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
-            endpoint_name (str): Name to use for creating an Amazon SageMaker
-                endpoint. If not specified, the name of the training job is
-                used.
-            use_compiled_model (bool): Flag to select whether to use compiled
-                (optimized) model. Default: False.
-            update_endpoint (bool): Flag to update the model in an existing
-                Amazon SageMaker endpoint. If True, this will deploy a new
-                EndpointConfig to an already existing endpoint and delete
-                resources corresponding to the previous EndpointConfig. Default:
-                False
-            wait (bool): Whether the call should wait until the deployment of
-                model completes (default: True).
-            model_name (str): Name to use for creating an Amazon SageMaker
-                model. If not specified, the name of the training job is used.
-            tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific
-                endpoint. Example:
-                >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]
-                For more information about tags, see
-                https://boto3.amazonaws.com/v1/documentation\
-                /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags
-            **kwargs: Passed to invocation of ``create_model()``.
-                Implementations may customize ``create_model()`` to accept
-                ``**kwargs`` to customize model creation during deploy.
+            initial_instance_count (int): Minimum number of EC2 instances to deploy to an endpoint for prediction.
+            instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,
+                for example, 'ml.c4.xlarge'.
+            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading
+                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator
+                will be attached to the endpoint.
+                For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
+            endpoint_name (str): Name to use for creating an Amazon SageMaker endpoint. If not specified, the name of
+                the training job is used.
+            use_compiled_model (bool): Flag to select whether to use compiled (optimized) model. Default: False.
+            update_endpoint (bool): Flag to update the model in an existing Amazon SageMaker endpoint.
+                If True, this will deploy a new EndpointConfig to an already existing endpoint and delete resources
+                corresponding to the previous EndpointConfig. Default: False
+            wait (bool): Whether the call should wait until the deployment of model completes (default: True).
+            model_name (str): Name to use for creating an Amazon SageMaker model. If not specified, the name of
+                the training job is used.
+            tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific endpoint. Example:
+                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]
+                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\
+                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags
+
+            **kwargs: Passed to invocation of ``create_model()``. Implementations may customize
+                ``create_model()`` to accept ``**kwargs`` to customize model creation during deploy.
                 For more, see the implementation docs.
 
         Returns:
             sagemaker.predictor.RealTimePredictor: A predictor that provides a ``predict()`` method,
-                which can be used to send requests to the Amazon SageMaker
-                endpoint and obtain inferences.
+                which can be used to send requests to the Amazon SageMaker endpoint and obtain inferences.
         """
         self._ensure_latest_training_job()
         endpoint_name = endpoint_name or self.latest_training_job.name
@@ -519,9 +458,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
 
     @property
     def model_data(self):
-        """str: The model location in S3. Only set if Estimator has been
-        ``fit()``.
-        """
+        """str: The model location in S3. Only set if Estimator has been ``fit()``."""
         if self.latest_training_job is not None:
             model_uri = self.sagemaker_session.sagemaker_client.describe_training_job(
                 TrainingJobName=self.latest_training_job.name
@@ -539,31 +476,26 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
 
     @abstractmethod
     def create_model(self, **kwargs):
-        """Create a SageMaker ``Model`` object that can be deployed to an
-        ``Endpoint``.
+        """Create a SageMaker ``Model`` object that can be deployed to an ``Endpoint``.
 
         Args:
-            **kwargs: Keyword arguments used by the implemented method for
-                creating the ``Model``.
+            **kwargs: Keyword arguments used by the implemented method for creating the ``Model``.
 
         Returns:
-            sagemaker.model.Model: A SageMaker ``Model`` object. See
-            :func:`~sagemaker.model.Model` for full details.
+            sagemaker.model.Model: A SageMaker ``Model`` object. See :func:`~sagemaker.model.Model` for full details.
         """
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
-        """Convert the job description to init params that can be handled by the
-        class constructor
+        """Convert the job description to init params that can be handled by the class constructor
 
         Args:
-            job_details: the returned job details from a describe_training_job
-                API call.
-            model_channel_name (str): Name of the channel where pre-trained
-                model data will be downloaded.
+            job_details: the returned job details from a describe_training_job API call.
+            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.
 
         Returns:
-            dictionary: The transformed init_params
+             dictionary: The transformed init_params
+
         """
         init_params = dict()
 
@@ -640,39 +572,29 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         role=None,
         volume_kms_key=None,
     ):
-        """Return a ``Transformer`` that uses a SageMaker Model based on the
-        training job. It reuses the SageMaker Session and base job name used by
-        the Estimator.
+        """Return a ``Transformer`` that uses a SageMaker Model based on the training job. It reuses the
+        SageMaker Session and base job name used by the Estimator.
 
         Args:
             instance_count (int): Number of EC2 instances to use.
-            instance_type (str): Type of EC2 instance to use, for example,
-                'ml.c4.xlarge'.
-            strategy (str): The strategy used to decide how to batch records in
-                a single request (default: None). Valid values: 'MULTI_RECORD'
-                and 'SINGLE_RECORD'.
-            assemble_with (str): How the output is assembled (default: None).
-                Valid values: 'Line' or 'None'.
-            output_path (str): S3 location for saving the transform result. If
-                not specified, results are stored to a default bucket.
-            output_kms_key (str): Optional. KMS key ID for encrypting the
-                transform output (default: None).
-            accept (str): The content type accepted by the endpoint deployed
-                during the transform job.
-            env (dict): Environment variables to be set for use during the
-                transform job (default: None).
-            max_concurrent_transforms (int): The maximum number of HTTP requests
-                to be made to each individual transform container at one time.
-            max_payload (int): Maximum size of the payload in a single HTTP
-                request to the container in MB.
-            tags (list[dict]): List of tags for labeling a transform job. If
-                none specified, then the tags used for the training job are used
-                for the transform job.
-            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,
-                which is also used during transform jobs. If not specified, the
-                role from the Estimator will be used.
-            volume_kms_key (str): Optional. KMS key ID for encrypting the volume
-                attached to the ML compute instance (default: None).
+            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.
+            strategy (str): The strategy used to decide how to batch records in a single request (default: None).
+                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.
+            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.
+            output_path (str): S3 location for saving the transform result. If not specified, results are stored to
+                a default bucket.
+            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).
+            accept (str): The content type accepted by the endpoint deployed during the transform job.
+            env (dict): Environment variables to be set for use during the transform job (default: None).
+            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to
+                each individual transform container at one time.
+            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.
+            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for
+                the training job are used for the transform job.
+            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during
+                transform jobs. If not specified, the role from the Estimator will be used.
+            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML
+                compute instance (default: None).
         """
         tags = tags or self.tags
 
@@ -707,8 +629,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
 
     @property
     def training_job_analytics(self):
-        """Return a ``TrainingJobAnalytics`` object for the current training
-        job.
+        """Return a ``TrainingJobAnalytics`` object for the current training job.
         """
         if self._current_job_name is None:
             raise ValueError("Estimator is not associated with a TrainingJob")
@@ -717,11 +638,9 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
         )
 
     def get_vpc_config(self, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT):
-        """Returns VpcConfig dict either from this Estimator's subnets and
-        security groups, or else validate and return an optional override value.
-
-        Args:
-            vpc_config_override:
+        """
+        Returns VpcConfig dict either from this Estimator's subnets and security groups,
+        or else validate and return an optional override value.
         """
         if vpc_config_override is vpc_utils.VPC_CONFIG_DEFAULT:
             return vpc_utils.to_dict(self.subnets, self.security_group_ids)
@@ -730,30 +649,22 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):
     def _ensure_latest_training_job(
         self, error_message="Estimator is not associated with a training job"
     ):
-        """
-        Args:
-            error_message:
-        """
         if self.latest_training_job is None:
             raise ValueError(error_message)
 
 
 class _TrainingJob(_Job):
-    """Placeholder docstring"""
-
     @classmethod
     def start_new(cls, estimator, inputs):
         """Create a new Amazon SageMaker training job from the estimator.
 
         Args:
-            estimator (sagemaker.estimator.EstimatorBase): Estimator object
-                created by the user.
-            inputs (str): Parameters used when called
-                :meth:`~sagemaker.estimator.EstimatorBase.fit`.
+            estimator (sagemaker.estimator.EstimatorBase): Estimator object created by the user.
+            inputs (str): Parameters used when called  :meth:`~sagemaker.estimator.EstimatorBase.fit`.
 
         Returns:
-            sagemaker.estimator._TrainingJob: Constructed object that captures
-            all information about the started training job.
+            sagemaker.estimator._TrainingJob: Constructed object that captures all information about the started
+            training job.
         """
 
         local_mode = estimator.sagemaker_session.local_mode
@@ -803,17 +714,9 @@ class _TrainingJob(_Job):
 
     @classmethod
     def _is_local_channel(cls, input_uri):
-        """
-        Args:
-            input_uri:
-        """
         return isinstance(input_uri, string_types) and input_uri.startswith("file://")
 
     def wait(self, logs=True):
-        """
-        Args:
-            logs:
-        """
         if logs:
             self.sagemaker_session.logs_for_job(self.job_name, wait=True)
         else:
@@ -821,8 +724,9 @@ class _TrainingJob(_Job):
 
 
 class Estimator(EstimatorBase):
-    """A generic Estimator to train using any supplied algorithm. This class is
-    designed for use with algorithms that don't have their own, custom class.
+    """
+    A generic Estimator to train using any supplied algorithm. This class is designed for use with
+    algorithms that don't have their own, custom class.
     """
 
     def __init__(
@@ -852,82 +756,58 @@ class Estimator(EstimatorBase):
 
         Args:
             image_name (str): The container image to use for training.
-            role (str): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role, if it needs to access an AWS resource.
-            train_instance_count (int): Number of Amazon EC2 instances to use
-                for training.
-            train_instance_type (str): Type of EC2 instance to use for training,
-                for example, 'ml.c4.xlarge'.
-            train_volume_size (int): Size in GB of the EBS volume to use for
-                storing input data during training (default: 30). Must be large
-                enough to store training data if File Mode is used (which is the
-                default).
-            train_volume_kms_key (str): Optional. KMS key ID for encrypting EBS
-                volume attached to the training instance (default: None).
-            train_max_run (int): Timeout in seconds for training (default: 24 *
-                60 * 60). After this amount of time Amazon SageMaker terminates
-                the job regardless of its current status.
-            input_mode (str): The input mode that the algorithm supports
-                (default: 'File'). Valid modes:
-
-                * 'File' - Amazon SageMaker copies the training dataset from the
-                  S3 location to a local directory.
-                * 'Pipe' - Amazon SageMaker streams data directly from S3 to the
-                  container via a Unix-named pipe.
-
-                This argument can be overriden on a per-channel basis using
-                ``sagemaker.session.s3_input.input_mode``.
-            output_path (str): S3 location for saving the training result (model
-                artifacts and output files). If not specified, results are
-                stored to a default bucket. If the bucket with the specific name
+            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs
+                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.
+                After the endpoint is created, the inference code might use the IAM role,
+                if it needs to access an AWS resource.
+            train_instance_count (int): Number of Amazon EC2 instances to use for training.
+            train_instance_type (str): Type of EC2 instance to use for training, for example, 'ml.c4.xlarge'.
+            train_volume_size (int): Size in GB of the EBS volume to use for storing input data
+                during training (default: 30). Must be large enough to store training data if File Mode is used
+                (which is the default).
+            train_volume_kms_key (str): Optional. KMS key ID for encrypting EBS volume attached to the
+                training instance (default: None).
+            train_max_run (int): Timeout in seconds for training (default: 24 * 60 * 60).
+                After this amount of time Amazon SageMaker terminates the job regardless of its current status.
+            input_mode (str): The input mode that the algorithm supports (default: 'File'). Valid modes:
+
+                * 'File' - Amazon SageMaker copies the training dataset from the S3 location to a local directory.
+                * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe.
+
+                This argument can be overriden on a per-channel basis using ``sagemaker.session.s3_input.input_mode``.
+            output_path (str): S3 location for saving the training result (model artifacts and output files).
+                If not specified, results are stored to a default bucket. If the bucket with the specific name
                 does not exist, the estimator creates the bucket during the
                 :meth:`~sagemaker.estimator.EstimatorBase.fit` method execution.
-            output_kms_key (str): Optional. KMS key ID for encrypting the
-                training output (default: None).
-            base_job_name (str): Prefix for training job name when the
-                :meth:`~sagemaker.estimator.EstimatorBase.fit` method launches.
-                If not specified, the estimator generates a default job name,
-                based on the training image name and current timestamp.
-            sagemaker_session (sagemaker.session.Session): Session object which
-                manages interactions with Amazon SageMaker APIs and any other
-                AWS services needed. If not specified, the estimator creates one
+            output_kms_key (str): Optional. KMS key ID for encrypting the training output (default: None).
+            base_job_name (str): Prefix for training job name when the :meth:`~sagemaker.estimator.EstimatorBase.fit`
+                method launches. If not specified, the estimator generates a default job name, based on
+                the training image name and current timestamp.
+            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with
+                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one
                 using the default AWS configuration chain.
-            hyperparameters (dict): Dictionary containing the hyperparameters to
-                initialize this estimator with.
-            tags (list[dict]): List of tags for labeling a training job. For
-                more, see
+            hyperparameters (dict): Dictionary containing the hyperparameters to initialize this estimator with.
+            tags (list[dict]): List of tags for labeling a training job. For more, see
                 https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
-            subnets (list[str]): List of subnet ids. If not specified training
-                job will be created without VPC config.
-            security_group_ids (list[str]): List of security group ids. If not
-                specified training job will be created without VPC config.
-            model_uri (str): URI where a pre-trained model is stored, either
-                locally or in S3 (default: None). If specified, the estimator
-                will create a channel pointing to the model so the training job
-                can download it. This model can be a 'model.tar.gz' from a
-                previous training job, or other artifacts coming from a
+            subnets (list[str]): List of subnet ids. If not specified training job will be created without VPC config.
+            security_group_ids (list[str]): List of security group ids. If not specified training job will be created
+                without VPC config.
+            model_uri (str): URI where a pre-trained model is stored, either locally or in S3 (default: None). If
+                specified, the estimator will create a channel pointing to the model so the training job can download
+                it. This model can be a 'model.tar.gz' from a previous training job, or other artifacts coming from a
                 different source.
 
-                In local mode, this should point to the path in which the model
-                is located and not the file itself, as local Docker containers
-                will try to mount the URI as a volume.
-
-                More information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
-            model_channel_name (str): Name of the channel where 'model_uri' will
-                be downloaded (default: 'model').
-            metric_definitions (list[dict]): A list of dictionaries that defines
-                the metric(s) used to evaluate the training jobs. Each
-                dictionary contains two keys: 'Name' for the name of the metric,
-                and 'Regex' for the regular expression used to extract the
-                metric from the logs. This should be defined only for jobs that
-                don't use an Amazon algorithm.
-            encrypt_inter_container_traffic (bool): Specifies whether traffic
-                between training containers is encrypted for the training job
-                (default: ``False``).
+                In local mode, this should point to the path in which the model is located and not the file itself,
+                as local Docker containers will try to mount the URI as a volume.
+
+                More information: https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
+            model_channel_name (str): Name of the channel where 'model_uri' will be downloaded (default: 'model').
+            metric_definitions (list[dict]): A list of dictionaries that defines the metric(s) used to evaluate the
+                training jobs. Each dictionary contains two keys: 'Name' for the name of the metric, and 'Regex' for
+                the regular expression used to extract the metric from the logs. This should be defined only
+                for jobs that don't use an Amazon algorithm.
+            encrypt_inter_container_traffic (bool): Specifies whether traffic between training containers is encrypted
+                for the training job (default: ``False``).
         """
         self.image_name = image_name
         self.hyperparam_dict = hyperparameters.copy() if hyperparameters else {}
@@ -953,26 +833,21 @@ class Estimator(EstimatorBase):
         )
 
     def train_image(self):
-        """Returns the docker image to use for training.
+        """
+        Returns the docker image to use for training.
 
-        The fit() method, that does the model training, calls this method to
-        find the image to use for model training.
+        The fit() method, that does the model training, calls this method to find the image to use for model training.
         """
         return self.image_name
 
     def set_hyperparameters(self, **kwargs):
-        """
-        Args:
-            **kwargs:
-        """
         for k, v in kwargs.items():
             self.hyperparam_dict[k] = v
 
     def hyperparameters(self):
         """Returns the hyperparameters as a dictionary to use for training.
 
-        The fit() method, that does the model training, calls this method to
-        find the hyperparameters you specified.
+       The fit() method, that does the model training, calls this method to find the hyperparameters you specified.
         """
         return self.hyperparam_dict
 
@@ -988,37 +863,29 @@ class Estimator(EstimatorBase):
         vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT,
         **kwargs
     ):
-        """Create a model to deploy.
-
-        The serializer, deserializer, content_type, and accept arguments are only used to define a default
-        RealTimePredictor. They are ignored if an explicit predictor class is passed in. Other arguments
-        are passed through to the Model class.
+        """
+        Create a model to deploy.
 
         Args:
-            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,
-                which is also used during transform jobs. If not specified, the
-                role from the Estimator will be used.
-            image (str): An container image to use for deploying the model.
-                Defaults to the image used for training.
-            predictor_cls (RealTimePredictor): The predictor class to use when
-                deploying the model.
-            serializer (callable): Should accept a single argument, the input
-                data, and return a sequence of bytes. May provide a content_type
-                attribute that defines the endpoint request content type
-            deserializer (callable): Should accept two arguments, the result
-                data and the response content type, and return a sequence of
-                bytes. May provide a content_type attribute that defines th
-                endpoint response Accept content type.
-            content_type (str): The invocation ContentType, overriding any
-                content_type from the serializer
-            accept (str): The invocation Accept, overriding any accept from the
-                deserializer.
-            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on
-                the model.
+            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during
+                transform jobs. If not specified, the role from the Estimator will be used.
+            image (str): An container image to use for deploying the model. Defaults to the image used for training.
+            predictor_cls (RealTimePredictor): The predictor class to use when deploying the model.
+            serializer (callable): Should accept a single argument, the input data, and return a sequence
+                of bytes. May provide a content_type attribute that defines the endpoint request content type
+            deserializer (callable): Should accept two arguments, the result data and the response content type,
+                and return a sequence of bytes. May provide a content_type attribute that defines th endpoint
+                response Accept content type.
+            content_type (str): The invocation ContentType, overriding any content_type from the serializer
+            accept (str): The invocation Accept, overriding any accept from the deserializer.
+            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.
                 Default: use subnets and security groups from this Estimator.
                 * 'Subnets' (list[str]): List of subnet ids.
                 * 'SecurityGroupIds' (list[str]): List of security group ids.
-            **kwargs:
+
+            The serializer, deserializer, content_type, and accept arguments are only used to define a default
+            RealTimePredictor. They are ignored if an explicit predictor class is passed in. Other arguments
+            are passed through to the Model class.
 
         Returns: a Model ready for deployment.
         """
@@ -1045,17 +912,15 @@ class Estimator(EstimatorBase):
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
-        """Convert the job description to init params that can be handled by the
-        class constructor
+        """Convert the job description to init params that can be handled by the class constructor
 
         Args:
-            job_details: the returned job details from a describe_training_job
-                API call.
-            model_channel_name (str): Name of the channel where pre-trained
-                model data will be downloaded
+            job_details: the returned job details from a describe_training_job API call.
+            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded
 
         Returns:
-            dictionary: The transformed init_params
+             dictionary: The transformed init_params
+
         """
         init_params = super(Estimator, cls)._prepare_init_params_from_job_description(
             job_details, model_channel_name
@@ -1094,16 +959,14 @@ class Framework(EstimatorBase):
         git_config=None,
         **kwargs
     ):
-        """Base class initializer. Subclasses which override ``__init__`` should
-        invoke ``super()``
+        """Base class initializer. Subclasses which override ``__init__`` should invoke ``super()``
 
         Args:
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. This should be compatible with either Python 2.7 or
-                Python 3.5. If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-                Example
+            entry_point (str): Path (absolute or relative) to the local Python source file which should be executed
+                as the entry point to training. This should be compatible with either Python 2.7 or Python 3.5.
+                If 'git_config' is provided, 'entry_point' should be a relative location to the Python source file in
+                the Git repo.
+                Example:
 
                     With the following GitHub repo directory structure:
 
@@ -1113,12 +976,45 @@ class Framework(EstimatorBase):
                     >>>         |----- test.py
 
                     You can assign entry_point='src/train.py'.
-            source_dir (str): Path (absolute or relative) to a directory with
-                any other training source code dependencies aside from the entry
-                point file (default: None). Structure within this directory are
-                preserved when training on Amazon SageMaker. If 'git_config' is
-                provided, 'source_dir' should be a relative location to a
-                directory in the Git repo. .. admonition:: Example
+            git_config (dict[str, str]): Git configurations used for cloning files, including ``repo``, ``branch``,
+                ``commit``, ``2FA_enabled``, ``username``, ``password`` and ``token``. The ``repo`` field is required.
+                All other fields are optional. ``repo`` specifies the Git repository where your training script is
+                stored. If you don't provide ``branch``, the default value  'master' is used. If you don't provide
+                ``commit``, the latest commit in the specified branch is used.
+                Example:
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then checkout the 'master' branch, and checkout
+                    the specified commit.
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are used for authentication. For GitHub
+                (or other Git) accounts, set ``2FA_enabled`` to 'True' if two-factor authentication is enabled for the
+                account, otherwise set it to 'False'. If you do not provide a value for ``2FA_enabled``, a default
+                value of 'False' is used. CodeCommit does not support two-factor authentication, so do not provide
+                "2FA_enabled" with CodeCommit repositories.
+
+                For GitHub and other Git repos, when SSH URLs are provided, it doesn't matter whether 2FA is
+                enabled or disabled; you should either have no passphrase for the SSH key pairs, or have the ssh-agent
+                configured so that you will not be prompted for SSH passphrase when you do 'git clone' command with SSH
+                URLs. When HTTPS URLs are provided: if 2FA is disabled, then either token or username+password will be
+                used for authentication if provided (token prioritized); if 2FA is enabled, only token will be used for
+                authentication if provided. If required authentication info is not provided, python SDK will try to use
+                local credentials storage to authenticate. If that fails either, an error message will be thrown.
+
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled' should not be provided. There is no token
+                in CodeCommit, so 'token' should not be provided too. When 'repo' is an SSH URL, the requirements are
+                the same as GitHub-like repos. When 'repo' is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will try to use either CodeCommit credential
+                helper or local credential storage for authentication.
+            source_dir (str): Path (absolute or relative) to a directory with any other training
+                source code dependencies aside from the entry point file (default: None). Structure within this
+                directory are preserved when training on Amazon SageMaker. If 'git_config' is provided,
+                'source_dir' should be a relative location to a directory in the Git repo.
+                Example:
 
                     With the following GitHub repo directory structure:
 
@@ -1127,42 +1023,33 @@ class Framework(EstimatorBase):
                     >>>         |----- train.py
                     >>>         |----- test.py
 
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            hyperparameters (dict): Hyperparameters that will be used for
-                training (default: None). The hyperparameters are made
-                accessible as a dict[str, str] to the training code on
-                SageMaker. For convenience, this accepts other types for keys
-                and values, but ``str()`` will be called to convert them before
-                training.
-            enable_cloudwatch_metrics (bool): [DEPRECATED] Now there are
-                cloudwatch metrics emitted by all SageMaker training jobs. This
-                will be ignored for now and removed in a further release.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None). The code file uploaded in S3 is
-                'code_location/source/sourcedir.tar.gz'. If not specified, the
-                default code location is s3://default_bucket/job-name/. And code
-                file uploaded to S3 is
-                s3://default_bucket/job-name/source/sourcedir.tar.gz
-            image_name (str): An alternate image name to use instead of the
-                official Sagemaker image for the framework. This is useful to
-                run one of the Sagemaker supported frameworks with an image
-                containing custom dependencies.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. .. admonition:: Example
-
-                    The following call >>> Estimator(entry_point='train.py',
-                    dependencies=['my/libs/common', 'virtual-env']) results in
-                    the following inside the container:
+                    and you need 'train.py' as entry point and 'test.py' as training source code as well, you can
+                    assign entry_point='train.py', source_dir='src'.
+            hyperparameters (dict): Hyperparameters that will be used for training (default: None).
+                The hyperparameters are made accessible as a dict[str, str] to the training code on SageMaker.
+                For convenience, this accepts other types for keys and values, but ``str()`` will be called
+                to convert them before training.
+            enable_cloudwatch_metrics (bool): [DEPRECATED] Now there are cloudwatch metrics emitted by all SageMaker
+                training jobs. This will be ignored for now and removed in a further release.
+            container_log_level (int): Log level to use within the container (default: logging.INFO).
+                Valid values are defined in the Python logging module.
+            code_location (str): The S3 prefix URI where custom code will be uploaded (default: None).
+                The code file uploaded in S3 is 'code_location/source/sourcedir.tar.gz'.
+                If not specified, the default code location is s3://default_bucket/job-name/. And code file
+                uploaded to S3 is s3://default_bucket/job-name/source/sourcedir.tar.gz
+            image_name (str): An alternate image name to use instead of the official Sagemaker image
+                for the framework. This is useful to run one of the Sagemaker supported frameworks
+                with an image containing custom dependencies.
+            dependencies (list[str]): A list of paths to directories (absolute or relative) with
+                any additional libraries that will be exported to the container (default: []).
+                The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+                If 'git_config' is provided, 'dependencies' should be a list of relative locations to directories
+                with any additional libraries needed in the Git repo.
+                Example:
+
+                    The following call
+                    >>> Estimator(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+                    results in the following inside the container:
 
                     >>> $ ls
 
@@ -1170,66 +1057,13 @@ class Framework(EstimatorBase):
                     >>>     |------ train.py
                     >>>     |------ common
                     >>>     |------ virtual-env
-            enable_network_isolation (bool): Specifies whether container will
-                run in network isolation mode. Network isolation mode restricts
-                the container access to outside networks (such as the internet).
-                The container does not make any inbound or outbound network
-                calls. If True, a channel named "code" will be created for any
-                user entry script for training. The user entry script, files in
-                source_dir (if specified), and dependencies will be uploaded in
-                a tar to S3. Also known as internet-free mode (default: `False`
-                ).
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
 
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            **kwargs: Additional kwargs passed to the ``EstimatorBase``
-                constructor.
+            enable_network_isolation (bool): Specifies whether container will run in network isolation mode. Network
+                isolation mode restricts the container access to outside networks (such as the internet). The container
+                does not make any inbound or outbound network calls. If True, a channel named "code" will be created
+                for any user entry script for training. The user entry script, files in source_dir (if specified), and
+                dependencies will be uploaded in a tar to S3. Also known as internet-free mode (default: `False`).
+            **kwargs: Additional kwargs passed to the ``EstimatorBase`` constructor.
         """
         super(Framework, self).__init__(**kwargs)
         if entry_point.startswith("s3://"):
@@ -1266,13 +1100,11 @@ class Framework(EstimatorBase):
         return self._enable_network_isolation
 
     def _prepare_for_training(self, job_name=None):
-        """Set hyperparameters needed for training. This method will also
-        validate ``source_dir``.
+        """Set hyperparameters needed for training. This method will also validate ``source_dir``.
 
         Args:
-           * job_name (str): Name of the training job to be created. If not
-                specified, one is generated, using the base name given to the
-                constructor if applicable.
+            * job_name (str): Name of the training job to be created. If not specified, one is generated,
+                using the base name given to the constructor if applicable.
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
@@ -1322,6 +1154,7 @@ class Framework(EstimatorBase):
         """Upload the user training script to s3 and return the location.
 
         Returns: s3 uri
+
         """
         local_mode = self.output_path.startswith("file://")
 
@@ -1352,12 +1185,10 @@ class Framework(EstimatorBase):
         )
 
     def _model_source_dir(self):
-        """Get the appropriate value to pass as source_dir to model constructor
-        on deploying
+        """Get the appropriate value to pass as source_dir to model constructor on deploying
 
         Returns:
-            str: Either a local or an S3 path pointing to the source_dir to be
-            used for code by the model to be deployed
+            str: Either a local or an S3 path pointing to the source_dir to be used for code by the model to be deployed
         """
         return (
             self.source_dir if self.sagemaker_session.local_mode else self.uploaded_code.s3_prefix
@@ -1366,8 +1197,8 @@ class Framework(EstimatorBase):
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
-        The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which
-        trains the model, calls this method to find the hyperparameters.
+        The  :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which trains the model, calls this method
+        to find the hyperparameters.
 
         Returns:
             dict[str, str]: The hyperparameters.
@@ -1376,17 +1207,15 @@ class Framework(EstimatorBase):
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
-        """Convert the job description to init params that can be handled by the
-        class constructor
+        """Convert the job description to init params that can be handled by the class constructor
 
         Args:
-            job_details: the returned job details from a describe_training_job
-                API call.
-            model_channel_name (str): Name of the channel where pre-trained
-                model data will be downloaded
+            job_details: the returned job details from a describe_training_job API call.
+            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded
 
         Returns:
-            dictionary: The transformed init_params
+             dictionary: The transformed init_params
+
         """
         init_params = super(Framework, cls)._prepare_init_params_from_job_description(
             job_details, model_channel_name
@@ -1420,9 +1249,8 @@ class Framework(EstimatorBase):
     def train_image(self):
         """Return the Docker image to use for training.
 
-        The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does
-        the model training, calls this method to find the image to use for model
-        training.
+        The  :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does the model training,
+        calls this method to find the image to use for model training.
 
         Returns:
             str: The URI of the Docker image.
@@ -1441,16 +1269,21 @@ class Framework(EstimatorBase):
     def attach(cls, training_job_name, sagemaker_session=None, model_channel_name="model"):
         """Attach to an existing training job.
 
-        Create an Estimator bound to an existing training job, each subclass
-        is responsible to implement
-        ``_prepare_init_params_from_job_description()`` as this method delegates
-        the actual conversion of a training job description to the arguments
-        that the class constructor expects. After attaching, if the training job
-        has a Complete status, it can be ``deploy()`` ed to create a SageMaker
-        Endpoint and return a ``Predictor``.
+        Create an Estimator bound to an existing training job, each subclass is responsible to implement
+        ``_prepare_init_params_from_job_description()`` as this method delegates the actual conversion of a training
+        job description to the arguments that the class constructor expects. After attaching, if the training job has a
+        Complete status, it can be ``deploy()`` ed to create a SageMaker Endpoint and return a ``Predictor``.
 
-        If the training job is in progress, attach will block and display log
-        messages from the training job, until the training job completes.
+        If the training job is in progress, attach will block and display log messages
+        from the training job, until the training job completes.
+
+        Args:
+            training_job_name (str): The name of the training job to attach to.
+            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with
+                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one
+                using the default AWS configuration chain.
+            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded (default:
+                'model'). If no channel with the same name exists in the training job, this option will be ignored.
 
         Examples:
             >>> my_estimator.fit(wait=False)
@@ -1459,20 +1292,8 @@ class Framework(EstimatorBase):
             >>> attached_estimator = Estimator.attach(training_job_name)
             >>> attached_estimator.deploy()
 
-        Args:
-            training_job_name (str): The name of the training job to attach to.
-            sagemaker_session (sagemaker.session.Session): Session object which
-                manages interactions with Amazon SageMaker APIs and any other
-                AWS services needed. If not specified, the estimator creates one
-                using the default AWS configuration chain.
-            model_channel_name (str): Name of the channel where pre-trained
-                model data will be downloaded (default: 'model'). If no channel
-                with the same name exists in the training job, this option will
-                be ignored.
-
         Returns:
-            Instance of the calling ``Estimator`` Class with the attached
-            training job.
+            Instance of the calling ``Estimator`` Class with the attached training job.
         """
         estimator = super(Framework, cls).attach(
             training_job_name, sagemaker_session, model_channel_name
@@ -1487,19 +1308,10 @@ class Framework(EstimatorBase):
 
     @staticmethod
     def _json_encode_hyperparameters(hyperparameters):
-        """
-        Args:
-            hyperparameters:
-        """
         return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}
 
     @classmethod
     def _update_init_params(cls, hp, tf_arguments):
-        """
-        Args:
-            hp:
-            tf_arguments:
-        """
         updated_params = {}
         for argument in tf_arguments:
             value = hp.pop(argument, None)
@@ -1525,42 +1337,31 @@ class Framework(EstimatorBase):
         model_server_workers=None,
         volume_kms_key=None,
     ):
-        """Return a ``Transformer`` that uses a SageMaker Model based on the
-        training job. It reuses the SageMaker Session and base job name used by
-        the Estimator.
+        """Return a ``Transformer`` that uses a SageMaker Model based on the training job. It reuses the
+        SageMaker Session and base job name used by the Estimator.
 
         Args:
             instance_count (int): Number of EC2 instances to use.
-            instance_type (str): Type of EC2 instance to use, for example,
-                'ml.c4.xlarge'.
-            strategy (str): The strategy used to decide how to batch records in
-                a single request (default: None). Valid values: 'MULTI_RECORD'
-                and 'SINGLE_RECORD'.
-            assemble_with (str): How the output is assembled (default: None).
-                Valid values: 'Line' or 'None'.
-            output_path (str): S3 location for saving the transform result. If
-                not specified, results are stored to a default bucket.
-            output_kms_key (str): Optional. KMS key ID for encrypting the
-                transform output (default: None).
-            accept (str): The content type accepted by the endpoint deployed
-                during the transform job.
-            env (dict): Environment variables to be set for use during the
-                transform job (default: None).
-            max_concurrent_transforms (int): The maximum number of HTTP requests
-                to be made to each individual transform container at one time.
-            max_payload (int): Maximum size of the payload in a single HTTP
-                request to the container in MB.
-            tags (list[dict]): List of tags for labeling a transform job. If
-                none specified, then the tags used for the training job are used
-                for the transform job.
-            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,
-                which is also used during transform jobs. If not specified, the
-                role from the Estimator will be used.
-            model_server_workers (int): Optional. The number of worker processes
-                used by the inference server. If None, server will use one
-                worker per vCPU.
-            volume_kms_key (str): Optional. KMS key ID for encrypting the volume
-                attached to the ML compute instance (default: None).
+            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.
+            strategy (str): The strategy used to decide how to batch records in a single request (default: None).
+                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.
+            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.
+            output_path (str): S3 location for saving the transform result. If not specified, results are stored to
+                a default bucket.
+            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).
+            accept (str): The content type accepted by the endpoint deployed during the transform job.
+            env (dict): Environment variables to be set for use during the transform job (default: None).
+            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to
+                each individual transform container at one time.
+            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.
+            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for
+                the training job are used for the transform job.
+            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during
+                transform jobs. If not specified, the role from the Estimator will be used.
+            model_server_workers (int): Optional. The number of worker processes used by the inference server.
+                If None, server will use one worker per vCPU.
+            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML
+                compute instance (default: None).
         """
         role = role or self.role
 
@@ -1606,11 +1407,6 @@ class Framework(EstimatorBase):
 
 
 def _s3_uri_prefix(channel_name, s3_data):
-    """
-    Args:
-        channel_name:
-        s3_data:
-    """
     if isinstance(s3_data, s3_input):
         s3_uri = s3_data.config["DataSource"]["S3DataSource"]["S3Uri"]
     else:
@@ -1624,10 +1420,6 @@ def _s3_uri_prefix(channel_name, s3_data):
 # Also accepts other valid input types, e.g. dict and s3_input.
 def _s3_uri_without_prefix_from_input(input_data):
     # Unpack an input_config object from a dict if a dict was passed in.
-    """
-    Args:
-        input_data:
-    """
     if isinstance(input_data, dict):
         response = {}
         for channel_name, channel_s3_uri in input_data.items():

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-18 18:16:06[0m
[92mHash: 356283e8c599d9b5acaa1bfff4c32fe20d4ad9c1[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: change: format and add missing docstring placeholders (#945)

This commit will format all existing docstring to follow Google
style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html
This commit will also add docstring placeholders to any class or method
previously missing it.

An ideal approach would be to take the time to include meaningful
docstrings in every file. However, since that is not a task that will
be prioritized, I've declared docstring bankruptcy on this package, in
order to enforce docstring on all future code changes to this package.[0m
@@ -10,7 +10,6 @@
 # distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
-"""Placeholder docstring"""
 from __future__ import absolute_import
 
 import json
@@ -71,37 +70,26 @@ class Model(object):
         """Initialize an SageMaker ``Model``.
 
         Args:
-            model_data (str): The S3 location of a SageMaker model data
-                ``.tar.gz`` file.
+            model_data (str): The S3 location of a SageMaker model data ``.tar.gz`` file.
             image (str): A Docker image URI.
-            role (str): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role if it needs to access some AWS resources.
-                It can be null if this is being used to create a Model to pass
-                to a ``PipelineModel`` which has its own Role field. (default:
-                None)
-            predictor_cls (callable[string, sagemaker.session.Session]): A
-                function to call to create a predictor (default: None). If not
-                None, ``deploy`` will return the result of invoking this
-                function on the created endpoint name.
-            env (dict[str, str]): Environment variables to run with ``image``
-                when hosted in SageMaker (default: None).
-            name (str): The model name. If None, a default model name will be
-                selected on each ``deploy``.
-            vpc_config (dict[str, list[str]]): The VpcConfig set on the model
-                (default: None)
+            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs
+                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.
+                After the endpoint is created, the inference code might use the IAM role if it needs to access some AWS
+                resources. It can be null if this is being used to create a Model to pass to a ``PipelineModel`` which
+                has its own Role field. (default: None)
+            predictor_cls (callable[string, sagemaker.session.Session]): A function to call to create
+               a predictor (default: None). If not None, ``deploy`` will return the result of invoking
+               this function on the created endpoint name.
+            env (dict[str, str]): Environment variables to run with ``image`` when hosted in SageMaker (default: None).
+            name (str): The model name. If None, a default model name will be selected on each ``deploy``.
+            vpc_config (dict[str, list[str]]): The VpcConfig set on the model (default: None)
                 * 'Subnets' (list[str]): List of subnet ids.
                 * 'SecurityGroupIds' (list[str]): List of security group ids.
-            sagemaker_session (sagemaker.session.Session): A SageMaker Session
-                object, used for SageMaker interactions (default: None). If not
-                specified, one is created using the default AWS configuration
-                chain.
-            enable_network_isolation (Boolean): Default False. if True, enables
-                network isolation in the endpoint, isolating the model
-                container. No inbound or outbound network calls can be made to
-                or from the model container.
+            sagemaker_session (sagemaker.session.Session): A SageMaker Session object, used for SageMaker
+               interactions (default: None). If not specified, one is created using the default AWS configuration chain.
+            enable_network_isolation (Boolean): Default False. if True, enables network isolation in the endpoint,
+                isolating the model container. No inbound or outbound network calls can be made to or from the
+                model container.
         """
         self.model_data = model_data
         self.image = image
@@ -119,18 +107,15 @@ class Model(object):
     def prepare_container_def(
         self, instance_type, accelerator_type=None
     ):  # pylint: disable=unused-argument
-        """Return a dict created by ``sagemaker.container_def()`` for deploying
-        this model to a specified instance type.
+        """Return a dict created by ``sagemaker.container_def()`` for deploying this model to a specified instance type.
 
-        Subclasses can override this to provide custom container definitions
-        for deployment to a specific instance type. Called by ``deploy()``.
+        Subclasses can override this to provide custom container definitions for
+        deployment to a specific instance type. Called by ``deploy()``.
 
         Args:
-            instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge'.
-            accelerator_type (str): The Elastic Inference accelerator type to
-                deploy to the instance for loading and making inferences to the
-                model. For example, 'ml.eia1.medium'.
+            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.
+            accelerator_type (str): The Elastic Inference accelerator type to deploy to the instance for loading and
+                making inferences to the model. For example, 'ml.eia1.medium'.
 
         Returns:
             dict: A container definition object usable with the CreateModel API.
@@ -149,18 +134,16 @@ class Model(object):
         """Create a SageMaker Model Entity
 
         Args:
-            instance_type (str): The EC2 instance type that this Model will be
-                used for, this is only used to determine if the image needs GPU
-                support or not.
-            accelerator_type (str): Type of Elastic Inference accelerator to
-                attach to an endpoint for model loading and inference, for
-                example, 'ml.eia1.medium'. If not specified, no Elastic
-                Inference accelerator will be attached to the endpoint.
-            tags (List[dict[str, str]]): Optional. The list of tags to add to
-                the model. Example: >>> tags = [{'Key': 'tagname', 'Value':
-                'tagvalue'}] For more information about tags, see
-                https://boto3.amazonaws.com/v1/documentation
-                /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags
+            instance_type (str): The EC2 instance type that this Model will be used for, this is only
+                used to determine if the image needs GPU support or not.
+            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading
+                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator
+                will be attached to the endpoint.
+            tags(List[dict[str, str]]): Optional. The list of tags to add to the model. Example:
+                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]
+                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\
+                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags
+
         """
         container_def = self.prepare_container_def(instance_type, accelerator_type=accelerator_type)
         self.name = self.name or utils.name_from_image(container_def["Image"])
@@ -175,11 +158,9 @@ class Model(object):
         )
 
     def _framework(self):
-        """Placeholder docstring"""
         return getattr(self, "__framework_name__", None)
 
     def _get_framework_version(self):
-        """Placeholder docstring"""
         return getattr(self, "framework_version", None)
 
     def _compilation_job_config(
@@ -193,17 +174,6 @@ class Model(object):
         framework,
         tags,
     ):
-        """
-        Args:
-            target_instance_type:
-            input_shape:
-            output_path:
-            role:
-            compile_max_run:
-            job_name:
-            framework:
-            tags:
-        """
         input_model_config = {
             "S3Uri": self.model_data,
             "DataInputConfig": input_shape
@@ -231,20 +201,14 @@ class Model(object):
 
         Args:
             region (str): Specifies the region where want to execute compilation
-
         Returns:
-            bool: boolean value whether if neo is available in the specified
-            region
+            bool: boolean value whether if neo is available in the specified region
         """
         if region in NEO_IMAGE_ACCOUNT:
             return True
         return False
 
     def _neo_image_account(self, region):
-        """
-        Args:
-            region:
-        """
         if region not in NEO_IMAGE_ACCOUNT:
             raise ValueError(
                 "Neo is not currently supported in {}, "
@@ -253,13 +217,6 @@ class Model(object):
         return NEO_IMAGE_ACCOUNT[region]
 
     def _neo_image(self, region, target_instance_type, framework, framework_version):
-        """
-        Args:
-            region:
-            target_instance_type:
-            framework:
-            framework_version:
-        """
         return fw_utils.create_image_uri(
             region,
             "neo-" + framework.lower(),
@@ -284,31 +241,24 @@ class Model(object):
         """Compile this ``Model`` with SageMaker Neo.
 
         Args:
-            target_instance_family (str): Identifies the device that you want to
-                run your model after compilation, for example: ml_c5. Allowed
-                strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2,
-                ml_p2, ml_p3, deeplens, rasp3b
-            input_shape (dict): Specifies the name and shape of the expected
-                inputs for your trained model in json dictionary form, for
-                example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28],
-                'var2':[1,1,28,28]}
+            target_instance_family (str): Identifies the device that you want to run your model after compilation, for
+                example: ml_c5. Allowed strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2, ml_p2, ml_p3,
+                deeplens, rasp3b
+            input_shape (dict): Specifies the name and shape of the expected inputs for your trained model in json
+                dictionary form, for example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28], 'var2':[1,1,28,28]}
             output_path (str): Specifies where to store the compiled model
             role (str): Execution role
-            tags (list[dict]): List of tags for labeling a compilation job. For
-                more, see
+            tags (list[dict]): List of tags for labeling a compilation job. For more, see
                 https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.
             job_name (str): The name of the compilation job
-            compile_max_run (int): Timeout in seconds for compilation (default:
-                3 * 60). After this amount of time Amazon SageMaker Neo
-                terminates the compilation job regardless of its current status.
-            framework (str): The framework that is used to train the original
-                model. Allowed values: 'mxnet', 'tensorflow', 'pytorch', 'onnx',
-                'xgboost'
-            framework_version (str):
-
+            compile_max_run (int): Timeout in seconds for compilation (default: 3 * 60).
+                After this amount of time Amazon SageMaker Neo terminates the compilation job regardless of its
+                current status.
+            framework (str): The framework that is used to train the original model. Allowed values: 'mxnet',
+                'tensorflow', 'pytorch', 'onnx', 'xgboost'
+            framework_version (str)
         Returns:
-            sagemaker.model.Model: A SageMaker ``Model`` object. See
-            :func:`~sagemaker.model.Model` for full details.
+            sagemaker.model.Model: A SageMaker ``Model`` object. See :func:`~sagemaker.model.Model` for full details.
         """
         framework = self._framework() or framework
         if framework is None:
@@ -365,50 +315,38 @@ class Model(object):
         kms_key=None,
         wait=True,
     ):
-        """Deploy this ``Model`` to an ``Endpoint`` and optionally return a
-        ``Predictor``.
+        """Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.
 
-        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an
-        ``Endpoint`` from this ``Model``. If ``self.predictor_cls`` is not None,
-        this method returns a the result of invoking ``self.predictor_cls`` on
-        the created endpoint name.
+        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.
+        If ``self.predictor_cls`` is not None, this method returns a the result of invoking
+        ``self.predictor_cls`` on the created endpoint name.
 
-        The name of the created model is accessible in the ``name`` field of
-        this ``Model`` after deploy returns
+        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns
 
-        The name of the created endpoint is accessible in the
-        ``endpoint_name`` field of this ``Model`` after deploy returns.
+        The name of the created endpoint is accessible in the ``endpoint_name``
+        field of this ``Model`` after deploy returns.
 
         Args:
-            initial_instance_count (int): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``.
-            instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge'.
-            accelerator_type (str): Type of Elastic Inference accelerator to
-                deploy this model for model loading and inference, for example,
-                'ml.eia1.medium'. If not specified, no Elastic Inference
-                accelerator will be attached to the endpoint. For more
-                information:
-                https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
-            endpoint_name (str): The name of the endpoint to create (default:
-                None). If not specified, a unique endpoint name will be created.
-            update_endpoint (bool): Flag to update the model in an existing
-                Amazon SageMaker endpoint. If True, this will deploy a new
-                EndpointConfig to an already existing endpoint and delete
-                resources corresponding to the previous EndpointConfig. If
-                False, a new endpoint will be created. Default: False
-            tags (List[dict[str, str]]): The list of tags to attach to this
-                specific endpoint.
-            kms_key (str): The ARN of the KMS key that is used to encrypt the
-                data on the storage volume attached to the instance hosting the
-                endpoint.
-            wait (bool): Whether the call should wait until the deployment of
-                this model completes (default: True).
+            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.
+            initial_instance_count (int): The initial number of instances to run in the
+                ``Endpoint`` created from this ``Model``.
+            accelerator_type (str): Type of Elastic Inference accelerator to deploy this model for model loading
+                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator
+                will be attached to the endpoint.
+                For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html
+            endpoint_name (str): The name of the endpoint to create (default: None).
+                If not specified, a unique endpoint name will be created.
+            update_endpoint (bool): Flag to update the model in an existing Amazon SageMaker endpoint.
+                If True, this will deploy a new EndpointConfig to an already existing endpoint and delete resources
+                corresponding to the previous EndpointConfig. If False, a new endpoint will be created. Default: False
+            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.
+            kms_key (str): The ARN of the KMS key that is used to encrypt the data on the
+                storage volume attached to the instance hosting the endpoint.
+            wait (bool): Whether the call should wait until the deployment of this model completes (default: True).
 
         Returns:
             callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on
-                the created endpoint name, if ``self.predictor_cls`` is not
-                None. Otherwise, return None.
+                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.
         """
         if not self.sagemaker_session:
             if instance_type in ("local", "local_gpu"):
@@ -473,30 +411,22 @@ class Model(object):
 
         Args:
             instance_count (int): Number of EC2 instances to use.
-            instance_type (str): Type of EC2 instance to use, for example,
-                'ml.c4.xlarge'.
-            strategy (str): The strategy used to decide how to batch records in
-                a single request (default: None). Valid values: 'MULTI_RECORD'
-                and 'SINGLE_RECORD'.
-            assemble_with (str): How the output is assembled (default: None).
-                Valid values: 'Line' or 'None'.
-            output_path (str): S3 location for saving the transform result. If
-                not specified, results are stored to a default bucket.
-            output_kms_key (str): Optional. KMS key ID for encrypting the
-                transform output (default: None).
-            accept (str): The content type accepted by the endpoint deployed
-                during the transform job.
-            env (dict): Environment variables to be set for use during the
-                transform job (default: None).
-            max_concurrent_transforms (int): The maximum number of HTTP requests
-                to be made to each individual transform container at one time.
-            max_payload (int): Maximum size of the payload in a single HTTP
-                request to the container in MB.
-            tags (list[dict]): List of tags for labeling a transform job. If
-                none specified, then the tags used for the training job are used
-                for the transform job.
-            volume_kms_key (str): Optional. KMS key ID for encrypting the volume
-                attached to the ML compute instance (default: None).
+            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.
+            strategy (str): The strategy used to decide how to batch records in a single request (default: None).
+                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.
+            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.
+            output_path (str): S3 location for saving the transform result. If not specified, results are stored to
+                a default bucket.
+            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).
+            accept (str): The content type accepted by the endpoint deployed during the transform job.
+            env (dict): Environment variables to be set for use during the transform job (default: None).
+            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to
+                each individual transform container at one time.
+            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.
+            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for
+                the training job are used for the transform job.
+            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML
+                compute instance (default: None).
         """
         self._create_sagemaker_model(instance_type)
         if self.enable_network_isolation():
@@ -525,6 +455,7 @@ class Model(object):
 
         Raises:
             ValueError: if the model is not created yet.
+
         """
         if self.name is None:
             raise ValueError(
@@ -546,8 +477,7 @@ SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
 class FrameworkModel(Model):
     """A Model for working with an SageMaker ``Framework``.
 
-    This class hosts user-defined code in S3 and sets code location and
-    configuration in model environment variables.
+    This class hosts user-defined code in S3 and sets code location and configuration in model environment variables.
     """
 
     def __init__(
@@ -571,17 +501,14 @@ class FrameworkModel(Model):
         """Initialize a ``FrameworkModel``.
 
         Args:
-            model_data (str): The S3 location of a SageMaker model data
-                ``.tar.gz`` file.
+            model_data (str): The S3 location of a SageMaker model data ``.tar.gz`` file.
             image (str): A Docker image URI.
-            role (str): An IAM role name or ARN for SageMaker to access AWS
-                resources on your behalf.
-            entry_point (str): Path (absolute or relative) to the Python source
-                file which should be executed as the entry point to model
-                hosting. This should be compatible with either Python 2.7 or
-                Python 3.5. If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-                Example
+            role (str): An IAM role name or ARN for SageMaker to access AWS resources on your behalf.
+            entry_point (str): Path (absolute or relative) to the Python source file which should be executed
+                as the entry point to model hosting. This should be compatible with either Python 2.7 or Python 3.5.
+                If 'git_config' is provided, 'entry_point' should be a relative location to the Python source file in
+                the Git repo.
+                Example:
 
                     With the following GitHub repo directory structure:
 
@@ -591,14 +518,46 @@ class FrameworkModel(Model):
                     >>>         |----- test.py
 
                     You can assign entry_point='src/inference.py'.
-            source_dir (str): Path (absolute or relative) to a directory with
-                any other training source code dependencies aside from the entry
-                point file (default: None). Structure within this directory will
-                be preserved when training on SageMaker. If 'git_config' is
-                provided, 'source_dir' should be a relative location to a
-                directory in the Git repo. If the directory points to S3, no
-                code will be uploaded and the S3 location will be used instead.
-                .. admonition:: Example
+            git_config (dict[str, str]): Git configurations used for cloning files, including ``repo``, ``branch``,
+                ``commit``, ``2FA_enabled``, ``username``, ``password`` and ``token``. The ``repo`` field is required.
+                All other fields are optional. ``repo`` specifies the Git repository where your training script is
+                stored. If you don't provide ``branch``, the default value  'master' is used. If you don't provide
+                ``commit``, the latest commit in the specified branch is used.
+                Example:
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then checkout the 'master' branch, and checkout
+                    the specified commit.
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are used for authentication. For GitHub
+                (or other Git) accounts, set ``2FA_enabled`` to 'True' if two-factor authentication is enabled for the
+                account, otherwise set it to 'False'. If you do not provide a value for ``2FA_enabled``, a default
+                value of 'False' is used. CodeCommit does not support two-factor authentication, so do not provide
+                "2FA_enabled" with CodeCommit repositories.
+
+                For GitHub and other Git repos, when SSH URLs are provided, it doesn't matter whether 2FA is
+                enabled or disabled; you should either have no passphrase for the SSH key pairs, or have the ssh-agent
+                configured so that you will not be prompted for SSH passphrase when you do 'git clone' command with SSH
+                URLs. When HTTPS URLs are provided: if 2FA is disabled, then either token or username+password will be
+                used for authentication if provided (token prioritized); if 2FA is enabled, only token will be used for
+                authentication if provided. If required authentication info is not provided, python SDK will try to use
+                local credentials storage to authenticate. If that fails either, an error message will be thrown.
+
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled' should not be provided. There is no token
+                in CodeCommit, so 'token' should not be provided too. When 'repo' is an SSH URL, the requirements are
+                the same as GitHub-like repos. When 'repo' is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will try to use either CodeCommit credential
+                helper or local credential storage for authentication.
+            source_dir (str): Path (absolute or relative) to a directory with any other training
+                source code dependencies aside from the entry point file (default: None). Structure within this
+                directory will be preserved when training on SageMaker. If 'git_config' is provided,
+                'source_dir' should be a relative location to a directory in the Git repo. If the directory points
+                to S3, no code will be uploaded and the S3 location will be used instead.
+                Example:
 
                     With the following GitHub repo directory structure:
 
@@ -608,40 +567,17 @@ class FrameworkModel(Model):
                     >>>         |----- test.py
 
                     You can assign entry_point='inference.py', source_dir='src'.
-            predictor_cls (callable[string, sagemaker.session.Session]): A
-                function to call to create a predictor (default: None). If not
-                None, ``deploy`` will return the result of invoking this
-                function on the created endpoint name.
-            env (dict[str, str]): Environment variables to run with ``image``
-                when hosted in SageMaker (default: None).
-            name (str): The model name. If None, a default model name will be
-                selected on each ``deploy``.
-            enable_cloudwatch_metrics (bool): Whether training and hosting
-                containers will generate CloudWatch metrics under the
-                AWS/SageMakerContainer namespace (default: False).
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): Name of the S3 bucket where custom code is
-                uploaded (default: None). If not specified, default bucket
-                created by ``sagemaker.session.Session`` is used.
-            sagemaker_session (sagemaker.session.Session): A SageMaker Session
-                object, used for SageMaker interactions (default: None). If not
-                specified, one is created using the default AWS configuration
-                chain.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead. .. admonition:: Example
-
-                    The following call >>> Estimator(entry_point='inference.py',
-                    dependencies=['my/libs/common', 'virtual-env']) results in
-                    the following inside the container:
+            dependencies (list[str]): A list of paths to directories (absolute or relative) with
+                any additional libraries that will be exported to the container (default: []).
+                The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+                If 'git_config' is provided, 'dependencies' should be a list of relative locations to directories
+                with any additional libraries needed in the Git repo. If the ```source_dir``` points to S3, code
+                will be uploaded and the S3 location will be used instead.
+                Example:
+
+                    The following call
+                    >>> Estimator(entry_point='inference.py', dependencies=['my/libs/common', 'virtual-env'])
+                    results in the following inside the container:
 
                     >>> $ ls
 
@@ -649,55 +585,21 @@ class FrameworkModel(Model):
                     >>>     |------ inference.py
                     >>>     |------ common
                     >>>     |------ virtual-env
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
 
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
+            predictor_cls (callable[string, sagemaker.session.Session]): A function to call to create
+               a predictor (default: None). If not None, ``deploy`` will return the result of invoking
+               this function on the created endpoint name.
+            env (dict[str, str]): Environment variables to run with ``image`` when hosted in SageMaker
+               (default: None).
+            name (str): The model name. If None, a default model name will be selected on each ``deploy``.
+            enable_cloudwatch_metrics (bool): Whether training and hosting containers will
+               generate CloudWatch metrics under the AWS/SageMakerContainer namespace (default: False).
+            container_log_level (int): Log level to use within the container (default: logging.INFO).
+                Valid values are defined in the Python logging module.
+            code_location (str): Name of the S3 bucket where custom code is uploaded (default: None).
+                If not specified, default bucket created by ``sagemaker.session.Session`` is used.
+            sagemaker_session (sagemaker.session.Session): A SageMaker Session object, used for SageMaker
+               interactions (default: None). If not specified, one is created using the default AWS configuration chain.
             **kwargs: Keyword arguments passed to the ``Model`` initializer.
         """
         super(FrameworkModel, self).__init__(
@@ -733,21 +635,17 @@ class FrameworkModel(Model):
     def prepare_container_def(
         self, instance_type, accelerator_type=None
     ):  # pylint disable=unused-argument
-        """Return a container definition with framework configuration set in
-        model environment variables.
+        """Return a container definition with framework configuration set in model environment variables.
 
         This also uploads user-supplied code to S3.
 
         Args:
-            instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge'.
-            accelerator_type (str): The Elastic Inference accelerator type to
-                deploy to the instance for loading and making inferences to the
-                model. For example, 'ml.eia1.medium'.
+            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.
+            accelerator_type (str): The Elastic Inference accelerator type to deploy to the instance for loading and
+                making inferences to the model. For example, 'ml.eia1.medium'.
 
         Returns:
-            dict[str, str]: A container definition object usable with the
-            CreateModel API.
+            dict[str, str]: A container definition object usable with the CreateModel API.
         """
         deploy_key_prefix = fw_utils.model_code_key_prefix(self.key_prefix, self.name, self.image)
         self._upload_code(deploy_key_prefix)
@@ -756,11 +654,6 @@ class FrameworkModel(Model):
         return sagemaker.container_def(self.image, self.model_data, deploy_env)
 
     def _upload_code(self, key_prefix, repack=False):
-        """
-        Args:
-            key_prefix:
-            repack:
-        """
         local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
         if self.sagemaker_session.local_mode and local_code:
             self.uploaded_code = None
@@ -794,7 +687,6 @@ class FrameworkModel(Model):
             )
 
     def _framework_env_vars(self):
-        """Placeholder docstring"""
         if self.uploaded_code:
             script_name = self.uploaded_code.script_name
             dir_name = self.uploaded_code.s3_prefix
@@ -818,19 +710,16 @@ class ModelPackage(Model):
         """Initialize a SageMaker ModelPackage.
 
         Args:
-            role (str): An AWS IAM role (either name or full ARN). The Amazon
-                SageMaker training jobs and APIs that create Amazon SageMaker
-                endpoints use this role to access training data and model
-                artifacts. After the endpoint is created, the inference code
-                might use the IAM role, if it needs to access an AWS resource.
-            model_data (str): The S3 location of a SageMaker model data
-                ``.tar.gz`` file. Must be provided if algorithm_arn is provided.
-            algorithm_arn (str): algorithm arn used to train the model, can be
-                just the name if your account owns the algorithm. Must also
-                provide ``model_data``.
-            model_package_arn (str): An existing SageMaker Model Package arn,
-                can be just the name if your account owns the Model Package.
-                ``model_data`` is not required.
+            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs
+                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.
+                After the endpoint is created, the inference code might use the IAM role,
+                if it needs to access an AWS resource.
+            model_data (str): The S3 location of a SageMaker model data ``.tar.gz`` file. Must be
+                provided if algorithm_arn is provided.
+            algorithm_arn (str): algorithm arn used to train the model, can be just the name if your
+                account owns the algorithm. Must also provide ``model_data``.
+            model_package_arn (str): An existing SageMaker Model Package arn, can be just the name if
+                your account owns the Model Package. ``model_data`` is not required.
             **kwargs: Additional kwargs passed to the Model constructor.
         """
         super(ModelPackage, self).__init__(role=role, model_data=model_data, image=None, **kwargs)
@@ -857,7 +746,6 @@ class ModelPackage(Model):
         self._created_model_package_name = None
 
     def _create_sagemaker_model_package(self):
-        """Placeholder docstring"""
         if self.algorithm_arn is None:
             raise ValueError("No algorithm_arn was provided to create a SageMaker Model Pacakge")
 
@@ -869,8 +757,7 @@ class ModelPackage(Model):
         return name
 
     def enable_network_isolation(self):
-        """Whether to enable network isolation when creating a model out of this
-        ModelPackage
+        """Whether to enable network isolation when creating a model out of this ModelPackage
 
         Returns:
             bool: If network isolation should be enabled or not.
@@ -878,7 +765,6 @@ class ModelPackage(Model):
         return self._is_marketplace()
 
     def _is_marketplace(self):
-        """Placeholder docstring"""
         model_package_name = self.model_package_arn or self._created_model_package_name
         if model_package_name is None:
             return True
@@ -899,8 +785,8 @@ class ModelPackage(Model):
         """Create a SageMaker Model Entity
 
         Args:
-            *args: Arguments coming from the caller. This class does not require
-                any so they are ignored.
+            *args: Arguments coming from the caller. This class
+                does not require any so they are ignored.
         """
         if self.algorithm_arn:
             # When ModelPackage is created using an algorithm_arn we need to first

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-12 17:50:12[0m
[92mHash: 2cf69b2afdba9d1b2722b6972ded438fe0222dbe[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Git integration for CodeCommit (#927)

* add functions, tests and doc for CodeCommit
[0m
@@ -30,17 +30,15 @@ from tests.integ import DATA_DIR, PYTHON_VERSION
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
+
 PRIVATE_GIT_REPO = "https://github.com/git-support-test/test-git.git"
 PRIVATE_BRANCH = "master"
 PRIVATE_COMMIT = "[93ma46d6f9add3532ca3e4e231e4108b6bad15b7373[0m"
+
 PRIVATE_GIT_REPO_2FA = "https://github.com/git-support-test-2fa/test-git.git"
 PRIVATE_GIT_REPO_2FA_SSH = "git@github.com:git-support-test-2fa/test-git.git"
 PRIVATE_BRANCH_2FA = "master"
 PRIVATE_COMMIT_2FA = "[93m52381dee030eb332a7e42d9992878d7261eb21d4[0m"
-CODECOMMIT_REPO = (
-    "https://git-codecommit.us-west-2.amazonaws.com/v1/repos/sagemaker-python-sdk-git-testing-repo/"
-)
-CODECOMMIT_BRANCH = "master"
 
 # Since personal access tokens will delete themselves if they are committed to GitHub repos,
 # we cannot hard code them here, but have to encrypt instead
@@ -227,65 +225,3 @@ def test_git_support_with_sklearn_ssh_passphrase_not_configured(
     with pytest.raises(subprocess.CalledProcessError) as error:
         sklearn.fit({"train": train_input, "test": test_input})
     assert "returned non-zero exit status" in str(error)
-
-
-@pytest.mark.local_mode
-def test_git_support_codecommit_with_mxnet(sagemaker_local_session):
-    script_path = "mnist.py"
-    data_path = os.path.join(DATA_DIR, "mxnet_mnist")
-    git_config = {
-        "repo": CODECOMMIT_REPO,
-        "branch": CODECOMMIT_BRANCH,
-        "username": "GitTest-at-142577830533",
-        "password": "[93m22LcZpWMtjpDG3fbOuHPooIoKoRxF36rQj7zdUvXooA=[0m",
-    }
-    source_dir = "mxnet"
-    dependencies = ["foo/bar.py"]
-    mx = MXNet(
-        entry_point=script_path,
-        role="SageMakerRole",
-        source_dir=source_dir,
-        dependencies=dependencies,
-        framework_version=MXNet.LATEST_VERSION,
-        py_version=PYTHON_VERSION,
-        train_instance_count=1,
-        train_instance_type="local",
-        sagemaker_session=sagemaker_local_session,
-        git_config=git_config,
-    )
-
-    mx.fit(
-        {
-            "train": "file://" + os.path.join(data_path, "train"),
-            "test": "file://" + os.path.join(data_path, "test"),
-        }
-    )
-
-    files = [file for file in os.listdir(mx.source_dir)]
-    assert "some_file" in files
-    assert "mnist.py" in files
-    assert os.path.exists(mx.dependencies[0])
-
-    with lock.lock(LOCK_PATH):
-        try:
-            client = sagemaker_local_session.sagemaker_client
-            desc = client.describe_training_job(TrainingJobName=mx.latest_training_job.name)
-            model_data = desc["ModelArtifacts"]["S3ModelArtifacts"]
-            model = MXNetModel(
-                model_data,
-                "SageMakerRole",
-                entry_point=script_path,
-                source_dir=source_dir,
-                dependencies=dependencies,
-                py_version=PYTHON_VERSION,
-                sagemaker_session=sagemaker_local_session,
-                framework_version=MXNet.LATEST_VERSION,
-                git_config=git_config,
-            )
-            predictor = model.deploy(1, "local")
-
-            data = numpy.zeros(shape=(1, 1, 28, 28))
-            result = predictor.predict(data)
-            assert result is not None
-        finally:
-            predictor.delete_endpoint()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-12 17:50:12[0m
[92mHash: 2cf69b2afdba9d1b2722b6972ded438fe0222dbe[0m
[92mFilepath: tests/unit/test_estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Git integration for CodeCommit (#927)

* add functions, tests and doc for CodeCommit
[0m
@@ -55,9 +55,6 @@ PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
 PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
 PRIVATE_BRANCH = "test-branch"
 PRIVATE_COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-CODECOMMIT_REPO = "https://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_REPO_SSH = "ssh://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_BRANCH = "master"
 REPO_DIR = "/tmp/repo_dir"
 
 DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
@@ -1134,63 +1131,6 @@ def test_git_support_ssh_passphrase_required(git_clone_repo, sagemaker_session):
     assert "returned non-zero exit status" in str(error)
 
 
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-def test_git_support_codecommit_with_username_and_password_succeed(
-    git_clone_repo, sagemaker_session
-):
-    git_config = {
-        "repo": CODECOMMIT_REPO,
-        "branch": CODECOMMIT_BRANCH,
-        "username": "username",
-        "password": "passw0rd!",
-    }
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-    assert fw.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-def test_git_support_codecommit_with_ssh_no_passphrase_needed(git_clone_repo, sagemaker_session):
-    git_config = {"repo": CODECOMMIT_REPO_SSH, "branch": CODECOMMIT_BRANCH}
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        # enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-    assert fw.entry_point == "/tmp/repo_dir/entry_point"
-
-
 @patch("time.strftime", return_value=TIMESTAMP)
 def test_init_with_source_dir_s3(strftime, sagemaker_session):
     fw = DummyFramework(

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-12 17:50:12[0m
[92mHash: 2cf69b2afdba9d1b2722b6972ded438fe0222dbe[0m
[92mFilepath: tests/unit/test_git_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Git integration for CodeCommit (#927)

* add functions, tests and doc for CodeCommit
[0m
@@ -27,9 +27,6 @@ PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
 PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
 PRIVATE_BRANCH = "test-branch"
 PRIVATE_COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-CODECOMMIT_REPO = "https://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_REPO_SSH = "ssh://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_BRANCH = "master"
 
 
 @patch("subprocess.check_call")
@@ -216,7 +213,7 @@ def test_git_clone_repo_with_token_no_2fa(isfile, mkdtemp, check_call):
         "repo": PRIVATE_GIT_REPO,
         "branch": PRIVATE_BRANCH,
         "commit": PRIVATE_COMMIT,
-        "token": "my-token",
+        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
         "2FA_enabled": False,
     }
     entry_point = "entry_point"
@@ -224,7 +221,12 @@ def test_git_clone_repo_with_token_no_2fa(isfile, mkdtemp, check_call):
     env["GIT_TERMINAL_PROMPT"] = "0"
     ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
     check_call.assert_any_call(
-        ["git", "clone", "https://my-token@github.com/testAccount/private-repo.git", REPO_DIR],
+        [
+            "git",
+            "clone",
+            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
+            REPO_DIR,
+        ],
         env=env,
     )
     check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
@@ -244,14 +246,19 @@ def test_git_clone_repo_with_token_2fa(isfile, mkdtemp, check_call):
         "commit": PRIVATE_COMMIT,
         "2FA_enabled": True,
         "username": "username",
-        "token": "my-token",
+        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
     }
     entry_point = "entry_point"
     env = os.environ.copy()
     env["GIT_TERMINAL_PROMPT"] = "0"
     ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
     check_call.assert_any_call(
-        ["git", "clone", "https://my-token@github.com/testAccount/private-repo.git", REPO_DIR],
+        [
+            "git",
+            "clone",
+            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
+            REPO_DIR,
+        ],
         env=env,
     )
     check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
@@ -283,7 +290,7 @@ def test_git_clone_repo_with_token_no_2fa_unnecessary_creds_provided(isfile, mkd
         "commit": PRIVATE_COMMIT,
         "username": "username",
         "password": "passw0rd!",
-        "token": "my-token",
+        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
     }
     entry_point = "entry_point"
     env = os.environ.copy()
@@ -295,7 +302,12 @@ def test_git_clone_repo_with_token_no_2fa_unnecessary_creds_provided(isfile, mkd
         in warn[0].message.args[0]
     )
     check_call.assert_any_call(
-        ["git", "clone", "https://my-token@github.com/testAccount/private-repo.git", REPO_DIR],
+        [
+            "git",
+            "clone",
+            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
+            REPO_DIR,
+        ],
         env=env,
     )
     check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
@@ -315,7 +327,7 @@ def test_git_clone_repo_with_token_2fa_unnecessary_creds_provided(isfile, mkdtem
         "commit": PRIVATE_COMMIT,
         "2FA_enabled": True,
         "username": "username",
-        "token": "my-token",
+        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
     }
     entry_point = "entry_point"
     env = os.environ.copy()
@@ -327,7 +339,12 @@ def test_git_clone_repo_with_token_2fa_unnecessary_creds_provided(isfile, mkdtem
         in warn[0].message.args[0]
     )
     check_call.assert_any_call(
-        ["git", "clone", "https://my-token@github.com/testAccount/private-repo.git", REPO_DIR],
+        [
+            "git",
+            "clone",
+            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
+            REPO_DIR,
+        ],
         env=env,
     )
     check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
@@ -405,62 +422,3 @@ def test_git_clone_repo_with_and_token_2fa_wrong_creds(mkdtemp, check_call):
     with pytest.raises(subprocess.CalledProcessError) as error:
         git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
     assert "returned non-zero exit status" in str(error)
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_codecommit_https_with_username_and_password(isfile, mkdtemp, check_call):
-    git_config = {
-        "repo": CODECOMMIT_REPO,
-        "branch": CODECOMMIT_BRANCH,
-        "username": "username",
-        "password": "my-codecommit-password",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://username:my-codecommit-password@git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", CODECOMMIT_BRANCH], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=128, cmd="git clone {} {}".format(CODECOMMIT_REPO_SSH, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_codecommit_ssh_passphrase_required(mkdtemp, check_call):
-    git_config = {"repo": CODECOMMIT_REPO_SSH, "branch": CODECOMMIT_BRANCH}
-    entry_point = "entry_point"
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config, entry_point)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=128, cmd="git clone {} {}".format(CODECOMMIT_REPO, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_codecommit_https_creds_not_stored_locally(mkdtemp, check_call):
-    git_config = {"repo": CODECOMMIT_REPO, "branch": CODECOMMIT_BRANCH}
-    entry_point = "entry_point"
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config, entry_point)
-    assert "returned non-zero exit status" in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-12 17:50:12[0m
[92mHash: 2cf69b2afdba9d1b2722b6972ded438fe0222dbe[0m
[92mFilepath: tests/unit/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: Git integration for CodeCommit (#927)

* add functions, tests and doc for CodeCommit
[0m
@@ -47,9 +47,6 @@ PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
 PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
 PRIVATE_BRANCH = "test-branch"
 PRIVATE_COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-CODECOMMIT_REPO = "https://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_REPO_SSH = "ssh://git-codecommit.us-west-2.amazonaws.com/v1/repos/test-repo/"
-CODECOMMIT_BRANCH = "master"
 REPO_DIR = "/tmp/repo_dir"
 
 
@@ -768,72 +765,3 @@ def test_git_support_ssh_passphrase_required(tar_and_upload_dir, git_clone_repo,
         )
         model.prepare_container_def(instance_type=INSTANCE_TYPE)
     assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_codecommit_with_username_and_password_succeed(
-    tar_and_upload_dir, git_clone_repo, sagemaker_session
-):
-    entry_point = "entry_point"
-    git_config = {
-        "repo": CODECOMMIT_REPO,
-        "branch": CODECOMMIT_BRANCH,
-        "username": "username",
-        "password": "passw0rd!",
-    }
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, None, [])
-    assert model.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_codecommit_ssh_no_passphrase_needed(
-    tar_and_upload_dir, git_clone_repo, sagemaker_session
-):
-    entry_point = "entry_point"
-    git_config = {"repo": CODECOMMIT_REPO_SSH, "branch": CODECOMMIT_BRANCH}
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, None, [])
-    assert model.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO_SSH, REPO_DIR)
-    ),
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_codecommit_ssh_passphrase_required(
-    tar_and_upload_dir, git_clone_repo, sagemaker_session
-):
-    entry_point = "entry_point"
-    git_config = {"repo": CODECOMMIT_REPO_SSH, "branch": CODECOMMIT_BRANCH}
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "returned non-zero exit status" in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-11 18:30:06[0m
[92mHash: fb309bcd8d57d462bdceee58a00108d6d8e42490[0m
[92mFilepath: doc/overview.rst[0m
[92mBranch: origin/master[0m
[92mCommit: feature: deal with credentials for Git support for GitHub (#914)

add authentication info[0m
@@ -183,43 +183,38 @@ Here is an example:
         # When you are done using your endpoint
         algo.delete_endpoint()
 
-Use Scripts Stored in a Git Repository
---------------------------------------
-When you create an estimator, you can specify a training script that is stored in a GitHub or other Git repository as the entry point for the estimator, so that you don't have to download the scripts locally.
-If you do so, source directory and dependencies should be in the same repo if they are needed. Git support can be enabled simply by providing ``git_config`` parameter
-when creating an ``Estimator`` object. If Git support is enabled, then ``entry_point``, ``source_dir`` and  ``dependencies``
-should be relative paths in the Git repo if provided.
+Git Support
+-----------
+If you have your training scripts in your GitHub repository, you can use them directly without the trouble to download
+them to local machine. Git support can be enabled simply by providing ``git_config`` parameter when initializing an
+estimator. If Git support is enabled, then ``entry_point``, ``source_dir`` and  ``dependencies`` should all be relative
+paths in the Git repo. Note that if you decided to use Git support, then everything you need for ``entry_point``,
+``source_dir`` and ``dependencies`` should be in a single Git repo.
 
-The ``git_config`` parameter includes fields ``repo``, ``branch``,  ``commit``, ``2FA_enabled``, ``username``,
-``password`` and ``token``. The ``repo`` field is required. All other fields are optional. ``repo`` specifies the Git
-repository where your training script is stored. If you don't provide ``branch``, the default value  'master' is used.
-If you don't provide ``commit``, the latest commit in the specified branch is used.
-
-``2FA_enabled``, ``username``, ``password`` and ``token`` are used for authentication. Set ``2FA_enabled`` to 'True' if
-two-factor authentication is enabled for the GitHub (or other Git) account, otherwise set it to 'False'.
-If you do not provide a value for ``2FA_enabled``, a default value of 'False' is used.
-
-If ``repo`` is an SSH URL, you should either have no passphrase for the SSH key pairs, or have the ``ssh-agent`` configured
-so that you are not prompted for the SSH passphrase when you run a ``git clone`` command with SSH URLs. For SSH URLs, it
-does not matter whether two-factor authentication is enabled.
-
-If ``repo`` is an https URL, 2FA matters. When 2FA is disabled, either ``token`` or ``username``+``password`` will be
-used for authentication if provided (``token`` prioritized). When 2FA is enabled, only token will be used for
-authentication if provided. If required authentication info is not provided, python SDK will try to use local
-credentials storage to authenticate. If that fails either, an error message will be thrown.
-
-Here are some examples of creating estimators with Git support:
+Here are ways to specify ``git_config``:
 
 .. code:: python
 
-        # Specifies the git_config parameter. This example does not provide Git credentials, so python SDK will try
-        # to use local credential storage.
+        # Specifies the git_config parameter
         git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
                       'branch': 'branch1',
                       'commit': '[93m4893e528afa4a790331e1b5286954f073b0f14a2[0m'}
 
+        # Alternatively, you can also specify git_config by providing only 'repo' and 'branch'.
+        # If this is the case, the latest commit in the branch will be used.
+        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
+                      'branch': 'branch1'}
+
+        # Only providing 'repo' is also allowed. If this is the case, latest commit in
+        # 'master' branch will be used.
+        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git'}
+
+The following are some examples to define estimators with Git support:
+
+.. code:: python
+
         # In this example, the source directory 'pytorch' contains the entry point 'mnist.py' and other source code.
-        # and it is relative path inside the Git repo.
+        # and it is  relative path inside the Git repo.
         pytorch_estimator = PyTorch(entry_point='mnist.py',
                                     role='SageMakerRole',
                                     source_dir='pytorch',
@@ -227,13 +222,6 @@ Here are some examples of creating estimators with Git support:
                                     train_instance_count=1,
                                     train_instance_type='ml.c4.xlarge')
 
-.. code:: python
-
-        # You can also specify git_config by providing only 'repo' and 'branch'.
-        # If this is the case, the latest commit in that branch will be used.
-        git_config = {'repo': 'git@github.com:username/repo-with-training-scripts.git',
-                      'branch': 'branch1'}
-
         # In this example, the entry point 'mnist.py' is all we need for source code.
         # We need to specify the path to it in the Git repo.
         mx_estimator = MXNet(entry_point='mxnet/mnist.py',
@@ -242,15 +230,6 @@ Here are some examples of creating estimators with Git support:
                              train_instance_count=1,
                              train_instance_type='ml.c4.xlarge')
 
-.. code:: python
-
-        # Only providing 'repo' is also allowed. If this is the case, latest commit in 'master' branch will be used.
-        # This example does not provide '2FA_enabled', so 2FA is treated as disabled by default. 'username' and
-        # 'password' are provided for authentication
-        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
-                      'username': 'username',
-                      'password': 'passw0rd!'}
-
         # In this example, besides entry point and other source code in source directory, we still need some
         # dependencies for the training job. Dependencies should also be paths inside the Git repo.
         pytorch_estimator = PyTorch(entry_point='mnist.py',
@@ -261,23 +240,7 @@ Here are some examples of creating estimators with Git support:
                                     train_instance_count=1,
                                     train_instance_type='ml.c4.xlarge')
 
-.. code:: python
-
-        # This example specifies that 2FA is enabled, and token is provided for authentication
-        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
-                      '2FA_enabled': True,
-                      'token': 'your-token'}
-
-        # In this exmaple, besides entry point, we also need some dependencies for the training job.
-        pytorch_estimator = PyTorch(entry_point='pytorch/mnist.py',
-                                    role='SageMakerRole',
-                                    dependencies=['dep.py'],
-                                    git_config=git_config,
-                                    train_instance_count=1,
-                                    train_instance_type='local')
-
-Git support can be used not only for training jobs, but also for hosting models. The usage is the same as the above,
-and ``git_config`` should be provided when creating model objects, e.g. ``TensorFlowModel``, ``MXNetModel``, ``PyTorchModel``.
+When Git support is enabled, users can still use local mode in the same way.
 
 Training Metrics
 ----------------

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-11 18:30:06[0m
[92mHash: fb309bcd8d57d462bdceee58a00108d6d8e42490[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: deal with credentials for Git support for GitHub (#914)

add authentication info[0m
@@ -16,7 +16,6 @@ import os
 
 import numpy
 import pytest
-import subprocess
 import tempfile
 
 from tests.integ import lock as lock
@@ -31,20 +30,6 @@ GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
 
-PRIVATE_GIT_REPO = "https://github.com/git-support-test/test-git.git"
-PRIVATE_BRANCH = "master"
-PRIVATE_COMMIT = "[93ma46d6f9add3532ca3e4e231e4108b6bad15b7373[0m"
-
-PRIVATE_GIT_REPO_2FA = "https://github.com/git-support-test-2fa/test-git.git"
-PRIVATE_GIT_REPO_2FA_SSH = "git@github.com:git-support-test-2fa/test-git.git"
-PRIVATE_BRANCH_2FA = "master"
-PRIVATE_COMMIT_2FA = "[93m52381dee030eb332a7e42d9992878d7261eb21d4[0m"
-
-# Since personal access tokens will delete themselves if they are committed to GitHub repos,
-# we cannot hard code them here, but have to encrypt instead
-ENCRYPTED_PRIVATE_REPO_TOKEN = "e-4_1-1dc_71-f0e_[93mf7b54a0f3b7db2757163da7b5e8c3[0m"
-PRIVATE_REPO_TOKEN = ENCRYPTED_PRIVATE_REPO_TOKEN.replace("-", "").replace("_", "")
-
 # endpoint tests all use the same port, so we use this lock to prevent concurrent execution
 LOCK_PATH = os.path.join(tempfile.gettempdir(), "sagemaker_test_git_lock")
 
@@ -71,6 +56,7 @@ def test_git_support_with_pytorch(sagemaker_local_session):
     with lock.lock(LOCK_PATH):
         try:
             predictor = pytorch.deploy(initial_instance_count=1, instance_type="local")
+
             data = numpy.zeros(shape=(1, 1, 28, 28)).astype(numpy.float32)
             result = predictor.predict(data)
             assert result is not None
@@ -80,17 +66,9 @@ def test_git_support_with_pytorch(sagemaker_local_session):
 
 @pytest.mark.local_mode
 def test_git_support_with_mxnet(sagemaker_local_session):
-
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": False,
-        "username": "git-support-test",
-        "password": "passw0rd@ %",
-    }
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     source_dir = "mxnet"
     dependencies = ["foo/bar.py"]
     mx = MXNet(
@@ -136,6 +114,7 @@ def test_git_support_with_mxnet(sagemaker_local_session):
                 git_config=git_config,
             )
             predictor = model.deploy(initial_instance_count=1, instance_type="local")
+
             data = numpy.zeros(shape=(1, 1, 28, 28))
             result = predictor.predict(data)
             assert result is not None
@@ -149,11 +128,9 @@ def test_git_support_with_sklearn(sagemaker_local_session, sklearn_full_version)
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "sklearn_mnist")
     git_config = {
-        "repo": PRIVATE_GIT_REPO_2FA,
-        "branch": PRIVATE_BRANCH_2FA,
-        "commit": PRIVATE_COMMIT_2FA,
-        "2FA_enabled": True,
-        "token": PRIVATE_REPO_TOKEN,
+        "repo": "https://github.com/GaryTu1020/python-sdk-testing.git",
+        "branch": "branch1",
+        "commit": "[93maafa4e96237dd78a015d5df22bfcfef46845c3c5[0m",
     }
     source_dir = "sklearn"
     sklearn = SKLearn(
@@ -194,34 +171,3 @@ def test_git_support_with_sklearn(sagemaker_local_session, sklearn_full_version)
             assert result is not None
         finally:
             predictor.delete_endpoint()
-
-
-@pytest.mark.local_mode
-def test_git_support_with_sklearn_ssh_passphrase_not_configured(
-    sagemaker_local_session, sklearn_full_version
-):
-    script_path = "mnist.py"
-    data_path = os.path.join(DATA_DIR, "sklearn_mnist")
-    git_config = {
-        "repo": PRIVATE_GIT_REPO_2FA_SSH,
-        "branch": PRIVATE_BRANCH_2FA,
-        "commit": PRIVATE_COMMIT_2FA,
-    }
-    source_dir = "sklearn"
-    sklearn = SKLearn(
-        entry_point=script_path,
-        role="SageMakerRole",
-        source_dir=source_dir,
-        py_version=PYTHON_VERSION,
-        train_instance_count=1,
-        train_instance_type="local",
-        sagemaker_session=sagemaker_local_session,
-        framework_version=sklearn_full_version,
-        hyperparameters={"epochs": 1},
-        git_config=git_config,
-    )
-    train_input = "file://" + os.path.join(data_path, "train")
-    test_input = "file://" + os.path.join(data_path, "test")
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        sklearn.fit({"train": train_input, "test": test_input})
-    assert "returned non-zero exit status" in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-11 18:30:06[0m
[92mHash: fb309bcd8d57d462bdceee58a00108d6d8e42490[0m
[92mFilepath: tests/unit/test_estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: deal with credentials for Git support for GitHub (#914)

add authentication info[0m
@@ -51,11 +51,16 @@ OUTPUT_PATH = "s3://bucket/prefix"
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
-PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
-PRIVATE_BRANCH = "test-branch"
-PRIVATE_COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-REPO_DIR = "/tmp/repo_dir"
+
+DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
+INSTANCE_TYPE = "c4.4xlarge"
+ACCELERATOR_TYPE = "ml.eia.medium"
+ROLE = "DummyRole"
+IMAGE_NAME = "fakeimage"
+REGION = "us-west-2"
+JOB_NAME = "{}-{}".format(IMAGE_NAME, TIMESTAMP)
+TAGS = [{"Name": "some-tag", "Value": "value-for-tag"}]
+OUTPUT_PATH = "s3://bucket/prefix"
 
 DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
 
@@ -887,9 +892,9 @@ def test_git_support_bad_repo_url_format(sagemaker_session):
         train_instance_type=INSTANCE_TYPE,
         enable_cloudwatch_metrics=True,
     )
-    with pytest.raises(ValueError) as error:
+    with pytest.raises(subprocess.CalledProcessError) as error:
         fw.fit()
-    assert "Invalid Git url provided." in str(error)
+    assert "returned non-zero exit status" in str(error)
 
 
 @patch(
@@ -1021,116 +1026,6 @@ def test_git_support_dependencies_not_exist(sagemaker_session):
     assert "Dependency", "does not exist in the repo." in str(error)
 
 
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-def test_git_support_with_username_password_no_2fa(git_clone_repo, sagemaker_session):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "username": "username",
-        "password": "passw0rd!",
-    }
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-    assert fw.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-def test_git_support_with_token_2fa(git_clone_repo, sagemaker_session):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "token": "my-token",
-        "2FA_enabled": True,
-    }
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-    assert fw.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-def test_git_support_ssh_no_passphrase_needed(git_clone_repo, sagemaker_session):
-    git_config = {"repo": PRIVATE_GIT_REPO_SSH, "branch": PRIVATE_BRANCH, "commit": PRIVATE_COMMIT}
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-    assert fw.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO_SSH, REPO_DIR)
-    ),
-)
-def test_git_support_ssh_passphrase_required(git_clone_repo, sagemaker_session):
-    git_config = {"repo": PRIVATE_GIT_REPO_SSH, "branch": PRIVATE_BRANCH, "commit": PRIVATE_COMMIT}
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        fw.fit()
-    assert "returned non-zero exit status" in str(error)
-
-
 @patch("time.strftime", return_value=TIMESTAMP)
 def test_init_with_source_dir_s3(strftime, sagemaker_session):
     fw = DummyFramework(

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-11 18:30:06[0m
[92mHash: fb309bcd8d57d462bdceee58a00108d6d8e42490[0m
[92mFilepath: tests/unit/test_git_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: deal with credentials for Git support for GitHub (#914)

add authentication info[0m
@@ -13,20 +13,15 @@
 from __future__ import absolute_import
 
 import pytest
-import os
 import subprocess
 from mock import patch
 
 from sagemaker import git_utils
 
 REPO_DIR = "/tmp/repo_dir"
-PUBLIC_GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-PUBLIC_BRANCH = "test-branch-git-config"
-PUBLIC_COMMIT = "[93m[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m[0m"
-PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
-PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
-PRIVATE_BRANCH = "test-branch"
-PRIVATE_COMMIT = "[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m"
+GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
+BRANCH = "test-branch-git-config"
+COMMIT = "[93m[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m[0m"
 
 
 @patch("subprocess.check_call")
@@ -35,58 +30,55 @@ PRIVATE_COMMIT = "[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m"
 @patch("os.path.isdir", return_value=True)
 @patch("os.path.exists", return_value=True)
 def test_git_clone_repo_succeed(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
     ret = git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    check_call.assert_any_call(["git", "clone", git_config["repo"], REPO_DIR], env=env)
-    check_call.assert_any_call(args=["git", "checkout", PUBLIC_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PUBLIC_COMMIT], cwd=REPO_DIR)
+    check_call.assert_any_call(["git", "clone", git_config["repo"], REPO_DIR])
+    check_call.assert_any_call(args=["git", "checkout", BRANCH], cwd=REPO_DIR)
+    check_call.assert_any_call(args=["git", "checkout", COMMIT], cwd=REPO_DIR)
     mkdtemp.assert_called_once()
     assert ret["entry_point"] == "entry_point"
     assert ret["source_dir"] == "/tmp/repo_dir/source_dir"
     assert ret["dependencies"] == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
 
 
-def test_git_clone_repo_repo_not_provided():
-    git_config = {"branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
-    entry_point = "entry_point_that_does_not_exist"
+def test_git_clone_repo_entry_point_not_provided():
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
     with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "Please provide a repo for git_config." in str(error)
+        git_utils.git_clone_repo(git_config=git_config, entry_point=None, source_dir=source_dir)
+    assert "Please provide an entry point." in str(error)
 
 
-def test_git_clone_repo_git_argument_wrong_format():
-    git_config = {
-        "repo": PUBLIC_GIT_REPO,
-        "branch": PUBLIC_BRANCH,
-        "commit": PUBLIC_COMMIT,
-        "token": 42,
-    }
-    entry_point = "entry_point"
+@patch("subprocess.check_call")
+@patch("tempfile.mkdtemp", return_value=REPO_DIR)
+@patch("os.path.isfile", return_value=True)
+@patch("os.path.isdir", return_value=True)
+@patch("os.path.exists", return_value=True)
+def test_git_clone_repo_repo_not_provided(exists, isdir, isfile, mkdtemp, check_call):
+    git_config = {"branch": BRANCH, "commit": COMMIT}
+    entry_point = "entry_point_that_does_not_exist"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
     with pytest.raises(ValueError) as error:
         git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "'token' must be a string." in str(error)
+    assert "Please provide a repo for git_config." in str(error)
 
 
 @patch(
     "subprocess.check_call",
     side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PUBLIC_GIT_REPO, REPO_DIR)
+        returncode=1, cmd="git clone {} {}".format(GIT_REPO, REPO_DIR)
     ),
 )
 @patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_clone_fail(mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+@patch("os.path.isfile", return_value=True)
+@patch("os.path.isdir", return_value=True)
+@patch("os.path.exists", return_value=True)
+def test_git_clone_repo_clone_fail(exists, isdir, isfile, mkdtemp, check_call):
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
@@ -100,8 +92,11 @@ def test_git_clone_repo_clone_fail(mkdtemp, check_call):
     side_effect=[True, subprocess.CalledProcessError(returncode=1, cmd="git checkout banana")],
 )
 @patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_branch_not_exist(mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+@patch("os.path.isfile", return_value=True)
+@patch("os.path.isdir", return_value=True)
+@patch("os.path.exists", return_value=True)
+def test_git_clone_repo_branch_not_exist(exists, isdir, isfile, mkdtemp, check_call):
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
@@ -115,12 +110,15 @@ def test_git_clone_repo_branch_not_exist(mkdtemp, check_call):
     side_effect=[
         True,
         True,
-        subprocess.CalledProcessError(returncode=1, cmd="git checkout {}".format(PUBLIC_COMMIT)),
+        subprocess.CalledProcessError(returncode=1, cmd="git checkout {}".format(COMMIT)),
     ],
 )
 @patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_commit_not_exist(mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+@patch("os.path.isfile", return_value=True)
+@patch("os.path.isdir", return_value=True)
+@patch("os.path.exists", return_value=True)
+def test_git_clone_repo_commit_not_exist(exists, isdir, isfile, mkdtemp, check_call):
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
@@ -134,8 +132,8 @@ def test_git_clone_repo_commit_not_exist(mkdtemp, check_call):
 @patch("os.path.isfile", return_value=False)
 @patch("os.path.isdir", return_value=True)
 @patch("os.path.exists", return_value=True)
-def test_git_clone_repo_entry_point_not_exist(exists, isdir, isfile, mkdtemp, heck_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+def test_git_clone_repo_entry_point_not_exist(exists, isdir, isfile, mkdtemp, check_call):
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point_that_does_not_exist"
     source_dir = "source_dir"
     dependencies = ["foo", "bar"]
@@ -150,7 +148,7 @@ def test_git_clone_repo_entry_point_not_exist(exists, isdir, isfile, mkdtemp, he
 @patch("os.path.isdir", return_value=False)
 @patch("os.path.exists", return_value=True)
 def test_git_clone_repo_source_dir_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir_that_does_not_exist"
     dependencies = ["foo", "bar"]
@@ -165,260 +163,10 @@ def test_git_clone_repo_source_dir_not_exist(exists, isdir, isfile, mkdtemp, che
 @patch("os.path.isdir", return_value=True)
 @patch("os.path.exists", side_effect=[True, False])
 def test_git_clone_repo_dependencies_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": PUBLIC_GIT_REPO, "branch": PUBLIC_BRANCH, "commit": PUBLIC_COMMIT}
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
     entry_point = "entry_point"
     source_dir = "source_dir"
     dependencies = ["foo", "dep_that_does_not_exist"]
     with pytest.raises(ValueError) as error:
         git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
     assert "does not exist in the repo." in str(error)
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_with_username_password_no_2fa(sfile, mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "username": "username",
-        "password": "passw0rd!",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://username:passw0rd%21@github.com/testAccount/private-repo.git",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_COMMIT], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_with_token_no_2fa(isfile, mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
-        "2FA_enabled": False,
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_COMMIT], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_with_token_2fa(isfile, mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": True,
-        "username": "username",
-        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_COMMIT], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_ssh(isfile, mkdtemp, check_call):
-    git_config = {"repo": PRIVATE_GIT_REPO_SSH, "branch": PRIVATE_BRANCH, "commit": PRIVATE_COMMIT}
-    entry_point = "entry_point"
-    ret = git_utils.git_clone_repo(git_config, entry_point)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_with_token_no_2fa_unnecessary_creds_provided(isfile, mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "username": "username",
-        "password": "passw0rd!",
-        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    with pytest.warns(UserWarning) as warn:
-        ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    assert (
-        "Using token for authentication, other credentials will be ignored."
-        in warn[0].message.args[0]
-    )
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_COMMIT], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-def test_git_clone_repo_with_token_2fa_unnecessary_creds_provided(isfile, mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": True,
-        "username": "username",
-        "token": "[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    with pytest.warns(UserWarning) as warn:
-        ret = git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    assert (
-        "Using token for authentication, other credentials will be ignored."
-        in warn[0].message.args[0]
-    )
-    check_call.assert_any_call(
-        [
-            "git",
-            "clone",
-            "https://[93m[93m[93m[93m[93m[93m[93m[93m08c13d80a861f37150cb5c64520bfe14a85ca191[0m[0m[0m[0m[0m[0m[0m[0m@github.com/testAccount/private-repo.git",
-            REPO_DIR,
-        ],
-        env=env,
-    )
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", PRIVATE_COMMIT], cwd=REPO_DIR)
-    assert ret["entry_point"] == "/tmp/repo_dir/entry_point"
-    assert ret["source_dir"] is None
-    assert ret["dependencies"] is None
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_with_username_and_password_wrong_creds(mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": False,
-        "username": "username",
-        "password": "wrong-password",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_with_token_wrong_creds(mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": False,
-        "token": "wrong-token",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-def test_git_clone_repo_with_and_token_2fa_wrong_creds(mkdtemp, check_call):
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "2FA_enabled": False,
-        "token": "wrong-token",
-    }
-    entry_point = "entry_point"
-    env = os.environ.copy()
-    env["GIT_TERMINAL_PROMPT"] = "0"
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config=git_config, entry_point=entry_point)
-    assert "returned non-zero exit status" in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-11 18:30:06[0m
[92mHash: fb309bcd8d57d462bdceee58a00108d6d8e42490[0m
[92mFilepath: tests/unit/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: deal with credentials for Git support for GitHub (#914)

add authentication info[0m
@@ -43,11 +43,6 @@ MODEL_NAME = "{}-{}".format(MODEL_IMAGE, TIMESTAMP)
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-PRIVATE_GIT_REPO_SSH = "git@github.com:testAccount/private-repo.git"
-PRIVATE_GIT_REPO = "https://github.com/testAccount/private-repo.git"
-PRIVATE_BRANCH = "test-branch"
-PRIVATE_COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-REPO_DIR = "/tmp/repo_dir"
 
 
 DESCRIBE_MODEL_PACKAGE_RESPONSE = {
@@ -671,97 +666,3 @@ def test_git_support_dependencies_not_exist(sagemaker_session):
         )
         model.prepare_container_def(instance_type=INSTANCE_TYPE)
     assert "Dependency", "does not exist in the repo." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_with_username_password_no_2fa(
-    tar_and_upload_dir, git_clone_repo, sagemaker_session
-):
-    entry_point = "entry_point"
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "username": "username",
-        "password": "passw0rd!",
-    }
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, None, [])
-    assert model.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_with_token_2fa(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    entry_point = "entry_point"
-    git_config = {
-        "repo": PRIVATE_GIT_REPO,
-        "branch": PRIVATE_BRANCH,
-        "commit": PRIVATE_COMMIT,
-        "token": "my-token",
-        "2FA_enabled": True,
-    }
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, None, [])
-    assert model.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    },
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_ssh_no_passphrase_needed(
-    tar_and_upload_dir, git_clone_repo, sagemaker_session
-):
-    entry_point = "entry_point"
-    git_config = {"repo": PRIVATE_GIT_REPO_SSH, "branch": PRIVATE_BRANCH, "commit": PRIVATE_COMMIT}
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, None, [])
-    assert model.entry_point == "/tmp/repo_dir/entry_point"
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(PRIVATE_GIT_REPO_SSH, REPO_DIR)
-    ),
-)
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_ssh_passphrase_required(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    entry_point = "entry_point"
-    git_config = {"repo": PRIVATE_GIT_REPO_SSH, "branch": PRIVATE_BRANCH, "commit": PRIVATE_COMMIT}
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "returned non-zero exit status" in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: doc/overview.rst[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -799,23 +799,6 @@ After that, invoke the ``deploy()`` method on the ``Model``:
 
 This returns a predictor the same way an ``Estimator`` does when ``deploy()`` is called. You can now get inferences just like with any other model deployed on Amazon SageMaker.
 
-Git support is also available when you bring your own model, through which you can use inference scripts stored in your
-Git repositories. The process is similar to using Git support for training jobs. You can simply provide ``git_config``
-when create the ``Model`` object, and let ``entry_point``, ``source_dir`` and ``dependencies`` (if needed) be relative
-paths inside the Git repository:
-
-.. code:: python
-
-    git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
-                  'branch': 'branch1',
-                  'commit': '[93m4893e528afa4a790331e1b5286954f073b0f14a2[0m'}
-
-    sagemaker_model = MXNetModel(model_data='s3://path/to/model.tar.gz',
-                                role='arn:aws:iam::accid:sagemaker-role',
-                                entry_point='inference.py',
-                                source_dir='mxnet',
-                                git_config=git_config)
-
 A full example is available in the `Amazon SageMaker examples repository <https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/mxnet_mnist_byom>`__.
 
 You can also find this notebook in the **Advanced Functionality** section of the **SageMaker Examples** section in a notebook instance.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -17,7 +17,7 @@ import logging
 import os
 
 import sagemaker
-from sagemaker import fw_utils, local, session, utils, git_utils
+from sagemaker import fw_utils, local, session, utils
 from sagemaker.fw_utils import UploadedCode
 from sagemaker.transformer import Transformer
 
@@ -494,7 +494,6 @@ class FrameworkModel(Model):
         code_location=None,
         sagemaker_session=None,
         dependencies=None,
-        git_config=None,
         **kwargs
     ):
         """Initialize a ``FrameworkModel``.
@@ -505,54 +504,15 @@ class FrameworkModel(Model):
             role (str): An IAM role name or ARN for SageMaker to access AWS resources on your behalf.
             entry_point (str): Path (absolute or relative) to the Python source file which should be executed
                 as the entry point to model hosting. This should be compatible with either Python 2.7 or Python 3.5.
-                If 'git_config' is provided, 'entry_point' should be a relative location to the Python source file in
-                the Git repo.
-                Example:
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-            git_config (dict[str, str]): Git configurations used for cloning files, including 'repo', 'branch'
-                and 'commit' (default: None).
-                'branch' and 'commit' are optional. If 'branch' is not specified, 'master' branch will be used. If
-                'commit' is not specified, the latest commit in the required branch will be used.
-                Example:
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then checkout the 'master' branch, and checkout
-                    the specified commit.
             source_dir (str): Path (absolute or relative) to a directory with any other training
                 source code dependencies aside from the entry point file (default: None). Structure within this
-                directory will be preserved when training on SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo. If the directory points
-                to S3, no code will be uploaded and the S3 location will be used instead.
-                Example:
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
+                directory will be preserved when training on SageMaker.
+                If the directory points to S3, no code will be uploaded and the S3 location will be used instead.
             dependencies (list[str]): A list of paths to directories (absolute or relative) with
                 any additional libraries that will be exported to the container (default: []).
                 The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
-                If 'git_config' is provided, 'dependencies' should be a list of relative locations to directories
-                with any additional libraries needed in the Git repo. If the ```source_dir``` points to S3, code
-                will be uploaded and the S3 location will be used instead.
-                Example:
+                If the ```source_dir``` points to S3, code will be uploaded and the S3 location will be used
+                instead. Example:
 
                     The following call
                     >>> Estimator(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
@@ -594,20 +554,12 @@ class FrameworkModel(Model):
         self.entry_point = entry_point
         self.source_dir = source_dir
         self.dependencies = dependencies or []
-        self.git_config = git_config
         self.enable_cloudwatch_metrics = enable_cloudwatch_metrics
         self.container_log_level = container_log_level
         if code_location:
             self.bucket, self.key_prefix = fw_utils.parse_s3_url(code_location)
         else:
             self.bucket, self.key_prefix = None, None
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
         self.uploaded_code = None
         self.repacked_model_data = None
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -21,14 +21,11 @@ import tempfile
 from tests.integ import lock as lock
 from sagemaker.mxnet.estimator import MXNet
 from sagemaker.pytorch.estimator import PyTorch
-from sagemaker.sklearn.estimator import SKLearn
-from sagemaker.mxnet.model import MXNetModel
-from sagemaker.sklearn.model import SKLearnModel
 from tests.integ import DATA_DIR, PYTHON_VERSION
 
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
+COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
 
 # endpoint tests all use the same port, so we use this lock to prevent concurrent execution
 LOCK_PATH = os.path.join(tempfile.gettempdir(), "sagemaker_test_git_lock")
@@ -65,16 +62,15 @@ def test_git_support_with_pytorch(sagemaker_local_session):
 
 
 @pytest.mark.local_mode
-def test_git_support_with_mxnet(sagemaker_local_session):
+def test_git_support_with_mxnet(sagemaker_local_session, mxnet_full_version):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")
     git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    source_dir = "mxnet"
     dependencies = ["foo/bar.py"]
     mx = MXNet(
         entry_point=script_path,
         role="SageMakerRole",
-        source_dir=source_dir,
+        source_dir="mxnet",
         dependencies=dependencies,
         framework_version=MXNet.LATEST_VERSION,
         py_version=PYTHON_VERSION,
@@ -98,76 +94,10 @@ def test_git_support_with_mxnet(sagemaker_local_session):
 
     with lock.lock(LOCK_PATH):
         try:
-            serving_script_path = "mnist_hosting_with_custom_handlers.py"
-            client = sagemaker_local_session.sagemaker_client
-            desc = client.describe_training_job(TrainingJobName=mx.latest_training_job.name)
-            model_data = desc["ModelArtifacts"]["S3ModelArtifacts"]
-            model = MXNetModel(
-                model_data,
-                "SageMakerRole",
-                entry_point=serving_script_path,
-                source_dir=source_dir,
-                dependencies=dependencies,
-                py_version=PYTHON_VERSION,
-                sagemaker_session=sagemaker_local_session,
-                framework_version=MXNet.LATEST_VERSION,
-                git_config=git_config,
-            )
-            predictor = model.deploy(initial_instance_count=1, instance_type="local")
+            predictor = mx.deploy(initial_instance_count=1, instance_type="local")
 
             data = numpy.zeros(shape=(1, 1, 28, 28))
             result = predictor.predict(data)
             assert result is not None
         finally:
             predictor.delete_endpoint()
-
-
-@pytest.mark.skipif(PYTHON_VERSION != "py3", reason="Scikit-learn image supports only python 3.")
-@pytest.mark.local_mode
-def test_git_support_with_sklearn(sagemaker_local_session, sklearn_full_version):
-    script_path = "mnist.py"
-    data_path = os.path.join(DATA_DIR, "sklearn_mnist")
-    git_config = {
-        "repo": "https://github.com/GaryTu1020/python-sdk-testing.git",
-        "branch": "branch1",
-        "commit": "[93maafa4e96237dd78a015d5df22bfcfef46845c3c5[0m",
-    }
-    source_dir = "sklearn"
-    sklearn = SKLearn(
-        entry_point=script_path,
-        role="SageMakerRole",
-        source_dir=source_dir,
-        py_version=PYTHON_VERSION,
-        train_instance_count=1,
-        train_instance_type="local",
-        sagemaker_session=sagemaker_local_session,
-        framework_version=sklearn_full_version,
-        hyperparameters={"epochs": 1},
-        git_config=git_config,
-    )
-    train_input = "file://" + os.path.join(data_path, "train")
-    test_input = "file://" + os.path.join(data_path, "test")
-    sklearn.fit({"train": train_input, "test": test_input})
-
-    assert os.path.isdir(sklearn.source_dir)
-
-    with lock.lock(LOCK_PATH):
-        try:
-            client = sagemaker_local_session.sagemaker_client
-            desc = client.describe_training_job(TrainingJobName=sklearn.latest_training_job.name)
-            model_data = desc["ModelArtifacts"]["S3ModelArtifacts"]
-            model = SKLearnModel(
-                model_data,
-                "SageMakerRole",
-                entry_point=script_path,
-                source_dir=source_dir,
-                sagemaker_session=sagemaker_local_session,
-                git_config=git_config,
-            )
-            predictor = model.deploy(1, "local")
-
-            data = numpy.zeros((100, 784), dtype="float32")
-            result = predictor.predict(data)
-            assert result is not None
-        finally:
-            predictor.delete_endpoint()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: tests/unit/test_estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -50,7 +50,7 @@ TAGS = [{"Name": "some-tag", "Value": "value-for-tag"}]
 OUTPUT_PATH = "s3://bucket/prefix"
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
+COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
 
 DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
 INSTANCE_TYPE = "c4.4xlarge"
@@ -898,12 +898,12 @@ def test_git_support_bad_repo_url_format(sagemaker_session):
 
 
 @patch(
-    "sagemaker.git_utils.git_clone_repo",
+    "subprocess.check_call",
     side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone https://github.com/aws/no-such-repo.git /tmp/repo_dir"
+        returncode=1, cmd="git clone https://github.com/aws/no-such-repo.git"
     ),
 )
-def test_git_support_git_clone_fail(sagemaker_session):
+def test_git_support_git_clone_fail(check_call, sagemaker_session):
     git_config = {"repo": "https://github.com/aws/no-such-repo.git", "branch": BRANCH}
     fw = DummyFramework(
         entry_point="entry_point",

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: tests/unit/test_git_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -21,7 +21,7 @@ from sagemaker import git_utils
 REPO_DIR = "/tmp/repo_dir"
 GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
+COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
 
 
 @patch("subprocess.check_call")
@@ -44,14 +44,6 @@ def test_git_clone_repo_succeed(exists, isdir, isfile, mkdtemp, check_call):
     assert ret["dependencies"] == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
 
 
-def test_git_clone_repo_entry_point_not_provided():
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    source_dir = "source_dir"
-    with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config=git_config, entry_point=None, source_dir=source_dir)
-    assert "Please provide an entry point." in str(error)
-
-
 @patch("subprocess.check_call")
 @patch("tempfile.mkdtemp", return_value=REPO_DIR)
 @patch("os.path.isfile", return_value=True)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-07-08 15:24:16[0m
[92mHash: 6f5f107f565aa18d577bb8e3ba01fcf536b8b83c[0m
[92mFilepath: tests/unit/test_model.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: git support for hosting models (#878)

* git integration for serving
[0m
@@ -14,7 +14,6 @@ from __future__ import absolute_import
 
 import copy
 import os
-import subprocess
 
 import sagemaker
 from sagemaker.model import FrameworkModel, ModelPackage
@@ -40,9 +39,6 @@ ACCELERATOR_TYPE = "ml.eia.medium"
 IMAGE_NAME = "fakeimage"
 REGION = "us-west-2"
 MODEL_NAME = "{}-{}".format(MODEL_IMAGE, TIMESTAMP)
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
 
 
 DESCRIBE_MODEL_PACKAGE_RESPONSE = {
@@ -98,21 +94,6 @@ class DummyFrameworkModel(FrameworkModel):
         return RealTimePredictor(endpoint_name, sagemaker_session=self.sagemaker_session)
 
 
-class DummyFrameworkModelForGit(FrameworkModel):
-    def __init__(self, sagemaker_session, entry_point, **kwargs):
-        super(DummyFrameworkModelForGit, self).__init__(
-            MODEL_DATA,
-            MODEL_IMAGE,
-            ROLE,
-            entry_point=entry_point,
-            sagemaker_session=sagemaker_session,
-            **kwargs
-        )
-
-    def create_predictor(self, endpoint_name):
-        return RealTimePredictor(endpoint_name, sagemaker_session=self.sagemaker_session)
-
-
 @pytest.fixture()
 def sagemaker_session():
     boto_mock = Mock(name="boto_session", region_name=REGION)
@@ -525,144 +506,3 @@ def test_check_neo_region(sagemaker_session, tmpdir):
             assert model.check_neo_region(region_name) is True
         else:
             assert model.check_neo_region(region_name) is False
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_succeed(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, sourcedir, dependency: {
-        "entry_point": "entry_point",
-        "source_dir": "/tmp/repo_dir/source_dir",
-        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"],
-    }
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    model = DummyFrameworkModelForGit(
-        sagemaker_session=sagemaker_session,
-        entry_point=entry_point,
-        source_dir=source_dir,
-        dependencies=dependencies,
-        git_config=git_config,
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, source_dir, dependencies)
-    assert model.entry_point == "entry_point"
-    assert model.source_dir == "/tmp/repo_dir/source_dir"
-    assert model.dependencies == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
-
-
-def test_git_support_repo_not_provided(sagemaker_session):
-    entry_point = "source_dir/entry_point"
-    git_config = {"branch": BRANCH, "commit": COMMIT}
-    with pytest.raises(ValueError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "Please provide a repo for git_config." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone https://github.com/aws/no-such-repo.git /tmp/repo_dir"
-    ),
-)
-def test_git_support_git_clone_fail(sagemaker_session):
-    entry_point = "source_dir/entry_point"
-    git_config = {"repo": "https://github.com/aws/no-such-repo.git", "branch": BRANCH}
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git checkout branch-that-does-not-exist"
-    ),
-)
-def test_git_support_branch_not_exist(git_clone_repo, sagemaker_session):
-    entry_point = "source_dir/entry_point"
-    git_config = {"repo": GIT_REPO, "branch": "branch-that-does-not-exist", "commit": COMMIT}
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git checkout commit-sha-that-does-not-exist"
-    ),
-)
-def test_git_support_commit_not_exist(git_clone_repo, sagemaker_session):
-    entry_point = "source_dir/entry_point"
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": "commit-sha-that-does-not-exist"}
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Entry point does not exist in the repo."),
-)
-def test_git_support_entry_point_not_exist(sagemaker_session):
-    entry_point = "source_dir/entry_point"
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    with pytest.raises(ValueError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session, entry_point=entry_point, git_config=git_config
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "Entry point does not exist in the repo." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Source directory does not exist in the repo."),
-)
-def test_git_support_source_dir_not_exist(sagemaker_session):
-    entry_point = "entry_point"
-    source_dir = "source_dir_that_does_not_exist"
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    with pytest.raises(ValueError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session,
-            entry_point=entry_point,
-            source_dir=source_dir,
-            git_config=git_config,
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "Source directory does not exist in the repo." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Dependency no-such-dir does not exist in the repo."),
-)
-def test_git_support_dependencies_not_exist(sagemaker_session):
-    entry_point = "entry_point"
-    dependencies = ["foo", "no_such_dir"]
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    with pytest.raises(ValueError) as error:
-        model = DummyFrameworkModelForGit(
-            sagemaker_session=sagemaker_session,
-            entry_point=entry_point,
-            dependencies=dependencies,
-            git_config=git_config,
-        )
-        model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    assert "Dependency", "does not exist in the repo." in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-26 13:00:31[0m
[92mHash: 9f5169b210894209feec8dba6500bcd21a8cccf0[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: fix: add pytest.mark.local_mode annotation to broken tests (#884)

[0m
@@ -15,7 +15,6 @@ from __future__ import absolute_import
 import os
 
 import numpy
-import pytest
 import tempfile
 
 from tests.integ import lock as lock
@@ -31,7 +30,6 @@ COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
 LOCK_PATH = os.path.join(tempfile.gettempdir(), "sagemaker_test_git_lock")
 
 
-@pytest.mark.local_mode
 def test_git_support_with_pytorch(sagemaker_local_session):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "pytorch_mnist")
@@ -61,7 +59,6 @@ def test_git_support_with_pytorch(sagemaker_local_session):
             predictor.delete_endpoint()
 
 
-@pytest.mark.local_mode
 def test_git_support_with_mxnet(sagemaker_local_session, mxnet_full_version):
     script_path = "mnist.py"
     data_path = os.path.join(DATA_DIR, "mxnet_mnist")

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 16:39:47[0m
[92mHash: a6596839deebecaa91690c91af3f56827ff6c5f0[0m
[92mFilepath: doc/overview.rst[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add git_config and git_clone, validate method (#832)

[0m
@@ -84,65 +84,6 @@ For more `information <https://boto3.amazonaws.com/v1/documentation/api/latest/r
     # Deletes the SageMaker model
     mxnet_predictor.delete_model()
 
-Git Support
-~~~~~~~~~~~
-If you have your training scripts in your GitHub repository, you can use them directly without the trouble to download
-them to local machine. Git support can be enabled simply by providing ``git_config`` parameter when initializing an
-estimator. If Git support is enabled, then ``entry_point``, ``source_dir`` and  ``dependencies`` should all be relative
-paths in the Git repo. Note that if you decided to use Git support, then everything you need for ``entry_point``,
-``source_dir`` and ``dependencies`` should be in a single Git repo.
-
-Here are ways to specify ``git_config``:
-
-.. code:: python
-
-        # Specifies the git_config parameter
-        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
-                      'branch': 'branch1',
-                      'commit': '[93m4893e528afa4a790331e1b5286954f073b0f14a2[0m'}
-
-        # Alternatively, you can also specify git_config by providing only 'repo' and 'branch'.
-        # If this is the case, the latest commit in the branch will be used.
-        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git',
-                      'branch': 'branch1'}
-
-        # Only providing 'repo' is also allowed. If this is the case, latest commit in
-        # 'master' branch will be used.
-        git_config = {'repo': 'https://github.com/username/repo-with-training-scripts.git'
-
-The following are some examples to define estimators with Git support:
-
-.. code:: python
-
-        # In this example, the source directory 'pytorch' contains the entry point 'mnist.py' and other source code.
-        # and it is  relative path inside the Git repo.
-        pytorch_estimator = PyTorch(entry_point='mnist.py',
-                                    role='SageMakerRole',
-                                    source_dir='pytorch',
-                                    git_config=git_config,
-                                    train_instance_count=1,
-                                    train_instance_type='ml.c4.xlarge')
-
-        # In this example, the entry point 'mnist.py' is all we need for source code.
-        # We need to specify the path to it in the Git repo.
-        mx_estimator = MXNet(entry_point='mxnet/mnist.py',
-                             role='SageMakerRole',
-                             git_config=git_config,
-                             train_instance_count=1,
-                             train_instance_type='ml.c4.xlarge')
-
-        # In this example, besides entry point and other source code in source directory, we still need some
-        # dependencies for the training job. Dependencies should also be paths inside the Git repo.
-        pytorch_estimator = PyTorch(entry_point='mnist.py',
-                                    role='SageMakerRole',
-                                    source_dir='pytorch',
-                                    dependencies=['dep.py', 'foo/bar.py'],
-                                    git_config=git_config,
-                                    train_instance_count=1,
-                                    train_instance_type='ml.c4.xlarge')
-
-When Git support is enabled, users can still use local mode in the same way.
-
 Training Metrics
 ~~~~~~~~~~~~~~~~
 The SageMaker Python SDK allows you to specify a name and a regular expression for metrics you want to track for training.
@@ -327,7 +268,6 @@ Currently, the following algorithms support incremental training:
 - Object Detection
 - Semantic Segmentation
 
-
 Using SageMaker AlgorithmEstimators
 -----------------------------------
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 16:39:47[0m
[92mHash: a6596839deebecaa91690c91af3f56827ff6c5f0[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add git_config and git_clone, validate method (#832)

[0m
@@ -22,7 +22,6 @@ from six import with_metaclass
 from six import string_types
 
 import sagemaker
-from sagemaker import git_utils
 from sagemaker.analytics import TrainingJobAnalytics
 from sagemaker.fw_utils import (
     create_image_uri,
@@ -934,7 +933,6 @@ class Framework(EstimatorBase):
     """
 
     __framework_name__ = None
-
     LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
     LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
     MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
@@ -951,7 +949,6 @@ class Framework(EstimatorBase):
         code_location=None,
         image_name=None,
         dependencies=None,
-        git_config=None,
         enable_network_isolation=False,
         **kwargs
     ):
@@ -960,47 +957,9 @@ class Framework(EstimatorBase):
         Args:
             entry_point (str): Path (absolute or relative) to the local Python source file which should be executed
                 as the entry point to training. This should be compatible with either Python 2.7 or Python 3.5.
-                If 'git_config' is provided, 'entry_point' should be a relative location to the Python source file in
-                the Git repo.
-                Example:
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            git_config (dict[str, str]): Git configurations used for cloning files, including 'repo', 'branch'
-                and 'commit' (default: None).
-                'branch' and 'commit' are optional. If 'branch' is not specified, 'master' branch will be used. If
-                'commit' is not specified, the latest commit in the required branch will be used.
-                Example:
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then checkout the 'master' branch, and checkout
-                    the specified commit.
             source_dir (str): Path (absolute or relative) to a directory with any other training
                 source code dependencies aside from the entry point file (default: None). Structure within this
-                directory are preserved when training on Amazon SageMaker. If 'git_config' is provided,
-                source_dir should be a relative location to a directory in the Git repo.
-                Example:
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as training source code as well, you can
-                    assign entry_point='train.py', source_dir='src'.
+                directory are preserved when training on Amazon SageMaker.
             hyperparameters (dict): Hyperparameters that will be used for training (default: None).
                 The hyperparameters are made accessible as a dict[str, str] to the training code on SageMaker.
                 For convenience, this accepts other types for keys and values, but ``str()`` will be called
@@ -1047,7 +1006,6 @@ class Framework(EstimatorBase):
                 )
             )
         self.entry_point = entry_point
-        self.git_config = git_config
         self.source_dir = source_dir
         self.dependencies = dependencies or []
         if enable_cloudwatch_metrics:
@@ -1080,14 +1038,6 @@ class Framework(EstimatorBase):
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
-
         # validate source dir will raise a ValueError if there is something wrong with the
         # source directory. We are intentionally not handling it because this is a critical error.
         if self.source_dir and not self.source_dir.lower().startswith("s3://"):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 16:39:47[0m
[92mHash: a6596839deebecaa91690c91af3f56827ff6c5f0[0m
[92mFilepath: tests/integ/test_git.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add git_config and git_clone, validate method (#832)

[0m
@@ -1,100 +0,0 @@
-# Copyright 2017-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-
-import os
-
-import numpy
-import tempfile
-
-from tests.integ import lock as lock
-from sagemaker.mxnet.estimator import MXNet
-from sagemaker.pytorch.estimator import PyTorch
-from tests.integ import DATA_DIR, PYTHON_VERSION
-
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-
-# endpoint tests all use the same port, so we use this lock to prevent concurrent execution
-LOCK_PATH = os.path.join(tempfile.gettempdir(), "sagemaker_test_git_lock")
-
-
-def test_git_support_with_pytorch(sagemaker_local_session):
-    script_path = "mnist.py"
-    data_path = os.path.join(DATA_DIR, "pytorch_mnist")
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    pytorch = PyTorch(
-        entry_point=script_path,
-        role="SageMakerRole",
-        source_dir="pytorch",
-        framework_version=PyTorch.LATEST_VERSION,
-        py_version=PYTHON_VERSION,
-        train_instance_count=1,
-        train_instance_type="local",
-        sagemaker_session=sagemaker_local_session,
-        git_config=git_config,
-    )
-
-    pytorch.fit({"training": "file://" + os.path.join(data_path, "training")})
-
-    with lock.lock(LOCK_PATH):
-        try:
-            predictor = pytorch.deploy(initial_instance_count=1, instance_type="local")
-
-            data = numpy.zeros(shape=(1, 1, 28, 28)).astype(numpy.float32)
-            result = predictor.predict(data)
-            assert result is not None
-        finally:
-            predictor.delete_endpoint()
-
-
-def test_git_support_with_mxnet(sagemaker_local_session, mxnet_full_version):
-    script_path = "mnist.py"
-    data_path = os.path.join(DATA_DIR, "mxnet_mnist")
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    dependencies = ["foo/bar.py"]
-    mx = MXNet(
-        entry_point=script_path,
-        role="SageMakerRole",
-        source_dir="mxnet",
-        dependencies=dependencies,
-        framework_version=MXNet.LATEST_VERSION,
-        py_version=PYTHON_VERSION,
-        train_instance_count=1,
-        train_instance_type="local",
-        sagemaker_session=sagemaker_local_session,
-        git_config=git_config,
-    )
-
-    mx.fit(
-        {
-            "train": "file://" + os.path.join(data_path, "train"),
-            "test": "file://" + os.path.join(data_path, "test"),
-        }
-    )
-
-    files = [file for file in os.listdir(mx.source_dir)]
-    assert "some_file" in files
-    assert "mnist.py" in files
-    assert os.path.exists(mx.dependencies[0])
-
-    with lock.lock(LOCK_PATH):
-        try:
-            predictor = mx.deploy(initial_instance_count=1, instance_type="local")
-
-            data = numpy.zeros(shape=(1, 1, 28, 28))
-            result = predictor.predict(data)
-            assert result is not None
-        finally:
-            predictor.delete_endpoint()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 16:39:47[0m
[92mHash: a6596839deebecaa91690c91af3f56827ff6c5f0[0m
[92mFilepath: tests/unit/test_estimator.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add git_config and git_clone, validate method (#832)

[0m
@@ -15,7 +15,6 @@ from __future__ import absolute_import
 import logging
 import json
 import os
-import subprocess
 from time import sleep
 
 import pytest
@@ -48,19 +47,6 @@ REGION = "us-west-2"
 JOB_NAME = "{}-{}".format(IMAGE_NAME, TIMESTAMP)
 TAGS = [{"Name": "some-tag", "Value": "value-for-tag"}]
 OUTPUT_PATH = "s3://bucket/prefix"
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-
-DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
-INSTANCE_TYPE = "c4.4xlarge"
-ACCELERATOR_TYPE = "ml.eia.medium"
-ROLE = "DummyRole"
-IMAGE_NAME = "fakeimage"
-REGION = "us-west-2"
-JOB_NAME = "{}-{}".format(IMAGE_NAME, TIMESTAMP)
-TAGS = [{"Name": "some-tag", "Value": "value-for-tag"}]
-OUTPUT_PATH = "s3://bucket/prefix"
 
 DESCRIBE_TRAINING_JOB_RESULT = {"ModelArtifacts": {"S3ModelArtifacts": MODEL_DATA}}
 
@@ -774,252 +760,6 @@ def test_prepare_for_training_force_name_generation(strftime, sagemaker_session)
     assert JOB_NAME == fw._current_job_name
 
 
-@patch("sagemaker.git_utils.git_clone_repo")
-def test_git_support_with_branch_and_commit_succeed(git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, source_dir=None, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    }
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-def test_git_support_with_branch_succeed(git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, source_dir, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/source_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    }
-    git_config = {"repo": GIT_REPO, "branch": BRANCH}
-    entry_point = "entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-def test_git_support_with_dependencies_succeed(git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, source_dir, dependencies: {
-        "entry_point": "/tmp/repo_dir/source_dir/entry_point",
-        "source_dir": None,
-        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/foo/bar"],
-    }
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "source_dir/entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        dependencies=["foo", "foo/bar"],
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, ["foo", "foo/bar"])
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-def test_git_support_without_branch_and_commit_succeed(git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, source_dir, dependencies=None: {
-        "entry_point": "/tmp/repo_dir/source_dir/entry_point",
-        "source_dir": None,
-        "dependencies": None,
-    }
-    git_config = {"repo": GIT_REPO}
-    entry_point = "source_dir/entry_point"
-    fw = DummyFramework(
-        entry_point=entry_point,
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    fw.fit()
-    git_clone_repo.assert_called_once_with(git_config, entry_point, None, [])
-
-
-def test_git_support_repo_not_provided(sagemaker_session):
-    git_config = {"branch": BRANCH, "commit": COMMIT}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        source_dir="source_dir",
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(ValueError) as error:
-        fw.fit()
-    assert "Please provide a repo for git_config." in str(error)
-
-
-def test_git_support_bad_repo_url_format(sagemaker_session):
-    git_config = {"repo": "hhttps://github.com/user/repo.git", "branch": BRANCH}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        source_dir="source_dir",
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        fw.fit()
-    assert "returned non-zero exit status" in str(error)
-
-
-def test_git_support_git_clone_fail(sagemaker_session):
-    git_config = {"repo": "https://github.com/aws/no-such-repo.git", "branch": BRANCH}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        fw.fit()
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git checkout branch-that-does-not-exist"
-    ),
-)
-def test_git_support_branch_not_exist(sagemaker_session):
-    git_config = {"repo": GIT_REPO, "branch": "branch-that-does-not-exist", "commit": COMMIT}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        fw.fit()
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git checkout commit-sha-that-does-not-exist"
-    ),
-)
-def test_git_support_commit_not_exist(sagemaker_session):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": "commit-sha-that-does-not-exist"}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        fw.fit()
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Entry point does not exist in the repo."),
-)
-def test_git_support_entry_point_not_exist(sagemaker_session):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    fw = DummyFramework(
-        entry_point="entry_point_that_does_not_exist",
-        git_config=git_config,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(ValueError) as error:
-        fw.fit()
-    assert "Entry point does not exist in the repo." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Source directory does not exist in the repo."),
-)
-def test_git_support_source_dir_not_exist(sagemaker_session):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        source_dir="source_dir_that_does_not_exist",
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(ValueError) as error:
-        fw.fit()
-    assert "Source directory does not exist in the repo." in str(error)
-
-
-@patch(
-    "sagemaker.git_utils.git_clone_repo",
-    side_effect=ValueError("Dependency no-such-dir does not exist in the repo."),
-)
-def test_git_support_dependencies_not_exist(sagemaker_session):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    fw = DummyFramework(
-        entry_point="entry_point",
-        git_config=git_config,
-        source_dir="source_dir",
-        dependencies=["foo", "no-such-dir"],
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        train_instance_count=INSTANCE_COUNT,
-        train_instance_type=INSTANCE_TYPE,
-        enable_cloudwatch_metrics=True,
-    )
-    with pytest.raises(ValueError) as error:
-        fw.fit()
-    assert "Dependency", "does not exist in the repo." in str(error)
-
-
 @patch("time.strftime", return_value=TIMESTAMP)
 def test_init_with_source_dir_s3(strftime, sagemaker_session):
     fw = DummyFramework(
@@ -1869,3 +1609,6 @@ def test_encryption_flag_in_non_vpc_mode_invalid(sagemaker_session):
         '"EnableInterContainerTrafficEncryption" and "VpcConfig" must be provided together'
         in str(error)
     )
+
+
+#################################################################################

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 16:39:47[0m
[92mHash: a6596839deebecaa91690c91af3f56827ff6c5f0[0m
[92mFilepath: tests/unit/test_git_utils.py[0m
[92mBranch: origin/master[0m
[92mCommit: feature: add git_config and git_clone, validate method (#832)

[0m
@@ -1,164 +0,0 @@
-# Copyright 2017-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-
-import pytest
-import subprocess
-from mock import patch
-
-from sagemaker import git_utils
-
-REPO_DIR = "/tmp/repo_dir"
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m"
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_succeed(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    ret = git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    check_call.assert_any_call(["git", "clone", git_config["repo"], REPO_DIR])
-    check_call.assert_any_call(args=["git", "checkout", BRANCH], cwd=REPO_DIR)
-    check_call.assert_any_call(args=["git", "checkout", COMMIT], cwd=REPO_DIR)
-    mkdtemp.assert_called_once()
-    assert ret["entry_point"] == "entry_point"
-    assert ret["source_dir"] == "/tmp/repo_dir/source_dir"
-    assert ret["dependencies"] == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_repo_not_provided(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point_that_does_not_exist"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "Please provide a repo for git_config." in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=subprocess.CalledProcessError(
-        returncode=1, cmd="git clone {} {}".format(GIT_REPO, REPO_DIR)
-    ),
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_clone_fail(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=[True, subprocess.CalledProcessError(returncode=1, cmd="git checkout banana")],
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_branch_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch(
-    "subprocess.check_call",
-    side_effect=[
-        True,
-        True,
-        subprocess.CalledProcessError(returncode=1, cmd="git checkout {}".format(COMMIT)),
-    ],
-)
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_commit_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(subprocess.CalledProcessError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "returned non-zero exit status" in str(error)
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=False)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_entry_point_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point_that_does_not_exist"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "Entry point does not exist in the repo." in str(error)
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=False)
-@patch("os.path.exists", return_value=True)
-def test_git_clone_repo_source_dir_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir_that_does_not_exist"
-    dependencies = ["foo", "bar"]
-    with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "Source directory does not exist in the repo." in str(error)
-
-
-@patch("subprocess.check_call")
-@patch("tempfile.mkdtemp", return_value=REPO_DIR)
-@patch("os.path.isfile", return_value=True)
-@patch("os.path.isdir", return_value=True)
-@patch("os.path.exists", side_effect=[True, False])
-def test_git_clone_repo_dependencies_not_exist(exists, isdir, isfile, mkdtemp, check_call):
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "dep_that_does_not_exist"]
-    with pytest.raises(ValueError) as error:
-        git_utils.git_clone_repo(git_config, entry_point, source_dir, dependencies)
-    assert "does not exist in the repo." in str(error)

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 10:45:26[0m
[92mHash: 6aa409bfdd1fc65212a787bb605dea558d033ab0[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/master[0m
[92mCommit: Integrate black into development process (#873)

* change: Add Black formatting tool as dependency

As of this commit, Black formatting tool can be run with 'tox -e black-format'.
Black does not run as part of any automated process, yet.

Black is pulled in as a test dependency only if the Python version
is greater than 3.6, as the tool is not vended as part of any
earlier Python version.

* change: Resolve Black formatting failures

Black is unable to handle trailing 'L' or 'l' which is no longer
supported as of python 3.8.

This commit removes those unnecessary 'long' identifiers.

https://www.python.org/dev/peps/pep-0237/

* change: Format all files using Black

This commit contains no functional changes.

* change: Manually resolve flake8 violations after formatting

* change: Manually resolve pylint violations after formatting

* change: Enable black locally and in automated build.

This commit enables black-format as part of "tox tests/unit", in order to
format all files.
It also enables black-check as part of the remote builds, in order to
verify that all files are properly formatted.
[0m
@@ -37,43 +37,37 @@ from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
 #
 # Both are  written by Amazon and are free to subscribe.
 
-ALGORITHM_ARN = (
-    "arn:aws:sagemaker:%s:%s:algorithm/scikit-decision-trees-"
-    "15423055-[93m[93m57b73412d2e93e9239e4e16f83298b8f[0m[0m"
-)
+ALGORITHM_ARN = 'arn:aws:sagemaker:%s:%s:algorithm/scikit-decision-trees-' \
+                '15423055-[93m[93m57b73412d2e93e9239e4e16f83298b8f[0m[0m'
 
-MODEL_PACKAGE_ARN = (
-    "arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-"
-    "154230595-[93m[93m8f00905c1f927a512b73ea29dd09ae30[0m[0m"
-)
+MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-' \
+                    '154230595-[93m[93m8f00905c1f927a512b73ea29dd09ae30[0m[0m'
 
 
 @pytest.mark.canary_quick
 def test_marketplace_estimator(sagemaker_session):
     with timeout(minutes=15):
-        data_path = os.path.join(DATA_DIR, "marketplace", "training")
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
         region = sagemaker_session.boto_region_name
         account = REGION_ACCOUNT_MAP[region]
         algorithm_arn = ALGORITHM_ARN % (region, account)
 
         algo = AlgorithmEstimator(
             algorithm_arn=algorithm_arn,
-            role="SageMakerRole",
+            role='SageMakerRole',
             train_instance_count=1,
-            train_instance_type="ml.c4.xlarge",
-            sagemaker_session=sagemaker_session,
-        )
+            train_instance_type='ml.c4.xlarge',
+            sagemaker_session=sagemaker_session)
 
         train_input = algo.sagemaker_session.upload_data(
-            path=data_path, key_prefix="integ-test-data/marketplace/train"
-        )
+            path=data_path, key_prefix='integ-test-data/marketplace/train')
 
-        algo.fit({"training": train_input})
+        algo.fit({'training': train_input})
 
-    endpoint_name = "test-marketplace-estimator{}".format(sagemaker_timestamp())
+    endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
     with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        predictor = algo.deploy(1, "ml.m4.xlarge", endpoint_name=endpoint_name)
-        shape = pandas.read_csv(os.path.join(data_path, "iris.csv"), header=None)
+        predictor = algo.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
 
         a = [50 * i for i in range(3)]
         b = [40 + i for i in range(10)]
@@ -82,48 +76,41 @@ def test_marketplace_estimator(sagemaker_session):
         test_data = shape.iloc[indices[:-1]]
         test_x = test_data.iloc[:, 1:]
 
-        print(predictor.predict(test_x.values).decode("utf-8"))
+        print(predictor.predict(test_x.values).decode('utf-8'))
 
 
 def test_marketplace_attach(sagemaker_session):
     with timeout(minutes=15):
-        data_path = os.path.join(DATA_DIR, "marketplace", "training")
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
         region = sagemaker_session.boto_region_name
         account = REGION_ACCOUNT_MAP[region]
         algorithm_arn = ALGORITHM_ARN % (region, account)
 
         mktplace = AlgorithmEstimator(
             algorithm_arn=algorithm_arn,
-            role="SageMakerRole",
+            role='SageMakerRole',
             train_instance_count=1,
-            train_instance_type="ml.c4.xlarge",
+            train_instance_type='ml.c4.xlarge',
             sagemaker_session=sagemaker_session,
-            base_job_name="test-marketplace",
-        )
+            base_job_name='test-marketplace')
 
         train_input = mktplace.sagemaker_session.upload_data(
-            path=data_path, key_prefix="integ-test-data/marketplace/train"
-        )
+            path=data_path, key_prefix='integ-test-data/marketplace/train')
 
-        mktplace.fit({"training": train_input}, wait=False)
+        mktplace.fit({'training': train_input}, wait=False)
         training_job_name = mktplace.latest_training_job.name
 
-        print("Waiting to re-attach to the training job: %s" % training_job_name)
+        print('Waiting to re-attach to the training job: %s' % training_job_name)
         time.sleep(20)
-        endpoint_name = "test-marketplace-estimator{}".format(sagemaker_timestamp())
+        endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
 
     with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        print("Re-attaching now to: %s" % training_job_name)
-        estimator = AlgorithmEstimator.attach(
-            training_job_name=training_job_name, sagemaker_session=sagemaker_session
-        )
-        predictor = estimator.deploy(
-            1,
-            "ml.m4.xlarge",
-            endpoint_name=endpoint_name,
-            serializer=sagemaker.predictor.csv_serializer,
-        )
-        shape = pandas.read_csv(os.path.join(data_path, "iris.csv"), header=None)
+        print('Re-attaching now to: %s' % training_job_name)
+        estimator = AlgorithmEstimator.attach(training_job_name=training_job_name,
+                                              sagemaker_session=sagemaker_session)
+        predictor = estimator.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name,
+                                     serializer=sagemaker.predictor.csv_serializer)
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
         a = [50 * i for i in range(3)]
         b = [40 + i for i in range(10)]
         indices = [i + j for i, j in itertools.product(a, b)]
@@ -131,7 +118,7 @@ def test_marketplace_attach(sagemaker_session):
         test_data = shape.iloc[indices[:-1]]
         test_x = test_data.iloc[:, 1:]
 
-        print(predictor.predict(test_x.values).decode("utf-8"))
+        print(predictor.predict(test_x.values).decode('utf-8'))
 
 
 @pytest.mark.canary_quick
@@ -145,18 +132,16 @@ def test_marketplace_model(sagemaker_session):
             endpoint, session, serializer=sagemaker.predictor.csv_serializer
         )
 
-    model = ModelPackage(
-        role="SageMakerRole",
-        model_package_arn=model_package_arn,
-        sagemaker_session=sagemaker_session,
-        predictor_cls=predict_wrapper,
-    )
+    model = ModelPackage(role='SageMakerRole',
+                         model_package_arn=model_package_arn,
+                         sagemaker_session=sagemaker_session,
+                         predictor_cls=predict_wrapper)
 
-    endpoint_name = "test-marketplace-model-endpoint{}".format(sagemaker_timestamp())
+    endpoint_name = 'test-marketplace-model-endpoint{}'.format(sagemaker_timestamp())
     with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        predictor = model.deploy(1, "ml.m4.xlarge", endpoint_name=endpoint_name)
-        data_path = os.path.join(DATA_DIR, "marketplace", "training")
-        shape = pandas.read_csv(os.path.join(data_path, "iris.csv"), header=None)
+        predictor = model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
         a = [50 * i for i in range(3)]
         b = [40 + i for i in range(10)]
         indices = [i + j for i, j in itertools.product(a, b)]
@@ -164,100 +149,90 @@ def test_marketplace_model(sagemaker_session):
         test_data = shape.iloc[indices[:-1]]
         test_x = test_data.iloc[:, 1:]
 
-        print(predictor.predict(test_x.values).decode("utf-8"))
+        print(predictor.predict(test_x.values).decode('utf-8'))
 
 
 def test_marketplace_tuning_job(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, "marketplace", "training")
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
     algorithm_arn = ALGORITHM_ARN % (region, account)
 
     mktplace = AlgorithmEstimator(
         algorithm_arn=algorithm_arn,
-        role="SageMakerRole",
+        role='SageMakerRole',
         train_instance_count=1,
-        train_instance_type="ml.c4.xlarge",
+        train_instance_type='ml.c4.xlarge',
         sagemaker_session=sagemaker_session,
-        base_job_name="test-marketplace",
-    )
+        base_job_name='test-marketplace')
 
     train_input = mktplace.sagemaker_session.upload_data(
-        path=data_path, key_prefix="integ-test-data/marketplace/train"
-    )
+        path=data_path, key_prefix='integ-test-data/marketplace/train')
 
     mktplace.set_hyperparameters(max_leaf_nodes=10)
 
-    hyperparameter_ranges = {"max_leaf_nodes": IntegerParameter(1, 100000)}
+    hyperparameter_ranges = {'max_leaf_nodes': IntegerParameter(1, 100000)}
 
-    tuner = HyperparameterTuner(
-        estimator=mktplace,
-        base_tuning_job_name="byo",
-        objective_metric_name="validation:accuracy",
-        hyperparameter_ranges=hyperparameter_ranges,
-        max_jobs=2,
-        max_parallel_jobs=2,
-    )
+    tuner = HyperparameterTuner(estimator=mktplace, base_tuning_job_name='byo',
+                                objective_metric_name='validation:accuracy',
+                                hyperparameter_ranges=hyperparameter_ranges,
+                                max_jobs=2, max_parallel_jobs=2)
 
-    tuner.fit({"training": train_input}, include_cls_metadata=False)
+    tuner.fit({'training': train_input}, include_cls_metadata=False)
     time.sleep(15)
     tuner.wait()
 
 
 def test_marketplace_transform_job(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, "marketplace", "training")
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
     algorithm_arn = ALGORITHM_ARN % (region, account)
 
     algo = AlgorithmEstimator(
         algorithm_arn=algorithm_arn,
-        role="SageMakerRole",
+        role='SageMakerRole',
         train_instance_count=1,
-        train_instance_type="ml.c4.xlarge",
+        train_instance_type='ml.c4.xlarge',
         sagemaker_session=sagemaker_session,
-        base_job_name="test-marketplace",
-    )
+        base_job_name='test-marketplace')
 
     train_input = algo.sagemaker_session.upload_data(
-        path=data_path, key_prefix="integ-test-data/marketplace/train"
-    )
+        path=data_path, key_prefix='integ-test-data/marketplace/train')
 
-    shape = pandas.read_csv(data_path + "/iris.csv", header=None).drop([0], axis=1)
+    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
 
-    transform_workdir = DATA_DIR + "/marketplace/transform"
-    shape.to_csv(transform_workdir + "/batchtransform_test.csv", index=False, header=False)
+    transform_workdir = DATA_DIR + '/marketplace/transform'
+    shape.to_csv(transform_workdir + '/batchtransform_test.csv', index=False, header=False)
     transform_input = algo.sagemaker_session.upload_data(
-        transform_workdir, key_prefix="integ-test-data/marketplace/transform"
-    )
+        transform_workdir,
+        key_prefix='integ-test-data/marketplace/transform')
 
-    algo.fit({"training": train_input})
+    algo.fit({'training': train_input})
 
-    transformer = algo.transformer(1, "ml.m4.xlarge")
-    transformer.transform(transform_input, content_type="text/csv")
+    transformer = algo.transformer(1, 'ml.m4.xlarge')
+    transformer.transform(transform_input, content_type='text/csv')
     transformer.wait()
 
 
 def test_marketplace_transform_job_from_model_package(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, "marketplace", "training")
-    shape = pandas.read_csv(data_path + "/iris.csv", header=None).drop([0], axis=1)
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
 
-    TRANSFORM_WORKDIR = DATA_DIR + "/marketplace/transform"
-    shape.to_csv(TRANSFORM_WORKDIR + "/batchtransform_test.csv", index=False, header=False)
+    TRANSFORM_WORKDIR = DATA_DIR + '/marketplace/transform'
+    shape.to_csv(TRANSFORM_WORKDIR + '/batchtransform_test.csv', index=False, header=False)
     transform_input = sagemaker_session.upload_data(
-        TRANSFORM_WORKDIR, key_prefix="integ-test-data/marketplace/transform"
-    )
+        TRANSFORM_WORKDIR,
+        key_prefix='integ-test-data/marketplace/transform')
 
     region = sagemaker_session.boto_region_name
     account = REGION_ACCOUNT_MAP[region]
     model_package_arn = MODEL_PACKAGE_ARN % (region, account)
 
-    model = ModelPackage(
-        role="SageMakerRole",
-        model_package_arn=model_package_arn,
-        sagemaker_session=sagemaker_session,
-    )
+    model = ModelPackage(role='SageMakerRole',
+                         model_package_arn=model_package_arn,
+                         sagemaker_session=sagemaker_session)
 
-    transformer = model.transformer(1, "ml.m4.xlarge")
-    transformer.transform(transform_input, content_type="text/csv")
+    transformer = model.transformer(1, 'ml.m4.xlarge')
+    transformer.transform(transform_input, content_type='text/csv')
     transformer.wait()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-24 10:45:26[0m
[92mHash: 6aa409bfdd1fc65212a787bb605dea558d033ab0[0m
[92mFilepath: tests/unit/test_algorithm.py[0m
[92mBranch: origin/master[0m
[92mCommit: Integrate black into development process (#873)

* change: Add Black formatting tool as dependency

As of this commit, Black formatting tool can be run with 'tox -e black-format'.
Black does not run as part of any automated process, yet.

Black is pulled in as a test dependency only if the Python version
is greater than 3.6, as the tool is not vended as part of any
earlier Python version.

* change: Resolve Black formatting failures

Black is unable to handle trailing 'L' or 'l' which is no longer
supported as of python 3.8.

This commit removes those unnecessary 'long' identifiers.

https://www.python.org/dev/peps/pep-0237/

* change: Format all files using Black

This commit contains no functional changes.

* change: Manually resolve flake8 violations after formatting

* change: Manually resolve pylint violations after formatting

* change: Enable black locally and in automated build.

This commit enables black-format as part of "tox tests/unit", in order to
format all files.
It also enables black-check as part of the remote builds, in order to
verify that all files are properly formatted.
[0m
@@ -23,159 +23,159 @@ from sagemaker.estimator import _TrainingJob
 from sagemaker.transformer import Transformer
 
 DESCRIBE_ALGORITHM_RESPONSE = {
-    "AlgorithmName": "scikit-decision-trees",
-    "AlgorithmArn": "arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-    "AlgorithmDescription": "Decision trees using Scikit",
-    "CreationTime": datetime.datetime(2018, 8, 3, 22, 44, 54, 437000),
-    "TrainingSpecification": {
-        "TrainingImage": "123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:12345",
-        "TrainingImageDigest": "sha256:[93m[93m206854b6ea2f0020d216311da732010515169820b898ec29720bcf1d2b46806a[0m[0m",
-        "SupportedHyperParameters": [
+    'AlgorithmName': 'scikit-decision-trees',
+    'AlgorithmArn': 'arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+    'AlgorithmDescription': 'Decision trees using Scikit',
+    'CreationTime': datetime.datetime(2018, 8, 3, 22, 44, 54, 437000),
+    'TrainingSpecification': {
+        'TrainingImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:12345',
+        'TrainingImageDigest': 'sha256:[93m[93m206854b6ea2f0020d216311da732010515169820b898ec29720bcf1d2b46806a[0m[0m',
+        'SupportedHyperParameters': [
             {
-                "Name": "max_leaf_nodes",
-                "Description": "Grow a tree with max_leaf_nodes in best-first fashion.",
-                "Type": "Integer",
-                "Range": {
-                    "IntegerParameterRangeSpecification": {"MinValue": "1", "MaxValue": "100000"}
+                'Name': 'max_leaf_nodes',
+                'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+                'Type': 'Integer',
+                'Range': {
+                    'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
                 },
-                "IsTunable": True,
-                "IsRequired": False,
-                "DefaultValue": "100",
+                'IsTunable': True,
+                'IsRequired': False,
+                'DefaultValue': '100',
             },
             {
-                "Name": "free_text_hp1",
-                "Description": "You can write anything here",
-                "Type": "FreeText",
-                "IsTunable": False,
-                "IsRequired": True,
-            },
+                'Name': 'free_text_hp1',
+                'Description': 'You can write anything here',
+                'Type': 'FreeText',
+                'IsTunable': False,
+                'IsRequired': True
+            }
         ],
-        "SupportedTrainingInstanceTypes": ["ml.m4.xlarge", "ml.m4.2xlarge", "ml.m4.4xlarge"],
-        "SupportsDistributedTraining": False,
-        "MetricDefinitions": [
-            {"Name": "validation:accuracy", "Regex": "validation-accuracy: (\\S+)"}
+        'SupportedTrainingInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge', 'ml.m4.4xlarge'],
+        'SupportsDistributedTraining': False,
+        'MetricDefinitions': [
+            {'Name': 'validation:accuracy', 'Regex': 'validation-accuracy: (\\S+)'}
         ],
-        "TrainingChannels": [
+        'TrainingChannels': [
             {
-                "Name": "training",
-                "Description": "Input channel that provides training data",
-                "IsRequired": True,
-                "SupportedContentTypes": ["text/csv"],
-                "SupportedCompressionTypes": ["None"],
-                "SupportedInputModes": ["File"],
+                'Name': 'training',
+                'Description': 'Input channel that provides training data',
+                'IsRequired': True,
+                'SupportedContentTypes': ['text/csv'],
+                'SupportedCompressionTypes': ['None'],
+                'SupportedInputModes': ['File'],
             }
         ],
-        "SupportedTuningJobObjectiveMetrics": [
-            {"Type": "Maximize", "MetricName": "validation:accuracy"}
+        'SupportedTuningJobObjectiveMetrics': [
+            {'Type': 'Maximize', 'MetricName': 'validation:accuracy'}
         ],
     },
-    "InferenceSpecification": {
-        "InferenceImage": "123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:123",
-        "SupportedTransformInstanceTypes": ["ml.m4.xlarge", "ml.m4.2xlarge"],
-        "SupportedContentTypes": ["text/csv"],
-        "SupportedResponseMIMETypes": ["text"],
+    'InferenceSpecification': {
+        'InferenceImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:123',
+        'SupportedTransformInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge'],
+        'SupportedContentTypes': ['text/csv'],
+        'SupportedResponseMIMETypes': ['text'],
     },
-    "ValidationSpecification": {
-        "ValidationRole": "arn:aws:iam::764419575721:role/SageMakerRole",
-        "ValidationProfiles": [
+    'ValidationSpecification': {
+        'ValidationRole': 'arn:aws:iam::764419575721:role/SageMakerRole',
+        'ValidationProfiles': [
             {
-                "ProfileName": "ValidationProfile1",
-                "TrainingJobDefinition": {
-                    "TrainingInputMode": "File",
-                    "HyperParameters": {},
-                    "InputDataConfig": [
+                'ProfileName': 'ValidationProfile1',
+                'TrainingJobDefinition': {
+                    'TrainingInputMode': 'File',
+                    'HyperParameters': {},
+                    'InputDataConfig': [
                         {
-                            "ChannelName": "training",
-                            "DataSource": {
-                                "S3DataSource": {
-                                    "S3DataType": "S3Prefix",
-                                    "S3Uri": "s3://sagemaker-us-east-2-7123/-scikit-byo-iris/training-input-data",
-                                    "S3DataDistributionType": "FullyReplicated",
+                            'ChannelName': 'training',
+                            'DataSource': {
+                                'S3DataSource': {
+                                    'S3DataType': 'S3Prefix',
+                                    'S3Uri': 's3://sagemaker-us-east-2-7123/-scikit-byo-iris/training-input-data',
+                                    'S3DataDistributionType': 'FullyReplicated',
                                 }
                             },
-                            "ContentType": "text/csv",
-                            "CompressionType": "None",
-                            "RecordWrapperType": "None",
+                            'ContentType': 'text/csv',
+                            'CompressionType': 'None',
+                            'RecordWrapperType': 'None',
                         }
                     ],
-                    "OutputDataConfig": {
-                        "KmsKeyId": "",
-                        "S3OutputPath": "s3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/training-output",
+                    'OutputDataConfig': {
+                        'KmsKeyId': '',
+                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/training-output',
                     },
-                    "ResourceConfig": {
-                        "InstanceType": "ml.c4.xlarge",
-                        "InstanceCount": 1,
-                        "VolumeSizeInGB": 10,
+                    'ResourceConfig': {
+                        'InstanceType': 'ml.c4.xlarge',
+                        'InstanceCount': 1,
+                        'VolumeSizeInGB': 10,
                     },
-                    "StoppingCondition": {"MaxRuntimeInSeconds": 3600},
+                    'StoppingCondition': {'MaxRuntimeInSeconds': 3600},
                 },
-                "TransformJobDefinition": {
-                    "MaxConcurrentTransforms": 0,
-                    "MaxPayloadInMB": 0,
-                    "TransformInput": {
-                        "DataSource": {
-                            "S3DataSource": {
-                                "S3DataType": "S3Prefix",
-                                "S3Uri": "s3://sagemaker-us-east-2/scikit-byo-iris/batch-inference/transform_test.csv",
+                'TransformJobDefinition': {
+                    'MaxConcurrentTransforms': 0,
+                    'MaxPayloadInMB': 0,
+                    'TransformInput': {
+                        'DataSource': {
+                            'S3DataSource': {
+                                'S3DataType': 'S3Prefix',
+                                'S3Uri': 's3://sagemaker-us-east-2/scikit-byo-iris/batch-inference/transform_test.csv',
                             }
                         },
-                        "ContentType": "text/csv",
-                        "CompressionType": "None",
-                        "SplitType": "Line",
+                        'ContentType': 'text/csv',
+                        'CompressionType': 'None',
+                        'SplitType': 'Line',
                     },
-                    "TransformOutput": {
-                        "S3OutputPath": "s3://sagemaker-us-east-2-764419575721/scikit-byo-iris/batch-transform-output",
-                        "Accept": "text/csv",
-                        "AssembleWith": "Line",
-                        "KmsKeyId": "",
+                    'TransformOutput': {
+                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/scikit-byo-iris/batch-transform-output',
+                        'Accept': 'text/csv',
+                        'AssembleWith': 'Line',
+                        'KmsKeyId': '',
                     },
-                    "TransformResources": {"InstanceType": "ml.c4.xlarge", "InstanceCount": 1},
+                    'TransformResources': {'InstanceType': 'ml.c4.xlarge', 'InstanceCount': 1},
                 },
             }
         ],
-        "ValidationOutputS3Prefix": "s3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/validation-output",
-        "ValidateForMarketplace": True,
+        'ValidationOutputS3Prefix': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/validation-output',
+        'ValidateForMarketplace': True,
     },
-    "AlgorithmStatus": "Completed",
-    "AlgorithmStatusDetails": {
-        "ValidationStatuses": [{"ProfileName": "ValidationProfile1", "Status": "Completed"}]
+    'AlgorithmStatus': 'Completed',
+    'AlgorithmStatusDetails': {
+        'ValidationStatuses': [{'ProfileName': 'ValidationProfile1', 'Status': 'Completed'}]
     },
-    "ResponseMetadata": {
-        "RequestId": "e04bc28b-61b6-4486-9106-0edf07f5649c",
-        "HTTPStatusCode": 200,
-        "HTTPHeaders": {
-            "x-amzn-requestid": "e04bc28b-61b6-4486-9106-0edf07f5649c",
-            "content-type": "application/x-amz-json-1.1",
-            "content-length": "3949",
-            "date": "Fri, 03 Aug 2018 23:08:43 GMT",
+    'ResponseMetadata': {
+        'RequestId': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
+        'HTTPStatusCode': 200,
+        'HTTPHeaders': {
+            'x-amzn-requestid': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
+            'content-type': 'application/x-amz-json-1.1',
+            'content-length': '3949',
+            'date': 'Fri, 03 Aug 2018 23:08:43 GMT',
         },
-        "RetryAttempts": 0,
+        'RetryAttempts': 0,
     },
 }
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_supported_input_mode_with_valid_input_types(session):
     # verify that the Estimator verifies the
     # input mode that an Algorithm supports.
 
     file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    file_mode_algo["TrainingSpecification"]["TrainingChannels"] = [
+    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
     ]
 
@@ -183,30 +183,30 @@ def test_algorithm_supported_input_mode_with_valid_input_types(session):
 
     # Creating a File mode Estimator with a File mode algorithm should work
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    pipe_mode_algo["TrainingSpecification"]["TrainingChannels"] = [
+    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["Pipe"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['Pipe'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
     ]
 
@@ -214,31 +214,31 @@ def test_algorithm_supported_input_mode_with_valid_input_types(session):
 
     # Creating a Pipe mode Estimator with a Pipe mode algorithm should work.
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
-        input_mode="Pipe",
+        input_mode='Pipe',
         sagemaker_session=session,
     )
 
     any_input_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    any_input_algo["TrainingSpecification"]["TrainingChannels"] = [
+    any_input_algo['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
     ]
 
@@ -247,36 +247,36 @@ def test_algorithm_supported_input_mode_with_valid_input_types(session):
     # Creating a File mode Estimator with an algorithm that supports both input modes
     # should work.
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_supported_input_mode_with_bad_input_types(session):
     # verify that the Estimator verifies raises exceptions when
     # attempting to train with an incorrect input type
 
     file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    file_mode_algo["TrainingSpecification"]["TrainingChannels"] = [
+    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
     ]
 
@@ -285,31 +285,31 @@ def test_algorithm_supported_input_mode_with_bad_input_types(session):
     # Creating a Pipe mode Estimator with a File mode algorithm should fail.
     with pytest.raises(ValueError):
         AlgorithmEstimator(
-            algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-            role="SageMakerRole",
-            train_instance_type="ml.m4.xlarge",
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.xlarge',
             train_instance_count=1,
-            input_mode="Pipe",
+            input_mode='Pipe',
             sagemaker_session=session,
         )
 
     pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    pipe_mode_algo["TrainingSpecification"]["TrainingChannels"] = [
+    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["Pipe"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['Pipe'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File", "Pipe"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
         },
     ]
 
@@ -318,171 +318,175 @@ def test_algorithm_supported_input_mode_with_bad_input_types(session):
     # Creating a File mode Estimator with a Pipe mode algorithm should fail.
     with pytest.raises(ValueError):
         AlgorithmEstimator(
-            algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-            role="SageMakerRole",
-            train_instance_type="ml.m4.xlarge",
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.xlarge',
             train_instance_count=1,
             sagemaker_session=session,
         )
 
 
-@patch("sagemaker.estimator.EstimatorBase.fit", Mock())
-@patch("sagemaker.Session")
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
+@patch('sagemaker.Session')
 def test_algorithm_trainining_channels_with_expected_channels(session):
     training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
 
-    training_channels["TrainingSpecification"]["TrainingChannels"] = [
+    training_channels['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
     ]
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     # Pass training and validation channels. This should work
-    estimator.fit({"training": "s3://some/place", "validation": "s3://some/other"})
+    estimator.fit({'training': 's3://some/place', 'validation': 's3://some/other'})
 
     # Passing only the training channel. Validation is optional so this should also work.
-    estimator.fit({"training": "s3://some/place"})
+    estimator.fit({'training': 's3://some/place'})
 
 
-@patch("sagemaker.estimator.EstimatorBase.fit", Mock())
-@patch("sagemaker.Session")
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
+@patch('sagemaker.Session')
 def test_algorithm_trainining_channels_with_invalid_channels(session):
     training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
 
-    training_channels["TrainingSpecification"]["TrainingChannels"] = [
+    training_channels['TrainingSpecification']['TrainingChannels'] = [
         {
-            "Name": "training",
-            "Description": "Input channel that provides training data",
-            "IsRequired": True,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
         {
-            "Name": "validation",
-            "Description": "Input channel that provides validation data",
-            "IsRequired": False,
-            "SupportedContentTypes": ["text/csv"],
-            "SupportedCompressionTypes": ["None"],
-            "SupportedInputModes": ["File"],
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
         },
     ]
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     # Passing only validation should fail as training is required.
     with pytest.raises(ValueError):
-        estimator.fit({"validation": "s3://some/thing"})
+        estimator.fit({'validation': 's3://some/thing'})
 
     # Passing an unknown channel should fail???
     with pytest.raises(ValueError):
-        estimator.fit({"training": "s3://some/data", "training2": "s3://some/other/data"})
+        estimator.fit({'training': 's3://some/data', 'training2': 's3://some/other/data'})
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_train_instance_types_valid_instance_types(session):
     describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    train_instance_types = ["ml.m4.xlarge", "ml.m5.2xlarge"]
+    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
 
-    describe_algo_response["TrainingSpecification"][
-        "SupportedTrainingInstanceTypes"
+    describe_algo_response['TrainingSpecification'][
+        'SupportedTrainingInstanceTypes'
     ] = train_instance_types
 
-    session.sagemaker_client.describe_algorithm = Mock(return_value=describe_algo_response)
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=describe_algo_response
+    )
 
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m5.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m5.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_train_instance_types_invalid_instance_types(session):
     describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    train_instance_types = ["ml.m4.xlarge", "ml.m5.2xlarge"]
+    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
 
-    describe_algo_response["TrainingSpecification"][
-        "SupportedTrainingInstanceTypes"
+    describe_algo_response['TrainingSpecification'][
+        'SupportedTrainingInstanceTypes'
     ] = train_instance_types
 
-    session.sagemaker_client.describe_algorithm = Mock(return_value=describe_algo_response)
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=describe_algo_response
+    )
 
     # invalid instance type, should fail
     with pytest.raises(ValueError):
         AlgorithmEstimator(
-            algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-            role="SageMakerRole",
-            train_instance_type="ml.m4.8xlarge",
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.8xlarge',
             train_instance_count=1,
             sagemaker_session=session,
         )
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_distributed_training_validation(session):
     distributed_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    distributed_algo["TrainingSpecification"]["SupportsDistributedTraining"] = True
+    distributed_algo['TrainingSpecification']['SupportsDistributedTraining'] = True
 
     single_instance_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    single_instance_algo["TrainingSpecification"]["SupportsDistributedTraining"] = False
+    single_instance_algo['TrainingSpecification']['SupportsDistributedTraining'] = False
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=distributed_algo)
 
     # Distributed training should work for Distributed and Single instance.
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=2,
         sagemaker_session=session,
     )
@@ -492,39 +496,39 @@ def test_algorithm_distributed_training_validation(session):
     # distributed training on a single instance algorithm should fail.
     with pytest.raises(ValueError):
         AlgorithmEstimator(
-            algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-            role="SageMakerRole",
-            train_instance_type="ml.m5.2xlarge",
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m5.2xlarge',
             train_instance_count=2,
             sagemaker_session=session,
         )
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_hyperparameter_integer_range_valid_range(session):
     hyperparameters = [
         {
-            "Description": "Grow a tree with max_leaf_nodes in best-first fashion.",
-            "Type": "Integer",
-            "Name": "max_leaf_nodes",
-            "Range": {
-                "IntegerParameterRangeSpecification": {"MinValue": "1", "MaxValue": "100000"}
+            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+            'Type': 'Integer',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
             },
-            "IsTunable": True,
-            "IsRequired": False,
-            "DefaultValue": "100",
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
         }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -533,31 +537,31 @@ def test_algorithm_hyperparameter_integer_range_valid_range(session):
     estimator.set_hyperparameters(max_leaf_nodes=100000)
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_hyperparameter_integer_range_invalid_range(session):
     hyperparameters = [
         {
-            "Description": "Grow a tree with max_leaf_nodes in best-first fashion.",
-            "Type": "Integer",
-            "Name": "max_leaf_nodes",
-            "Range": {
-                "IntegerParameterRangeSpecification": {"MinValue": "1", "MaxValue": "100000"}
+            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+            'Type': 'Integer',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
             },
-            "IsTunable": True,
-            "IsRequired": False,
-            "DefaultValue": "100",
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
         }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -569,31 +573,31 @@ def test_algorithm_hyperparameter_integer_range_invalid_range(session):
         estimator.set_hyperparameters(max_leaf_nodes=100001)
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_hyperparameter_continuous_range_valid_range(session):
     hyperparameters = [
         {
-            "Description": "A continuous hyperparameter",
-            "Type": "Continuous",
-            "Name": "max_leaf_nodes",
-            "Range": {
-                "ContinuousParameterRangeSpecification": {"MinValue": "0.0", "MaxValue": "1.0"}
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Continuous',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
             },
-            "IsTunable": True,
-            "IsRequired": False,
-            "DefaultValue": "100",
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
         }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -604,31 +608,31 @@ def test_algorithm_hyperparameter_continuous_range_valid_range(session):
     estimator.set_hyperparameters(max_leaf_nodes=1)
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_hyperparameter_continuous_range_invalid_range(session):
     hyperparameters = [
         {
-            "Description": "A continuous hyperparameter",
-            "Type": "Continuous",
-            "Name": "max_leaf_nodes",
-            "Range": {
-                "ContinuousParameterRangeSpecification": {"MinValue": "0.0", "MaxValue": "1.0"}
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Continuous',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
             },
-            "IsTunable": True,
-            "IsRequired": False,
-            "DefaultValue": "100",
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
         }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -640,159 +644,159 @@ def test_algorithm_hyperparameter_continuous_range_invalid_range(session):
         estimator.set_hyperparameters(max_leaf_nodes=-0.1)
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_hyperparameter_categorical_range(session):
     hyperparameters = [
         {
-            "Description": "A continuous hyperparameter",
-            "Type": "Categorical",
-            "Name": "hp1",
-            "Range": {"CategoricalParameterRangeSpecification": {"Values": ["TF", "MXNet"]}},
-            "IsTunable": True,
-            "IsRequired": False,
-            "DefaultValue": "100",
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
         }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
-    estimator.set_hyperparameters(hp1="MXNet")
-    estimator.set_hyperparameters(hp1="TF")
+    estimator.set_hyperparameters(hp1='MXNet')
+    estimator.set_hyperparameters(hp1='TF')
 
     with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp1="Chainer")
+        estimator.set_hyperparameters(hp1='Chainer')
 
     with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp1="MxNET")
+        estimator.set_hyperparameters(hp1='MxNET')
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_required_hyperparameters_not_provided(session):
     hyperparameters = [
         {
-            "Description": "A continuous hyperparameter",
-            "Type": "Categorical",
-            "Name": "hp1",
-            "Range": {"CategoricalParameterRangeSpecification": {"Values": ["TF", "MXNet"]}},
-            "IsTunable": True,
-            "IsRequired": True,
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': True,
         },
         {
-            "Name": "hp2",
-            "Description": "A continuous hyperparameter",
-            "Type": "Categorical",
-            "IsTunable": False,
-            "IsRequired": True,
-        },
+            'Name': 'hp2',
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'IsTunable': False,
+            'IsRequired': True
+        }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     # hp1 is required and was not provided
     with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp2="TF2")
+        estimator.set_hyperparameters(hp2='TF2')
 
     # Calling fit with unset required hyperparameters should fail
     # this covers the use case of not calling set_hyperparameters() explicitly
     with pytest.raises(ValueError):
-        estimator.fit({"training": "s3://some/place"})
+        estimator.fit({'training': 's3://some/place'})
 
 
-@patch("sagemaker.Session")
-@patch("sagemaker.estimator.EstimatorBase.fit", Mock())
+@patch('sagemaker.Session')
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
 def test_algorithm_required_hyperparameters_are_provided(session):
     hyperparameters = [
         {
-            "Description": "A categorical hyperparameter",
-            "Type": "Categorical",
-            "Name": "hp1",
-            "Range": {"CategoricalParameterRangeSpecification": {"Values": ["TF", "MXNet"]}},
-            "IsTunable": True,
-            "IsRequired": True,
+            'Description': 'A categorical hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': True,
         },
         {
-            "Name": "hp2",
-            "Description": "A categorical hyperparameter",
-            "Type": "Categorical",
-            "IsTunable": False,
-            "IsRequired": True,
+            'Name': 'hp2',
+            'Description': 'A categorical hyperparameter',
+            'Type': 'Categorical',
+            'IsTunable': False,
+            'IsRequired': True
         },
         {
-            "Name": "free_text_hp1",
-            "Description": "You can write anything here",
-            "Type": "FreeText",
-            "IsTunable": False,
-            "IsRequired": True,
-        },
+            'Name': 'free_text_hp1',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': True
+        }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     # All 3 Hyperparameters are provided
-    estimator.set_hyperparameters(hp1="TF", hp2="TF2", free_text_hp1="Hello!")
+    estimator.set_hyperparameters(hp1='TF', hp2='TF2', free_text_hp1='Hello!')
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_required_free_text_hyperparameter_not_provided(session):
     hyperparameters = [
         {
-            "Name": "free_text_hp1",
-            "Description": "You can write anything here",
-            "Type": "FreeText",
-            "IsTunable": False,
-            "IsRequired": True,
+            'Name': 'free_text_hp1',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': True
         },
         {
-            "Name": "free_text_hp2",
-            "Description": "You can write anything here",
-            "Type": "FreeText",
-            "IsTunable": False,
-            "IsRequired": False,
-        },
+            'Name': 'free_text_hp2',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': False
+        }
     ]
 
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo["TrainingSpecification"]["SupportedHyperParameters"] = hyperparameters
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -800,87 +804,91 @@ def test_algorithm_required_free_text_hyperparameter_not_provided(session):
     # Calling fit with unset required hyperparameters should fail
     # this covers the use case of not calling set_hyperparameters() explicitly
     with pytest.raises(ValueError):
-        estimator.fit({"training": "s3://some/place"})
+        estimator.fit({'training': 's3://some/place'})
 
     # hp1 is required and was not provided
     with pytest.raises(ValueError):
-        estimator.set_hyperparameters(free_text_hp2="some text")
+        estimator.set_hyperparameters(free_text_hp2='some text')
 
 
-@patch("sagemaker.Session")
-@patch("sagemaker.algorithm.AlgorithmEstimator.create_model")
+@patch('sagemaker.Session')
+@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
 def test_algorithm_create_transformer(create_model, session):
-    session.sagemaker_client.describe_algorithm = Mock(return_value=DESCRIBE_ALGORITHM_RESPONSE)
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
-    estimator.latest_training_job = _TrainingJob(session, "some-job-name")
+    estimator.latest_training_job = _TrainingJob(session, 'some-job-name')
     model = Mock()
-    model.name = "my-model"
+    model.name = 'my-model'
     create_model.return_value = model
 
-    transformer = estimator.transformer(instance_count=1, instance_type="ml.m4.xlarge")
+    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
 
     assert isinstance(transformer, Transformer)
     create_model.assert_called()
-    assert transformer.model_name == "my-model"
+    assert transformer.model_name == 'my-model'
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_create_transformer_without_completed_training_job(session):
-    session.sagemaker_client.describe_algorithm = Mock(return_value=DESCRIBE_ALGORITHM_RESPONSE)
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
     with pytest.raises(RuntimeError) as error:
-        estimator.transformer(instance_count=1, instance_type="ml.m4.xlarge")
-        assert "No finished training job found associated with this estimator" in str(error)
+        estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
+        assert 'No finished training job found associated with this estimator' in str(error)
 
 
-@patch("sagemaker.algorithm.AlgorithmEstimator.create_model")
-@patch("sagemaker.Session")
+@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
+@patch('sagemaker.Session')
 def test_algorithm_create_transformer_with_product_id(create_model, session):
     response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    response["ProductId"] = "some-product-id"
-    session.sagemaker_client.describe_algorithm = Mock(return_value=response)
+    response['ProductId'] = 'some-product-id'
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
 
-    estimator.latest_training_job = _TrainingJob(session, "some-job-name")
+    estimator.latest_training_job = _TrainingJob(session, 'some-job-name')
     model = Mock()
-    model.name = "my-model"
+    model.name = 'my-model'
     create_model.return_value = model
 
-    transformer = estimator.transformer(instance_count=1, instance_type="ml.m4.xlarge")
+    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
     assert transformer.env is None
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_enable_network_isolation_no_product_id(session):
-    session.sagemaker_client.describe_algorithm = Mock(return_value=DESCRIBE_ALGORITHM_RESPONSE)
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -889,16 +897,17 @@ def test_algorithm_enable_network_isolation_no_product_id(session):
     assert network_isolation is False
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_enable_network_isolation_with_product_id(session):
     response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    response["ProductId"] = "some-product-id"
-    session.sagemaker_client.describe_algorithm = Mock(return_value=response)
+    response['ProductId'] = 'some-product-id'
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )
@@ -907,29 +916,30 @@ def test_algorithm_enable_network_isolation_with_product_id(session):
     assert network_isolation is True
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_encrypt_inter_container_traffic(session):
     response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    response["encrypt_inter_container_traffic"] = True
-    session.sagemaker_client.describe_algorithm = Mock(return_value=response)
+    response['encrypt_inter_container_traffic'] = True
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
 
     estimator = AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
         train_instance_count=1,
         sagemaker_session=session,
-        encrypt_inter_container_traffic=True,
+        encrypt_inter_container_traffic=True
     )
 
     encrypt_inter_container_traffic = estimator.encrypt_inter_container_traffic
     assert encrypt_inter_container_traffic is True
 
 
-@patch("sagemaker.Session")
+@patch('sagemaker.Session')
 def test_algorithm_no_required_hyperparameters(session):
     some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    del some_algo["TrainingSpecification"]["SupportedHyperParameters"]
+    del some_algo['TrainingSpecification']['SupportedHyperParameters']
 
     session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
 
@@ -937,9 +947,9 @@ def test_algorithm_no_required_hyperparameters(session):
     # should fail if they are required.
     # Pass training and hyperparameters channels. This should work
     assert AlgorithmEstimator(
-        algorithm_arn="arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees",
-        role="SageMakerRole",
-        train_instance_type="ml.m4.2xlarge",
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
         train_instance_count=1,
         sagemaker_session=session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-04-04 12:01:43[0m
[92mHash: ba6240086b90860ca3a30aadc777dbc1e0a47f00[0m
[92mFilepath: CONTRIBUTING.md[0m
[92mBranch: origin/master[0m
[92mCommit: doc: add info about unique job names (#734)

[0m
@@ -65,10 +65,6 @@ Some of the prefixes allow abbreviation -- `break`, `feat`, `depr`, and `doc` ar
 
 For the rest of the message, use imperative style and keep things concise but informative. See [How to Write a Git Commit Message](https://chris.beams.io/posts/git-commit/) for guidance.
 
-### Integration tests
-
-Our CI system runs integration tests (the ones in the `tests/integ` directory) in parallel. If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or an endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tfs.py#L37) and 
-[here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.
 
 ## Finding contributions to work on
 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any ['help wanted'](https://github.com/aws/sagemaker-python-sdk/labels/help%20wanted) issues is a great place to start.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-03-14 01:08:19[0m
[92mHash: 5f5341807ea6def34f3035309275acb5f5c3e687[0m
[92mFilepath: src/sagemaker/tensorflow/README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Remove duplicate content and add links to readthedocs (#694)


[0m
@@ -22,7 +22,433 @@ Documentation of the previous Legacy Mode versions: `1.4.1 <https://github.com/a
 
 Supported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``.
 
-For information about using TensorFlow with the SageMaker Python SDK, see https://sagemaker.readthedocs.io/en/stable/using_tf.html.
+Training with TensorFlow
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Training TensorFlow models using ``sagemaker.tensorflow.TensorFlow`` is a two-step process.
+First, you prepare your training script, then second, you run it on
+SageMaker Learner via the ``sagemaker.tensorflow.TensorFlow`` estimator.
+
+Preparing a Script Mode training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your TensorFlow training script must be a Python 2.7- or 3.6-compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the local path where the training job can write the model artifacts to.
+  After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the ``model_dir``
+  argument passed in your training script which is a S3 location. ``SM_MODEL_DIR`` is always set to ``/opt/ml/model``.
+* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
+* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write output artifacts to.
+  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
+  These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the TensorFlow estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+
+For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``SM_CHANNEL_TRAIN`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch_size', type=int, default=100)
+        parser.add_argument('--learning_rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model_dir', type=str)
+        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, putting your training launching code in a main guard (``if __name__=='__main__':``)
+is good practice.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your TensorFlow estimator.
+
+Adapting your local TensorFlow script
+'''''''''''''''''''''''''''''''''''''
+
+If you have a TensorFlow training script that runs outside of SageMaker please follow the directions here:
+
+1. Make sure your script can handle ``--model_dir`` as an additional command line argument. If you did not specify a
+location when the TensorFlow estimator is constructed a S3 location under the default training job bucket will be passed
+in here. Distributed training with parameter servers requires you use the ``tf.estimator.train_and_evaluate`` API and
+a S3 location is needed as the model directory during training. Here is an example:
+
+.. code:: python
+
+    estimator = tf.estimator.Estimator(model_fn=my_model_fn, model_dir=args.model_dir)
+    ...
+    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
+    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
+    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)
+
+2. Load input data from the input channels. The input channels are defined when ``fit`` is called. For example:
+
+.. code:: python
+
+    estimator.fit({'train':'s3://my-bucket/my-training-data',
+                  'eval':'s3://my-bucket/my-evaluation-data'})
+
+In your training script the channels will be stored in environment variables ``SM_CHANNEL_TRAIN`` and
+``SM_CHANNEL_EVAL``. You can add them to your argument parsing logic like this:
+
+.. code:: python
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+    parser.add_argument('--eval', type=str, default=os.environ.get('SM_CHANNEL_EVAL'))
+
+3. Export your final model to path stored in environment variable ``SM_MODEL_DIR`` which should always be
+   ``/opt/ml/model``. At end of training SageMaker will upload the model file under ``/opt/ml/model`` to
+   ``output_path``.
+
+
+Training with TensorFlow estimator
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Calling fit
+'''''''''''
+
+To use Script Mode, set at least one of these args
+
+- ``py_version='py3'``
+- ``script_mode=True``
+
+Please note that when using Script Mode, your training script need to accept the following args:
+
+- ``model_dir``
+
+Please note that the following args are not permitted when using Script Mode:
+
+- ``checkpoint_path``
+- ``training_steps``
+- ``evaluation_steps``
+- ``requirements_file``
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.12', py_version='py3')
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The
+constructor keyword arguments define how SageMaker runs your training
+script which we discussed earlier.
+
+You start your training script by calling ``fit`` on a ``TensorFlow`` estimator. ``fit`` takes
+both required and optional arguments.
+
+Required argument
+"""""""""""""""""
+
+- ``inputs``: The S3 location(s) of datasets to be used for training. This can take one of two forms:
+
+  - ``str``: An S3 URI, for example ``s3://my-bucket/my-training-data``, which indicates the dataset's location.
+  - ``dict[str, str]``: A dictionary mapping channel names to S3 locations, for example ``{'train': 's3://my-bucket/my-training-data/train', 'test': 's3://my-bucket/my-training-data/test'}``
+  - ``sagemaker.session.s3_input``: channel configuration for S3 data sources that can provide additional information as well as the path to the training dataset. See `the API docs <https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input>`_ for full details.
+
+Optional arguments
+""""""""""""""""""
+
+- ``wait (bool)``: Defaults to True, whether to block and wait for the
+  training script to complete before returning.
+  If set to False, it will return immediately, and can later be attached to.
+- ``logs (bool)``: Defaults to True, whether to show logs produced by training
+  job in the Python session. Only meaningful when wait is True.
+- ``run_tensorboard_locally (bool)``: Defaults to False. If set to True a Tensorboard command will be printed out.
+- ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
+  based on the training image name and current timestamp.
+
+What happens when fit is called
+"""""""""""""""""""""""""""""""
+
+Calling ``fit`` starts a SageMaker training job. The training job will execute the following.
+
+- Starts ``train_instance_count`` EC2 instances of the type ``train_instance_type``.
+- On each instance, it will do the following steps:
+
+  - starts a Docker container optimized for TensorFlow.
+  - downloads the dataset.
+  - setup up training related environment varialbes
+  - setup up distributed training environment if configured to use parameter server
+  - starts asynchronous training
+
+If the ``wait=False`` flag is passed to ``fit``, then it will return immediately. The training job will continue running
+asynchronously. At a later time, a Tensorflow Estimator can be obtained by attaching to the existing training job. If
+the training job is not finished it will start showing the standard output of training and wait until it completes.
+After attaching, the estimator can be deployed as usual.
+
+.. code:: python
+
+    tf_estimator.fit(your_input_data, wait=False)
+    training_job_name = tf_estimator.latest_training_job.name
+
+    # after some time, or in a separate Python notebook, we can attach to it again.
+
+    tf_estimator = TensorFlow.attach(training_job_name=training_job_name)
+
+Distributed Training
+''''''''''''''''''''
+
+To run your training job with multiple instances in a distributed fashion, set ``train_instance_count``
+to a number larger than 1. We support two different types of distributed training, parameter server and Horovod.
+The ``distributions`` parameter is used to configure which distributed training strategy to use.
+
+Training with parameter servers
+"""""""""""""""""""""""""""""""
+
+If you specify parameter_server as the value of the distributions parameter, the container launches a parameter server
+thread on each instance in the training cluster, and then executes your training code. You can find more information on
+TensorFlow distributed training at `TensorFlow docs <https://www.tensorflow.org/deploy/distributed>`__.
+To enable parameter server training:
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=2, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.11', py_version='py3',
+                            distributions={'parameter_server': {'enabled': True}})
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Training with Horovod
+"""""""""""""""""""""
+
+Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version ``1.12`` or newer.
+You can find more details at `Horovod README <https://github.com/uber/horovod>`__.
+
+The container sets up the MPI environment and executes the ``mpirun`` command enabling you to run any Horovod
+training script with Script Mode.
+
+Training with ``MPI`` is configured by specifying following fields in ``distributions``:
+
+- ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.
+- ``processes_per_host (int)``: Number of processes MPI should launch on each host. Note, this should not be
+  greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu
+  training.
+- ``custom_mpi_options (str)``:  Any `mpirun` flag(s) can be passed in this field that will be added to the `mpirun`
+  command executed by SageMaker to launch distributed horovod training.
+
+
+In the below example we create an estimator to launch Horovod distributed training with 2 processes on one host:
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.12', py_version='py3',
+                              distributions={
+                                  'mpi': {
+                                      'enabled': True,
+                                      'processes_per_host': 2,
+                                      'custom_mpi_options': '--NCCL_DEBUG INFO'
+                                  }
+                              })
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+sagemaker.tensorflow.TensorFlow class
+'''''''''''''''''''''''''''''''''''''
+
+The ``TensorFlow`` constructor takes both required and optional arguments.
+
+Required:
+
+- ``entry_point (str)`` Path (absolute or relative) to the Python file which
+  should be executed as the entry point to training.
+- ``role (str)`` An AWS IAM role (either name or full ARN). The Amazon
+  SageMaker training jobs and APIs that create Amazon SageMaker
+  endpoints use this role to access training data and model artifacts.
+  After the endpoint is created, the inference code might use the IAM
+  role, if accessing AWS resource.
+- ``train_instance_count (int)`` Number of Amazon EC2 instances to use for
+  training.
+- ``train_instance_type (str)`` Type of EC2 instance to use for training, for
+  example, 'ml.c4.xlarge'.
+
+Optional:
+
+- ``source_dir (str)`` Path (absolute or relative) to a directory with any
+  other training source code dependencies including the entry point
+  file. Structure within this directory will be preserved when training
+  on SageMaker.
+- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+  any additional libraries that will be exported to the container (default: ``[]``).
+  The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+  If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+  instead. Example:
+
+  The following call
+
+  >>> TensorFlow(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+  results in the following inside the container:
+
+  >>> opt/ml/code
+  >>>     ├── train.py
+  >>>     ├── common
+  >>>     └── virtual-env
+
+- ``hyperparameters (dict[str, ANY])`` Hyperparameters that will be used for training.
+  Will be made accessible as command line arguments.
+- ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
+  input data during training. Must be large enough to the store training
+  data.
+- ``train_max_run (int)`` Timeout in seconds for training, after which Amazon
+  SageMaker terminates the job regardless of its current status.
+- ``output_path (str)`` S3 location where you want the training result (model
+  artifacts and optional output files) saved. If not specified, results
+  are stored to a default bucket. If the bucket with the specific name
+  does not exist, the estimator creates the bucket during the ``fit``
+  method execution.
+- ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+  output with.
+- ``base_job_name`` Name to assign for the training job that the ``fit``
+  method launches. If not specified, the estimator generates a default
+  job name, based on the training image name and current timestamp.
+- ``image_name`` An alternative docker image to use for training and
+  serving.  If specified, the estimator will use this image for training and
+  hosting, instead of selecting the appropriate SageMaker official image based on
+  ``framework_version`` and ``py_version``. Refer to: `SageMaker TensorFlow Docker Containers
+  <#sagemaker-tensorflow-docker-containers>`_ for details on what the official images support
+  and where to find the source code to build your custom image.
+- ``script_mode (bool)`` Whether to use Script Mode or not. Script mode is the only available training mode in Python 3,
+  setting ``py_version`` to ``py3`` automatically sets ``script_mode`` to True.
+- ``model_dir (str)`` Location where model data, checkpoint data, and TensorBoard checkpoints should be saved during training.
+  If not specified a S3 location will be generated under the training job's default bucket. And ``model_dir`` will be
+  passed in your training script as one of the command line arguments.
+- ``distributions (dict)`` Configure your distribution strategy with this argument.
+
+Training with Pipe Mode using PipeModeDataset
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Amazon SageMaker allows users to create training jobs using Pipe input mode.
+With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first.
+This means that your training jobs start sooner, finish quicker, and need less disk space.
+
+SageMaker TensorFlow provides an implementation of ``tf.data.Dataset`` that makes it easy to take advantage of Pipe
+input mode in SageMaker. You can replace your ``tf.data.Dataset`` with a ``sagemaker_tensorflow.PipeModeDataset`` to
+read TFRecords as they are streamed to your training instances.
+
+In your ``entry_point`` script, you can use ``PipeModeDataset`` like a ``Dataset``. In this example, we create a
+``PipeModeDataset`` to read TFRecords from the 'training' channel:
+
+
+.. code:: python
+
+    from sagemaker_tensorflow import PipeModeDataset
+
+    features = {
+        'data': tf.FixedLenFeature([], tf.string),
+        'labels': tf.FixedLenFeature([], tf.int64),
+    }
+
+    def parse(record):
+        parsed = tf.parse_single_example(record, features)
+        return ({
+            'data': tf.decode_raw(parsed['data'], tf.float64)
+        }, parsed['labels'])
+
+    def train_input_fn(training_dir, hyperparameters):
+        ds = PipeModeDataset(channel='training', record_format='TFRecord')
+        ds = ds.repeat(20)
+        ds = ds.prefetch(10)
+        ds = ds.map(parse, num_parallel_calls=10)
+        ds = ds.batch(64)
+        return ds
+
+
+To run training job with Pipe input mode, pass in ``input_mode='Pipe'`` to your TensorFlow Estimator:
+
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train-with-pipemodedataset.py', role='SageMakerRole',
+                              training_steps=10000, evaluation_steps=100,
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.10.0', input_mode='Pipe')
+
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+
+If your TFRecords are compressed, you can train on Gzipped TF Records by passing in ``compression='Gzip'`` to the call to
+``fit()``, and SageMaker will automatically unzip the records as data is streamed to your training instances:
+
+.. code:: python
+
+    from sagemaker.session import s3_input
+
+    train_s3_input = s3_input('s3://bucket/path/to/training/data', compression='Gzip')
+    tf_estimator.fit(train_s3_input)
+
+
+You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
+
+
+Training with MKL-DNN disabled
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
+
+In certain cases you might be able to get a better performance by disabling this optimization
+(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
+
+You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
+
+.. code:: python
+
+    import os
+
+    os.environ['TF_DISABLE_MKL'] = '1'
+    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
+
+
+Deploying TensorFlow Serving models
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After a TensorFlow estimator has been fit, it saves a TensorFlow SavedModel in
+the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
+estimator to create a SageMaker Endpoint.
+
+SageMaker provides two different options for deploying TensorFlow models to a SageMaker
+Endpoint:
+
+- The first option uses a Python-based server that allows you to specify your own custom
+  input and output handling functions in a Python script. This is the default option.
+
+  See `Deploying to Python-based Endpoints <deploying_python.rst>`_ to learn how to use this option.
+
+
+- The second option uses a TensorFlow Serving-based server to provide a super-set of the
+  `TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_. This option
+  does not require (or allow) a custom python script.
+
+  See `Deploying to TensorFlow Serving Endpoints <deploying_tensorflow_serving.rst>`_ to learn how to use this option.
+
 
 SageMaker TensorFlow Docker containers
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-03-10 15:04:40[0m
[92mHash: 129bdb37266ebd9a9135327e5ad135abebd1e6c7[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/master[0m
[92mCommit: Add region account map to marketplace test for canary (#688)

[0m
@@ -25,7 +25,6 @@ from sagemaker.tuner import IntegerParameter, HyperparameterTuner
 from sagemaker.utils import sagemaker_timestamp
 from tests.integ import DATA_DIR
 from tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name
-from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
 
 
 # All these tests require a manual 1 time subscription to the following Marketplace items:
@@ -37,10 +36,10 @@ from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
 #
 # Both are  written by Amazon and are free to subscribe.
 
-ALGORITHM_ARN = 'arn:aws:sagemaker:%s:%s:algorithm/scikit-decision-trees-' \
+ALGORITHM_ARN = 'arn:aws:sagemaker:%s:594846645681:algorithm/scikit-decision-trees-' \
                 '15423055-[93m57b73412d2e93e9239e4e16f83298b8f[0m'
 
-MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-' \
+MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:594846645681:model-package/scikit-iris-detector-' \
                     '154230595-[93m8f00905c1f927a512b73ea29dd09ae30[0m'
 
 
@@ -48,12 +47,9 @@ MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-
 def test_marketplace_estimator(sagemaker_session):
     with timeout(minutes=15):
         data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-        region = sagemaker_session.boto_region_name
-        account = REGION_ACCOUNT_MAP[region]
-        algorithm_arn = ALGORITHM_ARN % (region, account)
 
         algo = AlgorithmEstimator(
-            algorithm_arn=algorithm_arn,
+            algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
             role='SageMakerRole',
             train_instance_count=1,
             train_instance_type='ml.c4.xlarge',
@@ -82,12 +78,9 @@ def test_marketplace_estimator(sagemaker_session):
 def test_marketplace_attach(sagemaker_session):
     with timeout(minutes=15):
         data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-        region = sagemaker_session.boto_region_name
-        account = REGION_ACCOUNT_MAP[region]
-        algorithm_arn = ALGORITHM_ARN % (region, account)
 
         mktplace = AlgorithmEstimator(
-            algorithm_arn=algorithm_arn,
+            algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
             role='SageMakerRole',
             train_instance_count=1,
             train_instance_type='ml.c4.xlarge',
@@ -123,9 +116,6 @@ def test_marketplace_attach(sagemaker_session):
 
 @pytest.mark.canary_quick
 def test_marketplace_model(sagemaker_session):
-    region = sagemaker_session.boto_region_name
-    account = REGION_ACCOUNT_MAP[region]
-    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
 
     def predict_wrapper(endpoint, session):
         return sagemaker.RealTimePredictor(
@@ -133,7 +123,7 @@ def test_marketplace_model(sagemaker_session):
         )
 
     model = ModelPackage(role='SageMakerRole',
-                         model_package_arn=model_package_arn,
+                         model_package_arn=(MODEL_PACKAGE_ARN % sagemaker_session.boto_region_name),
                          sagemaker_session=sagemaker_session,
                          predictor_cls=predict_wrapper)
 
@@ -154,12 +144,9 @@ def test_marketplace_model(sagemaker_session):
 
 def test_marketplace_tuning_job(sagemaker_session):
     data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-    region = sagemaker_session.boto_region_name
-    account = REGION_ACCOUNT_MAP[region]
-    algorithm_arn = ALGORITHM_ARN % (region, account)
 
     mktplace = AlgorithmEstimator(
-        algorithm_arn=algorithm_arn,
+        algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
         role='SageMakerRole',
         train_instance_count=1,
         train_instance_type='ml.c4.xlarge',
@@ -185,12 +172,9 @@ def test_marketplace_tuning_job(sagemaker_session):
 
 def test_marketplace_transform_job(sagemaker_session):
     data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-    region = sagemaker_session.boto_region_name
-    account = REGION_ACCOUNT_MAP[region]
-    algorithm_arn = ALGORITHM_ARN % (region, account)
 
     algo = AlgorithmEstimator(
-        algorithm_arn=algorithm_arn,
+        algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
         role='SageMakerRole',
         train_instance_count=1,
         train_instance_type='ml.c4.xlarge',
@@ -225,12 +209,8 @@ def test_marketplace_transform_job_from_model_package(sagemaker_session):
         TRANSFORM_WORKDIR,
         key_prefix='integ-test-data/marketplace/transform')
 
-    region = sagemaker_session.boto_region_name
-    account = REGION_ACCOUNT_MAP[region]
-    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
-
     model = ModelPackage(role='SageMakerRole',
-                         model_package_arn=model_package_arn,
+                         model_package_arn=(MODEL_PACKAGE_ARN % sagemaker_session.boto_region_name),
                          sagemaker_session=sagemaker_session)
 
     transformer = model.transformer(1, 'ml.m4.xlarge')

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-03-05 20:57:32[0m
[92mHash: 3816a5658d3737c9767e01bc8d37fc3ed5551593[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/master[0m
[92mCommit: Classify canary_quick in integ tests (#677)

[0m
@@ -17,7 +17,6 @@ import os
 import time
 
 import pandas
-import pytest
 
 import sagemaker
 from sagemaker import AlgorithmEstimator, ModelPackage
@@ -43,7 +42,6 @@ MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:594846645681:model-package/scikit-iris
                     '154230595-[93m8f00905c1f927a512b73ea29dd09ae30[0m'
 
 
-@pytest.mark.canary_quick
 def test_marketplace_estimator(sagemaker_session):
     with timeout(minutes=15):
         data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
@@ -114,7 +112,6 @@ def test_marketplace_attach(sagemaker_session):
         print(predictor.predict(test_x.values).decode('utf-8'))
 
 
-@pytest.mark.canary_quick
 def test_marketplace_model(sagemaker_session):
 
     def predict_wrapper(endpoint, session):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-03-01 12:35:09[0m
[92mHash: 0a7a532401906c23bb8d7ee06dd46c7383feb939[0m
[92mFilepath: doc/using_tf.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Migrate tf using content into sphynx project (#673)


[0m
@@ -1,510 +0,0 @@
-==============================================
-Using TensorFlow with the SageMaker Python SDK
-==============================================
-
-TensorFlow SageMaker Estimators allow you to run your own TensorFlow
-training algorithms on SageMaker Learner, and to host your own TensorFlow
-models on SageMaker Hosting.
-
-**Note:** This topic describes how to use script mode for TensorFlow versions 1.11 and later.
-For Documentation of the previous Legacy Mode versions, see:
-* `1.4.1 <https://github.com/aws/sagemaker-python-sdk/tree/v1.0.0#tensorflow-sagemaker-estimators>`_
-* `1.5.0 <https://github.com/aws/sagemaker-python-sdk/tree/v1.1.0#tensorflow-sagemaker-estimators>`_
-* `1.6.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
-* `1.7.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
-* `1.8.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
-* `1.9.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.9.2/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
-* `1.10.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.10.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
-
-+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
-| WARNING                                                                                                                                                                     |
-+=============================================================================================================================================================================+
-| We have added a new format of your TensorFlow training script with TensorFlow version 1.11.                                                                                 |
-| This new way gives the user script more flexibility.                                                                                                                        |
-| This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions.                                         |
-| In addition we are adding Python 3 support with Script Mode.                                                                                                                |
-| Last supported version of Legacy Mode will be TensorFlow 1.12.                                                                                                              |
-| Script Mode is available with TensorFlow version 1.11 and newer.                                                                                                            |
-| Make sure you refer to the correct version of this README when you prepare your script.                                                                                     |
-| You can find the Legacy Mode README `here <https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#tensorflow-sagemaker-estimators-and-models>`_. |
-+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
-
-.. contents::
-
-Supported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``.
-
-Training with TensorFlow
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-Training TensorFlow models using ``sagemaker.tensorflow.TensorFlow`` is a two-step process.
-First, you prepare your training script, then second, you run it on
-SageMaker Learner via the ``sagemaker.tensorflow.TensorFlow`` estimator.
-
-Preparing a Script Mode training script
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Your TensorFlow training script must be a Python 2.7- or 3.6-compatible source file.
-
-The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
-
-* ``SM_MODEL_DIR``: A string that represents the local path where the training job can write the model artifacts to.
-  After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the ``model_dir``
-  argument passed in your training script which is a S3 location. ``SM_MODEL_DIR`` is always set to ``/opt/ml/model``.
-* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
-* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write output artifacts to.
-  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
-  These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.
-* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
-  For example, if you specify two input channels in the TensorFlow estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
-
-For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
-
-A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``SM_CHANNEL_TRAIN`` so that it can be deployed for inference later.
-Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
-For example, a training script might start with the following:
-
-.. code:: python
-
-    import argparse
-    import os
-
-    if __name__ =='__main__':
-
-        parser = argparse.ArgumentParser()
-
-        # hyperparameters sent by the client are passed as command-line arguments to the script.
-        parser.add_argument('--epochs', type=int, default=10)
-        parser.add_argument('--batch_size', type=int, default=100)
-        parser.add_argument('--learning_rate', type=float, default=0.1)
-
-        # input data and model directories
-        parser.add_argument('--model_dir', type=str)
-        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
-        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
-
-        args, _ = parser.parse_known_args()
-
-        # ... load from args.train and args.test, train a model, write model to args.model_dir.
-
-Because the SageMaker imports your training script, putting your training launching code in a main guard (``if __name__=='__main__':``)
-is good practice.
-
-Note that SageMaker doesn't support argparse actions.
-If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your TensorFlow estimator.
-
-Adapting your local TensorFlow script
-'''''''''''''''''''''''''''''''''''''
-
-If you have a TensorFlow training script that runs outside of SageMaker please follow the directions here:
-
-1. Make sure your script can handle ``--model_dir`` as an additional command line argument. If you did not specify a
-location when the TensorFlow estimator is constructed a S3 location under the default training job bucket will be passed
-in here. Distributed training with parameter servers requires you use the ``tf.estimator.train_and_evaluate`` API and
-a S3 location is needed as the model directory during training. Here is an example:
-
-.. code:: python
-
-    estimator = tf.estimator.Estimator(model_fn=my_model_fn, model_dir=args.model_dir)
-    ...
-    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
-    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
-    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)
-
-2. Load input data from the input channels. The input channels are defined when ``fit`` is called. For example:
-
-.. code:: python
-
-    estimator.fit({'train':'s3://my-bucket/my-training-data',
-                  'eval':'s3://my-bucket/my-evaluation-data'})
-
-In your training script the channels will be stored in environment variables ``SM_CHANNEL_TRAIN`` and
-``SM_CHANNEL_EVAL``. You can add them to your argument parsing logic like this:
-
-.. code:: python
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
-    parser.add_argument('--eval', type=str, default=os.environ.get('SM_CHANNEL_EVAL'))
-
-3. Export your final model to path stored in environment variable ``SM_MODEL_DIR`` which should always be
-   ``/opt/ml/model``. At end of training SageMaker will upload the model file under ``/opt/ml/model`` to
-   ``output_path``.
-
-
-Training with TensorFlow estimator
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Calling fit
-'''''''''''
-
-To use Script Mode, set at least one of these args
-
-- ``py_version='py3'``
-- ``script_mode=True``
-
-Please note that when using Script Mode, your training script need to accept the following args:
-
-- ``model_dir``
-
-Please note that the following args are not permitted when using Script Mode:
-
-- ``checkpoint_path``
-- ``training_steps``
-- ``evaluation_steps``
-- ``requirements_file``
-
-.. code:: python
-
-  from sagemaker.tensorflow import TensorFlow
-
-  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                            train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                            framework_version='1.12', py_version='py3')
-  tf_estimator.fit('s3://bucket/path/to/training/data')
-
-Where the S3 url is a path to your training data, within Amazon S3. The
-constructor keyword arguments define how SageMaker runs your training
-script which we discussed earlier.
-
-You start your training script by calling ``fit`` on a ``TensorFlow`` estimator. ``fit`` takes
-both required and optional arguments.
-
-Required argument
-"""""""""""""""""
-
-- ``inputs``: The S3 location(s) of datasets to be used for training. This can take one of two forms:
-
-  - ``str``: An S3 URI, for example ``s3://my-bucket/my-training-data``, which indicates the dataset's location.
-  - ``dict[str, str]``: A dictionary mapping channel names to S3 locations, for example ``{'train': 's3://my-bucket/my-training-data/train', 'test': 's3://my-bucket/my-training-data/test'}``
-  - ``sagemaker.session.s3_input``: channel configuration for S3 data sources that can provide additional information as well as the path to the training dataset. See `the API docs <https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input>`_ for full details.
-
-Optional arguments
-""""""""""""""""""
-
-- ``wait (bool)``: Defaults to True, whether to block and wait for the
-  training script to complete before returning.
-  If set to False, it will return immediately, and can later be attached to.
-- ``logs (bool)``: Defaults to True, whether to show logs produced by training
-  job in the Python session. Only meaningful when wait is True.
-- ``run_tensorboard_locally (bool)``: Defaults to False. If set to True a Tensorboard command will be printed out.
-- ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
-  based on the training image name and current timestamp.
-
-What happens when fit is called
-"""""""""""""""""""""""""""""""
-
-Calling ``fit`` starts a SageMaker training job. The training job will execute the following.
-
-- Starts ``train_instance_count`` EC2 instances of the type ``train_instance_type``.
-- On each instance, it will do the following steps:
-
-  - starts a Docker container optimized for TensorFlow.
-  - downloads the dataset.
-  - setup up training related environment varialbes
-  - setup up distributed training environment if configured to use parameter server
-  - starts asynchronous training
-
-If the ``wait=False`` flag is passed to ``fit``, then it will return immediately. The training job will continue running
-asynchronously. At a later time, a Tensorflow Estimator can be obtained by attaching to the existing training job. If
-the training job is not finished it will start showing the standard output of training and wait until it completes.
-After attaching, the estimator can be deployed as usual.
-
-.. code:: python
-
-    tf_estimator.fit(your_input_data, wait=False)
-    training_job_name = tf_estimator.latest_training_job.name
-
-    # after some time, or in a separate Python notebook, we can attach to it again.
-
-    tf_estimator = TensorFlow.attach(training_job_name=training_job_name)
-
-Distributed Training
-''''''''''''''''''''
-
-To run your training job with multiple instances in a distributed fashion, set ``train_instance_count``
-to a number larger than 1. We support two different types of distributed training, parameter server and Horovod.
-The ``distributions`` parameter is used to configure which distributed training strategy to use.
-
-Training with parameter servers
-"""""""""""""""""""""""""""""""
-
-If you specify parameter_server as the value of the distributions parameter, the container launches a parameter server
-thread on each instance in the training cluster, and then executes your training code. You can find more information on
-TensorFlow distributed training at `TensorFlow docs <https://www.tensorflow.org/deploy/distributed>`__.
-To enable parameter server training:
-
-.. code:: python
-
-  from sagemaker.tensorflow import TensorFlow
-
-  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                            train_instance_count=2, train_instance_type='ml.p2.xlarge',
-                            framework_version='1.11', py_version='py3',
-                            distributions={'parameter_server': {'enabled': True}})
-  tf_estimator.fit('s3://bucket/path/to/training/data')
-
-Training with Horovod
-"""""""""""""""""""""
-
-Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version ``1.12`` or newer.
-You can find more details at `Horovod README <https://github.com/uber/horovod>`__.
-
-The container sets up the MPI environment and executes the ``mpirun`` command enabling you to run any Horovod
-training script with Script Mode.
-
-Training with ``MPI`` is configured by specifying following fields in ``distributions``:
-
-- ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.
-- ``processes_per_host (int)``: Number of processes MPI should launch on each host. Note, this should not be
-  greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu
-  training.
-- ``custom_mpi_options (str)``:  Any `mpirun` flag(s) can be passed in this field that will be added to the `mpirun`
-  command executed by SageMaker to launch distributed horovod training.
-
-
-In the below example we create an estimator to launch Horovod distributed training with 2 processes on one host:
-
-.. code:: python
-
-    from sagemaker.tensorflow import TensorFlow
-
-    tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
-                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                              framework_version='1.12', py_version='py3',
-                              distributions={
-                                  'mpi': {
-                                      'enabled': True,
-                                      'processes_per_host': 2,
-                                      'custom_mpi_options': '--NCCL_DEBUG INFO'
-                                  }
-                              })
-    tf_estimator.fit('s3://bucket/path/to/training/data')
-
-sagemaker.tensorflow.TensorFlow class
-'''''''''''''''''''''''''''''''''''''
-
-The ``TensorFlow`` constructor takes both required and optional arguments.
-
-Required:
-
-- ``entry_point (str)`` Path (absolute or relative) to the Python file which
-  should be executed as the entry point to training.
-- ``role (str)`` An AWS IAM role (either name or full ARN). The Amazon
-  SageMaker training jobs and APIs that create Amazon SageMaker
-  endpoints use this role to access training data and model artifacts.
-  After the endpoint is created, the inference code might use the IAM
-  role, if accessing AWS resource.
-- ``train_instance_count (int)`` Number of Amazon EC2 instances to use for
-  training.
-- ``train_instance_type (str)`` Type of EC2 instance to use for training, for
-  example, 'ml.c4.xlarge'.
-
-Optional:
-
-- ``source_dir (str)`` Path (absolute or relative) to a directory with any
-  other training source code dependencies including the entry point
-  file. Structure within this directory will be preserved when training
-  on SageMaker.
-- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
-  any additional libraries that will be exported to the container (default: ``[]``).
-  The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
-  If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
-  instead. Example:
-
-  The following call
-
-  >>> TensorFlow(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
-
-  results in the following inside the container:
-
-  >>> opt/ml/code
-  >>>     ├── train.py
-  >>>     ├── common
-  >>>     └── virtual-env
-
-- ``hyperparameters (dict[str, ANY])`` Hyperparameters that will be used for training.
-  Will be made accessible as command line arguments.
-- ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
-  input data during training. Must be large enough to the store training
-  data.
-- ``train_max_run (int)`` Timeout in seconds for training, after which Amazon
-  SageMaker terminates the job regardless of its current status.
-- ``output_path (str)`` S3 location where you want the training result (model
-  artifacts and optional output files) saved. If not specified, results
-  are stored to a default bucket. If the bucket with the specific name
-  does not exist, the estimator creates the bucket during the ``fit``
-  method execution.
-- ``output_kms_key`` Optional KMS key ID to optionally encrypt training
-  output with.
-- ``base_job_name`` Name to assign for the training job that the ``fit``
-  method launches. If not specified, the estimator generates a default
-  job name, based on the training image name and current timestamp.
-- ``image_name`` An alternative docker image to use for training and
-  serving.  If specified, the estimator will use this image for training and
-  hosting, instead of selecting the appropriate SageMaker official image based on
-  ``framework_version`` and ``py_version``. Refer to: `SageMaker TensorFlow Docker Containers
-  <#sagemaker-tensorflow-docker-containers>`_ for details on what the official images support
-  and where to find the source code to build your custom image.
-- ``script_mode (bool)`` Whether to use Script Mode or not. Script mode is the only available training mode in Python 3,
-  setting ``py_version`` to ``py3`` automatically sets ``script_mode`` to True.
-- ``model_dir (str)`` Location where model data, checkpoint data, and TensorBoard checkpoints should be saved during training.
-  If not specified a S3 location will be generated under the training job's default bucket. And ``model_dir`` will be
-  passed in your training script as one of the command line arguments.
-- ``distributions (dict)`` Configure your distribution strategy with this argument.
-
-Training with Pipe Mode using PipeModeDataset
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Amazon SageMaker allows users to create training jobs using Pipe input mode.
-With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first.
-This means that your training jobs start sooner, finish quicker, and need less disk space.
-
-SageMaker TensorFlow provides an implementation of ``tf.data.Dataset`` that makes it easy to take advantage of Pipe
-input mode in SageMaker. You can replace your ``tf.data.Dataset`` with a ``sagemaker_tensorflow.PipeModeDataset`` to
-read TFRecords as they are streamed to your training instances.
-
-In your ``entry_point`` script, you can use ``PipeModeDataset`` like a ``Dataset``. In this example, we create a
-``PipeModeDataset`` to read TFRecords from the 'training' channel:
-
-
-.. code:: python
-
-    from sagemaker_tensorflow import PipeModeDataset
-
-    features = {
-        'data': tf.FixedLenFeature([], tf.string),
-        'labels': tf.FixedLenFeature([], tf.int64),
-    }
-
-    def parse(record):
-        parsed = tf.parse_single_example(record, features)
-        return ({
-            'data': tf.decode_raw(parsed['data'], tf.float64)
-        }, parsed['labels'])
-
-    def train_input_fn(training_dir, hyperparameters):
-        ds = PipeModeDataset(channel='training', record_format='TFRecord')
-        ds = ds.repeat(20)
-        ds = ds.prefetch(10)
-        ds = ds.map(parse, num_parallel_calls=10)
-        ds = ds.batch(64)
-        return ds
-
-
-To run training job with Pipe input mode, pass in ``input_mode='Pipe'`` to your TensorFlow Estimator:
-
-
-.. code:: python
-
-    from sagemaker.tensorflow import TensorFlow
-
-    tf_estimator = TensorFlow(entry_point='tf-train-with-pipemodedataset.py', role='SageMakerRole',
-                              training_steps=10000, evaluation_steps=100,
-                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
-                              framework_version='1.10.0', input_mode='Pipe')
-
-    tf_estimator.fit('s3://bucket/path/to/training/data')
-
-
-If your TFRecords are compressed, you can train on Gzipped TF Records by passing in ``compression='Gzip'`` to the call to
-``fit()``, and SageMaker will automatically unzip the records as data is streamed to your training instances:
-
-.. code:: python
-
-    from sagemaker.session import s3_input
-
-    train_s3_input = s3_input('s3://bucket/path/to/training/data', compression='Gzip')
-    tf_estimator.fit(train_s3_input)
-
-
-You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
-
-
-Training with MKL-DNN disabled
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
-
-In certain cases you might be able to get a better performance by disabling this optimization
-(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
-
-You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
-
-.. code:: python
-
-    import os
-
-    os.environ['TF_DISABLE_MKL'] = '1'
-    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
-
-
-Deploying TensorFlow Serving models
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After a TensorFlow estimator has been fit, it saves a TensorFlow SavedModel in
-the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
-estimator to create a SageMaker Endpoint.
-
-SageMaker provides two different options for deploying TensorFlow models to a SageMaker
-Endpoint:
-
-- The first option uses a Python-based server that allows you to specify your own custom
-  input and output handling functions in a Python script. This is the default option.
-
-  See `Deploying to Python-based Endpoints <deploying_python.rst>`_ to learn how to use this option.
-
-
-- The second option uses a TensorFlow Serving-based server to provide a super-set of the
-  `TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_. This option
-  does not require (or allow) a custom python script.
-
-  See `Deploying to TensorFlow Serving Endpoints <deploying_tensorflow_serving.rst>`_ to learn how to use this option.
-
-
-SageMaker TensorFlow Docker containers
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The containers include the following Python packages:
-
-+--------------------------------+---------------+-------------------+
-| Dependencies                   | Script Mode   | Legacy Mode       |
-+--------------------------------+---------------+-------------------+
-| boto3                          | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| botocore                       | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| CUDA (GPU image only)          | 9.0           | 9.0               |
-+--------------------------------+---------------+-------------------+
-| numpy                          | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| Pillow                         | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| scipy                          | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| sklean                         | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| h5py                           | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| pip                            | 18.1          | 18.1              |
-+--------------------------------+---------------+-------------------+
-| curl                           | Latest        | Latest            |
-+--------------------------------+---------------+-------------------+
-| tensorflow                     | 1.12.0        | 1.12.0            |
-+--------------------------------+---------------+-------------------+
-| tensorflow-serving-api         | 1.12.0        | None              |
-+--------------------------------+---------------+-------------------+
-| sagemaker-containers           | >=2.3.5       | >=2.3.5           |
-+--------------------------------+---------------+-------------------+
-| sagemaker-tensorflow-container | 1.0           | 1.0               |
-+--------------------------------+---------------+-------------------+
-| Python                         | 2.7 or 3.6    | 2.7               |
-+--------------------------------+---------------+-------------------+
-
-Legacy Mode TensorFlow Docker images support Python 2.7. Script Mode TensorFlow Docker images support both Python 2.7
-and Python 3.6. The Docker images extend Ubuntu 16.04.
-
-You can select version of TensorFlow by passing a ``framework_version`` keyword arg to the TensorFlow Estimator constructor. Currently supported versions are listed in the table above. You can also set ``framework_version`` to only specify major and minor version, e.g ``'1.6'``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.6.0.
-Alternatively, you can build your own image by following the instructions in the SageMaker TensorFlow containers
-repository, and passing ``image_name`` to the TensorFlow Estimator constructor.
-
-For more information on the contents of the images, see the SageMaker TensorFlow containers repository here: https://github.com/aws/sagemaker-tensorflow-containers/
\ No newline at end of file

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-02-26 11:04:33[0m
[92mHash: 15cc58f18e8a88ddea6b3a7df55551252cca7b9a[0m
[92mFilepath: src/sagemaker/mxnet/README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Remove duplicate content from mxnet/README.rst (#660)

This change removes duplicate content from /src/mxnet/README.rst and
adds a link to ReadTheDocs content.
[0m
@@ -8,7 +8,792 @@ Supported versions of MXNet: ``1.3.0``, ``1.2.1``, ``1.1.0``, ``1.0.0``, ``0.12.
 
 Supported versions of MXNet for Elastic Inference: ``1.3.0``.
 
-For information about using MXNet with the SageMaker Python SDK, see https://sagemaker.readthedocs.io/en/stable/using_mxnet.html.
++-----------------------+
+| **Table of Contents** |
++-----------------------+
+
+.. contents::
+  :local:
+  :depth: 3
+
+
+Training with MXNet
+-------------------
+
+Training MXNet models using ``MXNet`` Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an ``MXNet`` Estimator. You should prepare your script in a separate source file than the notebook, terminal session, or source file you're using to submit the script to SageMaker via an ``MXNet`` Estimator.
+
+Suppose that you already have an MXNet training script called
+``mxnet-train.py``. You can run this script in SageMaker as follows:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+    mxnet_estimator = MXNet('mxnet-train.py',
+                            role='SageMakerRole',
+                            train_instance_type='ml.p3.2xlarge',
+                            train_instance_count=1,
+                            framework_version='1.3.0')
+    mxnet_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The constructor keyword arguments define how SageMaker runs your training script and are discussed, in detail, in a later section.
+
+In the following sections, we'll discuss how to prepare a training script for execution on SageMaker, then how to run that script on SageMaker using an ``MXNet`` Estimator.
+
+Preparing the MXNet training script
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+| WARNING                                                                                                                                                  |
++==========================================================================================================================================================+
+| The structure for training scripts changed with MXNet version 1.3.                                                                                       |
+| Make sure you refer to the correct section of this README when you prepare your script.                                                                  |
+| For information on how to upgrade an old script to the new format, see `"Updating your MXNet training script" <#updating-your-mxnet-training-script>`__. |
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+
+For versions 1.3 and higher
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the path where the training job writes the model artifacts to.
+  After training, artifacts in this directory are uploaded to S3 for model hosting.
+* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the MXNet estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+* ``SM_HPS``: A json dump of the hyperparameters preserving json types (boolean, integer, etc.)
+
+For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``model_dir`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+    import json
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # an alternative way to load hyperparameters via SM_HPS environment variable.
+        parser.add_argument('--sm-hps', type=json.loads, default=os.environ['SM_HPS'])
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
+so that SageMaker does not inadvertently run your training code at the wrong point in execution.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your MXNet estimator.
+
+For more on training environment variables, please visit `SageMaker Containers <https://github.com/aws/sagemaker-containers>`_.
+
+For versions 1.2 and lower
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file. The MXNet training script must contain a function ``train``, which SageMaker invokes to run training. You can include other functions as well, but it must contain a ``train`` function.
+
+When you run your script on SageMaker via the ``MXNet`` Estimator, SageMaker injects information about the training environment into your training function via Python keyword arguments. You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
+
+-  ``hyperparameters (dict[string,string])``: The hyperparameters passed
+   to SageMaker TrainingJob that runs your MXNet training script. You
+   can use this to pass hyperparameters to your training script.
+-  ``input_data_config (dict[string,dict])``: The SageMaker TrainingJob
+   InputDataConfig object, that's set when the SageMaker TrainingJob is
+   created. This is discussed in more detail below.
+-  ``channel_input_dirs (dict[string,string])``: A collection of
+   directories containing training data. When you run training, you can
+   partition your training data into different logical "channels".
+   Depending on your problem, some common channel ideas are: "train",
+   "test", "evaluation" or "images',"labels".
+-  ``output_data_dir (str)``: A directory where your training script can
+   write data that will be moved to S3 after training is complete.
+-  ``num_gpus (int)``: The number of GPU devices available on your
+   training instance.
+-  ``num_cpus (int)``: The number of CPU devices available on your training instance.
+-  ``hosts (list[str])``: The list of host names running in the
+   SageMaker Training Job cluster.
+-  ``current_host (str)``: The name of the host executing the script.
+   When you use SageMaker for MXNet training, the script is run on each
+   host in the cluster.
+
+A training script that takes advantage of all arguments would have the following definition:
+
+.. code:: python
+
+    def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,
+              num_gpus, num_cpus, hosts, current_host):
+        pass
+
+You don't have to use all the arguments, arguments you don't care about can be ignored by including ``**kwargs``.
+
+.. code:: python
+
+    # Only work with hyperparameters and num_gpus, ignore all other hyperparameters
+    def train(hyperparameters, num_gpus, **kwargs):
+        pass
+
+**Note: Writing a training script that imports correctly**
+When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
+
+If you want to run your training script locally via the Python interpreter, look at using a ``___name__ == '__main__'`` guard, discussed in more detail here: https://stackoverflow.com/questions/419163/what-does-if-name-main-do .
+
+Distributed training
+''''''''''''''''''''
+
+When writing a distributed training script, you will want to use an MXNet kvstore to store and share model parameters.
+During training, SageMaker automatically starts an MXNet kvstore server and scheduler processes on hosts in your training job cluster.
+Your script runs as an MXNet worker task, with one server process on each host in your cluster.
+One host is selected arbitrarily to run the scheduler process.
+
+To learn more about writing distributed MXNet programs, please see `Distributed Training <http://newdocs.readthedocs.io/en/latest/distributed_training.html>`__ in the MXNet docs.
+
+Saving models
+'''''''''''''
+
+Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script. If your script includes a ``save`` function, SageMaker will invoke it with the return-value of ``train``. Model saving is a two-step process, firstly you return the model you want to save from
+``train``, then you define your model-serialization logic in ``save``.
+
+SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects. If your training script does not define a ``save`` function, then the default ``save`` function will be invoked on the return-value of your ``train`` function.
+
+The default serialization system generates three files:
+
+-  ``model-shapes.json``: A json list, containing a serialization of the
+   ``Module`` ``data_shapes`` property. Each object in the list contains
+   the serialization of one ``DataShape`` in the returned ``Module``.
+   Each object has a ``name`` property, containing the ``DataShape``
+   name and a ``shape`` property, which is a list of that dimensions for
+   the shape of that ``DataShape``. For example:
+
+.. code:: javascript
+
+    [
+        {"name":"images", "shape":[100, 1, 28, 28]},
+        {"name":"labels", "shape":[100, 1]}
+    ]
+
+-  ``model-symbol.json``: The MXNet ``Module`` ``Symbol`` serialization,
+   produced by invoking ``save`` on the ``symbol`` property of the
+   ``Module`` being saved.
+-  ``modle.params``: The MXNet ``Module`` parameters. Produced by
+   invoking ``save_params`` on the ``Module`` being saved.
+
+You can provide your own save function. This is useful if you are not working with the ``Module`` API or you need special processing.
+
+To provide your own save function, define a ``save`` function in your training script:
+
+.. code:: python
+
+    def save(model, model_dir):
+        pass
+
+The function should take two arguments:
+
+-  ``model``: This is the object that was returned from your ``train``
+   function. If your ``train`` function does not return an object, it
+   will be ``None``. You are free to return an object of any type from
+   ``train``, you do not have to return ``Module`` or ``Gluon`` API
+   specific objects.
+-  ``model_dir``: This is the string path on the SageMaker training host
+   where you save your model. Files created in this directory will be
+   accessible in S3 after your SageMaker Training Job completes.
+
+After your ``train`` function completes, SageMaker will invoke ``save`` with the object returned from ``train``.
+
+**Note: How to save Gluon models with SageMaker**
+
+If your train function returns a Gluon API ``net`` object as its model, you'll need to write your own ``save`` function. You will want to serialize the ``net`` parameters. Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
+
+Updating your MXNet training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The structure for training scripts changed with MXNet version 1.3.
+The ``train`` function is no longer be required; instead the training script must be able to be run as a standalone script.
+In this way, the training script is similar to a training script you might run outside of SageMaker.
+
+There are a few steps needed to make a training script with the old format compatible with the new format.
+
+First, add a `main guard <https://docs.python.org/3/library/__main__.html>`__ (``if __name__ == '__main__':``).
+The code executed from your main guard needs to:
+
+1. Set hyperparameters and directory locations
+2. Initiate training
+3. Save the model
+
+Hyperparameters will be passed as command-line arguments to your training script.
+In addition, the container will define the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
+You can find the full list of available environment variables in the `SageMaker Containers README <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+We recommend using `an argument parser <https://docs.python.org/3.5/howto/argparse.html>`__ for this part.
+Using the ``argparse`` library as an example, the code would look something like this:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ == '__main__':
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+The code in the main guard should also take care of training and saving the model.
+This can be as simple as just calling the ``train`` and ``save`` methods used in the previous training script format:
+
+.. code:: python
+
+    if __name__ == '__main__':
+        # arg parsing (shown above) goes here
+
+        model = train(args.batch_size, args.epochs, args.learning_rate, args.train, args.test)
+        save(args.model_dir, model)
+
+Note that saving the model will no longer be done by default; this must be done by the training script.
+If you were previously relying on the default save method, you can now import one from the container:
+
+.. code:: python
+
+    from sagemaker_mxnet_container.training_utils import save
+
+    if __name__ == '__main__':
+        # arg parsing and training (shown above) goes here
+
+        save(args.model_dir, model)
+
+Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must now set ``distributions`` to the following dictionary when creating an MXNet estimator:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+
+    estimator = MXNet('path-to-distributed-training-script.py',
+                      ...,
+                      distributions={'parameter_server': {'enabled': True}})
+
+
+Using third-party libraries
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When running your training script on SageMaker, it will have access to some pre-installed third-party libraries including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``. For more information on the runtime environment, including specific package versions, see `SageMaker MXNet Containers <#sagemaker-mxnet-containers>`__.
+
+If there are other packages you want to use with your script, you can include a `requirements.txt <https://pip.pypa.io/en/stable/user_guide/#requirements-files>`__ file in the same directory as your training script to install other dependencies at runtime.
+
+Running an MXNet training script in SageMaker
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You run MXNet training scripts on SageMaker by creating an ``MXNet`` estimators.
+When you call ``fit`` on an ``MXNet`` estimator, a SageMaker training job with your script is started.
+The following code sample shows how you train a custom MXNet script "train.py".
+
+.. code:: python
+
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.3.0',
+                            hyperparameters={'batch-size': 100,
+                                             'epochs': 10,
+                                             'learning-rate': 0.1})
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+MXNet Estimators
+^^^^^^^^^^^^^^^^
+
+The ``MXNet`` constructor takes both required and optional arguments.
+
+Required arguments
+''''''''''''''''''
+
+The following are required arguments to the ``MXNet`` constructor. When you create an MXNet object, you must include these in the constructor, either positionally or as keyword arguments.
+
+-  ``entry_point`` Path (absolute or relative) to the Python file which
+   should be executed as the entry point to training.
+-  ``role`` An AWS IAM role (either name or full ARN). The Amazon
+   SageMaker training jobs and APIs that create Amazon SageMaker
+   endpoints use this role to access training data and model artifacts.
+   After the endpoint is created, the inference code might use the IAM
+   role, if accessing AWS resource.
+-  ``train_instance_count`` Number of Amazon EC2 instances to use for
+   training.
+-  ``train_instance_type`` Type of EC2 instance to use for training, for
+   example, 'ml.c4.xlarge'.
+
+Optional arguments
+''''''''''''''''''
+
+The following are optional arguments. When you create an ``MXNet`` object, you can specify these as keyword arguments.
+
+-  ``source_dir`` Path (absolute or relative) to a directory with any
+   other training source code dependencies including the entry point
+   file. Structure within this directory will be preserved when training
+   on SageMaker.
+-  ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+   any additional libraries that will be exported to the container (default: ``[]``).
+   The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+   If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+   instead. For example, the following call
+
+   >>> MXNet(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+   results in the following inside the container:
+
+   .. code::
+       opt/ml/code
+         ├── train.py
+         ├── common
+         └── virtual-env
+
+-  ``hyperparameters`` Hyperparameters that will be used for training.
+   Will be made accessible as a dict[str, str] to the training code on
+   SageMaker. For convenience, accepts other types besides str, but
+   str() will be called on keys and values to convert them before
+   training.
+-  ``py_version`` Python version you want to use for executing your
+   model training code. Valid values: 'py2' and 'py3'.
+-  ``train_volume_size`` Size in GB of the EBS volume to use for storing
+   input data during training. Must be large enough to store training
+   data if input_mode='File' is used (which is the default).
+-  ``train_max_run`` Timeout in seconds for training, after which Amazon
+   SageMaker terminates the job regardless of its current status.
+-  ``input_mode`` The input mode that the algorithm supports. Valid
+   modes: 'File' - Amazon SageMaker copies the training dataset from the
+   S3 location to a directory in the Docker container. 'Pipe' - Amazon
+   SageMaker streams data directly from S3 to the container via a Unix
+   named pipe.
+-  ``output_path`` Location where you want the training result (model artifacts and optional output files) saved.
+   This should be an S3 location unless you're using Local Mode, which also supports local output paths.
+   If not specified, results are stored to a default S3 bucket.
+-  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+   output with.
+-  ``job_name`` Name to assign for the training job that the fit()
+   method launches. If not specified, the estimator generates a default
+   job name, based on the training image name and current timestamp
+-  ``image_name`` An alternative docker image to use for training and
+   serving.  If specified, the estimator will use this image for training and
+   hosting, instead of selecting the appropriate SageMaker official image based on
+   framework_version and py_version. Refer to: `SageMaker MXNet Docker Containers
+   <#sagemaker-mxnet-docker-containers>`_ for details on what the Official images support
+   and where to find the source code to build your custom image.
+-  ``distributions`` For versions 1.3 and above only.
+   Specifies information for how to run distributed training.
+   To launch a parameter server during training, set this argument to:
+
+.. code::
+
+    {
+      'parameter_server': {
+        'enabled': True
+      }
+    }
+
+Calling fit
+^^^^^^^^^^^
+
+You start your training script by calling ``fit`` on an ``MXNet`` Estimator. ``fit`` takes both required and optional arguments.
+
+Required argument
+'''''''''''''''''
+
+-  ``inputs``: This can take one of the following forms: A string
+   S3 URI, for example ``s3://my-bucket/my-training-data``. In this
+   case, the S3 objects rooted at the ``my-training-data`` prefix will
+   be available in the default ``training`` channel. A dict from
+   string channel names to S3 URIs. In this case, the objects rooted at
+   each S3 prefix will available as files in each channel directory.
+
+For example:
+
+.. code:: python
+
+    {'train':'s3://my-bucket/my-training-data',
+     'eval':'s3://my-bucket/my-evaluation-data'}
+
+.. optional-arguments-1:
+
+Optional arguments
+''''''''''''''''''
+
+-  ``wait``: Defaults to True, whether to block and wait for the
+   training script to complete before returning.
+-  ``logs``: Defaults to True, whether to show logs produced by training
+   job in the Python session. Only meaningful when wait is True.
+
+
+Deploying MXNet models
+----------------------
+
+After an MXNet Estimator has been fit, you can host the newly created model in SageMaker.
+
+After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to create a SageMaker Endpoint. The Endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script, which was run when you called ``fit``. This was the model object you returned from ``train`` and saved with either a custom save function or the default save function.
+
+``deploy`` returns a ``Predictor`` object, which you can use to do inference on the Endpoint hosting your MXNet model. Each ``Predictor`` provides a ``predict`` method which can do inference with numpy arrays or Python lists. Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
+
+``predict`` returns the result of inference against your model. By default, the inference result is either a Python list or dictionary.
+
+.. code:: python
+
+    # Train my estimator
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.2.1')
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+    # Deploy my estimator to a SageMaker Endpoint and get a Predictor
+    predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                       initial_instance_count=1)
+
+You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
+
+.. code:: python
+
+  predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                     initial_instance_count=1,
+                                     accelerator_type='ml.eia1.medium')
+
+The SageMaker MXNet Model Server
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model server. The model server loads the model that was saved by your training script and performs inference on the model in response to SageMaker InvokeEndpoint API calls.
+
+You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
+
+As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
+
+Model loading
+^^^^^^^^^^^^^
+
+Before a model can be served, it must be loaded. The SageMaker model server loads your model by invoking a ``model_fn`` function on your training script. If you don't provide a ``model_fn`` function, SageMaker will use a default ``model_fn`` function. The default function works with MXNet Module model objects, saved via the default ``save`` function.
+
+If you wrote a custom ``save`` function then you may need to write a custom ``model_fn`` function. If your save function serializes ``Module`` objects under the same format as the default ``save`` function, then you won't need to write a custom model_fn function. If you do write a ``model_fn`` function must have the following signature:
+
+.. code:: python
+
+    def model_fn(model_dir)
+
+SageMaker will inject the directory where your model files and sub-directories, saved by ``save``, have been mounted. Your model function should return a model object that can be used for model serving. SageMaker provides automated serving functions that work with Gluon API ``net`` objects and Module API ``Module`` objects. If you return either of these types of objects, then you will be able to use the default serving request handling functions.
+
+The following code-snippet shows an example custom ``model_fn`` implementation. This loads returns an MXNet Gluon net model for resnet-34 inference. It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
+        return net
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
+
+Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
+        return net
+
+The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93maabf018d906ed282a3c738377d19a8de[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
+
+For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
+
+Model serving
+^^^^^^^^^^^^^
+
+After the SageMaker model server loads your model by calling either the default ``model_fn`` or the implementation in your script, SageMaker serves your model.
+Model serving is the process of responding to inference requests received by SageMaker ``InvokeEndpoint`` API calls.
+Defining how to handle these requests can be done in one of two ways:
+
+- using ``input_fn``, ``predict_fn``, and ``output_fn``, some of which may be your own implementations
+- writing your own ``transform_fn`` for handling input processing, prediction, and output processing
+
+Using ``input_fn``, ``predict_fn``, and ``output_fn``
+'''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+The SageMaker MXNet model server breaks request handling into three steps:
+
+-  input processing
+-  prediction
+-  output processing
+
+Just like with ``model_fn``, you configure these steps by defining functions in your Python source file.
+
+Each step has its own Python function, which takes in information about the request and the return value from the previous function in the chain.
+Inside the SageMaker MXNet model server, the process looks like:
+
+.. code:: python
+
+    # Deserialize the Invoke request body into an object we can perform prediction on
+    input_object = input_fn(request_body, request_content_type)
+
+    # Perform prediction on the deserialized object, with the loaded model
+    prediction = predict_fn(input_object, model)
+
+    # Serialize the prediction result into the desired response content type
+    ouput = output_fn(prediction, response_content_type)
+
+The above code sample shows the three function definitions that correlate to the three steps mentioned above:
+
+-  ``input_fn``: Takes request data and deserializes the data into an
+   object for prediction.
+-  ``predict_fn``: Takes the deserialized request object and performs
+   inference against the loaded model.
+-  ``output_fn``: Takes the result of prediction and serializes this
+   according to the response content type.
+
+The SageMaker MXNet model server provides default implementations of these functions.
+These work with both Gluon API and Module API model objects.
+The following content types are supported:
+
+- Gluon API: 'application/json', 'application/x-npy'
+- Module API: 'application/json', 'application/x-npy', 'text-csv'
+
+You can also provide your own implementations for these functions in your training script.
+If you omit any definition then the SageMaker MXNet model server will use its default implementation for that function.
+
+If you rely solely on the SageMaker MXNet model server defaults, you get the following functionality:
+
+-  Prediction on MXNet Gluon API ``net`` and Module API ``Module``
+   objects.
+-  Deserialization from CSV and JSON to NDArrayIters.
+-  Serialization of NDArrayIters to CSV or JSON.
+
+In the following sections we describe the default implementations of input_fn, predict_fn, and output_fn. We describe the input arguments and expected return types of each, so you can define your own implementations.
+
+Input processing
+""""""""""""""""
+
+When an InvokeEndpoint operation is made against an Endpoint running a SageMaker MXNet model server, the model server receives two pieces of information:
+
+-  The request's content type, for example "application/json"
+-  The request data body as a byte array
+
+The SageMaker MXNet model server will invoke ``input_fn``, passing in this information. If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
+
+.. code:: python
+
+    def input_fn(request_body, request_content_type)
+
+Where ``request_body`` is a byte buffer and ``request_content_type`` is the content type of the request.
+
+The SageMaker MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
+
+Default JSON deserialization requires ``request_body`` contain a single json list. Sending multiple json objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
+
+Default CSV deserialization requires ``request_body`` contain one or more lines of CSV numerical data. The data is loaded into a two-dimensional array, where each line break defines the boundaries of the first dimension. This two-dimensional array is then re-shaped to be compatible with the shape expected by the model object. Specifically, the first dimension is kept unchanged, but the second dimension is reshaped to be consistent with the shape of all dimensions in the model, following the first dimension.
+
+If you provide your own implementation of input_fn, you should abide by the ``input_fn`` signature. If you want to use this with the default
+``predict_fn``, then you should return an ``NDArrayIter``. The ``NDArrayIter`` should have a shape identical to the shape of the model being predicted on. The example below shows a custom ``input_fn`` for preparing pickled numpy arrays.
+
+.. code:: python
+
+    import numpy as np
+    import mxnet as mx
+
+    def input_fn(request_body, request_content_type):
+        """An input_fn that loads a pickled numpy array"""
+        if request_content_type == 'application/python-pickle':
+            array = np.load(StringIO(request_body))
+            array.reshape(model.data_shpaes[0])
+            return mx.io.NDArrayIter(mx.ndarray(array))
+        else:
+            # Handle other content-types here or raise an Exception
+            # if the content type is not supported.
+            pass
+
+Prediction
+""""""""""
+
+After the inference request has been deserialized by ``input_fn``, the SageMaker MXNet model server invokes ``predict_fn``. As with ``input_fn``, you can define your own ``predict_fn`` or use the SageMaker Mxnet default.
+
+The ``predict_fn`` function has the following signature:
+
+.. code:: python
+
+    def predict_fn(input_object, model)
+
+Where ``input_object`` is the object returned from ``input_fn`` and
+``model`` is the model loaded by ``model_fn``.
+
+The default implementation of ``predict_fn`` requires ``input_object`` be an ``NDArrayIter``, which is the return-type of the default
+``input_fn``. It also requires that ``model`` be either an MXNet Gluon API ``net`` object or a Module API ``Module`` object.
+
+The default implementation performs inference with the input
+``NDArrayIter`` on the Gluon or Module object. If the model is a Gluon
+``net`` it performs: ``net.forward(input_object)``. If the model is a Module object it performs ``module.predict(input_object)``. In both cases, it returns the result of that call.
+
+If you implement your own prediction function, you should take care to ensure that:
+
+-  The first argument is expected to be the return value from input_fn.
+   If you use the default input_fn, this will be an ``NDArrayIter``.
+-  The second argument is the loaded model. If you use the default
+   ``model_fn`` implementation, this will be an MXNet Module object.
+   Otherwise, it will be the return value of your ``model_fn``
+   implementation.
+-  The return value should be of the correct type to be passed as the
+   first argument to ``output_fn``. If you use the default
+   ``output_fn``, this should be an ``NDArrayIter``.
+
+Output processing
+"""""""""""""""""
+
+After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return value from ``predict_fn`` and the InvokeEndpoint requested response content type.
+
+The ``output_fn`` has the following signature:
+
+.. code:: python
+
+    def output_fn(prediction, content_type)
+
+Where ``prediction`` is the result of invoking ``predict_fn`` and ``content_type`` is the requested response content type for ``InvokeEndpoint``.
+The function should return an array of bytes serialized to the expected content type.
+
+The default implementation expects ``prediction`` to be an ``NDArray`` and can serialize the result to either JSON or CSV. It accepts response content types of "application/json" and "text/csv".
+
+Using ``transform_fn``
+''''''''''''''''''''''
+
+If you would rather not structure your code around the three methods described above, you can instead define your own ``transform_fn`` to handle inference requests.
+This will override any implementation of ``input_fn``, ``predict_fn``, or ``output_fn``.
+``transform_fn`` has the following signature:
+
+.. code:: python
+
+    def transform_fn(model, request_body, content_type, accept_type)
+
+Where ``model`` is the model objected loaded by ``model_fn``, ``request_body`` is the data from the inference request, ``content_type`` is the content type of the request, and ``accept_type`` is the request content type for the response.
+
+This one function should handle processing the input, performing a prediction, and processing the output.
+The return object should be one of the following:
+
+- a tuple with two items: the response data and ``accept_type`` (the content type of the response data), or
+- a Flask response object: http://flask.pocoo.org/docs/1.0/api/#response-objects
+
+You can find examples of hosting scripts using this structure in the example notebooks, such as the `mxnet_gluon_sentiment <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_sentiment/sentiment.py#L344-L387>`__ notebook.
+
+Working with existing model data and training jobs
+--------------------------------------------------
+
+Attaching to existing training jobs
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You can attach an MXNet Estimator to an existing training job using the
+``attach`` method.
+
+.. code:: python
+
+    my_training_job_name = 'MyAwesomeMXNetTrainingJob'
+    mxnet_estimator = MXNet.attach(my_training_job_name)
+
+After attaching, if the training job is in a Complete status, it can be
+``deploy``\ ed to create a SageMaker Endpoint and return a
+``Predictor``. If the training job is in progress, attach will block and display log messages from the training job, until the training job completes.
+
+The ``attach`` method accepts the following arguments:
+
+-  ``training_job_name (str):`` The name of the training job to attach
+   to.
+-  ``sagemaker_session (sagemaker.Session or None):`` The Session used
+   to interact with SageMaker
+
+Deploying Endpoints from model data
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As well as attaching to existing training jobs, you can deploy models directly from model data in S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
+
+.. code:: python
+
+    mxnet_model = MXNetModel(model_data='s3://bucket/model.tar.gz', role='SageMakerRole', entry_point='trasform_script.py')
+
+    predictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
+
+The MXNetModel constructor takes the following arguments:
+
+-  ``model_data (str):`` An S3 location of a SageMaker model data
+   .tar.gz file
+-  ``image (str):`` A Docker image URI
+-  ``role (str):`` An IAM role name or Arn for SageMaker to access AWS
+   resources on your behalf.
+-  ``predictor_cls (callable[string,sagemaker.Session]):`` A function to
+   call to create a predictor. If not None, ``deploy`` will return the
+   result of invoking this function on the created endpoint name
+-  ``env (dict[string,string]):`` Environment variables to run with
+   ``image`` when hosted in SageMaker.
+-  ``name (str):`` The model name. If None, a default model name will be
+   selected on each ``deploy.``
+-  ``entry_point (str):`` Path (absolute or relative) to the Python file
+   which should be executed as the entry point to model hosting.
+-  ``source_dir (str):`` Optional. Path (absolute or relative) to a
+   directory with any other training source code dependencies including
+   tne entry point file. Structure within this directory will be
+   preserved when training on SageMaker.
+-  ``container_log_level (int):`` Log level to use within the container.
+   Valid values are defined in the Python logging module.
+-  ``code_location (str):`` Optional. Name of the S3 bucket where your
+   custom code will be uploaded to. If not specified, will use the
+   SageMaker default bucket created by sagemaker.Session.
+-  ``sagemaker_session (sagemaker.Session):`` The SageMaker Session
+   object, used for SageMaker interaction
+
+Your model data must be a .tar.gz file in S3. SageMaker Training Job model data is saved to .tar.gz files in S3, however if you have local data you want to deploy, you can prepare the data yourself.
+
+Assuming you have a local directory containg your model data named "my_model" you can tar and gzip compress the file and upload to S3 using the following commands:
+
+::
+
+    tar -czf model.tar.gz my_model
+    aws s3 cp model.tar.gz s3://my-bucket/my-path/model.tar.gz
+
+This uploads the contents of my_model to a gzip compressed tar file to S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
+
+To run this command, you'll need the aws cli tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
+
+Examples
+--------
+
+Amazon provides several example Jupyter notebooks that demonstrate end-to-end training on Amazon SageMaker using MXNet. Please refer to:
+
+https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk
+
+These are also available in SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
 
 SageMaker MXNet Containers
 --------------------------

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-02-21 13:06:20[0m
[92mHash: 9270da78e10a65206171741018feefe95d4005e3[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/master[0m
[92mCommit: add sphinx-build in tox (#653)

[0m
@@ -4,7 +4,7 @@
 # and then run "tox" from this directory.
 
 [tox]
-envlist = py27,py35,flake8,pylint,sphinx
+envlist = py27,py35,flake8,pylint
 skip_missing_interpreters = False
 
 [travis]
@@ -72,28 +72,3 @@ deps =
     pylint==2.1.1
 commands =
     python -m pylint --rcfile=.pylintrc -j 0 src/sagemaker
-
-[testenv:sphinx]
-basepython = python3
-changedir = doc
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
-install_command = python -m pip install --upgrade -I {packages}
-# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
-deps =
-    Pygments==2.2.0
-    setuptools<40
-    docutils==0.13.1
-    mock==1.0.1
-    pillow==2.6.1
-    alabaster>=0.7,<0.8,!=0.7.5
-    commonmark==0.5.4
-    recommonmark==0.4.0
-    sphinx<1.8
-    sphinx-rtd-theme<0.5
-    readthedocs-sphinx-ext<0.6
-# pip install requirements.txt is separate as RTD does it in separate steps
-# having the requirements.txt installed in deps above results in Double Requirement exception
-# https://github.com/pypa/pip/issues/988
-commands =
-    pip install --exists-action=w -r requirements.txt
-    sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-02-01 16:57:19[0m
[92mHash: e1b459f07f8f956f0088cc0405c927aa3d7879ed[0m
[92mFilepath: doc/using_mxnet.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Update information about saving models in the MXNet README (#616)

[0m
@@ -1,782 +1,822 @@
-=========================================
-Using MXNet with the SageMaker Python SDK
-=========================================
-
-.. contents::
-
-With the SageMaker Python SDK, you can train and host MXNet models on Amazon SageMaker.
-
-Supported versions of MXNet: ``1.3.0``, ``1.2.1``, ``1.1.0``, ``1.0.0``, ``0.12.1``.
-
-Supported versions of MXNet for Elastic Inference: ``1.3.0``.
-
-Training with MXNet
--------------------
-
-Training MXNet models using ``MXNet`` Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an ``MXNet`` Estimator. You should prepare your script in a separate source file than the notebook, terminal session, or source file you're using to submit the script to SageMaker via an ``MXNet`` Estimator.
-
-Suppose that you already have an MXNet training script called
-``mxnet-train.py``. You can run this script in SageMaker as follows:
-
-.. code:: python
-
-    from sagemaker.mxnet import MXNet
-    mxnet_estimator = MXNet('mxnet-train.py',
-                            role='SageMakerRole',
-                            train_instance_type='ml.p3.2xlarge',
-                            train_instance_count=1,
-                            framework_version='1.3.0')
-    mxnet_estimator.fit('s3://bucket/path/to/training/data')
-
-Where the S3 url is a path to your training data, within Amazon S3. The constructor keyword arguments define how SageMaker runs your training script and are discussed, in detail, in a later section.
-
-In the following sections, we'll discuss how to prepare a training script for execution on SageMaker, then how to run that script on SageMaker using an ``MXNet`` Estimator.
-
-Preparing the MXNet training script
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-+----------------------------------------------------------------------------------------------------------------------------------------------------------+
-| WARNING                                                                                                                                                  |
-+==========================================================================================================================================================+
-| The structure for training scripts changed with MXNet version 1.3.                                                                                       |
-| Make sure you refer to the correct section of this README when you prepare your script.                                                                  |
-| For information on how to upgrade an old script to the new format, see `"Updating your MXNet training script" <#updating-your-mxnet-training-script>`__. |
-+----------------------------------------------------------------------------------------------------------------------------------------------------------+
-
-For versions 1.3 and higher
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-Your MXNet training script must be a Python 2.7 or 3.5 compatible source file.
-
-The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
-
-* ``SM_MODEL_DIR``: A string that represents the path where the training job writes the model artifacts to.
-  After training, artifacts in this directory are uploaded to S3 for model hosting.
-* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
-* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
-  For example, if you specify two input channels in the MXNet estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
-* ``SM_HPS``: A json dump of the hyperparameters preserving json types (boolean, integer, etc.)
-
-For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
-
-A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``model_dir`` so that it can be deployed for inference later.
-Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
-For example, a training script might start with the following:
-
-.. code:: python
-
-    import argparse
-    import os
-    import json
-
-    if __name__ =='__main__':
-
-        parser = argparse.ArgumentParser()
-
-        # hyperparameters sent by the client are passed as command-line arguments to the script.
-        parser.add_argument('--epochs', type=int, default=10)
-        parser.add_argument('--batch-size', type=int, default=100)
-        parser.add_argument('--learning-rate', type=float, default=0.1)
-
-        # an alternative way to load hyperparameters via SM_HPS environment variable.
-        parser.add_argument('--sm-hps', type=json.loads, default=os.environ['SM_HPS'])
-
-        # input data and model directories
-        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
-        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
-        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
-
-        args, _ = parser.parse_known_args()
-
-        # ... load from args.train and args.test, train a model, write model to args.model_dir.
-
-Because the SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
-so that SageMaker does not inadvertently run your training code at the wrong point in execution.
-
-Note that SageMaker doesn't support argparse actions.
-If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your MXNet estimator.
-
-For more on training environment variables, please visit `SageMaker Containers <https://github.com/aws/sagemaker-containers>`_.
-
-For versions 1.2 and lower
-^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Your MXNet training script must be a Python 2.7 or 3.5 compatible source file. The MXNet training script must contain a function ``train``, which SageMaker invokes to run training. You can include other functions as well, but it must contain a ``train`` function.
-
-When you run your script on SageMaker via the ``MXNet`` Estimator, SageMaker injects information about the training environment into your training function via Python keyword arguments. You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
-
--  ``hyperparameters (dict[string,string])``: The hyperparameters passed
-   to SageMaker TrainingJob that runs your MXNet training script. You
-   can use this to pass hyperparameters to your training script.
--  ``input_data_config (dict[string,dict])``: The SageMaker TrainingJob
-   InputDataConfig object, that's set when the SageMaker TrainingJob is
-   created. This is discussed in more detail below.
--  ``channel_input_dirs (dict[string,string])``: A collection of
-   directories containing training data. When you run training, you can
-   partition your training data into different logical "channels".
-   Depending on your problem, some common channel ideas are: "train",
-   "test", "evaluation" or "images',"labels".
--  ``output_data_dir (str)``: A directory where your training script can
-   write data that will be moved to S3 after training is complete.
--  ``num_gpus (int)``: The number of GPU devices available on your
-   training instance.
--  ``num_cpus (int)``: The number of CPU devices available on your training instance.
--  ``hosts (list[str])``: The list of host names running in the
-   SageMaker Training Job cluster.
--  ``current_host (str)``: The name of the host executing the script.
-   When you use SageMaker for MXNet training, the script is run on each
-   host in the cluster.
-
-A training script that takes advantage of all arguments would have the following definition:
-
-.. code:: python
-
-    def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,
-              num_gpus, num_cpus, hosts, current_host):
-        pass
-
-You don't have to use all the arguments, arguments you don't care about can be ignored by including ``**kwargs``.
-
-.. code:: python
-
-    # Only work with hyperparameters and num_gpus, ignore all other hyperparameters
-    def train(hyperparameters, num_gpus, **kwargs):
-        pass
-
-**Note: Writing a training script that imports correctly**
-When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
-
-If you want to run your training script locally via the Python interpreter, look at using a ``___name__ == '__main__'`` guard, discussed in more detail here: https://stackoverflow.com/questions/419163/what-does-if-name-main-do .
-
-Distributed training
-''''''''''''''''''''
-
-When writing a distributed training script, you will want to use an MXNet kvstore to store and share model parameters.
-During training, SageMaker automatically starts an MXNet kvstore server and scheduler processes on hosts in your training job cluster.
-Your script runs as an MXNet worker task, with one server process on each host in your cluster.
-One host is selected arbitrarily to run the scheduler process.
-
-To learn more about writing distributed MXNet programs, please see `Distributed Training <http://newdocs.readthedocs.io/en/latest/distributed_training.html>`__ in the MXNet docs.
-
-Saving models
-'''''''''''''
-
-Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script. If your script includes a ``save`` function, SageMaker will invoke it with the return-value of ``train``. Model saving is a two-step process, firstly you return the model you want to save from
-``train``, then you define your model-serialization logic in ``save``.
-
-SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects. If your training script does not define a ``save`` function, then the default ``save`` function will be invoked on the return-value of your ``train`` function.
-
-The default serialization system generates three files:
-
--  ``model-shapes.json``: A json list, containing a serialization of the
-   ``Module`` ``data_shapes`` property. Each object in the list contains
-   the serialization of one ``DataShape`` in the returned ``Module``.
-   Each object has a ``name`` property, containing the ``DataShape``
-   name and a ``shape`` property, which is a list of that dimensions for
-   the shape of that ``DataShape``. For example:
-
-.. code:: javascript
-
-    [
-        {"name":"images", "shape":[100, 1, 28, 28]},
-        {"name":"labels", "shape":[100, 1]}
-    ]
-
--  ``model-symbol.json``: The MXNet ``Module`` ``Symbol`` serialization,
-   produced by invoking ``save`` on the ``symbol`` property of the
-   ``Module`` being saved.
--  ``modle.params``: The MXNet ``Module`` parameters. Produced by
-   invoking ``save_params`` on the ``Module`` being saved.
-
-You can provide your own save function. This is useful if you are not working with the ``Module`` API or you need special processing.
-
-To provide your own save function, define a ``save`` function in your training script:
-
-.. code:: python
-
-    def save(model, model_dir):
-        pass
-
-The function should take two arguments:
-
--  ``model``: This is the object that was returned from your ``train``
-   function. If your ``train`` function does not return an object, it
-   will be ``None``. You are free to return an object of any type from
-   ``train``, you do not have to return ``Module`` or ``Gluon`` API
-   specific objects.
--  ``model_dir``: This is the string path on the SageMaker training host
-   where you save your model. Files created in this directory will be
-   accessible in S3 after your SageMaker Training Job completes.
-
-After your ``train`` function completes, SageMaker will invoke ``save`` with the object returned from ``train``.
-
-**Note: How to save Gluon models with SageMaker**
-
-If your train function returns a Gluon API ``net`` object as its model, you'll need to write your own ``save`` function. You will want to serialize the ``net`` parameters. Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
-
-Updating your MXNet training script
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-The structure for training scripts changed with MXNet version 1.3.
-The ``train`` function is no longer be required; instead the training script must be able to be run as a standalone script.
-In this way, the training script is similar to a training script you might run outside of SageMaker.
-
-There are a few steps needed to make a training script with the old format compatible with the new format.
-
-First, add a `main guard <https://docs.python.org/3/library/__main__.html>`__ (``if __name__ == '__main__':``).
-The code executed from your main guard needs to:
-
-1. Set hyperparameters and directory locations
-2. Initiate training
-3. Save the model
-
-Hyperparameters will be passed as command-line arguments to your training script.
-In addition, the container will define the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
-You can find the full list of available environment variables in the `SageMaker Containers README <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
-
-We recommend using `an argument parser <https://docs.python.org/3.5/howto/argparse.html>`__ for this part.
-Using the ``argparse`` library as an example, the code would look something like this:
-
-.. code:: python
-
-    import argparse
-    import os
-
-    if __name__ == '__main__':
-        parser = argparse.ArgumentParser()
-
-        # hyperparameters sent by the client are passed as command-line arguments to the script.
-        parser.add_argument('--epochs', type=int, default=10)
-        parser.add_argument('--batch-size', type=int, default=100)
-        parser.add_argument('--learning-rate', type=float, default=0.1)
-
-        # input data and model directories
-        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
-        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
-        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
-
-        args, _ = parser.parse_known_args()
-
-The code in the main guard should also take care of training and saving the model.
-This can be as simple as just calling the ``train`` and ``save`` methods used in the previous training script format:
-
-.. code:: python
-
-    if __name__ == '__main__':
-        # arg parsing (shown above) goes here
-
-        model = train(args.batch_size, args.epochs, args.learning_rate, args.train, args.test)
-        save(args.model_dir, model)
-
-Note that saving the model will no longer be done by default; this must be done by the training script.
-If you were previously relying on the default save method, you can now import one from the container:
-
-.. code:: python
-
-    from sagemaker_mxnet_container.training_utils import save
-
-    if __name__ == '__main__':
-        # arg parsing and training (shown above) goes here
-
-        save(args.model_dir, model)
-
-Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must now set ``distributions`` to the following dictionary when creating an MXNet estimator:
-
-.. code:: python
-
-    from sagemaker.mxnet import MXNet
-
-    estimator = MXNet('path-to-distributed-training-script.py',
-                      ...,
-                      distributions={'parameter_server': {'enabled': True}})
-
-
-Using third-party libraries
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-When running your training script on SageMaker, it will have access to some pre-installed third-party libraries including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``. For more information on the runtime environment, including specific package versions, see `SageMaker MXNet Containers <#sagemaker-mxnet-containers>`__.
-
-If there are other packages you want to use with your script, you can include a `requirements.txt <https://pip.pypa.io/en/stable/user_guide/#requirements-files>`__ file in the same directory as your training script to install other dependencies at runtime.
-
-Running an MXNet training script in SageMaker
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You run MXNet training scripts on SageMaker by creating an ``MXNet`` estimators.
-When you call ``fit`` on an ``MXNet`` estimator, a SageMaker training job with your script is started.
-The following code sample shows how you train a custom MXNet script "train.py".
-
-.. code:: python
-
-    mxnet_estimator = MXNet('train.py',
-                            train_instance_type='ml.p2.xlarge',
-                            train_instance_count=1,
-                            framework_version='1.3.0',
-                            hyperparameters={'batch-size': 100,
-                                             'epochs': 10,
-                                             'learning-rate': 0.1})
-    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
-
-MXNet Estimators
-^^^^^^^^^^^^^^^^
-
-The ``MXNet`` constructor takes both required and optional arguments.
-
-Required arguments
-''''''''''''''''''
-
-The following are required arguments to the ``MXNet`` constructor. When you create an MXNet object, you must include these in the constructor, either positionally or as keyword arguments.
-
--  ``entry_point`` Path (absolute or relative) to the Python file which
-   should be executed as the entry point to training.
--  ``role`` An AWS IAM role (either name or full ARN). The Amazon
-   SageMaker training jobs and APIs that create Amazon SageMaker
-   endpoints use this role to access training data and model artifacts.
-   After the endpoint is created, the inference code might use the IAM
-   role, if accessing AWS resource.
--  ``train_instance_count`` Number of Amazon EC2 instances to use for
-   training.
--  ``train_instance_type`` Type of EC2 instance to use for training, for
-   example, 'ml.c4.xlarge'.
-
-Optional arguments
-''''''''''''''''''
-
-The following are optional arguments. When you create an ``MXNet`` object, you can specify these as keyword arguments.
-
--  ``source_dir`` Path (absolute or relative) to a directory with any
-   other training source code dependencies including the entry point
-   file. Structure within this directory will be preserved when training
-   on SageMaker.
--  ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
-   any additional libraries that will be exported to the container (default: ``[]``).
-   The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
-   If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
-   instead. For example, the following call
-
-   >>> MXNet(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
-
-   results in the following inside the container:
-
-   .. code::
-       opt/ml/code
-         ├── train.py
-         ├── common
-         └── virtual-env
-
--  ``hyperparameters`` Hyperparameters that will be used for training.
-   Will be made accessible as a dict[str, str] to the training code on
-   SageMaker. For convenience, accepts other types besides str, but
-   str() will be called on keys and values to convert them before
-   training.
--  ``py_version`` Python version you want to use for executing your
-   model training code. Valid values: 'py2' and 'py3'.
--  ``train_volume_size`` Size in GB of the EBS volume to use for storing
-   input data during training. Must be large enough to store training
-   data if input_mode='File' is used (which is the default).
--  ``train_max_run`` Timeout in seconds for training, after which Amazon
-   SageMaker terminates the job regardless of its current status.
--  ``input_mode`` The input mode that the algorithm supports. Valid
-   modes: 'File' - Amazon SageMaker copies the training dataset from the
-   S3 location to a directory in the Docker container. 'Pipe' - Amazon
-   SageMaker streams data directly from S3 to the container via a Unix
-   named pipe.
--  ``output_path`` Location where you want the training result (model artifacts and optional output files) saved.
-   This should be an S3 location unless you're using Local Mode, which also supports local output paths.
-   If not specified, results are stored to a default S3 bucket.
--  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
-   output with.
--  ``job_name`` Name to assign for the training job that the fit()
-   method launches. If not specified, the estimator generates a default
-   job name, based on the training image name and current timestamp
--  ``image_name`` An alternative docker image to use for training and
-   serving.  If specified, the estimator will use this image for training and
-   hosting, instead of selecting the appropriate SageMaker official image based on
-   framework_version and py_version. Refer to: `SageMaker MXNet Docker Containers
-   <#sagemaker-mxnet-docker-containers>`_ for details on what the Official images support
-   and where to find the source code to build your custom image.
--  ``distributions`` For versions 1.3 and above only.
-   Specifies information for how to run distributed training.
-   To launch a parameter server during training, set this argument to:
-
-.. code::
-
-    {
-      'parameter_server': {
-        'enabled': True
-      }
-    }
-
-Calling fit
-^^^^^^^^^^^
-
-You start your training script by calling ``fit`` on an ``MXNet`` Estimator. ``fit`` takes both required and optional arguments.
-
-Required argument
-'''''''''''''''''
-
--  ``inputs``: This can take one of the following forms: A string
-   S3 URI, for example ``s3://my-bucket/my-training-data``. In this
-   case, the S3 objects rooted at the ``my-training-data`` prefix will
-   be available in the default ``training`` channel. A dict from
-   string channel names to S3 URIs. In this case, the objects rooted at
-   each S3 prefix will available as files in each channel directory.
-
-For example:
-
-.. code:: python
-
-    {'train':'s3://my-bucket/my-training-data',
-     'eval':'s3://my-bucket/my-evaluation-data'}
-
-.. optional-arguments-1:
-
-Optional arguments
-''''''''''''''''''
-
--  ``wait``: Defaults to True, whether to block and wait for the
-   training script to complete before returning.
--  ``logs``: Defaults to True, whether to show logs produced by training
-   job in the Python session. Only meaningful when wait is True.
-
-
-Deploying MXNet models
-----------------------
-
-After an MXNet Estimator has been fit, you can host the newly created model in SageMaker.
-
-After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to create a SageMaker Endpoint. The Endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script, which was run when you called ``fit``. This was the model object you returned from ``train`` and saved with either a custom save function or the default save function.
-
-``deploy`` returns a ``Predictor`` object, which you can use to do inference on the Endpoint hosting your MXNet model. Each ``Predictor`` provides a ``predict`` method which can do inference with numpy arrays or Python lists. Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
-
-``predict`` returns the result of inference against your model. By default, the inference result is either a Python list or dictionary.
-
-.. code:: python
-
-    # Train my estimator
-    mxnet_estimator = MXNet('train.py',
-                            train_instance_type='ml.p2.xlarge',
-                            train_instance_count=1,
-                            framework_version='1.2.1')
-    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
-
-    # Deploy my estimator to a SageMaker Endpoint and get a Predictor
-    predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
-                                       initial_instance_count=1)
-
-You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
-
-MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
-
-.. code:: python
-
-  predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
-                                     initial_instance_count=1,
-                                     accelerator_type='ml.eia1.medium')
-
-The SageMaker MXNet Model Server
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model server. The model server loads the model that was saved by your training script and performs inference on the model in response to SageMaker InvokeEndpoint API calls.
-
-You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
-
-As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
-
-Model loading
-^^^^^^^^^^^^^
-
-Before a model can be served, it must be loaded. The SageMaker model server loads your model by invoking a ``model_fn`` function on your training script. If you don't provide a ``model_fn`` function, SageMaker will use a default ``model_fn`` function. The default function works with MXNet Module model objects, saved via the default ``save`` function.
-
-If you wrote a custom ``save`` function then you may need to write a custom ``model_fn`` function. If your save function serializes ``Module`` objects under the same format as the default ``save`` function, then you won't need to write a custom model_fn function. If you do write a ``model_fn`` function must have the following signature:
-
-.. code:: python
-
-    def model_fn(model_dir)
-
-SageMaker will inject the directory where your model files and sub-directories, saved by ``save``, have been mounted. Your model function should return a model object that can be used for model serving. SageMaker provides automated serving functions that work with Gluon API ``net`` objects and Module API ``Module`` objects. If you return either of these types of objects, then you will be able to use the default serving request handling functions.
-
-The following code-snippet shows an example custom ``model_fn`` implementation. This loads returns an MXNet Gluon net model for resnet-34 inference. It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
-
-.. code:: python
-
-    def model_fn(model_dir):
-        """
-        Load the gluon model. Called once when hosting service starts.
-        :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
-        return net
-
-MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
-
-Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
-
-.. code:: python
-
-    def model_fn(model_dir):
-        """
-        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
-        :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
-        return net
-
-The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
-
-For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
-
-Model serving
-^^^^^^^^^^^^^
-
-After the SageMaker model server has loaded your model, by calling either the default ``model_fn`` or the implementation in your training script, SageMaker will serve your model. Model serving is the process of responding to inference requests, received by SageMaker InvokeEndpoint API calls. The SageMaker MXNet model server breaks request handling into three steps:
-
-
--  input processing,
--  prediction, and
--  output processing.
-
-In a similar way to previous steps, you configure these steps by defining functions in your Python source file.
-
-Each step involves invoking a python function, with information about the request and the return-value from the previous function in the chain. Inside the SageMaker MXNet model server, the process looks like:
-
-.. code:: python
-
-    # Deserialize the Invoke request body into an object we can perform prediction on
-    input_object = input_fn(request_body, request_content_type, model)
-
-    # Perform prediction on the deserialized object, with the loaded model
-    prediction = predict_fn(input_object, model)
-
-    # Serialize the prediction result into the desired response content type
-    ouput = output_fn(prediction, response_content_type)
-
-The above code-sample shows the three function definitions:
-
--  ``input_fn``: Takes request data and deserializes the data into an
-   object for prediction.
--  ``predict_fn``: Takes the deserialized request object and performs
-   inference against the loaded model.
--  ``output_fn``: Takes the result of prediction and serializes this
-   according to the response content type.
-
-The SageMaker MXNet model server provides default implementations of these functions. These work with common-content types, and Gluon API and Module API model objects. You can provide your own implementations for these functions in your training script. If you omit any definition then the SageMaker MXNet model server will use its default implementation for that function.
-
-If you rely solely on the SageMaker MXNet model server defaults, you get the following functionality:
-
--  Prediction on MXNet Gluon API ``net`` and Module API ``Module``
-   objects.
--  Deserialization from CSV and JSON to NDArrayIters.
--  Serialization of NDArrayIters to CSV or JSON.
-
-In the following sections we describe the default implementations of input_fn, predict_fn, and output_fn. We describe the input arguments and expected return types of each, so you can define your own implementations.
-
-Input processing
-''''''''''''''''
-
-When an InvokeEndpoint operation is made against an Endpoint running a SageMaker MXNet model server, the model server receives two pieces of information:
-
--  The request Content-Type, for example "application/json"
--  The request data body, a byte array
-
-The SageMaker MXNet model server will invoke an ``input_fn`` function in your training script, passing in this information. If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
-
-.. code:: python
-
-    def input_fn(request_body, request_content_type, model)
-
-Where ``request_body`` is a byte buffer, ``request_content_type`` is a Python string, and model is the result of invoking ``model_fn``.
-
-The SageMaker MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
-
-Default json deserialization requires ``request_body`` contain a single json list. Sending multiple json objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
-
-Default csv deserialization requires ``request_body`` contain one or more lines of CSV numerical data. The data is loaded into a two-dimensional array, where each line break defines the boundaries of the first dimension. This two-dimensional array is then re-shaped to be compatible with the shape expected by the model object. Specifically, the first dimension is kept unchanged, but the second dimension is reshaped to be consistent with the shape of all dimensions in the model, following the first dimension.
-
-If you provide your own implementation of input_fn, you should abide by the ``input_fn`` signature. If you want to use this with the default
-``predict_fn``, then you should return an NDArrayIter. The NDArrayIter should have a shape identical to the shape of the model being predicted on. The example below shows a custom ``input_fn`` for preparing pickled numpy arrays.
-
-.. code:: python
-
-    import numpy as np
-    import mxnet as mx
-
-    def input_fn(request_body, request_content_type, model):
-        """An input_fn that loads a pickled numpy array"""
-        if request_content_type == 'application/python-pickle':
-            array = np.load(StringIO(request_body))
-            array.reshape(model.data_shpaes[0])
-            return mx.io.NDArrayIter(mx.ndarray(array))
-        else:
-            # Handle other content-types here or raise an Exception
-            # if the content type is not supported.
-            pass
-
-Prediction
-''''''''''
-
-After the inference request has been deserialized by ``input_fn``, the SageMaker MXNet model server invokes ``predict_fn``. As with ``input_fn``, you can define your own ``predict_fn`` or use the SageMaker Mxnet default.
-
-The ``predict_fn`` function has the following signature:
-
-.. code:: python
-
-    def predict_fn(input_object, model)
-
-Where ``input_object`` is the object returned from ``input_fn`` and
-``model`` is the model loaded by ``model_fn``.
-
-The default implementation of ``predict_fn`` requires ``input_object`` be an ``NDArrayIter``, which is the return-type of the default
-``input_fn``. It also requires that ``model`` be either an MXNet Gluon API ``net`` object or a Module API ``Module`` object.
-
-The default implementation performs inference with the input
-``NDArrayIter`` on the Gluon or Module object. If the model is a Gluon
-``net`` it performs: ``net.forward(input_object)``. If the model is a Module object it performs ``module.predict(input_object)``. In both cases, it returns the result of that call.
-
-If you implement your own prediction function, you should take care to ensure that:
-
--  The first argument is expected to be the return value from input_fn.
-   If you use the default input_fn, this will be an ``NDArrayIter``.
--  The second argument is the loaded model. If you use the default
-   ``model_fn`` implementation, this will be an MXNet Module object.
-   Otherwise, it will be the return value of your ``model_fn``
-   implementation.
--  The return value should be of the correct type to be passed as the
-   first argument to ``output_fn``. If you use the default
-   ``output_fn``, this should be an ``NDArrayIter``.
-
-Output processing
-'''''''''''''''''
-
-After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return-value from ``predict_fn`` and the InvokeEndpoint requested response content-type.
-
-The ``output_fn`` has the following signature:
-
-.. code:: python
-
-    def output_fn(prediction, content_type)
-
-Where ``prediction`` is the result of invoking ``predict_fn`` and
-``content_type`` is the InvokeEndpoint requested response content-type. The function should return a byte array of data serialized to content_type.
-
-The default implementation expects ``prediction`` to be an ``NDArray`` and can serialize the result to either JSON or CSV. It accepts response content types of "application/json" and "text/csv".
-
-Working with existing model data and training jobs
---------------------------------------------------
-
-Attaching to existing training jobs
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You can attach an MXNet Estimator to an existing training job using the
-``attach`` method.
-
-.. code:: python
-
-    my_training_job_name = 'MyAwesomeMXNetTrainingJob'
-    mxnet_estimator = MXNet.attach(my_training_job_name)
-
-After attaching, if the training job is in a Complete status, it can be
-``deploy``\ ed to create a SageMaker Endpoint and return a
-``Predictor``. If the training job is in progress, attach will block and display log messages from the training job, until the training job completes.
-
-The ``attach`` method accepts the following arguments:
-
--  ``training_job_name (str):`` The name of the training job to attach
-   to.
--  ``sagemaker_session (sagemaker.Session or None):`` The Session used
-   to interact with SageMaker
-
-Deploying Endpoints from model data
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-As well as attaching to existing training jobs, you can deploy models directly from model data in S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
-
-.. code:: python
-
-    mxnet_model = MXNetModel(model_data='s3://bucket/model.tar.gz', role='SageMakerRole', entry_point='trasform_script.py')
-
-    predictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
-
-The MXNetModel constructor takes the following arguments:
-
--  ``model_data (str):`` An S3 location of a SageMaker model data
-   .tar.gz file
--  ``image (str):`` A Docker image URI
--  ``role (str):`` An IAM role name or Arn for SageMaker to access AWS
-   resources on your behalf.
--  ``predictor_cls (callable[string,sagemaker.Session]):`` A function to
-   call to create a predictor. If not None, ``deploy`` will return the
-   result of invoking this function on the created endpoint name
--  ``env (dict[string,string]):`` Environment variables to run with
-   ``image`` when hosted in SageMaker.
--  ``name (str):`` The model name. If None, a default model name will be
-   selected on each ``deploy.``
--  ``entry_point (str):`` Path (absolute or relative) to the Python file
-   which should be executed as the entry point to model hosting.
--  ``source_dir (str):`` Optional. Path (absolute or relative) to a
-   directory with any other training source code dependencies including
-   tne entry point file. Structure within this directory will be
-   preserved when training on SageMaker.
--  ``container_log_level (int):`` Log level to use within the container.
-   Valid values are defined in the Python logging module.
--  ``code_location (str):`` Optional. Name of the S3 bucket where your
-   custom code will be uploaded to. If not specified, will use the
-   SageMaker default bucket created by sagemaker.Session.
--  ``sagemaker_session (sagemaker.Session):`` The SageMaker Session
-   object, used for SageMaker interaction
-
-Your model data must be a .tar.gz file in S3. SageMaker Training Job model data is saved to .tar.gz files in S3, however if you have local data you want to deploy, you can prepare the data yourself.
-
-Assuming you have a local directory containg your model data named "my_model" you can tar and gzip compress the file and upload to S3 using the following commands:
-
-::
-
-    tar -czf model.tar.gz my_model
-    aws s3 cp model.tar.gz s3://my-bucket/my-path/model.tar.gz
-
-This uploads the contents of my_model to a gzip compressed tar file to S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
-
-To run this command, you'll need the aws cli tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
-
-Examples
---------
-
-Amazon provides several example Jupyter notebooks that demonstrate end-to-end training on Amazon SageMaker using MXNet. Please refer to:
-
-https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk
-
-These are also available in SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
-
-SageMaker MXNet Containers
---------------------------
-
-When training and deploying training scripts, SageMaker runs your Python script in a Docker container with several libraries installed. When creating the Estimator and calling deploy to create the SageMaker Endpoint, you can control the environment your script runs in.
-
-SageMaker runs MXNet Estimator scripts in either Python 2.7 or Python 3.5. You can select the Python version by passing a ``py_version`` keyword arg to the MXNet Estimator constructor. Setting this to ``py2`` (the default) will cause your training script to be run on Python 2.7. Setting this to ``py3`` will cause your training script to be run on Python 3.5. This Python version applies to both the Training Job, created by fit, and the Endpoint, created by deploy.
-
-Your MXNet training script will be run on version 1.2.1 by default. (See below for how to choose a different version, and currently supported versions.) The decision to use the GPU or CPU version of MXNet is made by the ``train_instance_type``, set on the MXNet constructor. If you choose a GPU instance type, your training job will be run on a GPU version of MXNet. If you choose a CPU instance type, your training job will be run on a CPU version of MXNet. Similarly, when you call deploy, specifying a GPU or CPU deploy_instance_type, will control which MXNet build your Endpoint runs.
-
-The Docker images have the following dependencies installed:
-
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| Dependencies            | MXNet 0.12.1 | MXNet 1.0.0 | MXNet 1.1.0 | MXNet 1.2.1 | MXNet 1.3.0 |
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| Python                  |   2.7 or 3.5 |   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| CUDA (GPU image only)   |          9.0 |         9.0 |         9.0 |         9.0 |         9.0 |
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| numpy                   |       1.13.3 |      1.13.3 |      1.13.3 |      1.14.5 |      1.14.6 |
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| onnx                    |          N/A |         N/A |         N/A |       1.2.1 |       1.2.1 |
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-| keras-mxnet             |          N/A |         N/A |         N/A |         N/A |       2.2.2 |
-+-------------------------+--------------+-------------+-------------+-------------+-------------+
-
-The Docker images extend Ubuntu 16.04.
-
-You can select version of MXNet by passing a ``framework_version`` keyword arg to the MXNet Estimator constructor. Currently supported versions are listed in the above table. You can also set ``framework_version`` to only specify major and minor version, e.g ``1.2``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.2.1.
-Alternatively, you can build your own image by following the instructions in the SageMaker MXNet containers repository, and passing ``image_name`` to the MXNet Estimator constructor.
-
-You can visit the SageMaker MXNet containers repository here: https://github.com/aws/sagemaker-mxnet-container
+Using MXNet Estimators and Models
+---------------------------------
+
+-----------------
+Table of Contents
+-----------------
+.. contents::
+    :local:
+
+
+With MXNet estimators, you can train and host MXNet models on Amazon SageMaker.
+
+Supported versions of MXNet: ``1.3.0``, ``1.2.1``, ``1.1.0``, ``1.0.0``, ``0.12.1``.
+
+Supported versions of MXNet for Elastic Inference: ``1.3.0``.
+
+Training with MXNet
+~~~~~~~~~~~~~~~~~~~
+
+Training MXNet models using :class:`MXNet <sagemaker.mxnet.estimator.MXNet>` estimators is a two-step process. First, you prepare your training script, then you run the script on SageMaker by creating an ``MXNet`` estimator object.
+Prepare your script in a source file separate from the notebook, terminal session, or source file you're using to submit the script to SageMaker by creating the  ``MXNet`` estimator object.
+
+For example, suppose that you have an MXNet training script named
+``mxnet-train.py``. You can run this script in SageMaker as follows:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+    mxnet_estimator = MXNet('mxnet-train.py',
+                            role='SageMakerRole',
+                            train_instance_type='ml.p3.2xlarge',
+                            train_instance_count=1,
+                            framework_version='1.2.1')
+    mxnet_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The constructor arguments define how SageMaker runs your training script. These arguments are explained in a later section.
+
+In this topic, we  discuss how to prepare a training script and run that script on SageMaker using an ``MXNet`` estimator.
+
+Preparing the MXNet training script
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+| WARNING                                                                                                                                                  |
++==========================================================================================================================================================+
+| The structure for training scripts changed with MXNet version 1.3.                                                                                       |
+| Make sure you refer to the correct section of this topic when you prepare your script.                                                                   |
+| For information on how to upgrade an old script to the new format, see `"Updating your MXNet training script" <#updating-your-mxnet-training-script>`__. |
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+
+For versions 1.3 and higher
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the path where the training job writes the model artifacts.
+  After training, artifacts in this directory are uploaded to S3 for model deployment.
+* ``SM_NUM_GPUS``: The number of GPUs available to the host.
+* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write training output other than model artifacts.
+  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
+  These artifacts are compressed and uploaded to an S3 bucket with the same prefix as the bucket where the model artifacts are uploaded.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the MXNet estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+* ``SM_HPS``: A json dump of the hyperparameters preserving json types (boolean, integer, etc.)
+
+For a complete list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``model_dir`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+    import json
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # an alternative way to load hyperparameters via SM_HPS environment variable.
+        parser.add_argument('--sm-hps', type=json.loads, default=os.environ['SM_HPS'])
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
+so that SageMaker does not inadvertently run your training code at the wrong point in execution.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your MXNet estimator.
+
+For more on training environment variables, please visit `SageMaker Containers <https://github.com/aws/sagemaker-containers>`_.
+
+For versions 1.2 and lower
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file. The MXNet training script must contain a function ``train``, which SageMaker invokes to run training. You can include other functions as well, but it must contain a ``train`` function.
+
+When you run your script on SageMaker via the ``MXNet`` Estimator, SageMaker injects information about the training environment into your training function via Python keyword arguments. You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
+
+-  ``hyperparameters (dict[string,string])``: The hyperparameters passed
+   to SageMaker TrainingJob that runs your MXNet training script. You
+   can use this to pass hyperparameters to your training script.
+-  ``input_data_config (dict[string,dict])``: The SageMaker TrainingJob
+   InputDataConfig object, that's set when the SageMaker TrainingJob is
+   created. This is discussed in more detail below.
+-  ``channel_input_dirs (dict[string,string])``: A collection of
+   directories containing training data. When you run training, you can
+   partition your training data into different logical "channels".
+   Depending on your problem, some common channel ideas are: "train",
+   "test", "evaluation" or "images',"labels".
+-  ``output_data_dir (str)``: A directory where your training script can
+   write data that will be moved to s3 after training is complete.
+-  ``num_gpus (int)``: The number of GPU devices available on your
+   training instance.
+-  ``num_cpus (int)``: The number of CPU devices available on your training instance.
+-  ``hosts (list[str])``: The list of host names running in the
+   SageMaker Training Job cluster.
+-  ``current_host (str)``: The name of the host executing the script.
+   When you use SageMaker for MXNet training, the script is run on each
+   host in the cluster.
+
+A training script that takes advantage of all arguments would have the following definition:
+
+.. code:: python
+
+    def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,
+              num_gpus, num_cpus, hosts, current_host):
+        pass
+
+You don't have to use all the arguments, arguments you don't care about can be ignored by including ``**kwargs``.
+
+.. code:: python
+
+    # Only work with hyperparameters and num_gpus, ignore all other hyperparameters
+    def train(hyperparameters, num_gpus, **kwargs):
+        pass
+
+**Note: Writing a training script that imports correctly**
+When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
+
+If you want to run your training script locally via the Python interpreter, look at using a ``___name__ == '__main__'`` guard, discussed in more detail here: https://stackoverflow.com/questions/419163/what-does-if-name-main-do .
+
+Updating your MXNet training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The structure for training scripts changed with MXNet version 1.3.
+The ``train`` function is no longer be required; instead the training script must be able to be run as a standalone script.
+In this way, the training script is similar to a training script you might run outside of SageMaker.
+
+There are a few steps needed to make a training script with the old format compatible with the new format.
+
+First, add a `main guard <https://docs.python.org/3/library/__main__.html>`__ (``if __name__ == '__main__':``).
+The code executed from your main guard needs to:
+
+1. Set hyperparameters and directory locations
+2. Initiate training
+3. Save the model
+
+Hyperparameters will be passed as command-line arguments to your training script.
+In addition, the container will define the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
+You can find the full list of available environment variables in the `SageMaker Containers README <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+We recommend using `an argument parser <https://docs.python.org/3.5/howto/argparse.html>`__ for this part.
+Using the ``argparse`` library as an example, the code would look something like this:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ == '__main__':
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+The code in the main guard should also take care of training and saving the model.
+This can be as simple as just calling the ``train`` and ``save`` methods used in the previous training script format:
+
+.. code:: python
+
+    if __name__ == '__main__':
+        # arg parsing (shown above) goes here
+
+        model = train(args.batch_size, args.epochs, args.learning_rate, args.train, args.test)
+        save(args.model_dir, model)
+
+Note that saving the model will no longer be done by default; this must be done by the training script.
+If you were previously relying on the default save method, you can now import one from the container:
+
+.. code:: python
+
+    from sagemaker_mxnet_container.training_utils import save
+
+    if __name__ == '__main__':
+        # arg parsing and training (shown above) goes here
+
+        save(args.model_dir, model)
+
+Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must now set ``distributions`` to the following dictionary when creating an MXNet estimator:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+
+    estimator = MXNet('path-to-distributed-training-script.py',
+                      ...,
+                      distributions={'parameter_server': {'enabled': True}})
+
+
+Using third-party libraries
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When you run your training script on SageMaker, it has access to some pre-installed third-party libraries including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``.
+For more information on the runtime environment, including specific package versions, see `SageMaker MXNet Containers <#sagemaker-mxnet-containers>`__.
+
+If there are other packages you want to use with your script, you can include a `requirements.txt <https://pip.pypa.io/en/stable/user_guide/#requirements-files>`__ file in the same directory as your training script to install other dependencies at runtime.
+
+Running an MXNet training script in SageMaker
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You run MXNet training scripts on SageMaker by creating an :class:`MXNet <sagemaker.mxnet.estimator.MXNet>` estimator.
+When you call the :meth:`fit <sagemaker.estimator.EstimatorBase.fit>` method on an ``MXNet`` estimator, a SageMaker training job with your script is started.
+The following code sample shows how you train a custom MXNet script "train.py".
+
+.. code:: python
+
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.2.1')
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+MXNet Estimators
+^^^^^^^^^^^^^^^^
+
+The `MXNet <sagemaker.mxnet.estimator.MXNet>` constructor takes both required and optional arguments.
+
+Required arguments
+''''''''''''''''''
+
+The following are required arguments to the ``MXNet`` constructor. When you create an MXNet object, you must include these in the constructor, either positionally or as keyword arguments.
+
+-  ``entry_point`` Path (absolute or relative) to the Python file which
+   should be executed as the entry point to training.
+-  ``role`` An AWS IAM role (either name or full ARN). The Amazon
+   SageMaker training jobs and APIs that create Amazon SageMaker
+   endpoints use this role to access training data and model artifacts.
+   After the endpoint is created, the inference code might use the IAM
+   role, if accessing AWS resource.
+-  ``train_instance_count`` Number of Amazon EC2 instances to use for
+   training.
+-  ``train_instance_type`` Type of EC2 instance to use for training, for
+   example, 'ml.c4.xlarge'.
+
+Optional arguments
+''''''''''''''''''
+
+The following are optional arguments. When you create an ``MXNet`` object, you can specify these as keyword arguments.
+
+-  ``source_dir`` Path (absolute or relative) to a directory with any
+   other training source code dependencies including the entry point
+   file. Structure within this directory will be preserved when training
+   on SageMaker.
+- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+        any additional libraries that will be exported to the container (default: []).
+        The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+        If the ```source_dir``` points to S3, code will be uploaded and the S3 location will be used
+        instead. Example:
+
+            The following call
+            >>> MXNet(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+            results in the following inside the container:
+
+            >>> $ ls
+
+            >>> opt/ml/code
+            >>>     ├── train.py
+            >>>     ├── common
+            >>>     └── virtual-env
+
+-  ``hyperparameters`` Hyperparameters that will be used for training.
+   Will be made accessible as a dict[str, str] to the training code on
+   SageMaker. For convenience, accepts other types besides str, but
+   str() will be called on keys and values to convert them before
+   training.
+-  ``py_version`` Python version you want to use for executing your
+   model training code.
+-  ``train_volume_size`` Size in GB of the EBS volume to use for storing
+   input data during training. Must be large enough to store training
+   data if input_mode='File' is used (which is the default).
+-  ``train_max_run`` Timeout in seconds for training, after which Amazon
+   SageMaker terminates the job regardless of its current status.
+-  ``input_mode`` The input mode that the algorithm supports. Valid
+   modes: 'File' - Amazon SageMaker copies the training dataset from the
+   s3 location to a directory in the Docker container. 'Pipe' - Amazon
+   SageMaker streams data directly from s3 to the container via a Unix
+   named pipe.
+-  ``output_path`` s3 location where you want the training result (model
+   artifacts and optional output files) saved. If not specified, results
+   are stored to a default bucket. If the bucket with the specific name
+   does not exist, the estimator creates the bucket during the fit()
+   method execution.
+-  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+   output with.
+-  ``job_name`` Name to assign for the training job that the fit()
+   method launches. If not specified, the estimator generates a default
+   job name, based on the training image name and current timestamp
+-  ``image_name`` An alternative docker image to use for training and
+   serving.  If specified, the estimator will use this image for training and
+   hosting, instead of selecting the appropriate SageMaker official image based on
+   framework_version and py_version. Refer to: `SageMaker MXNet Docker Containers
+   <#sagemaker-mxnet-docker-containers>`_ for details on what the Official images support
+   and where to find the source code to build your custom image.
+-  ``distributions`` For versions 1.3 and above only.
+   Specifies information for how to run distributed training.
+   To launch a parameter server during training, set this argument to:
+
+.. code::
+
+    {
+      'parameter_server': {
+        'enabled': True
+      }
+    }
+
+Calling fit
+^^^^^^^^^^^
+
+You start your training script by calling ``fit`` on an ``MXNet`` Estimator. ``fit`` takes both required and optional arguments.
+
+Required argument
+'''''''''''''''''
+
+-  ``inputs``: This can take one of the following forms: A string
+   s3 URI, for example ``s3://my-bucket/my-training-data``. In this
+   case, the s3 objects rooted at the ``my-training-data`` prefix will
+   be available in the default ``training`` channel. A dict from
+   string channel names to s3 URIs. In this case, the objects rooted at
+   each s3 prefix will available as files in each channel directory.
+
+For example:
+
+.. code:: python
+
+    {'train':'s3://my-bucket/my-training-data',
+     'eval':'s3://my-bucket/my-evaluation-data'}
+
+.. optional-arguments-1:
+
+Optional arguments
+''''''''''''''''''
+
+-  ``wait``: Defaults to True, whether to block and wait for the
+   training script to complete before returning.
+-  ``logs``: Defaults to True, whether to show logs produced by training
+   job in the Python session. Only meaningful when wait is True.
+
+Saving models
+~~~~~~~~~~~~~
+
+When you run MXNet training, you often want to save or manipulate the models that MXNet produces. SageMaker estimators provide several ways to save MXNet models. The method used is driven by functions you define on your training script, run via the ``MXNet`` Estimator in SageMaker in response to ``fit``.
+
+Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script. If your script includes a ``save`` function, SageMaker will invoke it with the return-value of ``train``. Model saving is a two-step process, firstly you return the model you want to save from
+``train``, then you define your model-serialization logic in ``save``.
+
+SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects. If your training script does not define a ``save`` function, then the default ``save`` function will be invoked on the return-value of your ``train`` function.
+
+The following script demonstrates how to return a model from train, that's compatible with the default ``save`` function.
+
+.. code:: python
+
+    import mxnet as mx
+
+    def create_graph():
+        # Code to create graph omitted for brevity
+
+    def train(num_gpus, channel_input_dirs, **kwargs):
+        ctx = mx.cpu() if not num_gpus else [mx.gpu(i) for i in range(num_gpus)]
+        sym = create_graph()
+        mod = mx.mod.Module(symbol=sym, context=ctx)
+
+        # Code to fit mod omitted for brevity
+        # ...
+
+        # Return the Module object. SageMaker will save this.
+        return mod
+
+If you define your own ``save`` function, it should have the following signature:
+
+.. code:: python
+
+    def save(model, model_dir)
+
+Where ``model`` is the return-value from ``train`` and ``model_dir`` is the directory SageMaker requires you to save your model. If you write files into ``model_dir`` then they will be persisted to s3 after the SageMaker Training Job completes.
+
+After your training job is complete, your model data will available in the s3 ``output_path`` you specified when you created the MXNet Estimator. Handling of s3 output is discussed in: `Accessing SageMaker output and model data in s3 <#accessing%20-sagemaker-output-and-model-data-in-s3>`__.
+
+MXNet Module serialization in SageMaker
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+If you train function returns a ``Module`` object, it will be serialized by the default Module serialization system, unless you've specified a custom ``save`` function.
+
+The default serialization system generates three files:
+
+-  ``model-shapes.json``: A json list, containing a serialization of the
+   ``Module`` ``data_shapes`` property. Each object in the list contains
+   the serialization of one ``DataShape`` in the returned ``Module``.
+   Each object has a ``name`` property, containing the ``DataShape``
+   name and a ``shape`` property, which is a list of that dimensions for
+   the shape of that ``DataShape``. For example:
+
+.. code:: javascript
+
+    [
+        {"name":"images", "shape":[100, 1, 28, 28]},
+        {"name":"labels", "shape":[100, 1]}
+    ]
+
+-  ``model-symbol.json``: The MXNet ``Module`` ``Symbol`` serialization,
+   produced by invoking ``save`` on the ``symbol`` property of the
+   ``Module`` being saved.
+-  ``modle.params``: The MXNet ``Module`` parameters. Produced by
+   invoking ``save_params`` on the ``Module`` being saved.
+
+Writing a custom save function
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can provide your own save function. This is useful if you are not working with the ``Module`` API or you need special processing.
+
+To provide your own save function, define a ``save`` function in your training script. The function should take two arguments:
+
+-  model: This is the object that was returned from your ``train``
+   function. If your ``train`` function does not return an object, it
+   will be ``None``. You are free to return an object of any type from
+   ``train``, you do not have to return ``Module`` or ``Gluon`` API
+   specific objects.
+-  model_dir: This is the string path on the SageMaker training host
+   where you save your model. Files created in this directory will be
+   accessible in S3 after your SageMaker Training Job completes.
+
+After your ``train`` function completes, SageMaker will invoke ``save`` with the object returned from ``train``.
+
+**Note: How to save Gluon models with SageMaker**
+
+If your train function returns a Gluon API ``net`` object as its model, you'll need to write your own ``save`` function. You will want to serialize the ``net`` parameters. Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
+
+Deploying MXNet models
+~~~~~~~~~~~~~~~~~~~~~~
+
+After an MXNet Estimator has been fit, you can host the newly created model in SageMaker.
+
+After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to create a SageMaker Endpoint. The Endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script, which was run when you called ``fit``. This was the model object you returned from ``train`` and saved with either a custom save function or the default save function.
+
+``deploy`` returns a ``Predictor`` object, which you can use to do inference on the Endpoint hosting your MXNet model. Each ``Predictor`` provides a ``predict`` method which can do inference with numpy arrays or Python lists. Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
+
+``predict`` returns the result of inference against your model. By default, the inference result is either a Python list or dictionary.
+
+.. code:: python
+
+    # Train my estimator
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.2.1')
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+    # Deploy my estimator to a SageMaker Endpoint and get a Predictor
+    predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                       initial_instance_count=1)
+
+You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
+
+.. code:: python
+
+  predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                     initial_instance_count=1,
+                                     accelerator_type='ml.eia1.medium')
+
+The SageMaker MXNet Model Server
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model server. The model server loads the model that was saved by your training script and performs inference on the model in response to SageMaker InvokeEndpoint API calls.
+
+You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
+
+As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
+
+Model loading
+^^^^^^^^^^^^^
+
+Before a model can be served, it must be loaded. The SageMaker model server loads your model by invoking a ``model_fn`` function on your training script. If you don't provide a ``model_fn`` function, SageMaker will use a default ``model_fn`` function. The default function works with MXNet Module model objects, saved via the default ``save`` function.
+
+If you wrote a custom ``save`` function then you may need to write a custom ``model_fn`` function. If your save function serializes ``Module`` objects under the same format as the default ``save`` function, then you won't need to write a custom model_fn function. If you do write a ``model_fn`` function must have the following signature:
+
+.. code:: python
+
+    def model_fn(model_dir)
+
+SageMaker will inject the directory where your model files and sub-directories, saved by ``save``, have been mounted. Your model function should return a model object that can be used for model serving. SageMaker provides automated serving functions that work with Gluon API ``net`` objects and Module API ``Module`` objects. If you return either of these types of objects, then you will be able to use the default serving request handling functions.
+
+The following code-snippet shows an example custom ``model_fn`` implementation. This loads returns an MXNet Gluon net model for resnet-34 inference. It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
+        return net
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
+
+Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
+        return net
+
+The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93m[93maabf018d906ed282a3c738377d19a8de[0m[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
+
+For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
+
+Model serving
+^^^^^^^^^^^^^
+
+After the SageMaker model server has loaded your model, by calling either the default ``model_fn`` or the implementation in your training script, SageMaker will serve your model. Model serving is the process of responding to inference requests, received by SageMaker InvokeEndpoint API calls. The SageMaker MXNet model server breaks request handling into three steps:
+
+
+-  input processing,
+-  prediction, and
+-  output processing.
+
+In a similar way to previous steps, you configure these steps by defining functions in your Python source file.
+
+Each step involves invoking a python function, with information about the request and the return-value from the previous function in the chain. Inside the SageMaker MXNet model server, the process looks like:
+
+.. code:: python
+
+    # Deserialize the Invoke request body into an object we can perform prediction on
+    input_object = input_fn(request_body, request_content_type, model)
+
+    # Perform prediction on the deserialized object, with the loaded model
+    prediction = predict_fn(input_object, model)
+
+    # Serialize the prediction result into the desired response content type
+    ouput = output_fn(prediction, response_content_type)
+
+The above code-sample shows the three function definitions:
+
+-  ``input_fn``: Takes request data and deserializes the data into an
+   object for prediction.
+-  ``predict_fn``: Takes the deserialized request object and performs
+   inference against the loaded model.
+-  ``output_fn``: Takes the result of prediction and serializes this
+   according to the response content type.
+
+The SageMaker MXNet model server provides default implementations of these functions. These work with common-content types, and Gluon API and Module API model objects. You can provide your own implementations for these functions in your training script. If you omit any definition then the SageMaker MXNet model server will use its default implementation for that function.
+
+If you rely solely on the SageMaker MXNet model server defaults, you get the following functionality:
+
+-  Prediction on MXNet Gluon API ``net`` and Module API ``Module``
+   objects.
+-  Deserialization from CSV and JSON to NDArrayIters.
+-  Serialization of NDArrayIters to CSV or JSON.
+
+In the following sections we describe the default implementations of input_fn, predict_fn, and output_fn. We describe the input arguments and expected return types of each, so you can define your own implementations.
+
+Input processing
+''''''''''''''''
+
+When an InvokeEndpoint operation is made against an Endpoint running a SageMaker MXNet model server, the model server receives two pieces of information:
+
+-  The request Content-Type, for example "application/json"
+-  The request data body, a byte array
+
+The SageMaker MXNet model server will invoke an "input_fn" function in your training script, passing in this information. If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
+
+.. code:: python
+
+    def input_fn(request_body, request_content_type, model)
+
+Where ``request_body`` is a byte buffer, ``request_content_type`` is a Python string, and model is the result of invoking ``model_fn``.
+
+The SageMaker MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
+
+Default json deserialization requires ``request_body`` contain a single json list. Sending multiple json objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
+
+Default csv deserialization requires ``request_body`` contain one or more lines of CSV numerical data. The data is loaded into a two-dimensional array, where each line break defines the boundaries of the first dimension. This two-dimensional array is then re-shaped to be compatible with the shape expected by the model object. Specifically, the first dimension is kept unchanged, but the second dimension is reshaped to be consistent with the shape of all dimensions in the model, following the first dimension.
+
+If you provide your own implementation of input_fn, you should abide by the ``input_fn`` signature. If you want to use this with the default
+``predict_fn``, then you should return an NDArrayIter. The NDArrayIter should have a shape identical to the shape of the model being predicted on. The example below shows a custom ``input_fn`` for preparing pickled numpy arrays.
+
+.. code:: python
+
+    import numpy as np
+    import mxnet as mx
+
+    def input_fn(request_body, request_content_type, model):
+        """An input_fn that loads a pickled numpy array"""
+        if request_content_type == "application/python-pickle":
+            array = np.load(StringIO(request_body))
+            array.reshape(model.data_shpaes[0])
+            return mx.io.NDArrayIter(mx.ndarray(array))
+        else:
+            # Handle other content-types here or raise an Exception
+            # if the content type is not supported.
+            pass
+
+Prediction
+''''''''''
+
+After the inference request has been deserialized by ``input_fn``, the SageMaker MXNet model server invokes ``predict_fn``. As with ``input_fn``, you can define your own ``predict_fn`` or use the SageMaker Mxnet default.
+
+The ``predict_fn`` function has the following signature:
+
+.. code:: python
+
+    def predict_fn(input_object, model)
+
+Where ``input_object`` is the object returned from ``input_fn`` and
+``model`` is the model loaded by ``model_fn``.
+
+The default implementation of ``predict_fn`` requires ``input_object`` be an ``NDArrayIter``, which is the return-type of the default
+``input_fn``. It also requires that ``model`` be either an MXNet Gluon API ``net`` object or a Module API ``Module`` object.
+
+The default implementation performs inference with the input
+``NDArrayIter`` on the Gluon or Module object. If the model is a Gluon
+``net`` it performs: ``net.forward(input_object)``. If the model is a Module object it performs ``module.predict(input_object)``. In both cases, it returns the result of that call.
+
+If you implement your own prediction function, you should take care to ensure that:
+
+-  The first argument is expected to be the return value from input_fn.
+   If you use the default input_fn, this will be an ``NDArrayIter``.
+-  The second argument is the loaded model. If you use the default
+   ``model_fn`` implementation, this will be an MXNet Module object.
+   Otherwise, it will be the return value of your ``model_fn``
+   implementation.
+-  The return value should be of the correct type to be passed as the
+   first argument to ``output_fn``. If you use the default
+   ``output_fn``, this should be an ``NDArrayIter``.
+
+Output processing
+'''''''''''''''''
+
+After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return-value from ``predict_fn`` and the InvokeEndpoint requested response content-type.
+
+The ``output_fn`` has the following signature:
+
+.. code:: python
+
+    def output_fn(prediction, content_type)
+
+Where ``prediction`` is the result of invoking ``predict_fn`` and
+``content_type`` is the InvokeEndpoint requested response content-type. The function should return a byte array of data serialized to content_type.
+
+The default implementation expects ``prediction`` to be an ``NDArray`` and can serialize the result to either JSON or CSV. It accepts response content types of "application/json" and "text/csv".
+
+Distributed MXNet training
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You can run a multi-machine, distributed MXNet training using the MXNet Estimator. By default, MXNet objects will submit single-machine training jobs to SageMaker. If you set ``train_instance_count`` to be greater than one, multi-machine training jobs will be launched when ``fit`` is called. When you run multi-machine training, SageMaker will import your training script and invoke ``train`` on each host in the cluster.
+
+When you develop MXNet distributed learning algorithms, you often want to use an MXNet kvstore to store and share model parameters. To learn more about writing distributed MXNet programs, please see `Distributed Training <http://newdocs.readthedocs.io/en/latest/distributed_training.html>`__ in the MXNet docs.
+
+When using an MXNet Estimator, SageMaker automatically starts MXNet kvstore server and scheduler processes on hosts in your training job cluster. Your script runs as an MXNet worker task. SageMaker runs one server process on each host in your cluster. One host is selected arbitrarily to run the scheduler process.
+
+Working with existing model data and training jobs
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Attaching to existing training jobs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+You can attach an MXNet Estimator to an existing training job using the
+``attach`` method.
+
+.. code:: python
+
+    my_training_job_name = "MyAwesomeMXNetTrainingJob"
+    mxnet_estimator = MXNet.attach(my_training_job_name)
+
+After attaching, if the training job is in a Complete status, it can be
+``deploy``\ ed to create a SageMaker Endpoint and return a
+``Predictor``. If the training job is in progress, attach will block and display log messages from the training job, until the training job completes.
+
+The ``attach`` method accepts the following arguments:
+
+-  ``training_job_name (str):`` The name of the training job to attach
+   to.
+-  ``sagemaker_session (sagemaker.Session or None):`` The Session used
+   to interact with SageMaker
+
+Deploying Endpoints from model data
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+As well as attaching to existing training jobs, you can deploy models directly from model data in S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
+
+.. code:: python
+
+    mxnet_model = MXNetModel(model_data="s3://bucket/model.tar.gz", role="SageMakerRole", entry_point="trasform_script.py")
+
+    predictor = mxnet_model.deploy(instance_type="ml.c4.xlarge", initial_instance_count=1)
+
+The MXNetModel constructor takes the following arguments:
+
+-  ``model_data (str):`` An S3 location of a SageMaker model data
+   .tar.gz file
+-  ``image (str):`` A Docker image URI
+-  ``role (str):`` An IAM role name or Arn for SageMaker to access AWS
+   resources on your behalf.
+-  ``predictor_cls (callable[string,sagemaker.Session]):`` A function to
+   call to create a predictor. If not None, ``deploy`` will return the
+   result of invoking this function on the created endpoint name
+-  ``env (dict[string,string]):`` Environment variables to run with
+   ``image`` when hosted in SageMaker.
+-  ``name (str):`` The model name. If None, a default model name will be
+   selected on each ``deploy.``
+-  ``entry_point (str):`` Path (absolute or relative) to the Python file
+   which should be executed as the entry point to model hosting.
+-  ``source_dir (str):`` Optional. Path (absolute or relative) to a
+   directory with any other training source code dependencies including
+   tne entry point file. Structure within this directory will be
+   preserved when training on SageMaker.
+-  ``container_log_level (int):`` Log level to use within the container.
+   Valid values are defined in the Python logging module.
+-  ``code_location (str):`` Optional. Name of the S3 bucket where your
+   custom code will be uploaded to. If not specified, will use the
+   SageMaker default bucket created by sagemaker.Session.
+-  ``sagemaker_session (sagemaker.Session):`` The SageMaker Session
+   object, used for SageMaker interaction"""
+
+Your model data must be a .tar.gz file in S3. SageMaker Training Job model data is saved to .tar.gz files in S3, however if you have local data you want to deploy, you can prepare the data yourself.
+
+Assuming you have a local directory containg your model data named "my_model" you can tar and gzip compress the file and upload to S3 using the following commands:
+
+::
+
+    tar -czf model.tar.gz my_model
+    aws s3 cp model.tar.gz s3://my-bucket/my-path/model.tar.gz
+
+This uploads the contents of my_model to a gzip compressed tar file to S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
+
+To run this command, you'll need the aws cli tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
+
+MXNet Training Examples
+~~~~~~~~~~~~~~~~~~~~~~~
+
+Amazon provides several example Jupyter notebooks that demonstrate end-to-end training on Amazon SageMaker using MXNet. Please refer to:
+
+https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk
+
+These are also available in SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
+
+SageMaker MXNet Containers
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+When training and deploying training scripts, SageMaker runs your Python script in a Docker container with several libraries installed. When creating the Estimator and calling deploy to create the SageMaker Endpoint, you can control the environment your script runs in.
+
+SageMaker runs MXNet Estimator scripts in either Python 2.7 or Python 3.5. You can select the Python version by passing a ``py_version`` keyword arg to the MXNet Estimator constructor. Setting this to ``py2`` (the default) will cause your training script to be run on Python 2.7. Setting this to ``py3`` will cause your training script to be run on Python 3.5. This Python version applies to both the Training Job, created by fit, and the Endpoint, created by deploy.
+
+Your MXNet training script will be run on version 1.2.1 by default. (See below for how to choose a different version, and currently supported versions.) The decision to use the GPU or CPU version of MXNet is made by the ``train_instance_type``, set on the MXNet constructor. If you choose a GPU instance type, your training job will be run on a GPU version of MXNet. If you choose a CPU instance type, your training job will be run on a CPU version of MXNet. Similarly, when you call deploy, specifying a GPU or CPU deploy_instance_type, will control which MXNet build your Endpoint runs.
+
+The Docker images have the following dependencies installed:
+
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| Dependencies            | MXNet 0.12.1 | MXNet 1.0.0 | MXNet 1.1.0 | MXNet 1.2.1 | MXNet 1.3.0 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| Python                  |   2.7 or 3.5 |   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| CUDA (GPU image only)   |          9.0 |         9.0 |         9.0 |         9.0 |         9.0 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| numpy                   |       1.13.3 |      1.13.3 |      1.13.3 |      1.14.5 |      1.14.6 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| onnx                    |          N/A |         N/A |         N/A |       1.2.1 |       1.2.1 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+| keras-mxnet             |          N/A |         N/A |         N/A |         N/A |       2.2.2 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+
+
+The Docker images extend Ubuntu 16.04.
+
+You can select version of MXNet by passing a ``framework_version`` keyword arg to the MXNet Estimator constructor. Currently supported versions are listed in the above table. You can also set ``framework_version`` to only specify major and minor version, e.g ``1.2``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.2.1.
+Alternatively, you can build your own image by following the instructions in the SageMaker MXNet containers repository, and passing ``image_name`` to the MXNet Estimator constructor.
+
+You can visit the SageMaker MXNet containers repository here: https://github.com/aws/sagemaker-mxnet-containers/
\ No newline at end of file

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-01-31 14:03:24[0m
[92mHash: 4092b67977178913a7dcafb87a2494eef70d5429[0m
[92mFilepath: doc/using_mxnet.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Update EI section in regards to model loading within MXNet README (#618)

[0m
@@ -1,11 +1,8 @@
 Using MXNet Estimators and Models
 ---------------------------------
 
------------------
-Table of Contents
------------------
 .. contents::
-    :local:
+
 
 
 With MXNet estimators, you can train and host MXNet models on Amazon SageMaker.
@@ -533,31 +530,11 @@ The following code-snippet shows an example custom ``model_fn`` implementation.
         """
         Load the gluon model. Called once when hosting service starts.
         :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
-        return net
-
-MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
-
-Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
-
-.. code:: python
-
-    def model_fn(model_dir):
-        """
-        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
-        :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
-        return net
-
-The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93maabf018d906ed282a3c738377d19a8de[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
-
-For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
+            :return: a model (in this case a Gluon network)
+            """
+            net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
+            net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
+            return net
 
 Model serving
 ^^^^^^^^^^^^^

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-01-31 14:03:24[0m
[92mHash: 4092b67977178913a7dcafb87a2494eef70d5429[0m
[92mFilepath: src/sagemaker/mxnet/README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Update EI section in regards to model loading within MXNet README (#618)

[0m
@@ -487,7 +487,7 @@ After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to crea
 
 You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
 
-MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
 
 .. code:: python
 
@@ -525,31 +525,11 @@ The following code-snippet shows an example custom ``model_fn`` implementation.
         """
         Load the gluon model. Called once when hosting service starts.
         :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
-        return net
-
-MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
-
-Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
-
-.. code:: python
-
-    def model_fn(model_dir):
-        """
-        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
-        :param: model_dir The directory where model files are stored.
-        :return: a model (in this case a Gluon network)
-        """
-        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
-        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
-        return net
-
-The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93maabf018d906ed282a3c738377d19a8de[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
-
-For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
+            :return: a model (in this case a Gluon network)
+            """
+            net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
+            net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
+            return net
 
 Model serving
 ^^^^^^^^^^^^^

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-12-08 12:28:02[0m
[92mHash: 9cf38de523b2f9d3f91b469f6e4e315ca48f6255[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/master[0m
[92mCommit: Add AWS Marketplace Integration Tests (#526)

[0m
@@ -1,215 +0,0 @@
-# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-
-import itertools
-import os
-import time
-
-import pandas
-
-import sagemaker
-from sagemaker import AlgorithmEstimator, ModelPackage
-from sagemaker.tuner import IntegerParameter, HyperparameterTuner
-from sagemaker.utils import sagemaker_timestamp
-from tests.integ import DATA_DIR
-from tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name
-
-
-# All these tests require a manual 1 time subscription to the following Marketplace items:
-# Algorithm: Scikit Decision Trees
-# https://aws.amazon.com/marketplace/pp/prodview-ha4f3kqugba3u
-#
-# Pre-Trained Model: Scikit Decision Trees - Pretrained Model
-# https://aws.amazon.com/marketplace/pp/prodview-7qop4x5ahrdhe
-#
-# Both are  written by Amazon and are free to subscribe.
-
-ALGORITHM_ARN = 'arn:aws:sagemaker:%s:594846645681:algorithm/scikit-decision-trees-' \
-                '15423055-[93m57b73412d2e93e9239e4e16f83298b8f[0m'
-
-MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:594846645681:model-package/scikit-iris-detector-' \
-                    '154230595-[93m8f00905c1f927a512b73ea29dd09ae30[0m'
-
-
-def test_marketplace_estimator(sagemaker_session):
-    with timeout(minutes=15):
-        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-
-        algo = AlgorithmEstimator(
-            algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
-            role='SageMakerRole',
-            train_instance_count=1,
-            train_instance_type='ml.c4.xlarge',
-            sagemaker_session=sagemaker_session)
-
-        train_input = algo.sagemaker_session.upload_data(
-            path=data_path, key_prefix='integ-test-data/marketplace/train')
-
-        algo.fit({'training': train_input})
-
-    endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
-    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        predictor = algo.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
-        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
-
-        a = [50 * i for i in range(3)]
-        b = [40 + i for i in range(10)]
-        indices = [i + j for i, j in itertools.product(a, b)]
-
-        test_data = shape.iloc[indices[:-1]]
-        test_x = test_data.iloc[:, 1:]
-
-        print(predictor.predict(test_x.values).decode('utf-8'))
-
-
-def test_marketplace_attach(sagemaker_session):
-    with timeout(minutes=15):
-        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-
-        mktplace = AlgorithmEstimator(
-            algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
-            role='SageMakerRole',
-            train_instance_count=1,
-            train_instance_type='ml.c4.xlarge',
-            sagemaker_session=sagemaker_session,
-            base_job_name='test-marketplace')
-
-        train_input = mktplace.sagemaker_session.upload_data(
-            path=data_path, key_prefix='integ-test-data/marketplace/train')
-
-        mktplace.fit({'training': train_input}, wait=False)
-        training_job_name = mktplace.latest_training_job.name
-
-        print('Waiting to re-attach to the training job: %s' % training_job_name)
-        time.sleep(20)
-        endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
-
-    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        print('Re-attaching now to: %s' % training_job_name)
-        estimator = AlgorithmEstimator.attach(training_job_name=training_job_name,
-                                              sagemaker_session=sagemaker_session)
-        predictor = estimator.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name,
-                                     serializer=sagemaker.predictor.csv_serializer)
-        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
-        a = [50 * i for i in range(3)]
-        b = [40 + i for i in range(10)]
-        indices = [i + j for i, j in itertools.product(a, b)]
-
-        test_data = shape.iloc[indices[:-1]]
-        test_x = test_data.iloc[:, 1:]
-
-        print(predictor.predict(test_x.values).decode('utf-8'))
-
-
-def test_marketplace_model(sagemaker_session):
-
-    def predict_wrapper(endpoint, session):
-        return sagemaker.RealTimePredictor(
-            endpoint, session, serializer=sagemaker.predictor.csv_serializer
-        )
-
-    model = ModelPackage(role='SageMakerRole',
-                         model_package_arn=(MODEL_PACKAGE_ARN % sagemaker_session.boto_region_name),
-                         sagemaker_session=sagemaker_session,
-                         predictor_cls=predict_wrapper)
-
-    endpoint_name = 'test-marketplace-model-endpoint{}'.format(sagemaker_timestamp())
-    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
-        predictor = model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
-        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
-        a = [50 * i for i in range(3)]
-        b = [40 + i for i in range(10)]
-        indices = [i + j for i, j in itertools.product(a, b)]
-
-        test_data = shape.iloc[indices[:-1]]
-        test_x = test_data.iloc[:, 1:]
-
-        print(predictor.predict(test_x.values).decode('utf-8'))
-
-
-def test_marketplace_tuning_job(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-
-    mktplace = AlgorithmEstimator(
-        algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
-        role='SageMakerRole',
-        train_instance_count=1,
-        train_instance_type='ml.c4.xlarge',
-        sagemaker_session=sagemaker_session,
-        base_job_name='test-marketplace')
-
-    train_input = mktplace.sagemaker_session.upload_data(
-        path=data_path, key_prefix='integ-test-data/marketplace/train')
-
-    mktplace.set_hyperparameters(max_leaf_nodes=10)
-
-    hyperparameter_ranges = {'max_leaf_nodes': IntegerParameter(1, 100000)}
-
-    tuner = HyperparameterTuner(estimator=mktplace, base_tuning_job_name='byo',
-                                objective_metric_name='validation:accuracy',
-                                hyperparameter_ranges=hyperparameter_ranges,
-                                max_jobs=2, max_parallel_jobs=2)
-
-    tuner.fit({'training': train_input}, include_cls_metadata=False)
-    time.sleep(15)
-    tuner.wait()
-
-
-def test_marketplace_transform_job(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-
-    algo = AlgorithmEstimator(
-        algorithm_arn=(ALGORITHM_ARN % sagemaker_session.boto_region_name),
-        role='SageMakerRole',
-        train_instance_count=1,
-        train_instance_type='ml.c4.xlarge',
-        sagemaker_session=sagemaker_session,
-        base_job_name='test-marketplace')
-
-    train_input = algo.sagemaker_session.upload_data(
-        path=data_path, key_prefix='integ-test-data/marketplace/train')
-
-    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
-
-    transform_workdir = DATA_DIR + '/marketplace/transform'
-    shape.to_csv(transform_workdir + '/batchtransform_test.csv', index=False, header=False)
-    transform_input = algo.sagemaker_session.upload_data(
-        transform_workdir,
-        key_prefix='integ-test-data/marketplace/transform')
-
-    algo.fit({'training': train_input})
-
-    transformer = algo.transformer(1, 'ml.m4.xlarge')
-    transformer.transform(transform_input, content_type='text/csv')
-    transformer.wait()
-
-
-def test_marketplace_transform_job_from_model_package(sagemaker_session):
-    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
-    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
-
-    TRANSFORM_WORKDIR = DATA_DIR + '/marketplace/transform'
-    shape.to_csv(TRANSFORM_WORKDIR + '/batchtransform_test.csv', index=False, header=False)
-    transform_input = sagemaker_session.upload_data(
-        TRANSFORM_WORKDIR,
-        key_prefix='integ-test-data/marketplace/transform')
-
-    model = ModelPackage(role='SageMakerRole',
-                         model_package_arn=(MODEL_PACKAGE_ARN % sagemaker_session.boto_region_name),
-                         sagemaker_session=sagemaker_session)
-
-    transformer = model.transformer(1, 'ml.m4.xlarge')
-    transformer.transform(transform_input, content_type='text/csv')
-    transformer.wait()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-11-27 10:14:40[0m
[92mHash: 17820eae966de013731720702bcdea60e1c56c01[0m
[92mFilepath: tests/unit/test_algorithm.py[0m
[92mBranch: origin/master[0m
[92mCommit: add AlgorithmEstimator and ModelPackage support
[0m
@@ -1,896 +0,0 @@
-# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"). You
-# may not use this file except in compliance with the License. A copy of
-# the License is located at
-#
-#     http://aws.amazon.com/apache2.0/
-#
-# or in the "license" file accompanying this file. This file is
-# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
-# ANY KIND, either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-from __future__ import absolute_import
-
-import copy
-import datetime
-
-import pytest
-from mock import Mock, patch
-
-from sagemaker.algorithm import AlgorithmEstimator
-from sagemaker.estimator import _TrainingJob
-from sagemaker.transformer import Transformer
-
-DESCRIBE_ALGORITHM_RESPONSE = {
-    'AlgorithmName': 'scikit-decision-trees',
-    'AlgorithmArn': 'arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-    'AlgorithmDescription': 'Decision trees using Scikit',
-    'CreationTime': datetime.datetime(2018, 8, 3, 22, 44, 54, 437000),
-    'TrainingSpecification': {
-        'TrainingImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:12345',
-        'TrainingImageDigest': 'sha256:[93m206854b6ea2f0020d216311da732010515169820b898ec29720bcf1d2b46806a[0m',
-        'SupportedHyperParameters': [
-            {
-                'Name': 'max_leaf_nodes',
-                'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
-                'Type': 'Integer',
-                'Range': {
-                    'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
-                },
-                'IsTunable': True,
-                'IsRequired': False,
-                'DefaultValue': '100',
-            },
-            {
-                'Name': 'free_text_hp1',
-                'Description': 'You can write anything here',
-                'Type': 'FreeText',
-                'IsTunable': False,
-                'IsRequired': True
-            }
-        ],
-        'SupportedTrainingInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge', 'ml.m4.4xlarge'],
-        'SupportsDistributedTraining': False,
-        'MetricDefinitions': [
-            {'Name': 'validation:accuracy', 'Regex': 'validation-accuracy: (\\S+)'}
-        ],
-        'TrainingChannels': [
-            {
-                'Name': 'training',
-                'Description': 'Input channel that provides training data',
-                'IsRequired': True,
-                'SupportedContentTypes': ['text/csv'],
-                'SupportedCompressionTypes': ['None'],
-                'SupportedInputModes': ['File'],
-            }
-        ],
-        'SupportedTuningJobObjectiveMetrics': [
-            {'Type': 'Maximize', 'MetricName': 'validation:accuracy'}
-        ],
-    },
-    'InferenceSpecification': {
-        'InferenceImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:123',
-        'SupportedTransformInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge'],
-        'SupportedContentTypes': ['text/csv'],
-        'SupportedResponseMIMETypes': ['text'],
-    },
-    'ValidationSpecification': {
-        'ValidationRole': 'arn:aws:iam::764419575721:role/SageMakerRole',
-        'ValidationProfiles': [
-            {
-                'ProfileName': 'ValidationProfile1',
-                'TrainingJobDefinition': {
-                    'TrainingInputMode': 'File',
-                    'HyperParameters': {},
-                    'InputDataConfig': [
-                        {
-                            'ChannelName': 'training',
-                            'DataSource': {
-                                'S3DataSource': {
-                                    'S3DataType': 'S3Prefix',
-                                    'S3Uri': 's3://sagemaker-us-east-2-7123/-scikit-byo-iris/training-input-data',
-                                    'S3DataDistributionType': 'FullyReplicated',
-                                }
-                            },
-                            'ContentType': 'text/csv',
-                            'CompressionType': 'None',
-                            'RecordWrapperType': 'None',
-                        }
-                    ],
-                    'OutputDataConfig': {
-                        'KmsKeyId': '',
-                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/training-output',
-                    },
-                    'ResourceConfig': {
-                        'InstanceType': 'ml.c4.xlarge',
-                        'InstanceCount': 1,
-                        'VolumeSizeInGB': 10,
-                    },
-                    'StoppingCondition': {'MaxRuntimeInSeconds': 3600},
-                },
-                'TransformJobDefinition': {
-                    'MaxConcurrentTransforms': 0,
-                    'MaxPayloadInMB': 0,
-                    'TransformInput': {
-                        'DataSource': {
-                            'S3DataSource': {
-                                'S3DataType': 'S3Prefix',
-                                'S3Uri': 's3://sagemaker-us-east-2/scikit-byo-iris/batch-inference/transform_test.csv',
-                            }
-                        },
-                        'ContentType': 'text/csv',
-                        'CompressionType': 'None',
-                        'SplitType': 'Line',
-                    },
-                    'TransformOutput': {
-                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/scikit-byo-iris/batch-transform-output',
-                        'Accept': 'text/csv',
-                        'AssembleWith': 'Line',
-                        'KmsKeyId': '',
-                    },
-                    'TransformResources': {'InstanceType': 'ml.c4.xlarge', 'InstanceCount': 1},
-                },
-            }
-        ],
-        'ValidationOutputS3Prefix': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/validation-output',
-        'ValidateForMarketplace': True,
-    },
-    'AlgorithmStatus': 'Completed',
-    'AlgorithmStatusDetails': {
-        'ValidationStatuses': [{'ProfileName': 'ValidationProfile1', 'Status': 'Completed'}]
-    },
-    'ResponseMetadata': {
-        'RequestId': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
-        'HTTPStatusCode': 200,
-        'HTTPHeaders': {
-            'x-amzn-requestid': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
-            'content-type': 'application/x-amz-json-1.1',
-            'content-length': '3949',
-            'date': 'Fri, 03 Aug 2018 23:08:43 GMT',
-        },
-        'RetryAttempts': 0,
-    },
-}
-
-
-def test_algorithm_supported_input_mode_with_valid_input_types(sagemaker_session):
-    # verify that the Estimator verifies the
-    # input mode that an Algorithm supports.
-
-    file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=file_mode_algo)
-
-    # Creating a File mode Estimator with a File mode algorithm should work
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['Pipe'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=pipe_mode_algo)
-
-    # Creating a Pipe mode Estimator with a Pipe mode algorithm should work.
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        input_mode='Pipe',
-        sagemaker_session=sagemaker_session,
-    )
-
-    any_input_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    any_input_algo['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=any_input_algo)
-
-    # Creating a File mode Estimator with an algorithm that supports both input modes
-    # should work.
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-
-def test_algorithm_supported_input_mode_with_bad_input_types(sagemaker_session):
-    # verify that the Estimator verifies raises exceptions when
-    # attempting to train with an incorrect input type
-
-    file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=file_mode_algo)
-
-    # Creating a Pipe mode Estimator with a File mode algorithm should fail.
-    with pytest.raises(ValueError):
-        AlgorithmEstimator(
-            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-            role='SageMakerRole',
-            train_instance_type='ml.m4.xlarge',
-            train_instance_count=1,
-            input_mode='Pipe',
-            sagemaker_session=sagemaker_session,
-        )
-
-    pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['Pipe'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File', 'Pipe'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=pipe_mode_algo)
-
-    # Creating a File mode Estimator with a Pipe mode algorithm should fail.
-    with pytest.raises(ValueError):
-        AlgorithmEstimator(
-            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-            role='SageMakerRole',
-            train_instance_type='ml.m4.xlarge',
-            train_instance_count=1,
-            sagemaker_session=sagemaker_session,
-        )
-
-
-@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
-def test_algorithm_trainining_channels_with_expected_channels(sagemaker_session):
-    training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-
-    training_channels['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    # Pass training and validation channels. This should work
-    estimator.fit({'training': 's3://some/place', 'validation': 's3://some/other'})
-
-    # Passing only the training channel. Validation is optional so this should also work.
-    estimator.fit({'training': 's3://some/place'})
-
-
-@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
-def test_algorithm_trainining_channels_with_invalid_channels(sagemaker_session):
-    training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-
-    training_channels['TrainingSpecification']['TrainingChannels'] = [
-        {
-            'Name': 'training',
-            'Description': 'Input channel that provides training data',
-            'IsRequired': True,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-        {
-            'Name': 'validation',
-            'Description': 'Input channel that provides validation data',
-            'IsRequired': False,
-            'SupportedContentTypes': ['text/csv'],
-            'SupportedCompressionTypes': ['None'],
-            'SupportedInputModes': ['File'],
-        },
-    ]
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    # Passing only validation should fail as training is required.
-    with pytest.raises(ValueError):
-        estimator.fit({'validation': 's3://some/thing'})
-
-    # Passing an unknown channel should fail???
-    with pytest.raises(ValueError):
-        estimator.fit({'training': 's3://some/data', 'training2': 's3://some/other/data'})
-
-
-def test_algorithm_train_instance_types_valid_instance_types(sagemaker_session):
-    describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
-
-    describe_algo_response['TrainingSpecification'][
-        'SupportedTrainingInstanceTypes'
-    ] = train_instance_types
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=describe_algo_response
-    )
-
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m5.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-
-def test_algorithm_train_instance_types_invalid_instance_types(sagemaker_session):
-    describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
-
-    describe_algo_response['TrainingSpecification'][
-        'SupportedTrainingInstanceTypes'
-    ] = train_instance_types
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=describe_algo_response
-    )
-
-    # invalid instance type, should fail
-    with pytest.raises(ValueError):
-        AlgorithmEstimator(
-            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-            role='SageMakerRole',
-            train_instance_type='ml.m4.8xlarge',
-            train_instance_count=1,
-            sagemaker_session=sagemaker_session,
-        )
-
-
-def test_algorithm_distributed_training_validation(sagemaker_session):
-    distributed_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    distributed_algo['TrainingSpecification']['SupportsDistributedTraining'] = True
-
-    single_instance_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    single_instance_algo['TrainingSpecification']['SupportsDistributedTraining'] = False
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=distributed_algo)
-
-    # Distributed training should work for Distributed and Single instance.
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=2,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=single_instance_algo)
-
-    # distributed training on a single instance algorithm should fail.
-    with pytest.raises(ValueError):
-        AlgorithmEstimator(
-            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-            role='SageMakerRole',
-            train_instance_type='ml.m5.2xlarge',
-            train_instance_count=2,
-            sagemaker_session=sagemaker_session,
-        )
-
-
-def test_algorithm_hyperparameter_integer_range_valid_range(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
-            'Type': 'Integer',
-            'Name': 'max_leaf_nodes',
-            'Range': {
-                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
-            },
-            'IsTunable': True,
-            'IsRequired': False,
-            'DefaultValue': '100',
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    estimator.set_hyperparameters(max_leaf_nodes=1)
-    estimator.set_hyperparameters(max_leaf_nodes=100000)
-
-
-def test_algorithm_hyperparameter_integer_range_invalid_range(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
-            'Type': 'Integer',
-            'Name': 'max_leaf_nodes',
-            'Range': {
-                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
-            },
-            'IsTunable': True,
-            'IsRequired': False,
-            'DefaultValue': '100',
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(max_leaf_nodes=0)
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(max_leaf_nodes=100001)
-
-
-def test_algorithm_hyperparameter_continuous_range_valid_range(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'A continuous hyperparameter',
-            'Type': 'Continuous',
-            'Name': 'max_leaf_nodes',
-            'Range': {
-                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
-            },
-            'IsTunable': True,
-            'IsRequired': False,
-            'DefaultValue': '100',
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    estimator.set_hyperparameters(max_leaf_nodes=0)
-    estimator.set_hyperparameters(max_leaf_nodes=1.0)
-    estimator.set_hyperparameters(max_leaf_nodes=0.5)
-    estimator.set_hyperparameters(max_leaf_nodes=1)
-
-
-def test_algorithm_hyperparameter_continuous_range_invalid_range(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'A continuous hyperparameter',
-            'Type': 'Continuous',
-            'Name': 'max_leaf_nodes',
-            'Range': {
-                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
-            },
-            'IsTunable': True,
-            'IsRequired': False,
-            'DefaultValue': '100',
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(max_leaf_nodes=1.1)
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(max_leaf_nodes=-0.1)
-
-
-def test_algorithm_hyperparameter_categorical_range(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'A continuous hyperparameter',
-            'Type': 'Categorical',
-            'Name': 'hp1',
-            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
-            'IsTunable': True,
-            'IsRequired': False,
-            'DefaultValue': '100',
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    estimator.set_hyperparameters(hp1='MXNet')
-    estimator.set_hyperparameters(hp1='TF')
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp1='Chainer')
-
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp1='MxNET')
-
-
-def test_algorithm_required_hyperparameters_not_provided(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'A continuous hyperparameter',
-            'Type': 'Categorical',
-            'Name': 'hp1',
-            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
-            'IsTunable': True,
-            'IsRequired': True,
-        },
-        {
-            'Name': 'hp2',
-            'Description': 'A continuous hyperparameter',
-            'Type': 'Categorical',
-            'IsTunable': False,
-            'IsRequired': True
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    # hp1 is required and was not provided
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(hp2='TF2')
-
-    # Calling fit with unset required hyperparameters should fail
-    # this covers the use case of not calling set_hyperparameters() explicitly
-    with pytest.raises(ValueError):
-        estimator.fit({'training': 's3://some/place'})
-
-
-@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
-def test_algorithm_required_hyperparameters_are_provided(sagemaker_session):
-    hyperparameters = [
-        {
-            'Description': 'A categorical hyperparameter',
-            'Type': 'Categorical',
-            'Name': 'hp1',
-            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
-            'IsTunable': True,
-            'IsRequired': True,
-        },
-        {
-            'Name': 'hp2',
-            'Description': 'A categorical hyperparameter',
-            'Type': 'Categorical',
-            'IsTunable': False,
-            'IsRequired': True
-        },
-        {
-            'Name': 'free_text_hp1',
-            'Description': 'You can write anything here',
-            'Type': 'FreeText',
-            'IsTunable': False,
-            'IsRequired': True
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    # All 3 Hyperparameters are provided
-    estimator.set_hyperparameters(hp1='TF', hp2='TF2', free_text_hp1='Hello!')
-
-
-def test_algorithm_required_free_text_hyperparameter_not_provided(sagemaker_session):
-    hyperparameters = [
-        {
-            'Name': 'free_text_hp1',
-            'Description': 'You can write anything here',
-            'Type': 'FreeText',
-            'IsTunable': False,
-            'IsRequired': True
-        },
-        {
-            'Name': 'free_text_hp2',
-            'Description': 'You can write anything here',
-            'Type': 'FreeText',
-            'IsTunable': False,
-            'IsRequired': False
-        }
-    ]
-
-    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
-
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.2xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    # Calling fit with unset required hyperparameters should fail
-    # this covers the use case of not calling set_hyperparameters() explicitly
-    with pytest.raises(ValueError):
-        estimator.fit({'training': 's3://some/place'})
-
-    # hp1 is required and was not provided
-    with pytest.raises(ValueError):
-        estimator.set_hyperparameters(free_text_hp2='some text')
-
-
-@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
-def test_algorithm_create_transformer(create_model, sagemaker_session):
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=DESCRIBE_ALGORITHM_RESPONSE)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    estimator.latest_training_job = _TrainingJob(sagemaker_session, 'some-job-name')
-    model = Mock()
-    model.name = 'my-model'
-    create_model.return_value = model
-
-    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
-
-    assert isinstance(transformer, Transformer)
-    create_model.assert_called()
-    assert transformer.model_name == 'my-model'
-
-
-def test_algorithm_create_transformer_without_completed_training_job(sagemaker_session):
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=DESCRIBE_ALGORITHM_RESPONSE)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    with pytest.raises(RuntimeError) as error:
-        estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
-        assert 'No finished training job found associated with this estimator' in str(error)
-
-
-@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
-def test_algorithm_create_transformer_with_product_id(create_model, sagemaker_session):
-    response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    response['ProductId'] = 'some-product-id'
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=response)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    estimator.latest_training_job = _TrainingJob(sagemaker_session, 'some-job-name')
-    model = Mock()
-    model.name = 'my-model'
-    create_model.return_value = model
-
-    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
-    assert transformer.env is None
-
-
-def test_algorithm_enable_network_isolation_no_product_id(sagemaker_session):
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=DESCRIBE_ALGORITHM_RESPONSE)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    network_isolation = estimator.enable_network_isolation()
-    assert network_isolation is False
-
-
-def test_algorithm_enable_network_isolation_with_product_id(sagemaker_session):
-    response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
-    response['ProductId'] = 'some-product-id'
-    sagemaker_session.sagemaker_client.describe_algorithm = Mock(
-        return_value=response)
-
-    estimator = AlgorithmEstimator(
-        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
-        role='SageMakerRole',
-        train_instance_type='ml.m4.xlarge',
-        train_instance_count=1,
-        sagemaker_session=sagemaker_session,
-    )
-
-    network_isolation = estimator.enable_network_isolation()
-    assert network_isolation is True

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-08-23 18:10:15[0m
[92mHash: eefd0c92f3e493a8680a626054e7086334018f4b[0m
[92mFilepath: src/sagemaker/tensorflow/README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Support tensorflow 1.9 and bump version to 1.9.2 (#365)


[0m
@@ -6,7 +6,7 @@ TensorFlow SageMaker Estimators allow you to run your own TensorFlow
 training algorithms on SageMaker Learner, and to host your own TensorFlow
 models on SageMaker Hosting.
 
-Supported versions of TensorFlow: ``1.4.1``, ``1.5.0``, ``1.6.0``, ``1.7.0``, ``1.8.0``, ``1.9.0``.
+Supported versions of TensorFlow: ``1.4.1``, ``1.5.0``, ``1.6.0``, ``1.7.0``, ``1.8.0``.
 
 Training with TensorFlow
 ~~~~~~~~~~~~~~~~~~~~~~~~
@@ -833,7 +833,7 @@ SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimi
 In certain cases you might be able to get a better performance by disabling this optimization
 (`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
 
-You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
+You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` by setting two following environment variables:
 
 .. code:: python
 

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-08-14 19:34:12[0m
[92mHash: 146e171f8c6270f28ab4c728a9956b19d8bde19b[0m
[92mFilepath: README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: edited general repo readme (#351)

[0m
@@ -16,17 +16,14 @@ SageMaker Python SDK
 
 SageMaker Python SDK is an open source library for training and deploying machine learning models on Amazon SageMaker.
 
-With the SDK, you can train and deploy models using popular deep learning frameworks **Apache MXNet** and **TensorFlow**.
-You can also train and deploy models with **Amazon algorithms**,
-which are scalable implementations of core machine learning algorithms that are optimized for SageMaker and GPU training.
-If you have **your own algorithms** built into SageMaker compatible Docker containers, you can train and host models using these as well.
+With the SDK, you can train and deploy models using popular deep learning frameworks: **Apache MXNet** and **TensorFlow**. You can also train and deploy models with **Amazon algorithms**, these are scalable implementations of core machine learning algorithms that are optimized for SageMaker and GPU training. If you have **your own algorithms** built into SageMaker compatible Docker containers, you can train and host models using these as well.
 
 For detailed API reference please go to: `Read the Docs <https://readthedocs.org/projects/sagemaker/>`_
 
 Table of Contents
 -----------------
 
-1. `Installing SageMaker Python SDK <#installing-the-sagemaker-python-sdk>`__
+1. `Getting SageMaker Python SDK <#getting-sagemaker-python-sdk>`__
 2. `SageMaker Python SDK Overview <#sagemaker-python-sdk-overview>`__
 3. `MXNet SageMaker Estimators <#mxnet-sagemaker-estimators>`__
 4. `TensorFlow SageMaker Estimators <#tensorflow-sagemaker-estimators>`__
@@ -39,16 +36,16 @@ Table of Contents
 11. `BYO Model <#byo-model>`__
 
 
-Installing the SageMaker Python SDK
+Getting SageMaker Python SDK
 ----------------------------
 
-The SageMaker Python SDK is built to PyPI and can be installed with pip as follows:
+SageMaker Python SDK is built to PyPI and can be installed with pip.
 
 ::
 
     pip install sagemaker
 
-You can install from source by cloning this repository and running a pip install command in the root directory of the repository:
+You can install from source by cloning this repository and issuing a pip install command in the root directory of the repository.
 
 ::
 
@@ -86,13 +83,12 @@ tox is a prerequisite for running unit tests so you need to make sure you have i
 
 **Integrations tests**
 
-To run the integration tests, the following prerequisites must be met
+To be able to run the integration tests, the following prerequisites must be met
 
 1. Access to an AWS account to run the tests on
-2. AWS account credentials available to boto3 clients used in the tests
-3. The AWS account has an IAM role named :code:`SageMakerRole`
-4. The libraries listed in the ``extra_require`` object in in ``setup.py`` for ``test`` are installed. You can do this by running the following command:
- :code:`pip install --upgrade .[test]`
+2. Make the AWS account credentials available to boto3 clients used in the tests
+3. Ensure the AWS account has an IAM role named :code:`SageMakerRole`
+4. Ensure the libraries mentioned in setup.py extra_require for test are installed which can be achieved using :code:`pip install --upgrade .[test]`
 
 You can run integ tests by issuing the following command:
 
@@ -123,15 +119,14 @@ SageMaker Python SDK Overview
 
 SageMaker Python SDK provides several high-level abstractions for working with Amazon SageMaker. These are:
 
-- **Estimators**: Encapsulate training on SageMaker.
-- **Models**: Encapsulate built ML models.
-- **Predictors**: Provide real-time inference and transformation using Python data-types against a SageMaker endpoint.
-- **Session**: Provides a collection of methods for working with SageMaker resources.
+- **Estimators**: Encapsulate training on SageMaker. Can be ``fit()`` to run training, then the resulting model ``deploy()`` ed to a SageMaker Endpoint.
+- **Models**: Encapsulate built ML models. Can be ``deploy()`` ed to a SageMaker Endpoint.
+- **Predictors**: Provide real-time inference and transformation using Python data-types against a SageMaker Endpoint.
+- **Session**: Provides a collection of convenience methods for working with SageMaker resources.
 
-``Estimator`` and ``Model`` implementations for MXNet, TensorFlow, Chainer, PyTorch, and Amazon ML algorithms are included.
-There's also an ``Estimator`` that runs SageMaker compatible custom Docker containers, enabling you to run your own ML algorithms by using the SageMaker Python SDK.
+Estimator and Model implementations for MXNet, TensorFlow, and Amazon ML algorithms are included. There's also an Estimator that runs SageMaker compatible custom Docker containers, allowing you to run your own ML algorithms via SageMaker Python SDK.
 
-The following sections of this document explain how to use the different estimators and models:
+Later sections of this document explain how to use the different Estimators and Models. These are:
 
 * `MXNet SageMaker Estimators and Models <#mxnet-sagemaker-estimators>`__
 * `TensorFlow SageMaker Estimators and Models <#tensorflow-sagemaker-estimators>`__
@@ -141,10 +136,10 @@ The following sections of this document explain how to use the different estimat
 * `Custom SageMaker Estimators and Models <#byo-docker-containers-with-sagemaker-estimators>`__
 
 
-Using Estimators
+Estimator Usage
 ---------------
 
-Here is an end to end example of how to use a SageMaker Estimator:
+Here is an end to end example of how to use a SageMaker Estimator.
 
 .. code:: python
 
@@ -158,22 +153,24 @@ Here is an end to end example of how to use a SageMaker Estimator:
     # Starts a SageMaker training job and waits until completion.
     mxnet_estimator.fit('s3://my_bucket/my_training_data/')
 
-    # Deploys the model that was generated by fit() to a SageMaker endpoint
+    # Deploys the model that was generated by fit() to a SageMaker Endpoint
     mxnet_predictor = mxnet_estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')
 
-    # Serializes data and makes a prediction request to the SageMaker endpoint
+    # Serializes data and makes a prediction request to the SageMaker Endpoint
     response = mxnet_predictor.predict(data)
 
-    # Tears down the SageMaker endpoint
+    # Tears down the SageMaker Endpoint
     mxnet_estimator.delete_endpoint()
 
 Local Mode
 ~~~~~~~~~~
 
-The SageMaker Python SDK supports local mode, which allows you to create estimators and deploy them to your local environment.
-This is a great way to test your deep learning scripts before running them in SageMaker's managed training or hosting environments.
+The SageMaker Python SDK now supports local mode, which allows you to create TensorFlow, MXNet and BYO estimators and
+deploy to your local environment. This is a great way to test your deep learning script before running in
+SageMaker's managed training or hosting environments.
 
-We can take the example in  `Using Estimators <#using-estimators>`__ , and use either ``local`` or ``local_gpu`` as the instance type.
+We can take the example in  `Estimator Usage <#estimator-usage>`__ , and use either ``local`` or ``local_gpu`` as the
+instance type.
 
 .. code:: python
 
@@ -184,7 +181,7 @@ We can take the example in  `Using Estimators <#using-estimators>`__ , and use e
                             train_instance_type='local',
                             train_instance_count=1)
 
-    # In Local Mode, fit will pull the MXNet container Docker image and run it locally
+    # In Local Mode, fit will pull the MXNet container docker image and run it locally
     mxnet_estimator.fit('s3://my_bucket/my_training_data/')
 
     # Alternatively, you can train using data in your local file system. This is only supported in Local mode.
@@ -200,10 +197,11 @@ We can take the example in  `Using Estimators <#using-estimators>`__ , and use e
     mxnet_estimator.delete_endpoint()
 
 
-If you have an existing model and want to deploy it locally, don't specify a sagemaker_session argument to the ``MXNetModel`` constructor.
-The correct session is generated when you call ``model.deploy()``.
+If you have an existing model and would like to deploy it locally you can do that as well. If you don't
+specify a sagemaker_session argument to the MXNetModel constructor, the right session will be generated
+when calling model.deploy()
 
-Here is an end-to-end example:
+Here is an end to end example:
 
 .. code:: python
 
@@ -223,28 +221,28 @@ Here is an end-to-end example:
     predictor.delete_endpoint()
 
 
-For detailed examples of running Docker in local mode, see:
+For detailed examples of running docker in local mode, see:
 
 - `TensorFlow local mode example notebook <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_local_mode_mnist.ipynb>`__.
 - `MXNet local mode example notebook <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_mnist/mnist_with_gluon_local_mode.ipynb>`__.
 
 A few important notes:
 
-- Only one local mode endpoint can be running at a time.
-- If you are using S3 data as input, it is pulled from S3 to your local environment. Ensure you have sufficient space to store the data locally.
-- If you run into problems it often due to different Docker containers conflicting. Killing these containers and re-running often solves your problems.
-- Local Mode requires Docker Compose and `nvidia-docker2 <https://github.com/NVIDIA/nvidia-docker>`__ for ``local_gpu``.
+- Only one local mode endpoint can be running at a time
+- If you are using s3 data as input, it will be pulled from S3 to your local environment, please ensure you have sufficient space.
+- If you run into problems, this is often due to different docker containers conflicting. Killing these containers and re-running often solves your problems.
+- Local Mode requires docker-compose and `nvidia-docker2 <https://github.com/NVIDIA/nvidia-docker>`__ for ``local_gpu``.
 - Distributed training is not yet supported for ``local_gpu``.
 
 
 MXNet SageMaker Estimators
 --------------------------
 
-By using MXNet SageMaker ``Estimators``, you can train and host MXNet models on Amazon SageMaker.
+With MXNet Estimators, you can train and host MXNet models on Amazon SageMaker.
 
 Supported versions of MXNet: ``1.2.1``, ``1.1.0``, ``1.0.0``, ``0.12.1``.
 
-For more information, see `MXNet SageMaker Estimators and Models`_.
+More details at `MXNet SageMaker Estimators and Models`_.
 
 .. _MXNet SageMaker Estimators and Models: src/sagemaker/mxnet/README.rst
 
@@ -252,11 +250,13 @@ For more information, see `MXNet SageMaker Estimators and Models`_.
 TensorFlow SageMaker Estimators
 -------------------------------
 
-By using TensorFlow SageMaker ``Estimators``, you can train and host TensorFlow models on Amazon SageMaker.
+TensorFlow SageMaker Estimators allow you to run your own TensorFlow
+training algorithms on SageMaker Learner, and to host your own TensorFlow
+models on SageMaker Hosting.
 
 Supported versions of TensorFlow: ``1.4.1``, ``1.5.0``, ``1.6.0``, ``1.7.0``, ``1.8.0``.
 
-For more information, see `TensorFlow SageMaker Estimators and Models`_.
+More details at `TensorFlow SageMaker Estimators and Models`_.
 
 .. _TensorFlow SageMaker Estimators and Models: src/sagemaker/tensorflow/README.rst
 
@@ -264,13 +264,13 @@ For more information, see `TensorFlow SageMaker Estimators and Models`_.
 Chainer SageMaker Estimators
 -------------------------------
 
-By using Chainer SageMaker ``Estimators``, you can train and host Chainer models on Amazon SageMaker.
+With Chainer Estimators, you can train and host Chainer models on Amazon SageMaker.
 
 Supported versions of Chainer: ``4.0.0``, ``4.1.0``.
 
-For more information about Chainer, see https://github.com/chainer/chainer.
+You can visit the Chainer repository at https://github.com/chainer/chainer.
 
-For more information about  Chainer SageMaker ``Estimators``, see `Chainer SageMaker Estimators and Models`_.
+More details at `Chainer SageMaker Estimators and Models`_.
 
 .. _Chainer SageMaker Estimators and Models: src/sagemaker/chainer/README.rst
 
@@ -278,27 +278,26 @@ For more information about  Chainer SageMaker ``Estimators``, see `Chainer SageM
 PyTorch SageMaker Estimators
 -------------------------------
 
-With PyTorch SageMaker ``Estimators``, you can train and host PyTorch models on Amazon SageMaker.
+With PyTorch Estimators, you can train and host PyTorch models on Amazon SageMaker.
 
 Supported versions of PyTorch: ``0.4.0``
 
-For more information about PyTorch, see https://github.com/pytorch/pytorch.
+You can visit the PyTorch repository at https://github.com/pytorch/pytorch.
 
-For more information about PyTorch SageMaker ``Estimators``, see `PyTorch SageMaker Estimators and Models`_.
+More details at `PyTorch SageMaker Estimators and Models`_.
 
 .. _PyTorch SageMaker Estimators and Models: src/sagemaker/pytorch/README.rst
 
 
 AWS SageMaker Estimators
 ------------------------
-Amazon SageMaker provides several built-in machine learning algorithms that you can use to solve a variety of problems.
+Amazon SageMaker provides several built-in machine learning algorithms that you can use for a variety of problem types.
 
-The full list of algorithms is available at: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html
+The full list of algorithms is available on the AWS website: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html
 
-The SageMaker Python SDK includes estimator wrappers for the AWS K-means, Principal Components Analysis (PCA), Linear Learner, Factorization Machines,
-Latent Dirichlet Allocation (LDA), Neural Topic Model (NTM) Random Cut Forest and k-nearest neighbors (k-NN) algorithms.
+SageMaker Python SDK includes Estimator wrappers for the AWS K-means, Principal Components Analysis(PCA), Linear Learner, Factorization Machines, Latent Dirichlet Allocation(LDA), Neural Topic Model(NTM) Random Cut Forest and k-nearest neighbors (k-NN) algorithms.
 
-For more information, see `AWS SageMaker Estimators and Models`_.
+More details at `AWS SageMaker Estimators and Models`_.
 
 .. _AWS SageMaker Estimators and Models: src/sagemaker/amazon/README.rst
 
@@ -306,8 +305,7 @@ For more information, see `AWS SageMaker Estimators and Models`_.
 BYO Docker Containers with SageMaker Estimators
 -----------------------------------------------
 
-To use a Docker image that you created and use the SageMaker SDK for training, the easiest way is to use the dedicated ``Estimator`` class.
-You can create an instance of the ``Estimator`` class with desired Docker image and use it as described in previous sections.
+When you want to use a Docker image prepared earlier and use SageMaker SDK for training the easiest way is to use dedicated ``Estimator`` class. You will be able to instantiate it with desired image and use it in same way as described in previous sections.
 
 Please refer to the full example in the examples repo:
 
@@ -324,11 +322,10 @@ SageMaker Automatic Model Tuning
 --------------------------------
 
 All of the estimators can be used with SageMaker Automatic Model Tuning, which performs hyperparameter tuning jobs.
-A hyperparameter tuning job finds the best version of a model by running many training jobs on your dataset using the algorithm with different values of hyperparameters within ranges
-that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.
-If you're not using an Amazon SageMaker built-in algorithm, then the metric is defined by a regular expression (regex) you provide. 
-The hyperparameter tuning job parses the training job's logs to find metrics that match the regex you defined.
-For more information about SageMaker Automatic Model Tuning, see `AWS documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html>`__.
+A hyperparameter tuning job runs multiple training jobs that differ by the values of their hyperparameters to find the best training job.
+It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.
+If you're not using an Amazon ML algorithm, then the metric is defined by a regular expression (regex) you provide for going through the training job's logs.
+You can read more about SageMaker Automatic Model Tuning in the `AWS documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html>`__.
 
 The SageMaker Python SDK contains a ``HyperparameterTuner`` class for creating and interacting with hyperparameter training jobs.
 Here is a basic example of how to use it:
@@ -357,15 +354,15 @@ Here is a basic example of how to use it:
     # Tear down the SageMaker endpoint
     my_tuner.delete_endpoint()
 
-This example shows a hyperparameter tuning job that creates up to 100 training jobs, running up to 10 training jobs at a time.
-Each training job's learning rate is a value between 0.05 and 0.06, but this value will differ between training jobs.
+This example shows a hyperparameter tuning job that creates up to 100 training jobs, running up to 10 at a time.
+Each training job's learning rate will be a value between 0.05 and 0.06, but this value will differ between training jobs.
 You can read more about how these values are chosen in the `AWS documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html>`__.
 
 A hyperparameter range can be one of three types: continuous, integer, or categorical.
 The SageMaker Python SDK provides corresponding classes for defining these different types.
 You can define up to 20 hyperparameters to search over, but each value of a categorical hyperparameter range counts against that limit.
 
-If you are using an Amazon SageMaker built-in algorithm, you don't need to pass in anything for ``metric_definitions``.
+If you are using an Amazon ML algorithm, you don't need to pass in anything for ``metric_definitions``.
 In addition, the ``fit()`` call uses a list of ``RecordSet`` objects instead of a dictionary:
 
 .. code:: python
@@ -377,18 +374,15 @@ In addition, the ``fit()`` call uses a list of ``RecordSet`` objects instead of
     # Start hyperparameter tuning job
     my_tuner.fit([train_records, test_records])
 
-To help attach a previously-started hyperparameter tuning job to a ``HyperparameterTuner`` instance, 
-``fit()`` adds the module path of the class used to create the tuner to the list of static hyperparameters by default.
-If the algorithm you are using cannot handle unknown hyperparameters
-(for example, an Amazon SageMaker built-in algorithm that does not have a custom estimator in the Python SDK),
-set ``include_cls_metadata`` to ``False`` when you call ``fit``, so that it does not add the module path as a static hyperparameter:
+To aid with attaching a previously-started hyperparameter tuning job with a ``HyperparameterTuner`` instance, ``fit()`` injects metadata in the hyperparameters by default.
+If the algorithm you are using cannot handle unknown hyperparameters (e.g. an Amazon ML algorithm that does not have a custom estimator in the Python SDK), then you can set ``include_cls_metadata`` to ``False`` when calling fit:
 
 .. code:: python
 
     my_tuner.fit({'train': 's3://my_bucket/my_training_data', 'test': 's3://my_bucket_my_testing_data'},
                  include_cls_metadata=False)
 
-There is also an analytics object associated with each ``HyperparameterTuner`` instance that contains useful information about the hyperparameter tuning job.
+There is also an analytics object associated with each ``HyperparameterTuner`` instance that presents useful information about the hyperparameter tuning job.
 For example, the ``dataframe`` method gets a pandas dataframe summarizing the associated training jobs:
 
 .. code:: python
@@ -414,18 +408,17 @@ For more detailed explanations of the classes that this library provides for aut
 SageMaker Batch Transform
 -------------------------
 
-After you train a model, you can use Amazon SageMaker Batch Transform to perform inferences with the model.
-Batch Transform manages all necessary compute resources, including launching instances to deploy endpoints and deleting them afterward.
+Once you have a trained model, you can use Amazon SageMaker Batch Transform to perform inferences with the model.
+Batch Transform manages all compute resources necessary, including launching instances to deploy endpoints and deleting them afterward.
 You can read more about SageMaker Batch Transform in the `AWS documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html>`__.
 
-If you trained the model using a SageMaker Python SDK estimator,
-you can invoke the estimator's ``transformer()`` method to create a transform job for a model based on the training job:
+If you have trained the model using a SageMaker Python SDK Estimator, you can simply invoke ``transformer()`` to create a ``Transformer`` for the training job:
 
 .. code:: python
 
     transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
 
-Alternatively, if you already have a SageMaker model, you can create an instance of the ``Transformer`` class by calling its constructor:
+Alternatively, if you already have a SageMaker Model, you can instantiate a ``Transformer`` directly with its constructor:
 
 .. code:: python
 
@@ -433,16 +426,16 @@ Alternatively, if you already have a SageMaker model, you can create an instance
                               instance_count=1,
                               instance_type='ml.m4.xlarge')
 
-For a full list of the possible options to configure by using either of these methods, see the API docs for `Estimator <https://sagemaker.readthedocs.io/en/latest/estimators.html#sagemaker.estimator.Estimator.transformer>`__ or `Transformer <https://sagemaker.readthedocs.io/en/latest/transformer.html#sagemaker.transformer.Transformer>`__.
+For a full list of the possible options to configure through either of these methods, please refer to the API docs for `Estimator <https://sagemaker.readthedocs.io/en/latest/estimators.html#sagemaker.estimator.Estimator.transformer>`__ or `Transformer <https://sagemaker.readthedocs.io/en/latest/transformer.html#sagemaker.transformer.Transformer>`__.
 
-After you create a ``Transformer`` object, you can invoke ``transform()`` to start a batch transform job with the S3 location of your data.
-You can also specify other attributes of your data, such as the content type.
+Once you've created a ``Transformer`` object, you can invoke ``transform()`` to being a batch transform job with the S3 location of your data.
+You can also specify other attributes about your data, such as the content type.
 
 .. code:: python
 
     transformer.transform('s3://my-bucket/batch-transform-input')
 
-For more details about what can be specified here, see `API docs <https://sagemaker.readthedocs.io/en/latest/transformer.html#sagemaker.transformer.Transformer.transform>`__.
+For more details about what can be specified here, please refer to the `API docs <https://sagemaker.readthedocs.io/en/latest/transformer.html#sagemaker.transformer.Transformer.transform>`__.
 
 
 FAQ
@@ -451,7 +444,7 @@ FAQ
 I want to train a SageMaker Estimator with local data, how do I do this?
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-Upload the data to S3 before training. You can use the AWS Command Line Tool (the aws cli) to achieve this.
+You'll need to upload the data to S3 before training. You can use the AWS Command Line Tool (the aws cli) to achieve this.
 
 If you don't have the aws cli, you can install it using pip:
 
@@ -459,24 +452,22 @@ If you don't have the aws cli, you can install it using pip:
 
     pip install awscli --upgrade --user
 
-If you don't have pip or want to learn more about installing the aws cli, see the official `Amazon aws cli installation guide <http://docs.aws.amazon.com/cli/latest/userguide/installing.html>`__.
+If you don't have pip or want to learn more about installing the aws cli, please refer to the official `Amazon aws cli installation guide <http://docs.aws.amazon.com/cli/latest/userguide/installing.html>`__.
 
-After you install the AWS cli, you can upload a directory of files to S3 with the following command:
+Once you have the aws cli installed, you can upload a directory of files to S3 with the following command:
 
 ::
 
     aws s3 cp /tmp/foo/ s3://bucket/path
 
-For more information about using the aws cli for manipulating S3 resources, see `AWS cli command reference <http://docs.aws.amazon.com/cli/latest/reference/s3/index.html>`__.
+You can read more about using the aws cli for manipulating S3 resources in the `AWS cli command reference <http://docs.aws.amazon.com/cli/latest/reference/s3/index.html>`__.
 
 
 How do I make predictions against an existing endpoint?
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-Create a ``Predictor`` object and provide it with your endpoint name,
-then call its ``predict()`` method with your input.
+Create a Predictor object and provide it your endpoint name. Then, simply call its predict() method with your input.
 
-You can use either the generic ``RealTimePredictor`` class, which by default does not perform any serialization/deserialization transformations on your input,
-but can be configured to do so through constructor arguments:
+You can either use the generic RealTimePredictor class, which by default does not perform any serialization/deserialization transformations on your input, but can be configured to do so through constructor arguments:
 http://sagemaker.readthedocs.io/en/latest/predictors.html
 
 Or you can use the TensorFlow / MXNet specific predictor classes, which have default serialization/deserialization logic:
@@ -495,8 +486,7 @@ Example code using the TensorFlow predictor:
 
 BYO Model
 ---------
-You can also create an endpoint from an existing model rather than training one.
-That is, you can bring your own model:
+You can also create an endpoint from an existing model rather than training one - i.e. bring your own model.
 
 First, package the files for the trained model into a ``.tar.gz`` file, and upload the archive to S3.
 
@@ -521,4 +511,4 @@ After that, invoke the ``deploy()`` method on the ``Model``:
 
 This returns a predictor the same way an ``Estimator`` does when ``deploy()`` is called. You can now get inferences just like with any other model deployed on Amazon SageMaker.
 
-A full example is available in the `Amazon SageMaker examples repository <https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/mxnet_mnist_byom>`__.
\ No newline at end of file
+A full example is available in the `Amazon SageMaker examples repository <https://github.com/ragavvenkatesan/amazon-sagemaker-examples/tree/[93m3c8394f21ee357da0b553b0ab024c5c5e425182a[0m/advanced_functionality/mxnet_mnist_byom>`__.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-08-13 15:01:22[0m
[92mHash: fa883258d63018536e261cb790e0aa9d43bf6413[0m
[92mFilepath: src/sagemaker/tensorflow/README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Add documentation about how to disable MKL-DNN optimization for tensroflow. (#348)

[0m
@@ -826,23 +826,6 @@ If your TFRecords are compressed, you can train on Gzipped TF Records by passing
 You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
 
 
-Training with MKL-DNN disabled
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
-
-In certain cases you might be able to get a better performance by disabling this optimization
-(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
-
-You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` by setting two following environment variables:
-
-.. code:: python
-
-    import os
-
-    os.environ['TF_DISABLE_MKL'] = '1'
-    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
-
 
 SageMaker TensorFlow Docker containers
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2018-01-18 11:13:50[0m
[92mHash: a23028a8831eb729ed33fe67f549a5f388e5c76d[0m
[92mFilepath: README.rst[0m
[92mBranch: origin/master[0m
[92mCommit: Add documentation about BYO Models (#47)

[0m
@@ -21,7 +21,6 @@ Table of Contents
 4. `TensorFlow SageMaker Estimators <#tensorflow-sagemaker-estimators>`__
 5. `AWS SageMaker Estimators <#aws-sagemaker-estimators>`__
 6. `BYO Docker Containers with SageMaker Estimators <#byo-docker-containers-with-sagemaker-estimators>`__
-7. `BYO Model <#byo-model>`__
 
 
 Getting SageMaker Python SDK
@@ -94,7 +93,7 @@ SageMaker Python SDK Overview
 
 SageMaker Python SDK provides several high-level abstractions for working with Amazon SageMaker. These are:
 
-- **Estimators**: Encapsulate training on SageMaker. Can be ``fit()`` to run training, then the resulting model ``deploy()`` ed to a SageMaker Endpoint.
+- **Estimators**: Encapsulate training on SageMaker. Can be ``fit()`` to run training, then the resulting model ``deploy()`` ed to a SageMaker Endpoint. 
 - **Models**: Encapsulate built ML models. Can be ``deploy()`` ed to a SageMaker Endpoint.
 - **Predictors**: Provide real-time inference and transformation using Python data-types against a SageMaker Endpoint.
 - **Session**: Provides a collection of convience methods for working with SageMaker resources.
@@ -178,7 +177,7 @@ You don't have to use all the arguments, arguments you don't care about can be i
         pass
 
 **Note: Writing a training script that imports correctly**
-When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
+When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script. 
 
 If you want to run your training script locally via the Python interpreter, look at using a ``___name__ == '__main__'`` guard, discussed in more detail here: https://stackoverflow.com/questions/419163/what-does-if-name-main-do .
 
@@ -411,7 +410,7 @@ The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model serve
 
 You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
 
-As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
+As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor. 
 
 Model loading
 ^^^^^^^^^^^^^
@@ -698,19 +697,19 @@ The Docker images extend Ubuntu 16.04.
 TensorFlow SageMaker Estimators
 -------------------------------
 
-TensorFlow SageMaker Estimators allow you to run your own TensorFlow
-training algorithms on SageMaker Learner, and to host your own TensorFlow
+TensorFlow SageMaker Estimators allow you to run your own TensorFlow 
+training algorithms on SageMaker Learner, and to host your own TensorFlow 
 models on SageMaker Hosting.
 
 Training with TensorFlow
 ~~~~~~~~~~~~~~~~~~~~~~~~
 
-Training TensorFlow models using a ``sagemaker.tensorflow.TensorFlow``
+Training TensorFlow models using a ``sagemaker.tensorflow.TensorFlow`` 
 is a two-step process.
-First, you prepare your training script, then second, you run it on
+First, you prepare your training script, then second, you run it on 
 SageMaker Learner via the ``sagemaker.tensorflow.TensorFlow`` estimator.
 
-Suppose that you already have a TensorFlow training script called
+Suppose that you already have a TensorFlow training script called 
 ``tf-train.py``. You can train this script in SageMaker Learner as
 follows:
 
@@ -728,7 +727,7 @@ constructor keyword arguments define how SageMaker runs your training
 script and are discussed, in detail, in a later section.
 
 In the following sections, we'll discuss how to prepare a training script for execution on
-SageMaker, then how to run that script on SageMaker using a ``sagemaker.tensorflow.TensorFlow``
+SageMaker, then how to run that script on SageMaker using a ``sagemaker.tensorflow.TensorFlow`` 
 estimator.
 
 Preparing the TensorFlow training script
@@ -745,7 +744,7 @@ version is **1.4.0**. This training script **must contain** the following functi
 Creating a ``model_fn``
 ^^^^^^^^^^^^^^^^^^^^^^^
 
-A ``model_fn`` is a function that contains all the logic to support training, evaluation,
+A ``model_fn`` is a function that contains all the logic to support training, evaluation, 
 and prediction. The basic skeleton for a ``model_fn`` looks like this:
 
 .. code:: python
@@ -772,8 +771,8 @@ The ``model_fn`` must accept four positional arguments:
   - ``TRAIN``: the ``model_fn`` was invoked in **training** mode.
   - ``EVAL``: the ``model_fn`` was invoked in **evaluation** mode.
   - ``PREDICT``: the ``model_fn`` was invoked in **predict** mode.
-- ``hyperparameters``: The hyperparameters passed to SageMaker TrainingJob that runs
-  your TensorFlow training script. You can use this to pass hyperparameters to your
+- ``hyperparameters``: The hyperparameters passed to SageMaker TrainingJob that runs 
+  your TensorFlow training script. You can use this to pass hyperparameters to your 
   training script.
 
 Example of a complete ``model_fn``
@@ -822,21 +821,21 @@ Example of a complete ``model_fn``
 Distributed training
 ''''''''''''''''''''
 
-When distributed training happens, a copy of the same neural network will be sent to
-multiple training instances. Each instance will train with a batch of the dataset,
+When distributed training happens, a copy of the same neural network will be sent to 
+multiple training instances. Each instance will train with a batch of the dataset, 
 calculate loss and minimize the optimizer. One entire loop of this process is called training step.
 
 A `global step <https://www.tensorflow.org/api_docs/python/tf/train/global_step>`_ is a global
 counter shared between the instances. It is necessary for distributed training, so the optimizer
-can keep track of the number of training steps across instances. The only change in the
-previous complete ``model_fn`` to enable distributed training is to pass in the global
+can keep track of the number of training steps across instances. The only change in the 
+previous complete ``model_fn`` to enable distributed training is to pass in the global 
 step into the ``optimizer.minimize`` function:
 
 .. code:: python
-
+  
   train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())
 
-More information about distributed training can be find in talk from the TensorFlow Dev Summit 2017
+More information about distributed training can be find in talk from the TensorFlow Dev Summit 2017 
 `Distributed TensorFlow <https://www.youtube.com/watch?time_continue=1&v=la_M6bCV91M>`_.
 
 
@@ -846,8 +845,8 @@ More details on how to create a ``model_fn`` can be find in `Constructing the mo
 Creating ``train_input_fn`` and ``eval_input_fn`` functions
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-The ``train_input_fn`` is used to pass ``features`` and ``labels`` to the ``model_fn``
-in **training** mode. The ``eval_input_fn`` is used to ``features`` and ``labels`` to the
+The ``train_input_fn`` is used to pass ``features`` and ``labels`` to the ``model_fn`` 
+in **training** mode. The ``eval_input_fn`` is used to ``features`` and ``labels`` to the 
 ``model_fn`` in **evaluation** mode.
 
 The basic skeleton for the ``train_input_fn`` looks like this:
@@ -860,7 +859,7 @@ The basic skeleton for the ``train_input_fn`` looks like this:
     # 2. Preprocess the dataset
     # 3. Return 1)  a mapping of feature columns to Tensors with
     # the corresponding feature data, and 2) a Tensor containing labels
-    return feature_cols, labels
+    return feature_cols, labels 
 
 An ``eval_input_fn`` follows the same format:
 
@@ -872,7 +871,7 @@ An ``eval_input_fn`` follows the same format:
     # 2. Preprocess the dataset
     # 3. Return 1)  a mapping of feature columns to Tensors with
     # the corresponding feature data, and 2) a Tensor containing labels
-    return feature_cols, labels
+    return feature_cols, labels 
 
 Example of a complete ``train_input_fn`` and ``eval_input_fn``
 ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
@@ -905,12 +904,12 @@ More details on how to create input functions can be find in `Building Input Fun
 Creating a ``serving_input_fn``
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-During training, ``train_input_fn`` ingests data and prepares it for use by the model.
-At the end of training, similarly, ``serving_input_fn`` is used to create the model that
+During training, ``train_input_fn`` ingests data and prepares it for use by the model. 
+At the end of training, similarly, ``serving_input_fn`` is used to create the model that 
 is exported for TensorFlow Serving. This function has the following purposes:
 
 - To add placeholders to the graph that the serving system will feed with inference requests.
-- To add any additional ops needed to convert data from the input format into the feature Tensors
+- To add any additional ops needed to convert data from the input format into the feature Tensors 
   expected by the model.
 
 The basic skeleton for the ``serving_input_fn`` looks like this:
@@ -921,7 +920,7 @@ The basic skeleton for the ``serving_input_fn`` looks like this:
     # Logic to the following:
     # 1. Defines placeholders that TensorFlow serving will feed with inference requests
     # 2. Preprocess input data
-    # 3. Returns a tf.estimator.export.ServingInputReceiver object, which packages the placeholders
+    # 3. Returns a tf.estimator.export.ServingInputReceiver object, which packages the placeholders 
     and the resulting feature Tensors together.
 
 Example of a complete ``serving_input_fn``
@@ -944,7 +943,7 @@ More examples on how to create a TensorFlow training script can be find in the `
 Support for pre-made ``tf.estimator`` and ``Keras`` models
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-In addition to ``model_fn``, ``sagemaker.tensorflow.TensorFlow`` supports pre-canned ``tf.estimator``
+In addition to ``model_fn``, ``sagemaker.tensorflow.TensorFlow`` supports pre-canned ``tf.estimator`` 
 and ``Keras`` models.
 
 Using a pre-made ``tensorflow.estimator`` instead of a ``model_fn``
@@ -953,20 +952,20 @@ Using a pre-made ``tensorflow.estimator`` instead of a ``model_fn``
 Pre-canned estimators are machine learning estimators premade for general purpose problems.
 ``tf.estimator`` provides the following pre-canned estimators:
 
-- `tf.estimator.LinearClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier>`_: Constructs
+- `tf.estimator.LinearClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier>`_: Constructs 
   a linear classification model.
-- `tf.estimator.LinearRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor>`_: Constructs
+- `tf.estimator.LinearRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor>`_: Constructs 
   a linear regression model.
-- `tf.estimator.DNNClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier>`_: Constructs
+- `tf.estimator.DNNClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier>`_: Constructs 
   a neural network classification model.
-- `tf.estimator.DNNRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor>`_: Construct
+- `tf.estimator.DNNRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor>`_: Construct 
   a neural network regression model.
-- `tf.estimator.DNNLinearCombinedClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier>`_: Constructs
+- `tf.estimator.DNNLinearCombinedClassifier <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier>`_: Constructs 
   a neural network and linear combined classification model.
-- `tf.estimator.DNNLinearCombinedRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor>`_: Constructs
+- `tf.estimator.DNNLinearCombinedRegressor <https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor>`_: Constructs 
   a neural network and linear combined regression model.
 
-To use a pre-canned ``tensorflow.estimator`` instead of creating a ``model_fn``, you need to write a ``estimator_fn``.
+To use a pre-canned ``tensorflow.estimator`` instead of creating a ``model_fn``, you need to write a ``estimator_fn``. 
 The base skeleton for the ``estimator_fn`` looks like this:
 
 .. code:: python
@@ -999,7 +998,7 @@ An example on how to create a TensorFlow training script with an ``estimator_fn`
 Using a ``Keras`` model instead of a ``model_fn``
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-``tf.keras`` is an full implementation inside TensorFlow of the Keras API. To use a ``tf.keras``
+``tf.keras`` is an full implementation inside TensorFlow of the Keras API. To use a ``tf.keras`` 
 model for training instead of ``model_fn``, you need to write a ``keras_model_fn``. The base skeleton of
 a ``keras_model_fn`` looks like this:
 
@@ -1055,7 +1054,7 @@ The ``TensorFlow`` constructor takes both required and optional arguments.
 Required arguments
 ''''''''''''''''''
 
-The following are required arguments to the TensorFlow constructor.
+The following are required arguments to the TensorFlow constructor. 
 
 -  ``entry_point (str)`` Path (absolute or relative) to the Python file which
    should be executed as the entry point to training.
@@ -1068,7 +1067,7 @@ The following are required arguments to the TensorFlow constructor.
    training.
 -  ``train_instance_type (str)`` Type of EC2 instance to use for training, for
    example, 'ml.c4.xlarge'.
-- ``training_steps (int)`` Perform this many steps of training. ``None``, means train forever.
+- ``training_steps (int)`` Perform this many steps of training. ``None``, means train forever. 
 - ``evaluation_steps (int)`` Perform this many steps of evaluation. ``None``, means
   that evaluation runs until input from ``eval_input_fn`` is exhausted (or another exception is raised).
 
@@ -1084,7 +1083,7 @@ you can specify these as keyword arguments.
    on SageMaker.
 -  ``hyperparameters (dict[str,ANY])`` Hyperparameters that will be used for training.
    Will be made accessible as a dict[] to the training code on
-   SageMaker. Some hyperparameters will be interpreted by TensorFlow and can be use to
+   SageMaker. Some hyperparameters will be interpreted by TensorFlow and can be use to 
    fine tune training. See `Optional Hyperparameters <#optional-hyperparameters>`_.
 -  ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
    input data during training. Must be large enough to the store training
@@ -1096,10 +1095,10 @@ you can specify these as keyword arguments.
    are stored to a default bucket. If the bucket with the specific name
    does not exist, the estimator creates the bucket during the ``fit``
    method execution.
--  ``checkpoint_path`` S3 location where checkpoint data will saved and restored.
-   The default location is *bucket_name/job_name/checkpoint*. If the location
-   already has checkpoints before the training starts, the model will restore
-   state from the last saved checkpoint. It is very useful to restart a training.
+-  ``checkpoint_path`` S3 location where checkpoint data will saved and restored. 
+   The default location is *bucket_name/job_name/checkpoint*. If the location 
+   already has checkpoints before the training starts, the model will restore 
+   state from the last saved checkpoint. It is very useful to restart a training. 
    See `Restoring from checkpoints <#restoring-from-checkpoints>`_.
 -  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
    output with.
@@ -1111,8 +1110,8 @@ you can specify these as keyword arguments.
 Optional Hyperparameters
 ''''''''''''''''''''''''
 
-These hyperparameters are used by TensorFlow to fine tune the training.
-You need to add them inside the hyperparameters dictionary in the
+These hyperparameters are used by TensorFlow to fine tune the training. 
+You need to add them inside the hyperparameters dictionary in the 
 ``TensorFlow`` estimator constructor.
 
 -  ``save_summary_steps (int)`` Save summaries every this many steps.
@@ -1139,10 +1138,10 @@ both required and optional arguments.
 Required argument
 '''''''''''''''''
 
--  ``inputs (str)``: A S3 URI, for example ``s3://my-bucket/my-training-data``, which contains
+-  ``inputs (str)``: A S3 URI, for example ``s3://my-bucket/my-training-data``, which contains 
    the dataset that will be used for training. When the training job starts in SageMaker the
    container will download the dataset. Both ``train_input_fn`` and ``eval_input_fn`` functions
-   have a parameter called ``training_dir`` which contains the directory inside the container
+   have a parameter called ``training_dir`` which contains the directory inside the container 
    where the dataset was saved into. See `Creating train_input_fn and eval_input_fn functions`_.
 
 Optional arguments
@@ -1152,8 +1151,8 @@ Optional arguments
    training script to complete before returning.
 -  ``logs (bool)``: Defaults to True, whether to show logs produced by training
    job in the Python session. Only meaningful when wait is True.
-- ``run_tensorboard_locally (bool)``: Defaults to False. Executes TensorBoard in a different
-  process with downloaded checkpoint information. Requires modules TensorBoard and AWS CLI.
+- ``run_tensorboard_locally (bool)``: Defaults to False. Executes TensorBoard in a different 
+  process with downloaded checkpoint information. Requires modules TensorBoard and AWS CLI. 
   installed. Terminates TensorBoard when the execution ends. See `Running TensorBoard`_.
 - ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
   based on the training image name and current timestamp.
@@ -1169,15 +1168,15 @@ Calling ``fit`` starts a SageMaker training job. The training job will execute t
   - starts a Docker container optimized for TensorFlow, see `SageMaker TensorFlow Docker containers`_.
   - downloads the dataset.
   - setup up distributed training.
-  - starts asynchronous training, executing the ``model_fn`` function defined in your script
-    in **training** mode; i.e., ``features`` and ``labels`` are fed by a batch of the
+  - starts asynchronous training, executing the ``model_fn`` function defined in your script 
+    in **training** mode; i.e., ``features`` and ``labels`` are fed by a batch of the 
     training dataset defined by ``train_input_fn``. See `Creating train_input_fn and eval_input_fn functions`_.
 
 The training job finishes after the number of training steps reaches the value defined by
 the ``TensorFlow`` estimator parameter ``training_steps`` is finished or when the training
 job execution time reaches the ``TensorFlow`` estimator parameter ``train_max_run``.
 
-When the training job finishes, a `TensorFlow serving <https://www.tensorflow.org/serving/serving_basic>`_
+When the training job finishes, a `TensorFlow serving <https://www.tensorflow.org/serving/serving_basic>`_ 
 with the result of the training is generated and saved to the S3 location define by
 the ``TensorFlow`` estimator parameter ``output_path``.
 
@@ -1189,7 +1188,7 @@ During the training job, the first EC2 instance that is executing the training i
 All instances execute the training loop, feeding the ``model_fn`` with ``train_input_fn``.
 Every ``min_eval_frequency`` steps (see `Optional Hyperparameters`_), the ``master`` instance
 will execute the ``model_fn`` in **evaluation** mode; i.e., ``features`` and ``labels`` are
-fed with the evaluation dataset defined by ``eval_input_fn``. See `Creating train_input_fn and eval_input_fn functions`_.
+fed with the evaluation dataset defined by ``eval_input_fn``. See `Creating train_input_fn and eval_input_fn functions`_. 
 
 For more information on training and evaluation process, see `tf.estimator.train_and_evaluate <https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/estimator/training.py#L256>`_.
 
@@ -1198,20 +1197,20 @@ For more information on fit, see `SageMaker Python SDK Overview <#sagemaker-pyth
 TensorFlow serving models
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 
-After your training job is complete in SageMaker and the ``fit`` call ends, the training job
-will generate a `TensorFlow serving <https://www.tensorflow.org/serving/serving_basic>`_
-model ready for deployment. Your TensorFlow serving model will be available in the S3 location
-``output_path`` that you specified when you created your `sagemaker.tensorflow.TensorFlow`
+After your training job is complete in SageMaker and the ``fit`` call ends, the training job 
+will generate a `TensorFlow serving <https://www.tensorflow.org/serving/serving_basic>`_ 
+model ready for deployment. Your TensorFlow serving model will be available in the S3 location 
+``output_path`` that you specified when you created your `sagemaker.tensorflow.TensorFlow` 
 estimator.
 
 Restoring from checkpoints
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 While your training job is executing, TensorFlow will generate checkpoints and save them in the S3
-location defined by ``checkpoint_path`` parameter in the ``TensorFlow`` constructor.
+location defined by ``checkpoint_path`` parameter in the ``TensorFlow`` constructor. 
 These checkpoints can be used to restore a previous session or to evaluate the current training using ``TensorBoard``.
 
-To restore a previous session, you just need to create a new ``sagemaker.tensorflow.TensorFlow``
+To restore a previous session, you just need to create a new ``sagemaker.tensorflow.TensorFlow`` 
 estimator pointing to the previous checkpoint path:
 
 .. code:: python
@@ -1228,20 +1227,20 @@ estimator pointing to the previous checkpoint path:
 Running TensorBoard
 ^^^^^^^^^^^^^^^^^^^
 
-When the ``fit`` parameter ``run_tensorboard_locally`` is set ``True``, all the checkpoint data
-located in ``checkpoint_path`` will be downloaded to a local temporary folder and a local
-``TensorBoard`` application will be watching that temporary folder.
+When the ``fit`` parameter ``run_tensorboard_locally`` is set ``True``, all the checkpoint data 
+located in ``checkpoint_path`` will be downloaded to a local temporary folder and a local 
+``TensorBoard`` application will be watching that temporary folder. 
 Every time a new checkpoint is created by the training job in the S3 bucket, ``fit`` will download that checkpoint to the same temporary folder and update ``TensorBoard``.
 
-When the ``fit`` method starts the training, it will log the port that ``TensorBoard`` is using
-to display metrics. The default port is **6006**, but another port can be chosen depending on
+When the ``fit`` method starts the training, it will log the port that ``TensorBoard`` is using 
+to display metrics. The default port is **6006**, but another port can be chosen depending on 
 availability. The port number will increase until finds an available port. After that, the port
 number will be printed in stdout.
 
 It takes a few minutes to provision containers and start the training job. TensorBoard will start to display metrics shortly after that.
 
-You can access TensorBoard locally at http://localhost:6006 or using your SakeMaker workspace at
-`https*workspace_base_url*proxy/6006/ <proxy/6006/>`_ (TensorBoard will not work if you forget to put the slash,
+You can access TensorBoard locally at http://localhost:6006 or using your SakeMaker workspace at 
+`https*workspace_base_url*proxy/6006/ <proxy/6006/>`_ (TensorBoard will not work if you forget to put the slash, 
 '/', in end of the url). If TensorBoard started on a different port, adjust these URLs to match.
 
 
@@ -1250,7 +1249,7 @@ Deploying TensorFlow Serving models
 
 After a ``TensorFlow`` Estimator has been fit, it saves a ``TensorFlow Serving`` model in
 the S3 location defined by ``output_path``. You can call ``deploy`` on a ``TensorFlow``
-estimator to create a SageMaker Endpoint.
+estimator to create a SageMaker Endpoint. 
 
 A common usage of the ``deploy`` method, after the ``TensorFlow`` estimator has been fit look
 like this:
@@ -1266,7 +1265,7 @@ like this:
   predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
 
 
-The code block above deploys a SageMaker Endpoint with one instance of the type 'ml.c4.xlarge'.
+The code block above deploys a SageMaker Endpoint with one instance of the type 'ml.c4.xlarge'. 
 
 What happens when deploy is called
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -1300,18 +1299,18 @@ The following code adds a prediction request to the previous code example:
 
   result = predictor.predict([6.4, 3.2, 4.5, 1.5])
 
-The ``predictor.predict`` method call takes one parameter, the input ``data`` for which you want the ``SageMaker Endpoint``
+The ``predictor.predict`` method call takes one parameter, the input ``data`` for which you want the ``SageMaker Endpoint`` 
 to provide inference. ``predict`` will serialize the input data, and send it in as request to the ``SageMaker Endpoint`` by
 an ``InvokeEndpoint`` SageMaker operation. ``InvokeEndpoint`` operation requests can be made by ``predictor.predict``, by
-boto3 ``SageMaker.runtime`` client or by AWS CLI.
+boto3 ``SageMaker.runtime`` client or by AWS CLI. 
 
-The ``SageMaker Endpoint`` web server will process the request, make an inference using the deployed model, and return a response.
+The ``SageMaker Endpoint`` web server will process the request, make an inference using the deployed model, and return a response. 
 The ``result`` returned by ``predict`` is
 a Python dictionary with the model prediction. In the code example above, the prediction ``result`` looks like this:
 
 .. code:: python
 
-  {'result':
+  {'result': 
     {'classifications': [
       {'classes': [
         {'label': '0', 'score': 0.0012890376383438706},
@@ -1324,8 +1323,8 @@ a Python dictionary with the model prediction. In the code example above, the pr
 Specifying the output of a prediction request
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-The format of the prediction ``result`` is determined by the parameter ``export_outputs`` of the `tf.estimator.EstimatorSpec <https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec>`_ that you returned when you created your ``model_fn``, see
-`Example of a complete model_fn`_ for an example of ``export_outputs``.
+The format of the prediction ``result`` is determined by the parameter ``export_outputs`` of the `tf.estimator.EstimatorSpec <https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec>`_ that you returned when you created your ``model_fn``, see 
+`Example of a complete model_fn`_ for an example of ``export_outputs``. 
 
 More information on how to create ``export_outputs`` can find in `specifying the outputs of a custom model <https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/docs_src/programmers_guide/saved_model.md#specifying-the-outputs-of-a-custom-model>`_.
 
@@ -1374,7 +1373,7 @@ An example of ``input_fn`` for the content-type "application/python-pickle" can
         else:
             # Handle other content-types here or raise an Exception
             # if the content type is not supported.
-            pass
+            pass  
 
 Overriding output precessing with an ``output_fn``
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -1392,7 +1391,7 @@ An example of ``output_fn`` for the accept type "application/python-pickle" can
         else:
             # Handle other content-types here or raise an Exception
             # if the content type is not supported.
-            pass
+            pass  
 
 A example with ``input_fn`` and ``output_fn`` above can be found in
 `here <https://github.com/aws/sagemaker-python-sdk/blob/master/tests/data/cifar_10/source/resnet_cifar_10.py#L143>`_.
@@ -1424,7 +1423,7 @@ The Docker images extend Ubuntu 16.04.
 
 AWS SageMaker Estimators
 ------------------------
-Amazon SageMaker provides several built-in machine learning algorithms that you can use for a variety of problem types.
+Amazon SageMaker provides several built-in machine learning algorithms that you can use for a variety of problem types. 
 
 The full list of algorithms is available on the AWS website: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html
 
@@ -1525,36 +1524,6 @@ Example code using the TensorFlow predictor:
 ::
 
     from sagemaker.tensorflow import TensorFlowPredictor
-
+    
     predictor = TensorFlowPredictor('myexistingendpoint')
     result = predictor.predict(['my request body'])
-
-
-BYO Model
------------------------------------------------
-You can also create an endpoint from an existing model rather than training one - i.e. bring your own model.
-
-First, package the files for the trained model into a ``.tar.gz`` file, and upload the archive to S3.
-
-Next, create a ``Model`` object that corresponds to the framework that you are using: `MXNetModel <https://sagemaker.readthedocs.io/en/latest/sagemaker.mxnet.html#mxnet-model>`__ or `TensorFlowModel <https://sagemaker.readthedocs.io/en/latest/sagemaker.tensorflow.html#tensorflow-model>`__.
-
-Example code using ``MXNetModel``:
-
-.. code:: python
-
-   from sagemaker.mxnet.model import MXNetModel
-
-   sagemaker_model = MXNetModel(model_data='s3://path/to/model.tar.gz',
-                                role='arn:aws:iam::accid:sagemaker-role',
-                                entry_point='entry_point.py')
-
-After that, invoke the ``deploy()`` method on the ``Model``:
-
-.. code:: python
-
-   predictor = sagemaker_model.deploy(initial_instance_count=1,
-                                      instance_type='ml.m4.xlarge')
-
-This returns a predictor the same way an ``Estimator`` does when ``deploy()`` is called. You can now get inferences just like with any other model deployed on Amazon SageMaker.
-
-A full example is available in the `Amazon SageMaker examples repository <https://github.com/ragavvenkatesan/amazon-sagemaker-examples/tree/[93m3c8394f21ee357da0b553b0ab024c5c5e425182a[0m/advanced_functionality/mxnet_mnist_byom>`__.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-03-07 11:51:36[0m
[92mHash: d610bfb5e7d1d4844468edb3c322eff6175adbda[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/dev[0m
[92mCommit: fix: container env generation for S3 URI and add test for the same (#2971)

[0m
@@ -26,8 +26,6 @@ from sagemaker.pytorch.model import PyTorchModel
 from sagemaker.sklearn.model import SKLearnModel
 from sagemaker.tensorflow.model import TensorFlowModel
 from sagemaker.xgboost.model import XGBoostModel
-from sagemaker.workflow.properties import Properties
-
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -44,6 +42,7 @@ GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
 BRANCH = "test-branch-git-config"
 COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
 ENTRY_POINT_INFERENCE = "inference.py"
+
 SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
 IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
 
@@ -72,23 +71,6 @@ def sagemaker_session():
     return sms
 
 
-@patch("shutil.rmtree", MagicMock())
-@patch("tarfile.open", MagicMock())
-@patch("os.listdir", MagicMock(return_value=[ENTRY_POINT_INFERENCE]))
-def test_prepare_container_def_with_model_src_s3_returns_correct_url(sagemaker_session):
-    model = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=MODEL_IMAGE,
-        model_data=Properties("Steps.MyStep"),
-    )
-    container_def = model.prepare_container_def(INSTANCE_TYPE, "ml.eia.medium")
-
-    assert container_def["Environment"]["SAGEMAKER_SUBMIT_DIRECTORY"] == SCRIPT_URI
-
-
 def test_prepare_container_def_with_model_data():
     model = Model(MODEL_IMAGE)
     container_def = model.prepare_container_def(INSTANCE_TYPE, "ml.eia.medium")

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 09:30:45[0m
[92mHash: b8d0badb7d75cbca7b7e4bcd380c5a0eb1582cb6[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: Syncing master-jumpstart with dev (#2887)

* feature: allow conditional parellel builds (#2727)

* fix endpoint bug (#2772)

Co-authored-by: Basil Beirouti <beirb@amazon.com>

* fix: local mode - support relative file structure (#2768)

* prepare release v2.72.0

* update development version to v2.72.1.dev0

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* prepare release v2.72.1

* update development version to v2.72.2.dev0

* fix: Model Registration with BYO scripts (#2797)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* fix: Add ContentType in test_auto_ml_describe

* fix: Re-deploy static integ test endpoint if it is not found

* documentation :SageMaker model parallel library 1.6.0 API doc (#2814)

* update smdmp change log, archive api doc for 1.4.0 and 1.5.0

* add no-index flags

* finish api doc archive

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* add all api docs

* add appendix, fix links

* structural changes, fix links

* incorporate feedback

* prepare release v2.72.1

* update development version to v2.72.2.dev0

Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>

* fix: fix kmeans test deletion sequence, increment lineage statics (#2815)

* fix: Increment static lineage pipeline (#2817)

* fix: Update CHANGELOG.md (#2832)

* prepare release v2.72.2

* update development version to v2.72.3.dev0

* change: update master from dev (#2836)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* prepare release v2.72.3

* update development version to v2.72.4.dev0

* fix: fixes unnecessary session call while generating pipeline definition for lambda step (#2824)

* feature: Add models_v2 under lineage context (#2800)

* feature: enable python 3.9 (#2802)

Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* change: Update CHANGELOG.md (#2842)

* fix: update pricing link (#2805)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* doc: Document the available ExecutionVariables (#2807)

* fix: Remove duplicate vertex/edge in query lineage (#2784)

* feature: Support model pipelines in CreateModelStep (#2845)

Co-authored-by: Payton Staub <pstaub@amazon.com>

* feature: support JsonGet/Join parameterization in tuning step Hyperparameters (#2833)

* doc: Enhance smddp 1.2.2 doc (#2852)

* feature: support checkpoint to be passed from estimator (#2849)

Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>

* fix: allow kms_key to be passed for processing step (#2779)

* feature: Adds support for Serverless inference (#2831)

* feature: Add support for SageMaker lineage queries in action (#2853)

* feature: Adds Lineage queries in artifact, context and trial components (#2838)

* feature: Add EMRStep support in Sagemaker pipeline (#2848)

Co-authored-by: chenxy <chenxy@amazon.com>

* prepare release v2.73.0

* update development version to v2.73.1.dev0

* feature: Add support for SageMaker lineage queries context (#2830)

* fix: support specifying a facet by its column index

Currently the Clarify BiasConfig only accepts facet name. Actually
Clarify analysis configuration supports both name and index. This
commit adds the same support to BiasConfig.

* doc: more documentation for serverless inference (#2859)

* prepare release v2.74.0

* update development version to v2.74.1.dev0

* Add deprecation warning in Clarify DataConfig (#2847)

* feature: Update instance types for integ test (#2881)

* feature: Adds support for async inference (#2846)

* fix: update to incorporate black v22, pin tox versions (#2889)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>

* make black happy

Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: ci <ci>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -16,7 +16,6 @@ from __future__ import absolute_import, print_function
 import json
 import logging
 import os
-from typing import Any, Dict
 import uuid
 from abc import ABCMeta, abstractmethod
 
@@ -48,10 +47,6 @@ from sagemaker.fw_utils import (
 )
 from sagemaker.inputs import TrainingInput
 from sagemaker.job import _Job
-from sagemaker.jumpstart.utils import (
-    add_jumpstart_tags,
-    update_inference_tags_with_jumpstart_training_tags,
-)
 from sagemaker.local import LocalSession
 from sagemaker.model import (
     CONTAINER_LOG_LEVEL_PARAM_NAME,
@@ -91,15 +86,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
     instance.
     """
 
-    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
-    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
-    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
-    INSTANCE_TYPE = "sagemaker_instance_type"
-    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
-    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
-    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
-    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
-
     def __init__(
         self,
         role,
@@ -133,13 +119,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        hyperparameters=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``EstimatorBase`` instance.
@@ -291,133 +270,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
                 use during training job (default: ``None``)
-            max_retry_attempts (int): The number of times to move a job to the STARTING status.
+             max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            hyperparameters (dict): Dictionary containing the hyperparameters to
-                initialize this estimator with. (Default: None).
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
 
         """
         instance_count = renamed_kwargs(
@@ -446,22 +305,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self.volume_kms_key = volume_kms_key
         self.max_run = max_run
         self.input_mode = input_mode
+        self.tags = tags
         self.metric_definitions = metric_definitions
         self.model_uri = model_uri
         self.model_channel_name = model_channel_name
         self.code_uri = None
         self.code_channel_name = "code"
-        self.source_dir = source_dir
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
-        self.code_location = code_location
-        self.entry_point = entry_point
-        self.dependencies = dependencies
-        self.uploaded_code = None
-        self.tags = add_jumpstart_tags(
-            tags=tags, training_model_uri=self.model_uri, training_script_uri=self.source_dir
-        )
+
         if self.instance_type in ("local", "local_gpu"):
             if self.instance_type == "local_gpu" and self.instance_count > 1:
                 raise RuntimeError("Distributed Training in Local GPU is not supported")
@@ -587,21 +437,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._ensure_base_job_name()
         return name_from_base(self.base_job_name)
 
-    @staticmethod
-    def _json_encode_hyperparameters(hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
-        """Applies Json encoding for certain Hyperparameter types, returns hyperparameters.
-
-        Args:
-            hyperparameters (dict): Dictionary of hyperparameters.
-        """
-        current_hyperparameters = hyperparameters
-        if current_hyperparameters is not None:
-            hyperparameters = {
-                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
-                for (k, v) in current_hyperparameters.items()
-            }
-        return hyperparameters
-
     def _prepare_for_training(self, job_name=None):
         """Set any values in the estimator that need to be set before training.
 
@@ -621,105 +456,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             else:
                 self.output_path = "s3://{}/".format(self.sagemaker_session.default_bucket())
 
-        if self.git_config:
-            updated_paths = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updated_paths["entry_point"]
-            self.source_dir = updated_paths["source_dir"]
-            self.dependencies = updated_paths["dependencies"]
-
-        if self.source_dir or self.entry_point or self.dependencies:
-
-            # validate source dir will raise a ValueError if there is something wrong with
-            # the source directory. We are intentionally not handling it because this is a
-            # critical error.
-            if self.source_dir and not self.source_dir.lower().startswith("s3://"):
-                validate_source_dir(self.entry_point, self.source_dir)
-
-            # if we are in local mode with local_code=True. We want the container to just
-            # mount the source dir instead of uploading to S3.
-            local_code = get_config_value("local.local_code", self.sagemaker_session.config)
-
-            if self.sagemaker_session.local_mode and local_code:
-                # if there is no source dir, use the directory containing the entry point.
-                if self.source_dir is None:
-                    self.source_dir = os.path.dirname(self.entry_point)
-                self.entry_point = os.path.basename(self.entry_point)
-
-                code_dir = "file://" + self.source_dir
-                script = self.entry_point
-            elif self.enable_network_isolation() and self.entry_point:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
-                script = self.uploaded_code.script_name
-                self.code_uri = self.uploaded_code.s3_prefix
-            else:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.uploaded_code.s3_prefix
-                script = self.uploaded_code.script_name
-
-            # Modify hyperparameters in-place to point to the right code directory and
-            # script URIs
-            self._script_mode_hyperparam_update(code_dir, script)
-
         self._prepare_rules()
         self._prepare_debugger_for_training()
         self._prepare_profiler_for_training()
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
-
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
-
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(hyperparams))
-
-    def _stage_user_code_in_s3(self) -> str:
-        """Upload the user training script to s3 and return the s3 URI.
-
-        Returns: s3 uri
-        """
-        local_mode = self.output_path.startswith("file://")
-
-        if self.code_location is None and local_mode:
-            code_bucket = self.sagemaker_session.default_bucket()
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = None
-        elif self.code_location is None:
-            code_bucket, _ = parse_s3_url(self.output_path)
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = self.output_kms_key
-        elif local_mode:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-            kms_key = None
-        else:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-
-            output_bucket, _ = parse_s3_url(self.output_path)
-            kms_key = self.output_kms_key if code_bucket == output_bucket else None
-
-        return tar_and_upload_dir(
-            session=self.sagemaker_session.boto_session,
-            bucket=code_bucket,
-            s3_key_prefix=code_s3_prefix,
-            script=self.entry_point,
-            directory=self.source_dir,
-            dependencies=self.dependencies,
-            kms_key=kms_key,
-            s3_resource=self.sagemaker_session.s3_resource,
-        )
-
     def _prepare_rules(self):
         """Rules list includes both debugger and profiler rules.
 
@@ -1226,10 +966,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
         model.name = model_name
 
-        tags = update_inference_tags_with_jumpstart_training_tags(
-            inference_tags=tags, training_tags=self.tags
-        )
-
         return model.deploy(
             instance_type=instance_type,
             initial_instance_count=initial_instance_count,
@@ -2003,12 +1739,6 @@ class Estimator(EstimatorBase):
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``Estimator`` instance.
@@ -2166,127 +1896,9 @@ class Estimator(EstimatorBase):
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
         """
         self.image_uri = image_uri
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
+        self.hyperparam_dict = hyperparameters.copy() if hyperparameters else {}
         super(Estimator, self).__init__(
             role,
             instance_count,
@@ -2319,13 +1931,6 @@ class Estimator(EstimatorBase):
             disable_profiler=disable_profiler,
             environment=environment,
             max_retry_attempts=max_retry_attempts,
-            container_log_level=container_log_level,
-            source_dir=source_dir,
-            git_config=git_config,
-            code_location=code_location,
-            entry_point=entry_point,
-            dependencies=dependencies,
-            hyperparameters=hyperparameters,
             **kwargs,
         )
 
@@ -2346,7 +1951,7 @@ class Estimator(EstimatorBase):
         training.
         """
         for k, v in kwargs.items():
-            self._hyperparameters[k] = v
+            self.hyperparam_dict[k] = v
 
     def hyperparameters(self):
         """Returns the hyperparameters as a dictionary to use for training.
@@ -2354,7 +1959,7 @@ class Estimator(EstimatorBase):
         The fit() method, that does the model training, calls this method to
         find the hyperparameters you specified.
         """
-        return self._hyperparameters
+        return self.hyperparam_dict
 
     def create_model(
         self,
@@ -2430,6 +2035,15 @@ class Framework(EstimatorBase):
 
     _framework_name = None
 
+    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
+    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
+    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
+    INSTANCE_TYPE = "sagemaker_instance_type"
+    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
+    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
+    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
+    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
+
     def __init__(
         self,
         entry_point,
@@ -2643,23 +2257,48 @@ class Framework(EstimatorBase):
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
-        self._validate_and_set_debugger_configs()
+        if self.git_config:
+            updated_paths = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updated_paths["entry_point"]
+            self.source_dir = updated_paths["source_dir"]
+            self.dependencies = updated_paths["dependencies"]
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
+        # validate source dir will raise a ValueError if there is something wrong with the
+        # source directory. We are intentionally not handling it because this is a critical error.
+        if self.source_dir and not self.source_dir.lower().startswith("s3://"):
+            validate_source_dir(self.entry_point, self.source_dir)
+
+        # if we are in local mode with local_code=True. We want the container to just
+        # mount the source dir instead of uploading to S3.
+        local_code = get_config_value("local.local_code", self.sagemaker_session.config)
+        if self.sagemaker_session.local_mode and local_code:
+            # if there is no source dir, use the directory containing the entry point.
+            if self.source_dir is None:
+                self.source_dir = os.path.dirname(self.entry_point)
+            self.entry_point = os.path.basename(self.entry_point)
+
+            code_dir = "file://" + self.source_dir
+            script = self.entry_point
+        elif self.enable_network_isolation() and self.entry_point:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
+            script = self.uploaded_code.script_name
+            self.code_uri = self.uploaded_code.s3_prefix
+        else:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.uploaded_code.s3_prefix
+            script = self.uploaded_code.script_name
 
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
+        # Modify hyperparameters in-place to point to the right code directory and script URIs
+        self._hyperparameters[DIR_PARAM_NAME] = code_dir
+        self._hyperparameters[SCRIPT_PARAM_NAME] = script
+        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
+        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name
+        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
 
-        self._hyperparameters.update(hyperparams)
+        self._validate_and_set_debugger_configs()
 
     def _validate_and_set_debugger_configs(self):
         """Set defaults for debugging."""
@@ -2689,6 +2328,44 @@ class Framework(EstimatorBase):
                 self.environment = {}
             self.environment[DEBUGGER_FLAG] = "0"
 
+    def _stage_user_code_in_s3(self):
+        """Upload the user training script to s3 and return the location.
+
+        Returns: s3 uri
+        """
+        local_mode = self.output_path.startswith("file://")
+
+        if self.code_location is None and local_mode:
+            code_bucket = self.sagemaker_session.default_bucket()
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = None
+        elif self.code_location is None:
+            code_bucket, _ = parse_s3_url(self.output_path)
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = self.output_kms_key
+        elif local_mode:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+            kms_key = None
+        else:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+
+            output_bucket, _ = parse_s3_url(self.output_path)
+            kms_key = self.output_kms_key if code_bucket == output_bucket else None
+
+        return tar_and_upload_dir(
+            session=self.sagemaker_session.boto_session,
+            bucket=code_bucket,
+            s3_key_prefix=code_s3_prefix,
+            script=self.entry_point,
+            directory=self.source_dir,
+            dependencies=self.dependencies,
+            kms_key=kms_key,
+            s3_resource=self.sagemaker_session.s3_resource,
+            settings=self.sagemaker_session.settings,
+        )
+
     def _model_source_dir(self):
         """Get the appropriate value to pass as ``source_dir`` to a model constructor.
 
@@ -2719,10 +2396,6 @@ class Framework(EstimatorBase):
 
         return None
 
-    def set_hyperparameters(self, **kwargs):
-        """Escape the dict argument as JSON, update the private hyperparameter attribute."""
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(kwargs))
-
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
@@ -2732,7 +2405,7 @@ class Framework(EstimatorBase):
         Returns:
             dict[str, str]: The hyperparameters.
         """
-        return EstimatorBase._json_encode_hyperparameters(self._hyperparameters)
+        return self._json_encode_hyperparameters(self._hyperparameters)
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
@@ -2773,32 +2446,51 @@ class Framework(EstimatorBase):
 
         return init_params
 
-    def training_image_uri(self, region=None):
+    def training_image_uri(self):
         """Return the Docker image to use for training.
 
         The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does
         the model training, calls this method to find the image to use for model
         training.
 
-        Args:
-            region (str): Optional. AWS region to use for image URI. Default: AWS region associated
-                with the SageMaker session.
-
         Returns:
             str: The URI of the Docker image.
         """
+        if self.image_uri:
+            return self.image_uri
+        if hasattr(self, "distribution"):
+            distribution = self.distribution  # pylint: disable=no-member
+        else:
+            distribution = None
+        compiler_config = getattr(self, "compiler_config", None)
+
+        if hasattr(self, "tensorflow_version") or hasattr(self, "pytorch_version"):
+            processor = image_uris._processor(self.instance_type, ["cpu", "gpu"])
+            is_native_huggingface_gpu = processor == "gpu" and not compiler_config
+            container_version = "cu110-ubuntu18.04" if is_native_huggingface_gpu else None
+            if self.tensorflow_version is not None:  # pylint: disable=no-member
+                base_framework_version = (
+                    f"tensorflow{self.tensorflow_version}"  # pylint: disable=no-member
+                )
+            else:
+                base_framework_version = (
+                    f"pytorch{self.pytorch_version}"  # pylint: disable=no-member
+                )
+        else:
+            container_version = None
+            base_framework_version = None
 
-        return image_uris.get_training_image_uri(
-            region=region or self.sagemaker_session.boto_region_name,
-            framework=self._framework_name,
-            framework_version=self.framework_version,  # pylint: disable=no-member
-            py_version=self.py_version,  # pylint: disable=no-member
-            image_uri=self.image_uri,
-            distribution=getattr(self, "distribution", None),
-            compiler_config=getattr(self, "compiler_config", None),
-            tensorflow_version=getattr(self, "tensorflow_version", None),
-            pytorch_version=getattr(self, "pytorch_version", None),
+        return image_uris.retrieve(
+            self._framework_name,
+            self.sagemaker_session.boto_region_name,
             instance_type=self.instance_type,
+            version=self.framework_version,  # pylint: disable=no-member
+            py_version=self.py_version,  # pylint: disable=no-member
+            image_scope="training",
+            distribution=distribution,
+            base_framework_version=base_framework_version,
+            container_version=container_version,
+            training_compiler_config=compiler_config,
         )
 
     @classmethod
@@ -2851,6 +2543,17 @@ class Framework(EstimatorBase):
         )
         return estimator
 
+    @staticmethod
+    def _json_encode_hyperparameters(hyperparameters):
+        """Placeholder docstring"""
+        current_hyperparameters = hyperparameters
+        if current_hyperparameters is not None:
+            hyperparameters = {
+                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
+                for (k, v) in current_hyperparameters.items()
+            }
+        return hyperparameters
+
     @classmethod
     def _update_init_params(cls, hp, tf_arguments):
         """Placeholder docstring"""

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 09:30:45[0m
[92mHash: b8d0badb7d75cbca7b7e4bcd380c5a0eb1582cb6[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: Syncing master-jumpstart with dev (#2887)

* feature: allow conditional parellel builds (#2727)

* fix endpoint bug (#2772)

Co-authored-by: Basil Beirouti <beirb@amazon.com>

* fix: local mode - support relative file structure (#2768)

* prepare release v2.72.0

* update development version to v2.72.1.dev0

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* prepare release v2.72.1

* update development version to v2.72.2.dev0

* fix: Model Registration with BYO scripts (#2797)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* fix: Add ContentType in test_auto_ml_describe

* fix: Re-deploy static integ test endpoint if it is not found

* documentation :SageMaker model parallel library 1.6.0 API doc (#2814)

* update smdmp change log, archive api doc for 1.4.0 and 1.5.0

* add no-index flags

* finish api doc archive

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* add all api docs

* add appendix, fix links

* structural changes, fix links

* incorporate feedback

* prepare release v2.72.1

* update development version to v2.72.2.dev0

Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>

* fix: fix kmeans test deletion sequence, increment lineage statics (#2815)

* fix: Increment static lineage pipeline (#2817)

* fix: Update CHANGELOG.md (#2832)

* prepare release v2.72.2

* update development version to v2.72.3.dev0

* change: update master from dev (#2836)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* prepare release v2.72.3

* update development version to v2.72.4.dev0

* fix: fixes unnecessary session call while generating pipeline definition for lambda step (#2824)

* feature: Add models_v2 under lineage context (#2800)

* feature: enable python 3.9 (#2802)

Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* change: Update CHANGELOG.md (#2842)

* fix: update pricing link (#2805)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* doc: Document the available ExecutionVariables (#2807)

* fix: Remove duplicate vertex/edge in query lineage (#2784)

* feature: Support model pipelines in CreateModelStep (#2845)

Co-authored-by: Payton Staub <pstaub@amazon.com>

* feature: support JsonGet/Join parameterization in tuning step Hyperparameters (#2833)

* doc: Enhance smddp 1.2.2 doc (#2852)

* feature: support checkpoint to be passed from estimator (#2849)

Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>

* fix: allow kms_key to be passed for processing step (#2779)

* feature: Adds support for Serverless inference (#2831)

* feature: Add support for SageMaker lineage queries in action (#2853)

* feature: Adds Lineage queries in artifact, context and trial components (#2838)

* feature: Add EMRStep support in Sagemaker pipeline (#2848)

Co-authored-by: chenxy <chenxy@amazon.com>

* prepare release v2.73.0

* update development version to v2.73.1.dev0

* feature: Add support for SageMaker lineage queries context (#2830)

* fix: support specifying a facet by its column index

Currently the Clarify BiasConfig only accepts facet name. Actually
Clarify analysis configuration supports both name and index. This
commit adds the same support to BiasConfig.

* doc: more documentation for serverless inference (#2859)

* prepare release v2.74.0

* update development version to v2.74.1.dev0

* Add deprecation warning in Clarify DataConfig (#2847)

* feature: Update instance types for integ test (#2881)

* feature: Adds support for async inference (#2846)

* fix: update to incorporate black v22, pin tox versions (#2889)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>

* make black happy

Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: ci <ci>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -18,7 +18,6 @@ import json
 import logging
 import os
 import re
-import copy
 
 import sagemaker
 from sagemaker import (
@@ -35,7 +34,6 @@ from sagemaker.deprecations import removed_kwargs
 from sagemaker.predictor import PredictorBase
 from sagemaker.serverless import ServerlessInferenceConfig
 from sagemaker.transformer import Transformer
-from sagemaker.jumpstart.utils import add_jumpstart_tags
 from sagemaker.utils import unique_name_from_base
 from sagemaker.async_inference import AsyncInferenceConfig
 from sagemaker.predictor_async import AsyncPredictor
@@ -63,15 +61,6 @@ class ModelBase(abc.ABC):
         """Destroy resources associated with this model."""
 
 
-SCRIPT_PARAM_NAME = "sagemaker_program"
-DIR_PARAM_NAME = "sagemaker_submit_directory"
-CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
-JOB_NAME_PARAM_NAME = "sagemaker_job_name"
-MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
-SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
-SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
-
-
 class Model(ModelBase):
     """A SageMaker ``Model`` that can be deployed to an ``Endpoint``."""
 
@@ -88,12 +77,6 @@ class Model(ModelBase):
         enable_network_isolation=False,
         model_kms_key=None,
         image_config=None,
-        source_dir=None,
-        code_location=None,
-        entry_point=None,
-        container_log_level=logging.INFO,
-        dependencies=None,
-        git_config=None,
     ):
         """Initialize an SageMaker ``Model``.
 
@@ -135,124 +118,6 @@ class Model(ModelBase):
                 model container is pulled from ECR, or private registry in your
                 VPC. By default it is set to pull model container image from
                 ECR. (default: None).
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code will be uploaded and the S3 location
-                will be used instead.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
-            code_location (str): Name of the S3 bucket where custom code is
-                uploaded (default: None). If not specified, default bucket
-                created by ``sagemaker.session.Session`` is used.
-            entry_point (str): Path (absolute or relative) to the Python source
-                file which should be executed as the entry point to model
-                hosting (default: None). If ``source_dir`` is specified,
-                then ``entry_point`` must point to a file located at the root of
-                ``source_dir``. If 'git_config' is provided, 'entry_point' should
-                be a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Model(entry_point='inference.py',
-                    ...       dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ inference.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-
         """
         self.model_data = model_data
         self.image_uri = image_uri
@@ -270,24 +135,6 @@ class Model(ModelBase):
         self._enable_network_isolation = enable_network_isolation
         self.model_kms_key = model_kms_key
         self.image_config = image_config
-        self.entry_point = entry_point
-        self.source_dir = source_dir
-        self.dependencies = dependencies or []
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        if code_location:
-            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
-        else:
-            self.bucket, self.key_prefix = None, None
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
-        self.uploaded_code = None
-        self.repacked_model_data = None
 
     def register(
         self,
@@ -399,91 +246,10 @@ class Model(ModelBase):
         Returns:
             dict: A container definition object usable with the CreateModel API.
         """
-        deploy_key_prefix = fw_utils.model_code_key_prefix(
-            self.key_prefix, self.name, self.image_uri
-        )
-        deploy_env = copy.deepcopy(self.env)
-        if self.source_dir or self.dependencies or self.entry_point or self.git_config:
-            is_repack = (
-                self.source_dir and self.entry_point and not (self.key_prefix or self.git_config)
-            )
-            self._upload_code(deploy_key_prefix, repack=is_repack)
-            deploy_env.update(self._script_mode_env_vars())
         return sagemaker.container_def(
-            self.image_uri,
-            self.repacked_model_data or self.model_data,
-            deploy_env,
-            image_config=self.image_config,
+            self.image_uri, self.model_data, self.env, image_config=self.image_config
         )
 
-    def _upload_code(self, key_prefix: str, repack: bool = False) -> None:
-        """Uploads code to S3 to be used with script mode with SageMaker inference.
-
-        Args:
-            key_prefix (str): The S3 key associated with the ``code_location`` parameter of the
-                ``Model`` class.
-            repack (bool): Optional. Set to ``True`` to indicate that the source code and model
-                artifact should be repackaged into a new S3 object. (default: False).
-        """
-        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
-        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
-            self.uploaded_code = None
-        elif not repack:
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            self.uploaded_code = fw_utils.tar_and_upload_dir(
-                session=self.sagemaker_session.boto_session,
-                bucket=bucket,
-                s3_key_prefix=key_prefix,
-                script=self.entry_point,
-                directory=self.source_dir,
-                dependencies=self.dependencies,
-            )
-
-        if repack and self.model_data is not None and self.entry_point is not None:
-            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
-                # model is not yet there, defer repacking to later during pipeline execution
-                return
-
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
-
-            utils.repack_model(
-                inference_script=self.entry_point,
-                source_directory=self.source_dir,
-                dependencies=self.dependencies,
-                model_uri=self.model_data,
-                repacked_model_uri=repacked_model_data,
-                sagemaker_session=self.sagemaker_session,
-                kms_key=self.model_kms_key,
-            )
-
-            self.repacked_model_data = repacked_model_data
-            self.uploaded_code = fw_utils.UploadedCode(
-                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
-            )
-
-    def _script_mode_env_vars(self):
-        """Placeholder docstring"""
-        script_name = None
-        dir_name = None
-        if self.uploaded_code:
-            script_name = self.uploaded_code.script_name
-            if self.enable_network_isolation():
-                dir_name = "/opt/ml/model/code"
-            else:
-                dir_name = self.uploaded_code.s3_prefix
-        elif self.entry_point is not None:
-            script_name = self.entry_point
-            if self.source_dir is not None:
-                dir_name = "file://" + self.source_dir
-
-        return {
-            SCRIPT_PARAM_NAME.upper(): script_name or str(),
-            DIR_PARAM_NAME.upper(): dir_name or str(),
-            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
-            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
-        }
-
     def enable_network_isolation(self):
         """Whether to enable network isolation when creating this Model
 
@@ -1016,10 +782,6 @@ class Model(ModelBase):
         removed_kwargs("update_endpoint", kwargs)
         self._init_sagemaker_session_if_does_not_exist(instance_type)
 
-        tags = add_jumpstart_tags(
-            tags=tags, inference_model_uri=self.model_data, inference_script_uri=self.source_dir
-        )
-
         if self.role is None:
             raise ValueError("Role can not be null for deploying a model")
 
@@ -1194,6 +956,15 @@ class Model(ModelBase):
         self.sagemaker_session.delete_model(self.name)
 
 
+SCRIPT_PARAM_NAME = "sagemaker_program"
+DIR_PARAM_NAME = "sagemaker_submit_directory"
+CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
+JOB_NAME_PARAM_NAME = "sagemaker_job_name"
+MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
+SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
+SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
+
+
 class FrameworkModel(Model):
     """A Model for working with an SageMaker ``Framework``.
 
@@ -1371,14 +1142,113 @@ class FrameworkModel(Model):
             env=env,
             name=name,
             sagemaker_session=sagemaker_session,
-            source_dir=source_dir,
-            code_location=code_location,
-            entry_point=entry_point,
-            container_log_level=container_log_level,
-            dependencies=dependencies,
-            git_config=git_config,
             **kwargs,
         )
+        self.entry_point = entry_point
+        self.source_dir = source_dir
+        self.dependencies = dependencies or []
+        self.git_config = git_config
+        self.container_log_level = container_log_level
+        if code_location:
+            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
+        else:
+            self.bucket, self.key_prefix = None, None
+        if self.git_config:
+            updates = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updates["entry_point"]
+            self.source_dir = updates["source_dir"]
+            self.dependencies = updates["dependencies"]
+        self.uploaded_code = None
+        self.repacked_model_data = None
+
+    def prepare_container_def(self, instance_type=None, accelerator_type=None):
+        """Return a container definition with framework configuration.
+
+        Framework configuration is set in model environment variables.
+        This also uploads user-supplied code to S3.
+
+        Args:
+            instance_type (str): The EC2 instance type to deploy this Model to.
+                For example, 'ml.p2.xlarge'.
+            accelerator_type (str): The Elastic Inference accelerator type to
+                deploy to the instance for loading and making inferences to the
+                model. For example, 'ml.eia1.medium'.
+
+        Returns:
+            dict[str, str]: A container definition object usable with the
+            CreateModel API.
+        """
+        deploy_key_prefix = fw_utils.model_code_key_prefix(
+            self.key_prefix, self.name, self.image_uri
+        )
+        self._upload_code(deploy_key_prefix)
+        deploy_env = dict(self.env)
+        deploy_env.update(self._framework_env_vars())
+        return sagemaker.container_def(self.image_uri, self.model_data, deploy_env)
+
+    def _upload_code(self, key_prefix, repack=False):
+        """Placeholder Docstring"""
+        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
+        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
+            self.uploaded_code = None
+        elif not repack:
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            self.uploaded_code = fw_utils.tar_and_upload_dir(
+                session=self.sagemaker_session.boto_session,
+                bucket=bucket,
+                s3_key_prefix=key_prefix,
+                script=self.entry_point,
+                directory=self.source_dir,
+                dependencies=self.dependencies,
+                settings=self.sagemaker_session.settings,
+            )
+
+        if repack and self.model_data is not None and self.entry_point is not None:
+            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
+                # model is not yet there, defer repacking to later during pipeline execution
+                return
+
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
+
+            utils.repack_model(
+                inference_script=self.entry_point,
+                source_directory=self.source_dir,
+                dependencies=self.dependencies,
+                model_uri=self.model_data,
+                repacked_model_uri=repacked_model_data,
+                sagemaker_session=self.sagemaker_session,
+                kms_key=self.model_kms_key,
+            )
+
+            self.repacked_model_data = repacked_model_data
+            self.uploaded_code = fw_utils.UploadedCode(
+                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
+            )
+
+    def _framework_env_vars(self):
+        """Placeholder docstring"""
+        script_name = None
+        dir_name = None
+        if self.uploaded_code:
+            script_name = self.uploaded_code.script_name
+            if self.enable_network_isolation():
+                dir_name = "/opt/ml/model/code"
+            else:
+                dir_name = self.uploaded_code.s3_prefix
+        elif self.entry_point is not None:
+            script_name = self.entry_point
+            if self.source_dir is not None:
+                dir_name = "file://" + self.source_dir
+
+        return {
+            SCRIPT_PARAM_NAME.upper(): script_name or str(),
+            DIR_PARAM_NAME.upper(): dir_name or str(),
+            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
+            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
+        }
 
 
 class ModelPackage(Model):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-02-03 09:30:45[0m
[92mHash: b8d0badb7d75cbca7b7e4bcd380c5a0eb1582cb6[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: Syncing master-jumpstart with dev (#2887)

* feature: allow conditional parellel builds (#2727)

* fix endpoint bug (#2772)

Co-authored-by: Basil Beirouti <beirb@amazon.com>

* fix: local mode - support relative file structure (#2768)

* prepare release v2.72.0

* update development version to v2.72.1.dev0

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* prepare release v2.72.1

* update development version to v2.72.2.dev0

* fix: Model Registration with BYO scripts (#2797)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* fix: Add ContentType in test_auto_ml_describe

* fix: Re-deploy static integ test endpoint if it is not found

* documentation :SageMaker model parallel library 1.6.0 API doc (#2814)

* update smdmp change log, archive api doc for 1.4.0 and 1.5.0

* add no-index flags

* finish api doc archive

* fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

* fix: Prevent repack_model script from referencing nonexistent directories (#2755)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* fix: S3Input - add support for instance attributes (#2754)

* fix: typos and broken link (#2765)

Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>

* add all api docs

* add appendix, fix links

* structural changes, fix links

* incorporate feedback

* prepare release v2.72.1

* update development version to v2.72.2.dev0

Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>

* fix: fix kmeans test deletion sequence, increment lineage statics (#2815)

* fix: Increment static lineage pipeline (#2817)

* fix: Update CHANGELOG.md (#2832)

* prepare release v2.72.2

* update development version to v2.72.3.dev0

* change: update master from dev (#2836)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* prepare release v2.72.3

* update development version to v2.72.4.dev0

* fix: fixes unnecessary session call while generating pipeline definition for lambda step (#2824)

* feature: Add models_v2 under lineage context (#2800)

* feature: enable python 3.9 (#2802)

Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>

* change: Update CHANGELOG.md (#2842)

* fix: update pricing link (#2805)

Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>

* doc: Document the available ExecutionVariables (#2807)

* fix: Remove duplicate vertex/edge in query lineage (#2784)

* feature: Support model pipelines in CreateModelStep (#2845)

Co-authored-by: Payton Staub <pstaub@amazon.com>

* feature: support JsonGet/Join parameterization in tuning step Hyperparameters (#2833)

* doc: Enhance smddp 1.2.2 doc (#2852)

* feature: support checkpoint to be passed from estimator (#2849)

Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>

* fix: allow kms_key to be passed for processing step (#2779)

* feature: Adds support for Serverless inference (#2831)

* feature: Add support for SageMaker lineage queries in action (#2853)

* feature: Adds Lineage queries in artifact, context and trial components (#2838)

* feature: Add EMRStep support in Sagemaker pipeline (#2848)

Co-authored-by: chenxy <chenxy@amazon.com>

* prepare release v2.73.0

* update development version to v2.73.1.dev0

* feature: Add support for SageMaker lineage queries context (#2830)

* fix: support specifying a facet by its column index

Currently the Clarify BiasConfig only accepts facet name. Actually
Clarify analysis configuration supports both name and index. This
commit adds the same support to BiasConfig.

* doc: more documentation for serverless inference (#2859)

* prepare release v2.74.0

* update development version to v2.74.1.dev0

* Add deprecation warning in Clarify DataConfig (#2847)

* feature: Update instance types for integ test (#2881)

* feature: Adds support for async inference (#2846)

* fix: update to incorporate black v22, pin tox versions (#2889)

Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>

* make black happy

Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: ci <ci>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>
Co-authored-by: Xinghan Chen <47259301+xchen909@users.noreply.github.com>
Co-authored-by: Navin Soni <navinsoni89@gmail.com>
Co-authored-by: Tulio Casagrande <tuliocasagrande@gmail.com>
Co-authored-by: jerrypeng7773 <50377760+jerrypeng7773@users.noreply.github.com>
Co-authored-by: marckarp <mkarpmar@gmail.com>
Co-authored-by: marckarp <karpmar@8c859028ba7e.ant.amazon.com>
Co-authored-by: jayatalr <69013381+jayatalr@users.noreply.github.com>
Co-authored-by: bhaoz <96764005+bhaoz@users.noreply.github.com>
Co-authored-by: Ethan Cheng <shouhc@amazon.com>
Co-authored-by: chenxy <chenxy@amazon.com>
Co-authored-by: Xiaoguang Chen <xgchen@amazon.com>
Co-authored-by: keerthanvasist <kvasist@amazon.com>
Co-authored-by: Mufaddal Rohawala <muffi179@gmail.com>
Co-authored-by: Shreya Pandit <pandishr@amazon.com>[0m
@@ -11,21 +11,12 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
-from unittest.mock import MagicMock
 
 import pytest
 from mock import Mock, patch
 
 import sagemaker
-from sagemaker.model import FrameworkModel, Model
-from sagemaker.huggingface.model import HuggingFaceModel
-from sagemaker.jumpstart.constants import JUMPSTART_BUCKET_NAME_SET
-from sagemaker.jumpstart.enums import JumpStartTag
-from sagemaker.mxnet.model import MXNetModel
-from sagemaker.pytorch.model import PyTorchModel
-from sagemaker.sklearn.model import SKLearnModel
-from sagemaker.tensorflow.model import TensorFlowModel
-from sagemaker.xgboost.model import XGBoostModel
+from sagemaker.model import Model
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -36,39 +27,10 @@ INSTANCE_COUNT = 2
 INSTANCE_TYPE = "ml.c4.4xlarge"
 ROLE = "some-role"
 
-REGION = "us-west-2"
-BUCKET_NAME = "some-bucket-name"
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-ENTRY_POINT_INFERENCE = "inference.py"
 
-SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
-IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
-
-
-class DummyFrameworkModel(FrameworkModel):
-    def __init__(self, **kwargs):
-        super(DummyFrameworkModel, self).__init__(
-            **kwargs,
-        )
-
-
-@pytest.fixture()
+@pytest.fixture
 def sagemaker_session():
-    boto_mock = Mock(name="boto_session", region_name=REGION)
-    sms = MagicMock(
-        name="sagemaker_session",
-        boto_session=boto_mock,
-        boto_region_name=REGION,
-        config=None,
-        local_mode=False,
-        s3_client=None,
-        s3_resource=None,
-    )
-    sms.default_bucket = Mock(name="default_bucket", return_value=BUCKET_NAME)
-
-    return sms
+    return Mock()
 
 
 def test_prepare_container_def_with_model_data():
@@ -383,171 +345,3 @@ def test_delete_model_no_name(sagemaker_session):
     ):
         model.delete_model()
     sagemaker_session.delete_model.assert_not_called()
-
-
-@patch("time.strftime", MagicMock(return_value=TIMESTAMP))
-@patch("sagemaker.utils.repack_model")
-def test_script_mode_model_same_calls_as_framework(repack_model, sagemaker_session):
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert len(sagemaker_session.create_model.call_args_list) == 1
-    assert len(sagemaker_session.endpoint_from_production_variants.call_args_list) == 1
-    assert len(repack_model.call_args_list) == 1
-
-    generic_model_create_model_args = sagemaker_session.create_model.call_args_list
-    generic_model_endpoint_from_production_variants_args = (
-        sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    generic_model_repack_model_args = repack_model.call_args_list
-
-    sagemaker_session.create_model.reset_mock()
-    sagemaker_session.endpoint_from_production_variants.reset_mock()
-    repack_model.reset_mock()
-
-    t = DummyFrameworkModel(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert generic_model_create_model_args == sagemaker_session.create_model.call_args_list
-    assert (
-        generic_model_endpoint_from_production_variants_args
-        == sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    assert generic_model_repack_model_args == repack_model.call_args_list
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_succeed_model_class(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, sourcedir, dependency: {
-        "entry_point": "entry_point",
-        "source_dir": "/tmp/repo_dir/source_dir",
-        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"],
-    }
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    model = Model(
-        sagemaker_session=sagemaker_session,
-        entry_point=entry_point,
-        source_dir=source_dir,
-        dependencies=dependencies,
-        git_config=git_config,
-        image_uri=IMAGE_URI,
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, source_dir, dependencies)
-    assert model.entry_point == "entry_point"
-    assert model.source_dir == "/tmp/repo_dir/source_dir"
-    assert model.dependencies == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
-
-
-@patch("sagemaker.utils.repack_model")
-def test_script_mode_model_tags_jumpstart_models(repack_model, sagemaker_session):
-
-    jumpstart_source_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/source_dirs/source.tar.gz"
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=jumpstart_source_dir,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert sagemaker_session.create_model.call_args_list[0][1]["tags"] == [
-        {
-            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-            "Value": jumpstart_source_dir,
-        },
-    ]
-    assert sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"] == [
-        {
-            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-            "Value": jumpstart_source_dir,
-        },
-    ]
-
-    non_jumpstart_source_dir = "s3://blah/blah/blah"
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=non_jumpstart_source_dir,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert {
-        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-        "Value": non_jumpstart_source_dir,
-    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-    assert {
-        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
-        "Value": non_jumpstart_source_dir,
-    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-
-@patch("sagemaker.utils.repack_model")
-@patch("sagemaker.fw_utils.tar_and_upload_dir")
-def test_all_framework_models_add_jumpstart_tags(
-    repack_model, tar_and_uload_dir, sagemaker_session
-):
-    framework_model_classes_to_kwargs = {
-        PyTorchModel: {"framework_version": "1.5.0", "py_version": "py3"},
-        TensorFlowModel: {
-            "framework_version": "2.3",
-        },
-        HuggingFaceModel: {
-            "pytorch_version": "1.7.1",
-            "py_version": "py36",
-            "transformers_version": "4.6.1",
-        },
-        MXNetModel: {"framework_version": "1.7.0", "py_version": "py3"},
-        SKLearnModel: {
-            "framework_version": "0.23-1",
-        },
-        XGBoostModel: {
-            "framework_version": "1.3-1",
-        },
-    }
-    jumpstart_model_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/model_dirs/model.tar.gz"
-    for framework_model_class, kwargs in framework_model_classes_to_kwargs.items():
-        framework_model_class(
-            entry_point=ENTRY_POINT_INFERENCE,
-            role=ROLE,
-            sagemaker_session=sagemaker_session,
-            model_data=jumpstart_model_dir,
-            **kwargs,
-        ).deploy(instance_type="ml.m2.xlarge", initial_instance_count=INSTANCE_COUNT)
-
-        assert {
-            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
-            "Value": jumpstart_model_dir,
-        } in sagemaker_session.create_model.call_args_list[0][1]["tags"]
-
-        assert {
-            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
-            "Value": jumpstart_model_dir,
-        } in sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"]
-
-        sagemaker_session.create_model.reset_mock()
-        sagemaker_session.endpoint_from_production_variants.reset_mock()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-25 18:14:53[0m
[92mHash: 109730e30e6787c86f82e3489caa037efad5dd53[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: Add support for SageMaker lineage queries context (#2830)

[0m
@@ -16,6 +16,7 @@ from __future__ import absolute_import, print_function
 import json
 import logging
 import os
+from typing import Any, Dict
 import uuid
 from abc import ABCMeta, abstractmethod
 
@@ -47,6 +48,10 @@ from sagemaker.fw_utils import (
 )
 from sagemaker.inputs import TrainingInput
 from sagemaker.job import _Job
+from sagemaker.jumpstart.utils import (
+    add_jumpstart_tags,
+    update_inference_tags_with_jumpstart_training_tags,
+)
 from sagemaker.local import LocalSession
 from sagemaker.model import (
     CONTAINER_LOG_LEVEL_PARAM_NAME,
@@ -86,6 +91,15 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
     instance.
     """
 
+    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
+    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
+    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
+    INSTANCE_TYPE = "sagemaker_instance_type"
+    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
+    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
+    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
+    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
+
     def __init__(
         self,
         role,
@@ -119,6 +133,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
+        source_dir=None,
+        git_config=None,
+        hyperparameters=None,
+        container_log_level=logging.INFO,
+        code_location=None,
+        entry_point=None,
+        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``EstimatorBase`` instance.
@@ -270,13 +291,133 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
                 use during training job (default: ``None``)
-             max_retry_attempts (int): The number of times to move a job to the STARTING status.
+            max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
+                with any other training source code dependencies aside from the entry
+                point file (default: None). If ``source_dir`` is an S3 URI, it must
+                point to a tar.gz file. Structure within this directory are preserved
+                when training on Amazon SageMaker. If 'git_config' is provided,
+                'source_dir' should be a relative location to a directory in the Git
+                repo.
+
+                .. admonition:: Example
+
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- train.py
+                    >>>         |----- test.py
+
+                    and you need 'train.py' as entry point and 'test.py' as
+                    training source code as well, you can assign
+                    entry_point='train.py', source_dir='src'.
+            git_config (dict[str, str]): Git configurations used for cloning
+                files, including ``repo``, ``branch``, ``commit``,
+                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
+                ``repo`` field is required. All other fields are optional.
+                ``repo`` specifies the Git repository where your training script
+                is stored. If you don't provide ``branch``, the default value
+                'master' is used. If you don't provide ``commit``, the latest
+                commit in the specified branch is used. .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then
+                    checkout the 'master' branch, and checkout the specified
+                    commit.
+
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
+                used for authentication. For GitHub (or other Git) accounts, set
+                ``2FA_enabled`` to 'True' if two-factor authentication is
+                enabled for the account, otherwise set it to 'False'. If you do
+                not provide a value for ``2FA_enabled``, a default value of
+                'False' is used. CodeCommit does not support two-factor
+                authentication, so do not provide "2FA_enabled" with CodeCommit
+                repositories.
+
+                For GitHub and other Git repos, when SSH URLs are provided, it
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
+                for authentication if provided. If required authentication info
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
+
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
+                should not be provided. There is no token in CodeCommit, so
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
+            hyperparameters (dict): Dictionary containing the hyperparameters to
+                initialize this estimator with. (Default: None).
+            container_log_level (int): Log level to use within the container
+                (default: logging.INFO). Valid values are defined in the Python
+                logging module.
+            code_location (str): The S3 prefix URI where custom code will be
+                uploaded (default: None) - don't include a trailing slash since
+                a string prepended with a "/" is appended to ``code_location``. The code
+                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
+                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
+            entry_point (str): Path (absolute or relative) to the local Python
+                source file which should be executed as the entry point to
+                training. (Default: None). If ``source_dir`` is specified, then ``entry_point``
+                must point to a file located at the root of ``source_dir``.
+                If 'git_config' is provided, 'entry_point' should be
+                a relative location to the Python source file in the Git repo.
+
+                Example:
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- train.py
+                    >>>         |----- test.py
+
+                    You can assign entry_point='src/train.py'.
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
+                copied to SageMaker in the same folder where the entrypoint is
+                copied. If 'git_config' is provided, 'dependencies' should be a
+                list of relative locations to directories with any additional
+                libraries needed in the Git repo.
+
+                .. admonition:: Example
+
+                    The following call
+
+                    >>> Estimator(entry_point='train.py',
+                    ...           dependencies=['my/libs/common', 'virtual-env'])
+
+                    results in the following inside the container:
+
+                    >>> $ ls
+
+                    >>> opt/ml/code
+                    >>>     |------ train.py
+                    >>>     |------ common
+                    >>>     |------ virtual-env
+
+                This is not supported with "local code" in Local Mode.
 
         """
         instance_count = renamed_kwargs(
@@ -305,13 +446,22 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self.volume_kms_key = volume_kms_key
         self.max_run = max_run
         self.input_mode = input_mode
-        self.tags = tags
         self.metric_definitions = metric_definitions
         self.model_uri = model_uri
         self.model_channel_name = model_channel_name
         self.code_uri = None
         self.code_channel_name = "code"
-
+        self.source_dir = source_dir
+        self.git_config = git_config
+        self.container_log_level = container_log_level
+        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
+        self.code_location = code_location
+        self.entry_point = entry_point
+        self.dependencies = dependencies
+        self.uploaded_code = None
+        self.tags = add_jumpstart_tags(
+            tags=tags, training_model_uri=self.model_uri, training_script_uri=self.source_dir
+        )
         if self.instance_type in ("local", "local_gpu"):
             if self.instance_type == "local_gpu" and self.instance_count > 1:
                 raise RuntimeError("Distributed Training in Local GPU is not supported")
@@ -437,6 +587,21 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._ensure_base_job_name()
         return name_from_base(self.base_job_name)
 
+    @staticmethod
+    def _json_encode_hyperparameters(hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
+        """Applies Json encoding for certain Hyperparameter types, returns hyperparameters.
+
+        Args:
+            hyperparameters (dict): Dictionary of hyperparameters.
+        """
+        current_hyperparameters = hyperparameters
+        if current_hyperparameters is not None:
+            hyperparameters = {
+                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
+                for (k, v) in current_hyperparameters.items()
+            }
+        return hyperparameters
+
     def _prepare_for_training(self, job_name=None):
         """Set any values in the estimator that need to be set before training.
 
@@ -456,10 +621,105 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             else:
                 self.output_path = "s3://{}/".format(self.sagemaker_session.default_bucket())
 
+        if self.git_config:
+            updated_paths = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updated_paths["entry_point"]
+            self.source_dir = updated_paths["source_dir"]
+            self.dependencies = updated_paths["dependencies"]
+
+        if self.source_dir or self.entry_point or self.dependencies:
+
+            # validate source dir will raise a ValueError if there is something wrong with
+            # the source directory. We are intentionally not handling it because this is a
+            # critical error.
+            if self.source_dir and not self.source_dir.lower().startswith("s3://"):
+                validate_source_dir(self.entry_point, self.source_dir)
+
+            # if we are in local mode with local_code=True. We want the container to just
+            # mount the source dir instead of uploading to S3.
+            local_code = get_config_value("local.local_code", self.sagemaker_session.config)
+
+            if self.sagemaker_session.local_mode and local_code:
+                # if there is no source dir, use the directory containing the entry point.
+                if self.source_dir is None:
+                    self.source_dir = os.path.dirname(self.entry_point)
+                self.entry_point = os.path.basename(self.entry_point)
+
+                code_dir = "file://" + self.source_dir
+                script = self.entry_point
+            elif self.enable_network_isolation() and self.entry_point:
+                self.uploaded_code = self._stage_user_code_in_s3()
+                code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
+                script = self.uploaded_code.script_name
+                self.code_uri = self.uploaded_code.s3_prefix
+            else:
+                self.uploaded_code = self._stage_user_code_in_s3()
+                code_dir = self.uploaded_code.s3_prefix
+                script = self.uploaded_code.script_name
+
+            # Modify hyperparameters in-place to point to the right code directory and
+            # script URIs
+            self._script_mode_hyperparam_update(code_dir, script)
+
         self._prepare_rules()
         self._prepare_debugger_for_training()
         self._prepare_profiler_for_training()
 
+    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
+        """Applies in-place update to hyperparameters required for script mode with training.
+
+        Args:
+            code_dir (str): The directory hosting the training scripts.
+            script (str): The relative filepath of the training entry-point script.
+        """
+        hyperparams: Dict[str, str] = {}
+        hyperparams[DIR_PARAM_NAME] = code_dir
+        hyperparams[SCRIPT_PARAM_NAME] = script
+        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
+        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
+        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
+
+        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(hyperparams))
+
+    def _stage_user_code_in_s3(self) -> str:
+        """Upload the user training script to s3 and return the s3 URI.
+
+        Returns: s3 uri
+        """
+        local_mode = self.output_path.startswith("file://")
+
+        if self.code_location is None and local_mode:
+            code_bucket = self.sagemaker_session.default_bucket()
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = None
+        elif self.code_location is None:
+            code_bucket, _ = parse_s3_url(self.output_path)
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = self.output_kms_key
+        elif local_mode:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+            kms_key = None
+        else:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+
+            output_bucket, _ = parse_s3_url(self.output_path)
+            kms_key = self.output_kms_key if code_bucket == output_bucket else None
+
+        return tar_and_upload_dir(
+            session=self.sagemaker_session.boto_session,
+            bucket=code_bucket,
+            s3_key_prefix=code_s3_prefix,
+            script=self.entry_point,
+            directory=self.source_dir,
+            dependencies=self.dependencies,
+            kms_key=kms_key,
+            s3_resource=self.sagemaker_session.s3_resource,
+        )
+
     def _prepare_rules(self):
         """Rules list includes both debugger and profiler rules.
 
@@ -852,8 +1112,8 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
     def deploy(
         self,
-        initial_instance_count=None,
-        instance_type=None,
+        initial_instance_count,
+        instance_type,
         serializer=None,
         deserializer=None,
         accelerator_type=None,
@@ -864,7 +1124,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         kms_key=None,
         data_capture_config=None,
         tags=None,
-        serverless_inference_config=None,
         **kwargs,
     ):
         """Deploy the trained model to an Amazon SageMaker endpoint.
@@ -875,14 +1134,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html
 
         Args:
-            initial_instance_count (int): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``. If not using
-                serverless inference, then it need to be a number larger or equals
-                to 1 (default: None)
-            instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge', or 'local' for local mode. If not using
-                serverless inference, then it is required to deploy a model.
-                (default: None)
+            initial_instance_count (int): Minimum number of EC2 instances to
+                deploy to an endpoint for prediction.
+            instance_type (str): Type of EC2 instance to deploy to an endpoint
+                for prediction, for example, 'ml.c4.xlarge'.
             serializer (:class:`~sagemaker.serializers.BaseSerializer`): A
                 serializer object, used to encode data for an inference endpoint
                 (default: None). If ``serializer`` is not None, then
@@ -915,11 +1170,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies
                 configuration related to Endpoint data capture for use with
                 Amazon SageMaker Model Monitoring. Default: None.
-            serverless_inference_config (sagemaker.serverless.ServerlessInferenceConfig):
-                Specifies configuration related to serverless endpoint. Use this configuration
-                when trying to create serverless endpoint and make serverless inference. If
-                empty object passed through, we will use pre-defined values in
-                ``ServerlessInferenceConfig`` class to deploy serverless endpoint (default: None)
             tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific
                 endpoint. Example:
                 >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]
@@ -937,7 +1187,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 endpoint and obtain inferences.
         """
         removed_kwargs("update_endpoint", kwargs)
-        is_serverless = serverless_inference_config is not None
         self._ensure_latest_training_job()
         self._ensure_base_job_name()
         default_name = name_from_base(self.base_job_name)
@@ -945,7 +1194,7 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         model_name = model_name or default_name
 
         self.deploy_instance_type = instance_type
-        if use_compiled_model and not is_serverless:
+        if use_compiled_model:
             family = "_".join(instance_type.split(".")[:-1])
             if family not in self._compiled_models:
                 raise ValueError(
@@ -959,6 +1208,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
 
         model.name = model_name
 
+        tags = update_inference_tags_with_jumpstart_training_tags(
+            inference_tags=tags, training_tags=self.tags
+        )
+
         return model.deploy(
             instance_type=instance_type,
             initial_instance_count=initial_instance_count,
@@ -970,7 +1223,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             wait=wait,
             kms_key=kms_key,
             data_capture_config=data_capture_config,
-            serverless_inference_config=serverless_inference_config,
         )
 
     def register(
@@ -1731,6 +1983,12 @@ class Estimator(EstimatorBase):
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
+        source_dir=None,
+        git_config=None,
+        container_log_level=logging.INFO,
+        code_location=None,
+        entry_point=None,
+        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``Estimator`` instance.
@@ -1888,9 +2146,127 @@ class Estimator(EstimatorBase):
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
+                with any other training source code dependencies aside from the entry
+                point file (default: None). If ``source_dir`` is an S3 URI, it must
+                point to a tar.gz file. Structure within this directory are preserved
+                when training on Amazon SageMaker. If 'git_config' is provided,
+                'source_dir' should be a relative location to a directory in the Git
+                repo.
+
+                .. admonition:: Example
+
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- train.py
+                    >>>         |----- test.py
+
+                    and you need 'train.py' as entry point and 'test.py' as
+                    training source code as well, you can assign
+                    entry_point='train.py', source_dir='src'.
+            git_config (dict[str, str]): Git configurations used for cloning
+                files, including ``repo``, ``branch``, ``commit``,
+                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
+                ``repo`` field is required. All other fields are optional.
+                ``repo`` specifies the Git repository where your training script
+                is stored. If you don't provide ``branch``, the default value
+                'master' is used. If you don't provide ``commit``, the latest
+                commit in the specified branch is used. .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
+
+                    results in cloning the repo specified in 'repo', then
+                    checkout the 'master' branch, and checkout the specified
+                    commit.
+
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
+                used for authentication. For GitHub (or other Git) accounts, set
+                ``2FA_enabled`` to 'True' if two-factor authentication is
+                enabled for the account, otherwise set it to 'False'. If you do
+                not provide a value for ``2FA_enabled``, a default value of
+                'False' is used. CodeCommit does not support two-factor
+                authentication, so do not provide "2FA_enabled" with CodeCommit
+                repositories.
+
+                For GitHub and other Git repos, when SSH URLs are provided, it
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
+                for authentication if provided. If required authentication info
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
+
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
+                should not be provided. There is no token in CodeCommit, so
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
+            container_log_level (int): Log level to use within the container
+                (default: logging.INFO). Valid values are defined in the Python
+                logging module.
+            code_location (str): The S3 prefix URI where custom code will be
+                uploaded (default: None) - don't include a trailing slash since
+                a string prepended with a "/" is appended to ``code_location``. The code
+                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
+                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
+            entry_point (str): Path (absolute or relative) to the local Python
+                source file which should be executed as the entry point to
+                training. If ``source_dir`` is specified, then ``entry_point``
+                must point to a file located at the root of ``source_dir``.
+                If 'git_config' is provided, 'entry_point' should be
+                a relative location to the Python source file in the Git repo.
+
+                Example:
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- train.py
+                    >>>         |----- test.py
+
+                    You can assign entry_point='src/train.py'.
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
+                copied to SageMaker in the same folder where the entrypoint is
+                copied. If 'git_config' is provided, 'dependencies' should be a
+                list of relative locations to directories with any additional
+                libraries needed in the Git repo.
+
+                .. admonition:: Example
+
+                    The following call
+
+                    >>> Estimator(entry_point='train.py',
+                    ...           dependencies=['my/libs/common', 'virtual-env'])
+
+                    results in the following inside the container:
+
+                    >>> $ ls
+
+                    >>> opt/ml/code
+                    >>>     |------ train.py
+                    >>>     |------ common
+                    >>>     |------ virtual-env
+
+                This is not supported with "local code" in Local Mode.
         """
         self.image_uri = image_uri
-        self.hyperparam_dict = hyperparameters.copy() if hyperparameters else {}
+        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
         super(Estimator, self).__init__(
             role,
             instance_count,
@@ -1923,6 +2299,13 @@ class Estimator(EstimatorBase):
             disable_profiler=disable_profiler,
             environment=environment,
             max_retry_attempts=max_retry_attempts,
+            container_log_level=container_log_level,
+            source_dir=source_dir,
+            git_config=git_config,
+            code_location=code_location,
+            entry_point=entry_point,
+            dependencies=dependencies,
+            hyperparameters=hyperparameters,
             **kwargs,
         )
 
@@ -1943,7 +2326,7 @@ class Estimator(EstimatorBase):
         training.
         """
         for k, v in kwargs.items():
-            self.hyperparam_dict[k] = v
+            self._hyperparameters[k] = v
 
     def hyperparameters(self):
         """Returns the hyperparameters as a dictionary to use for training.
@@ -1951,7 +2334,7 @@ class Estimator(EstimatorBase):
         The fit() method, that does the model training, calls this method to
         find the hyperparameters you specified.
         """
-        return self.hyperparam_dict
+        return self._hyperparameters
 
     def create_model(
         self,
@@ -2027,15 +2410,6 @@ class Framework(EstimatorBase):
 
     _framework_name = None
 
-    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
-    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
-    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
-    INSTANCE_TYPE = "sagemaker_instance_type"
-    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
-    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
-    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
-    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
-
     def __init__(
         self,
         entry_point,
@@ -2249,48 +2623,23 @@ class Framework(EstimatorBase):
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
-        if self.git_config:
-            updated_paths = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updated_paths["entry_point"]
-            self.source_dir = updated_paths["source_dir"]
-            self.dependencies = updated_paths["dependencies"]
+        self._validate_and_set_debugger_configs()
 
-        # validate source dir will raise a ValueError if there is something wrong with the
-        # source directory. We are intentionally not handling it because this is a critical error.
-        if self.source_dir and not self.source_dir.lower().startswith("s3://"):
-            validate_source_dir(self.entry_point, self.source_dir)
-
-        # if we are in local mode with local_code=True. We want the container to just
-        # mount the source dir instead of uploading to S3.
-        local_code = get_config_value("local.local_code", self.sagemaker_session.config)
-        if self.sagemaker_session.local_mode and local_code:
-            # if there is no source dir, use the directory containing the entry point.
-            if self.source_dir is None:
-                self.source_dir = os.path.dirname(self.entry_point)
-            self.entry_point = os.path.basename(self.entry_point)
-
-            code_dir = "file://" + self.source_dir
-            script = self.entry_point
-        elif self.enable_network_isolation() and self.entry_point:
-            self.uploaded_code = self._stage_user_code_in_s3()
-            code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
-            script = self.uploaded_code.script_name
-            self.code_uri = self.uploaded_code.s3_prefix
-        else:
-            self.uploaded_code = self._stage_user_code_in_s3()
-            code_dir = self.uploaded_code.s3_prefix
-            script = self.uploaded_code.script_name
+    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
+        """Applies in-place update to hyperparameters required for script mode with training.
 
-        # Modify hyperparameters in-place to point to the right code directory and script URIs
-        self._hyperparameters[DIR_PARAM_NAME] = code_dir
-        self._hyperparameters[SCRIPT_PARAM_NAME] = script
-        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name
-        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
+        Args:
+            code_dir (str): The directory hosting the training scripts.
+            script (str): The relative filepath of the training entry-point script.
+        """
+        hyperparams: Dict[str, str] = {}
+        hyperparams[DIR_PARAM_NAME] = code_dir
+        hyperparams[SCRIPT_PARAM_NAME] = script
+        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
+        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
+        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
 
-        self._validate_and_set_debugger_configs()
+        self._hyperparameters.update(hyperparams)
 
     def _validate_and_set_debugger_configs(self):
         """Set defaults for debugging."""
@@ -2320,44 +2669,6 @@ class Framework(EstimatorBase):
                 self.environment = {}
             self.environment[DEBUGGER_FLAG] = "0"
 
-    def _stage_user_code_in_s3(self):
-        """Upload the user training script to s3 and return the location.
-
-        Returns: s3 uri
-        """
-        local_mode = self.output_path.startswith("file://")
-
-        if self.code_location is None and local_mode:
-            code_bucket = self.sagemaker_session.default_bucket()
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = None
-        elif self.code_location is None:
-            code_bucket, _ = parse_s3_url(self.output_path)
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = self.output_kms_key
-        elif local_mode:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-            kms_key = None
-        else:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-
-            output_bucket, _ = parse_s3_url(self.output_path)
-            kms_key = self.output_kms_key if code_bucket == output_bucket else None
-
-        return tar_and_upload_dir(
-            session=self.sagemaker_session.boto_session,
-            bucket=code_bucket,
-            s3_key_prefix=code_s3_prefix,
-            script=self.entry_point,
-            directory=self.source_dir,
-            dependencies=self.dependencies,
-            kms_key=kms_key,
-            s3_resource=self.sagemaker_session.s3_resource,
-            settings=self.sagemaker_session.settings,
-        )
-
     def _model_source_dir(self):
         """Get the appropriate value to pass as ``source_dir`` to a model constructor.
 
@@ -2388,6 +2699,10 @@ class Framework(EstimatorBase):
 
         return None
 
+    def set_hyperparameters(self, **kwargs):
+        """Escape the dict argument as JSON, update the private hyperparameter attribute."""
+        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(kwargs))
+
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
@@ -2397,7 +2712,7 @@ class Framework(EstimatorBase):
         Returns:
             dict[str, str]: The hyperparameters.
         """
-        return self._json_encode_hyperparameters(self._hyperparameters)
+        return EstimatorBase._json_encode_hyperparameters(self._hyperparameters)
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
@@ -2438,51 +2753,32 @@ class Framework(EstimatorBase):
 
         return init_params
 
-    def training_image_uri(self):
+    def training_image_uri(self, region=None):
         """Return the Docker image to use for training.
 
         The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does
         the model training, calls this method to find the image to use for model
         training.
 
+        Args:
+            region (str): Optional. AWS region to use for image URI. Default: AWS region associated
+                with the SageMaker session.
+
         Returns:
             str: The URI of the Docker image.
         """
-        if self.image_uri:
-            return self.image_uri
-        if hasattr(self, "distribution"):
-            distribution = self.distribution  # pylint: disable=no-member
-        else:
-            distribution = None
-        compiler_config = getattr(self, "compiler_config", None)
-
-        if hasattr(self, "tensorflow_version") or hasattr(self, "pytorch_version"):
-            processor = image_uris._processor(self.instance_type, ["cpu", "gpu"])
-            is_native_huggingface_gpu = processor == "gpu" and not compiler_config
-            container_version = "cu110-ubuntu18.04" if is_native_huggingface_gpu else None
-            if self.tensorflow_version is not None:  # pylint: disable=no-member
-                base_framework_version = (
-                    f"tensorflow{self.tensorflow_version}"  # pylint: disable=no-member
-                )
-            else:
-                base_framework_version = (
-                    f"pytorch{self.pytorch_version}"  # pylint: disable=no-member
-                )
-        else:
-            container_version = None
-            base_framework_version = None
 
-        return image_uris.retrieve(
-            self._framework_name,
-            self.sagemaker_session.boto_region_name,
-            instance_type=self.instance_type,
-            version=self.framework_version,  # pylint: disable=no-member
+        return image_uris.get_training_image_uri(
+            region=region or self.sagemaker_session.boto_region_name,
+            framework=self._framework_name,
+            framework_version=self.framework_version,  # pylint: disable=no-member
             py_version=self.py_version,  # pylint: disable=no-member
-            image_scope="training",
-            distribution=distribution,
-            base_framework_version=base_framework_version,
-            container_version=container_version,
-            training_compiler_config=compiler_config,
+            image_uri=self.image_uri,
+            distribution=getattr(self, "distribution", None),
+            compiler_config=getattr(self, "compiler_config", None),
+            tensorflow_version=getattr(self, "tensorflow_version", None),
+            pytorch_version=getattr(self, "pytorch_version", None),
+            instance_type=self.instance_type,
         )
 
     @classmethod
@@ -2535,17 +2831,6 @@ class Framework(EstimatorBase):
         )
         return estimator
 
-    @staticmethod
-    def _json_encode_hyperparameters(hyperparameters):
-        """Placeholder docstring"""
-        current_hyperparameters = hyperparameters
-        if current_hyperparameters is not None:
-            hyperparameters = {
-                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
-                for (k, v) in current_hyperparameters.items()
-            }
-        return hyperparameters
-
     @classmethod
     def _update_init_params(cls, hp, tf_arguments):
         """Placeholder docstring"""

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-25 18:14:53[0m
[92mHash: 109730e30e6787c86f82e3489caa037efad5dd53[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: Add support for SageMaker lineage queries context (#2830)

[0m
@@ -18,6 +18,7 @@ import json
 import logging
 import os
 import re
+import copy
 
 import sagemaker
 from sagemaker import (
@@ -32,8 +33,8 @@ from sagemaker import (
 from sagemaker.inputs import CompilationInput
 from sagemaker.deprecations import removed_kwargs
 from sagemaker.predictor import PredictorBase
-from sagemaker.serverless import ServerlessInferenceConfig
 from sagemaker.transformer import Transformer
+from sagemaker.jumpstart.utils import add_jumpstart_tags
 
 LOGGER = logging.getLogger("sagemaker")
 
@@ -58,6 +59,15 @@ class ModelBase(abc.ABC):
         """Destroy resources associated with this model."""
 
 
+SCRIPT_PARAM_NAME = "sagemaker_program"
+DIR_PARAM_NAME = "sagemaker_submit_directory"
+CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
+JOB_NAME_PARAM_NAME = "sagemaker_job_name"
+MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
+SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
+SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
+
+
 class Model(ModelBase):
     """A SageMaker ``Model`` that can be deployed to an ``Endpoint``."""
 
@@ -74,6 +84,12 @@ class Model(ModelBase):
         enable_network_isolation=False,
         model_kms_key=None,
         image_config=None,
+        source_dir=None,
+        code_location=None,
+        entry_point=None,
+        container_log_level=logging.INFO,
+        dependencies=None,
+        git_config=None,
     ):
         """Initialize an SageMaker ``Model``.
 
@@ -115,6 +131,124 @@ class Model(ModelBase):
                 model container is pulled from ECR, or private registry in your
                 VPC. By default it is set to pull model container image from
                 ECR. (default: None).
+            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
+                with any other training source code dependencies aside from the entry
+                point file (default: None). If ``source_dir`` is an S3 URI, it must
+                point to a tar.gz file. Structure within this directory are preserved
+                when training on Amazon SageMaker. If 'git_config' is provided,
+                'source_dir' should be a relative location to a directory in the Git repo.
+                If the directory points to S3, no code will be uploaded and the S3 location
+                will be used instead.
+
+                .. admonition:: Example
+
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- inference.py
+                    >>>         |----- test.py
+
+                    You can assign entry_point='inference.py', source_dir='src'.
+            code_location (str): Name of the S3 bucket where custom code is
+                uploaded (default: None). If not specified, default bucket
+                created by ``sagemaker.session.Session`` is used.
+            entry_point (str): Path (absolute or relative) to the Python source
+                file which should be executed as the entry point to model
+                hosting (default: None). If ``source_dir`` is specified,
+                then ``entry_point`` must point to a file located at the root of
+                ``source_dir``. If 'git_config' is provided, 'entry_point' should
+                be a relative location to the Python source file in the Git repo.
+
+                Example:
+                    With the following GitHub repo directory structure:
+
+                    >>> |----- README.md
+                    >>> |----- src
+                    >>>         |----- inference.py
+                    >>>         |----- test.py
+
+                    You can assign entry_point='src/inference.py'.
+            container_log_level (int): Log level to use within the container
+                (default: logging.INFO). Valid values are defined in the Python
+                logging module.
+            dependencies (list[str]): A list of paths to directories (absolute
+                or relative) with any additional libraries that will be exported
+                to the container (default: []). The library folders will be
+                copied to SageMaker in the same folder where the entrypoint is
+                copied. If 'git_config' is provided, 'dependencies' should be a
+                list of relative locations to directories with any additional
+                libraries needed in the Git repo. If the ```source_dir``` points
+                to S3, code will be uploaded and the S3 location will be used
+                instead.
+
+                .. admonition:: Example
+
+                    The following call
+
+                    >>> Model(entry_point='inference.py',
+                    ...       dependencies=['my/libs/common', 'virtual-env'])
+
+                    results in the following inside the container:
+
+                    >>> $ ls
+
+                    >>> opt/ml/code
+                    >>>     |------ inference.py
+                    >>>     |------ common
+                    >>>     |------ virtual-env
+
+                This is not supported with "local code" in Local Mode.
+            git_config (dict[str, str]): Git configurations used for cloning
+                files, including ``repo``, ``branch``, ``commit``,
+                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
+                ``repo`` field is required. All other fields are optional.
+                ``repo`` specifies the Git repository where your training script
+                is stored. If you don't provide ``branch``, the default value
+                'master' is used. If you don't provide ``commit``, the latest
+                commit in the specified branch is used. .. admonition:: Example
+
+                    The following config:
+
+                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
+                    >>>               'branch': 'test-branch-git-config',
+                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
+
+                    results in cloning the repo specified in 'repo', then
+                    checkout the 'master' branch, and checkout the specified
+                    commit.
+
+                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
+                used for authentication. For GitHub (or other Git) accounts, set
+                ``2FA_enabled`` to 'True' if two-factor authentication is
+                enabled for the account, otherwise set it to 'False'. If you do
+                not provide a value for ``2FA_enabled``, a default value of
+                'False' is used. CodeCommit does not support two-factor
+                authentication, so do not provide "2FA_enabled" with CodeCommit
+                repositories.
+
+                For GitHub and other Git repos, when SSH URLs are provided, it
+                doesn't matter whether 2FA is enabled or disabled; you should
+                either have no passphrase for the SSH key pairs, or have the
+                ssh-agent configured so that you will not be prompted for SSH
+                passphrase when you do 'git clone' command with SSH URLs. When
+                HTTPS URLs are provided: if 2FA is disabled, then either token
+                or username+password will be used for authentication if provided
+                (token prioritized); if 2FA is enabled, only token will be used
+                for authentication if provided. If required authentication info
+                is not provided, python SDK will try to use local credentials
+                storage to authenticate. If that fails either, an error message
+                will be thrown.
+
+                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
+                should not be provided. There is no token in CodeCommit, so
+                'token' should not be provided too. When 'repo' is an SSH URL,
+                the requirements are the same as GitHub-like repos. When 'repo'
+                is an HTTPS URL, username+password will be used for
+                authentication if they are provided; otherwise, python SDK will
+                try to use either CodeCommit credential helper or local
+                credential storage for authentication.
+
         """
         self.model_data = model_data
         self.image_uri = image_uri
@@ -132,6 +266,24 @@ class Model(ModelBase):
         self._enable_network_isolation = enable_network_isolation
         self.model_kms_key = model_kms_key
         self.image_config = image_config
+        self.entry_point = entry_point
+        self.source_dir = source_dir
+        self.dependencies = dependencies or []
+        self.git_config = git_config
+        self.container_log_level = container_log_level
+        if code_location:
+            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
+        else:
+            self.bucket, self.key_prefix = None, None
+        if self.git_config:
+            updates = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updates["entry_point"]
+            self.source_dir = updates["source_dir"]
+            self.dependencies = updates["dependencies"]
+        self.uploaded_code = None
+        self.repacked_model_data = None
 
     def register(
         self,
@@ -210,7 +362,7 @@ class Model(ModelBase):
             model_package_arn=model_package.get("ModelPackageArn"),
         )
 
-    def _init_sagemaker_session_if_does_not_exist(self, instance_type=None):
+    def _init_sagemaker_session_if_does_not_exist(self, instance_type):
         """Set ``self.sagemaker_session`` to ``LocalSession`` or ``Session`` if it's not already.
 
         The type of session object is determined by the instance type.
@@ -243,10 +395,91 @@ class Model(ModelBase):
         Returns:
             dict: A container definition object usable with the CreateModel API.
         """
+        deploy_key_prefix = fw_utils.model_code_key_prefix(
+            self.key_prefix, self.name, self.image_uri
+        )
+        deploy_env = copy.deepcopy(self.env)
+        if self.source_dir or self.dependencies or self.entry_point or self.git_config:
+            is_repack = (
+                self.source_dir and self.entry_point and not (self.key_prefix or self.git_config)
+            )
+            self._upload_code(deploy_key_prefix, repack=is_repack)
+            deploy_env.update(self._script_mode_env_vars())
         return sagemaker.container_def(
-            self.image_uri, self.model_data, self.env, image_config=self.image_config
+            self.image_uri,
+            self.repacked_model_data or self.model_data,
+            deploy_env,
+            image_config=self.image_config,
         )
 
+    def _upload_code(self, key_prefix: str, repack: bool = False) -> None:
+        """Uploads code to S3 to be used with script mode with SageMaker inference.
+
+        Args:
+            key_prefix (str): The S3 key associated with the ``code_location`` parameter of the
+                ``Model`` class.
+            repack (bool): Optional. Set to ``True`` to indicate that the source code and model
+                artifact should be repackaged into a new S3 object. (default: False).
+        """
+        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
+        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
+            self.uploaded_code = None
+        elif not repack:
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            self.uploaded_code = fw_utils.tar_and_upload_dir(
+                session=self.sagemaker_session.boto_session,
+                bucket=bucket,
+                s3_key_prefix=key_prefix,
+                script=self.entry_point,
+                directory=self.source_dir,
+                dependencies=self.dependencies,
+            )
+
+        if repack and self.model_data is not None and self.entry_point is not None:
+            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
+                # model is not yet there, defer repacking to later during pipeline execution
+                return
+
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
+
+            utils.repack_model(
+                inference_script=self.entry_point,
+                source_directory=self.source_dir,
+                dependencies=self.dependencies,
+                model_uri=self.model_data,
+                repacked_model_uri=repacked_model_data,
+                sagemaker_session=self.sagemaker_session,
+                kms_key=self.model_kms_key,
+            )
+
+            self.repacked_model_data = repacked_model_data
+            self.uploaded_code = fw_utils.UploadedCode(
+                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
+            )
+
+    def _script_mode_env_vars(self):
+        """Placeholder docstring"""
+        script_name = None
+        dir_name = None
+        if self.uploaded_code:
+            script_name = self.uploaded_code.script_name
+            if self.enable_network_isolation():
+                dir_name = "/opt/ml/model/code"
+            else:
+                dir_name = self.uploaded_code.s3_prefix
+        elif self.entry_point is not None:
+            script_name = self.entry_point
+            if self.source_dir is not None:
+                dir_name = "file://" + self.source_dir
+
+        return {
+            SCRIPT_PARAM_NAME.upper(): script_name or str(),
+            DIR_PARAM_NAME.upper(): dir_name or str(),
+            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
+            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
+        }
+
     def enable_network_isolation(self):
         """Whether to enable network isolation when creating this Model
 
@@ -689,8 +922,8 @@ class Model(ModelBase):
 
     def deploy(
         self,
-        initial_instance_count=None,
-        instance_type=None,
+        initial_instance_count,
+        instance_type,
         serializer=None,
         deserializer=None,
         accelerator_type=None,
@@ -699,7 +932,6 @@ class Model(ModelBase):
         kms_key=None,
         wait=True,
         data_capture_config=None,
-        serverless_inference_config=None,
         **kwargs,
     ):
         """Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.
@@ -717,13 +949,9 @@ class Model(ModelBase):
 
         Args:
             initial_instance_count (int): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``. If not using
-                serverless inference, then it need to be a number larger or equals
-                to 1 (default: None)
+                in the ``Endpoint`` created from this ``Model``.
             instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge', or 'local' for local mode. If not using
-                serverless inference, then it is required to deploy a model.
-                (default: None)
+                For example, 'ml.p2.xlarge', or 'local' for local mode.
             serializer (:class:`~sagemaker.serializers.BaseSerializer`): A
                 serializer object, used to encode data for an inference endpoint
                 (default: None). If ``serializer`` is not None, then
@@ -752,17 +980,7 @@ class Model(ModelBase):
             data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies
                 configuration related to Endpoint data capture for use with
                 Amazon SageMaker Model Monitoring. Default: None.
-            serverless_inference_config (sagemaker.serverless.ServerlessInferenceConfig):
-                Specifies configuration related to serverless endpoint. Use this configuration
-                when trying to create serverless endpoint and make serverless inference. If
-                empty object passed through, we will use pre-defined values in
-                ``ServerlessInferenceConfig`` class to deploy serverless endpoint (default: None)
-        Raises:
-             ValueError: If arguments combination check failed in these circumstances:
-                - If no role is specified or
-                - If serverless inference config is not specified and instance type and instance
-                    count are also not specified or
-                - If a wrong type of object is provided as serverless inference config
+
         Returns:
             callable[string, sagemaker.session.Session] or None: Invocation of
                 ``self.predictor_cls`` on the created endpoint name, if ``self.predictor_cls``
@@ -771,50 +989,34 @@ class Model(ModelBase):
         removed_kwargs("update_endpoint", kwargs)
         self._init_sagemaker_session_if_does_not_exist(instance_type)
 
+        tags = add_jumpstart_tags(
+            tags=tags, inference_model_uri=self.model_data, inference_script_uri=self.source_dir
+        )
+
         if self.role is None:
             raise ValueError("Role can not be null for deploying a model")
 
-        is_serverless = serverless_inference_config is not None
-        if not is_serverless and not (instance_type and initial_instance_count):
-            raise ValueError(
-                "Must specify instance type and instance count unless using serverless inference"
-            )
-
-        if is_serverless and not isinstance(serverless_inference_config, ServerlessInferenceConfig):
-            raise ValueError(
-                "serverless_inference_config needs to be a ServerlessInferenceConfig object"
-            )
-
-        if instance_type and instance_type.startswith("ml.inf") and not self._is_compiled_model:
+        if instance_type.startswith("ml.inf") and not self._is_compiled_model:
             LOGGER.warning(
                 "Your model is not compiled. Please compile your model before using Inferentia."
             )
 
-        compiled_model_suffix = None if is_serverless else "-".join(instance_type.split(".")[:-1])
-        if self._is_compiled_model and not is_serverless:
+        compiled_model_suffix = "-".join(instance_type.split(".")[:-1])
+        if self._is_compiled_model:
             self._ensure_base_name_if_needed(self.image_uri)
             if self._base_name is not None:
                 self._base_name = "-".join((self._base_name, compiled_model_suffix))
 
         self._create_sagemaker_model(instance_type, accelerator_type, tags)
-
-        serverless_inference_config_dict = (
-            serverless_inference_config._to_request_dict() if is_serverless else None
-        )
         production_variant = sagemaker.production_variant(
-            self.name,
-            instance_type,
-            initial_instance_count,
-            accelerator_type=accelerator_type,
-            serverless_inference_config=serverless_inference_config_dict,
+            self.name, instance_type, initial_instance_count, accelerator_type=accelerator_type
         )
         if endpoint_name:
             self.endpoint_name = endpoint_name
         else:
             base_endpoint_name = self._base_name or utils.base_from_name(self.name)
-            if self._is_compiled_model and not is_serverless:
-                if not base_endpoint_name.endswith(compiled_model_suffix):
-                    base_endpoint_name = "-".join((base_endpoint_name, compiled_model_suffix))
+            if self._is_compiled_model and not base_endpoint_name.endswith(compiled_model_suffix):
+                base_endpoint_name = "-".join((base_endpoint_name, compiled_model_suffix))
             self.endpoint_name = utils.name_from_base(base_endpoint_name)
 
         data_capture_config_dict = None
@@ -921,15 +1123,6 @@ class Model(ModelBase):
         self.sagemaker_session.delete_model(self.name)
 
 
-SCRIPT_PARAM_NAME = "sagemaker_program"
-DIR_PARAM_NAME = "sagemaker_submit_directory"
-CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
-JOB_NAME_PARAM_NAME = "sagemaker_job_name"
-MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
-SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
-SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
-
-
 class FrameworkModel(Model):
     """A Model for working with an SageMaker ``Framework``.
 
@@ -1107,113 +1300,14 @@ class FrameworkModel(Model):
             env=env,
             name=name,
             sagemaker_session=sagemaker_session,
+            source_dir=source_dir,
+            code_location=code_location,
+            entry_point=entry_point,
+            container_log_level=container_log_level,
+            dependencies=dependencies,
+            git_config=git_config,
             **kwargs,
         )
-        self.entry_point = entry_point
-        self.source_dir = source_dir
-        self.dependencies = dependencies or []
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        if code_location:
-            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
-        else:
-            self.bucket, self.key_prefix = None, None
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
-        self.uploaded_code = None
-        self.repacked_model_data = None
-
-    def prepare_container_def(self, instance_type=None, accelerator_type=None):
-        """Return a container definition with framework configuration.
-
-        Framework configuration is set in model environment variables.
-        This also uploads user-supplied code to S3.
-
-        Args:
-            instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge'.
-            accelerator_type (str): The Elastic Inference accelerator type to
-                deploy to the instance for loading and making inferences to the
-                model. For example, 'ml.eia1.medium'.
-
-        Returns:
-            dict[str, str]: A container definition object usable with the
-            CreateModel API.
-        """
-        deploy_key_prefix = fw_utils.model_code_key_prefix(
-            self.key_prefix, self.name, self.image_uri
-        )
-        self._upload_code(deploy_key_prefix)
-        deploy_env = dict(self.env)
-        deploy_env.update(self._framework_env_vars())
-        return sagemaker.container_def(self.image_uri, self.model_data, deploy_env)
-
-    def _upload_code(self, key_prefix, repack=False):
-        """Placeholder Docstring"""
-        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
-        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
-            self.uploaded_code = None
-        elif not repack:
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            self.uploaded_code = fw_utils.tar_and_upload_dir(
-                session=self.sagemaker_session.boto_session,
-                bucket=bucket,
-                s3_key_prefix=key_prefix,
-                script=self.entry_point,
-                directory=self.source_dir,
-                dependencies=self.dependencies,
-                settings=self.sagemaker_session.settings,
-            )
-
-        if repack and self.model_data is not None and self.entry_point is not None:
-            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
-                # model is not yet there, defer repacking to later during pipeline execution
-                return
-
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
-
-            utils.repack_model(
-                inference_script=self.entry_point,
-                source_directory=self.source_dir,
-                dependencies=self.dependencies,
-                model_uri=self.model_data,
-                repacked_model_uri=repacked_model_data,
-                sagemaker_session=self.sagemaker_session,
-                kms_key=self.model_kms_key,
-            )
-
-            self.repacked_model_data = repacked_model_data
-            self.uploaded_code = fw_utils.UploadedCode(
-                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
-            )
-
-    def _framework_env_vars(self):
-        """Placeholder docstring"""
-        script_name = None
-        dir_name = None
-        if self.uploaded_code:
-            script_name = self.uploaded_code.script_name
-            if self.enable_network_isolation():
-                dir_name = "/opt/ml/model/code"
-            else:
-                dir_name = self.uploaded_code.s3_prefix
-        elif self.entry_point is not None:
-            script_name = self.entry_point
-            if self.source_dir is not None:
-                dir_name = "file://" + self.source_dir
-
-        return {
-            SCRIPT_PARAM_NAME.upper(): script_name or str(),
-            DIR_PARAM_NAME.upper(): dir_name or str(),
-            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
-            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
-        }
 
 
 class ModelPackage(Model):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-25 18:14:53[0m
[92mHash: 109730e30e6787c86f82e3489caa037efad5dd53[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: Add support for SageMaker lineage queries context (#2830)

[0m
@@ -11,11 +11,9 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
-import unittest.mock
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
 from sagemaker.lineage.action import Action
-from sagemaker.lineage.lineage_trial_component import LineageTrialComponent
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -34,38 +32,6 @@ def test_lineage_query(sagemaker_session):
         start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
     )
 
-    assert len(response.edges) == 1
-    assert response.edges[0].source_arn == "arn1"
-    assert response.edges[0].destination_arn == "arn2"
-    assert response.edges[0].association_type == "Produced"
-    assert len(response.vertices) == 2
-
-    assert response.vertices[0].arn == "arn1"
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-    assert response.vertices[1].arn == "arn2"
-    assert response.vertices[1].lineage_source == "Model"
-    assert response.vertices[1].lineage_entity == "Context"
-
-
-def test_lineage_query_duplication(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {"Arn": "arn1", "Type": "Endpoint", "LineageType": "Artifact"},
-            {"Arn": "arn1", "Type": "Endpoint", "LineageType": "Artifact"},
-            {"Arn": "arn2", "Type": "Model", "LineageType": "Context"},
-        ],
-        "Edges": [
-            {"SourceArn": "arn1", "DestinationArn": "arn2", "AssociationType": "Produced"},
-            {"SourceArn": "arn1", "DestinationArn": "arn2", "AssociationType": "Produced"},
-        ],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-
     assert len(response.edges) == 1
     assert response.edges[0].source_arn == "arn1"
     assert response.edges[0].destination_arn == "arn2"
@@ -288,49 +254,6 @@ def test_vertex_to_object_context(sagemaker_session):
     assert isinstance(context, Context)
 
 
-def test_vertex_to_object_trial_component(sagemaker_session):
-
-    tc_arn = "arn:aws:sagemaker:us-west-2:963951943925:trial-component/abaloneprocess-ixyt08z3ru-aws-processing-job"
-    vertex = Vertex(
-        arn=tc_arn,
-        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
-        lineage_source=LineageSourceEnum.TRANSFORM_JOB.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_trial_component.return_value = {
-        "TrialComponentName": "MyTrialComponent",
-        "TrialComponentArn": tc_arn,
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/my_trial_component",
-            "SourceType": "ARN",
-            "SourceId": "Thu Dec 17 17:16:24 UTC 2020",
-        },
-        "TrialComponentType": "ModelDeployment",
-        "Properties": {
-            "PipelineExecutionArn": "arn:aws:sagemaker:us-west-2:0123456789012:\
-                pipeline/mypipeline/execution/0irnteql64d0",
-            "PipelineStepName": "MyStep",
-            "Status": "Completed",
-        },
-        "CreationTime": 1608225384.0,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608225384.0,
-        "LastModifiedBy": {},
-    }
-
-    trial_component = vertex.to_lineage_object()
-
-    expected_calls = [
-        unittest.mock.call(TrialComponentName="abaloneprocess-ixyt08z3ru-aws-processing-job"),
-    ]
-    assert expected_calls == sagemaker_session.sagemaker_client.describe_trial_component.mock_calls
-
-    assert trial_component.trial_component_arn == tc_arn
-    assert trial_component.trial_component_name == "MyTrialComponent"
-    assert isinstance(trial_component, LineageTrialComponent)
-
-
 def test_vertex_to_object_model_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -362,37 +285,6 @@ def test_vertex_to_object_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
-def test_vertex_to_object_artifact(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.ARTIFACT.value,
-        lineage_source=LineageSourceEnum.MODEL.value,
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
-        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        "Source": {
-            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/mymodel",
-            "SourceTypes": [],
-        },
-        "ArtifactType": None,
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    artifact = vertex.to_lineage_object()
-
-    assert (
-        artifact.artifact_arn
-        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert isinstance(artifact, Artifact)
-
-
 def test_vertex_to_dataset_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -455,7 +347,7 @@ def test_vertex_to_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
-def test_vertex_to_object_image_artifact(sagemaker_session):
+def test_vertex_to_object_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
         lineage_entity=LineageEntityEnum.ARTIFACT.value,
@@ -517,7 +409,7 @@ def test_vertex_to_object_action(sagemaker_session):
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.TRIAL.value,
+        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-25 18:14:53[0m
[92mHash: 109730e30e6787c86f82e3489caa037efad5dd53[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: Add support for SageMaker lineage queries context (#2830)

[0m
@@ -11,12 +11,21 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
+from unittest.mock import MagicMock
 
 import pytest
 from mock import Mock, patch
 
 import sagemaker
-from sagemaker.model import Model
+from sagemaker.model import FrameworkModel, Model
+from sagemaker.huggingface.model import HuggingFaceModel
+from sagemaker.jumpstart.constants import JUMPSTART_BUCKET_NAME_SET
+from sagemaker.jumpstart.enums import JumpStartTag
+from sagemaker.mxnet.model import MXNetModel
+from sagemaker.pytorch.model import PyTorchModel
+from sagemaker.sklearn.model import SKLearnModel
+from sagemaker.tensorflow.model import TensorFlowModel
+from sagemaker.xgboost.model import XGBoostModel
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -27,10 +36,39 @@ INSTANCE_COUNT = 2
 INSTANCE_TYPE = "ml.c4.4xlarge"
 ROLE = "some-role"
 
+REGION = "us-west-2"
+BUCKET_NAME = "some-bucket-name"
+GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
+BRANCH = "test-branch-git-config"
+COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
+ENTRY_POINT_INFERENCE = "inference.py"
 
-@pytest.fixture
+SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
+IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
+
+
+class DummyFrameworkModel(FrameworkModel):
+    def __init__(self, **kwargs):
+        super(DummyFrameworkModel, self).__init__(
+            **kwargs,
+        )
+
+
+@pytest.fixture()
 def sagemaker_session():
-    return Mock()
+    boto_mock = Mock(name="boto_session", region_name=REGION)
+    sms = MagicMock(
+        name="sagemaker_session",
+        boto_session=boto_mock,
+        boto_region_name=REGION,
+        config=None,
+        local_mode=False,
+        s3_client=None,
+        s3_resource=None,
+    )
+    sms.default_bucket = Mock(name="default_bucket", return_value=BUCKET_NAME)
+
+    return sms
 
 
 def test_prepare_container_def_with_model_data():
@@ -345,3 +383,171 @@ def test_delete_model_no_name(sagemaker_session):
     ):
         model.delete_model()
     sagemaker_session.delete_model.assert_not_called()
+
+
+@patch("time.strftime", MagicMock(return_value=TIMESTAMP))
+@patch("sagemaker.utils.repack_model")
+def test_script_mode_model_same_calls_as_framework(repack_model, sagemaker_session):
+    t = Model(
+        entry_point=ENTRY_POINT_INFERENCE,
+        role=ROLE,
+        sagemaker_session=sagemaker_session,
+        source_dir=SCRIPT_URI,
+        image_uri=IMAGE_URI,
+        model_data=MODEL_DATA,
+    )
+    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
+
+    assert len(sagemaker_session.create_model.call_args_list) == 1
+    assert len(sagemaker_session.endpoint_from_production_variants.call_args_list) == 1
+    assert len(repack_model.call_args_list) == 1
+
+    generic_model_create_model_args = sagemaker_session.create_model.call_args_list
+    generic_model_endpoint_from_production_variants_args = (
+        sagemaker_session.endpoint_from_production_variants.call_args_list
+    )
+    generic_model_repack_model_args = repack_model.call_args_list
+
+    sagemaker_session.create_model.reset_mock()
+    sagemaker_session.endpoint_from_production_variants.reset_mock()
+    repack_model.reset_mock()
+
+    t = DummyFrameworkModel(
+        entry_point=ENTRY_POINT_INFERENCE,
+        role=ROLE,
+        sagemaker_session=sagemaker_session,
+        source_dir=SCRIPT_URI,
+        image_uri=IMAGE_URI,
+        model_data=MODEL_DATA,
+    )
+    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
+
+    assert generic_model_create_model_args == sagemaker_session.create_model.call_args_list
+    assert (
+        generic_model_endpoint_from_production_variants_args
+        == sagemaker_session.endpoint_from_production_variants.call_args_list
+    )
+    assert generic_model_repack_model_args == repack_model.call_args_list
+
+
+@patch("sagemaker.git_utils.git_clone_repo")
+@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
+def test_git_support_succeed_model_class(tar_and_upload_dir, git_clone_repo, sagemaker_session):
+    git_clone_repo.side_effect = lambda gitconfig, entrypoint, sourcedir, dependency: {
+        "entry_point": "entry_point",
+        "source_dir": "/tmp/repo_dir/source_dir",
+        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"],
+    }
+    entry_point = "entry_point"
+    source_dir = "source_dir"
+    dependencies = ["foo", "bar"]
+    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
+    model = Model(
+        sagemaker_session=sagemaker_session,
+        entry_point=entry_point,
+        source_dir=source_dir,
+        dependencies=dependencies,
+        git_config=git_config,
+        image_uri=IMAGE_URI,
+    )
+    model.prepare_container_def(instance_type=INSTANCE_TYPE)
+    git_clone_repo.assert_called_with(git_config, entry_point, source_dir, dependencies)
+    assert model.entry_point == "entry_point"
+    assert model.source_dir == "/tmp/repo_dir/source_dir"
+    assert model.dependencies == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]
+
+
+@patch("sagemaker.utils.repack_model")
+def test_script_mode_model_tags_jumpstart_models(repack_model, sagemaker_session):
+
+    jumpstart_source_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/source_dirs/source.tar.gz"
+    t = Model(
+        entry_point=ENTRY_POINT_INFERENCE,
+        role=ROLE,
+        sagemaker_session=sagemaker_session,
+        source_dir=jumpstart_source_dir,
+        image_uri=IMAGE_URI,
+        model_data=MODEL_DATA,
+    )
+    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
+
+    assert sagemaker_session.create_model.call_args_list[0][1]["tags"] == [
+        {
+            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
+            "Value": jumpstart_source_dir,
+        },
+    ]
+    assert sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"] == [
+        {
+            "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
+            "Value": jumpstart_source_dir,
+        },
+    ]
+
+    non_jumpstart_source_dir = "s3://blah/blah/blah"
+    t = Model(
+        entry_point=ENTRY_POINT_INFERENCE,
+        role=ROLE,
+        sagemaker_session=sagemaker_session,
+        source_dir=non_jumpstart_source_dir,
+        image_uri=IMAGE_URI,
+        model_data=MODEL_DATA,
+    )
+    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
+
+    assert {
+        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
+        "Value": non_jumpstart_source_dir,
+    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
+
+    assert {
+        "Key": JumpStartTag.INFERENCE_SCRIPT_URI.value,
+        "Value": non_jumpstart_source_dir,
+    } not in sagemaker_session.create_model.call_args_list[0][1]["tags"]
+
+
+@patch("sagemaker.utils.repack_model")
+@patch("sagemaker.fw_utils.tar_and_upload_dir")
+def test_all_framework_models_add_jumpstart_tags(
+    repack_model, tar_and_uload_dir, sagemaker_session
+):
+    framework_model_classes_to_kwargs = {
+        PyTorchModel: {"framework_version": "1.5.0", "py_version": "py3"},
+        TensorFlowModel: {
+            "framework_version": "2.3",
+        },
+        HuggingFaceModel: {
+            "pytorch_version": "1.7.1",
+            "py_version": "py36",
+            "transformers_version": "4.6.1",
+        },
+        MXNetModel: {"framework_version": "1.7.0", "py_version": "py3"},
+        SKLearnModel: {
+            "framework_version": "0.23-1",
+        },
+        XGBoostModel: {
+            "framework_version": "1.3-1",
+        },
+    }
+    jumpstart_model_dir = f"s3://{list(JUMPSTART_BUCKET_NAME_SET)[0]}/model_dirs/model.tar.gz"
+    for framework_model_class, kwargs in framework_model_classes_to_kwargs.items():
+        framework_model_class(
+            entry_point=ENTRY_POINT_INFERENCE,
+            role=ROLE,
+            sagemaker_session=sagemaker_session,
+            model_data=jumpstart_model_dir,
+            **kwargs,
+        ).deploy(instance_type="ml.m2.xlarge", initial_instance_count=INSTANCE_COUNT)
+
+        assert {
+            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
+            "Value": jumpstart_model_dir,
+        } in sagemaker_session.create_model.call_args_list[0][1]["tags"]
+
+        assert {
+            "Key": JumpStartTag.INFERENCE_MODEL_URI.value,
+            "Value": jumpstart_model_dir,
+        } in sagemaker_session.endpoint_from_production_variants.call_args_list[0][1]["tags"]
+
+        sagemaker_session.create_model.reset_mock()
+        sagemaker_session.endpoint_from_production_variants.reset_mock()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-20 18:22:42[0m
[92mHash: d9d8c68b4bfba9b1adf1d8356de092331d70039b[0m
[92mFilepath: src/sagemaker/estimator.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feat: Script mode support for Estimator class (#2834)

[0m
@@ -16,7 +16,6 @@ from __future__ import absolute_import, print_function
 import json
 import logging
 import os
-from typing import Any, Dict
 import uuid
 from abc import ABCMeta, abstractmethod
 
@@ -87,15 +86,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
     instance.
     """
 
-    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
-    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
-    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
-    INSTANCE_TYPE = "sagemaker_instance_type"
-    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
-    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
-    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
-    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
-
     def __init__(
         self,
         role,
@@ -129,13 +119,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        hyperparameters=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``EstimatorBase`` instance.
@@ -287,133 +270,13 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
                 will be disabled (default: ``False``).
             environment (dict[str, str]) : Environment variables to be set for
                 use during training job (default: ``None``)
-            max_retry_attempts (int): The number of times to move a job to the STARTING status.
+             max_retry_attempts (int): The number of times to move a job to the STARTING status.
                 You can specify between 1 and 30 attempts.
                 If the value of attempts is greater than zero,
                 the job is retried on InternalServerFailure
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            hyperparameters (dict): Dictionary containing the hyperparameters to
-                initialize this estimator with. (Default: None).
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. (Default: None). If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
 
         """
         instance_count = renamed_kwargs(
@@ -448,14 +311,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self.model_channel_name = model_channel_name
         self.code_uri = None
         self.code_channel_name = "code"
-        self.source_dir = source_dir
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
-        self.code_location = code_location
-        self.entry_point = entry_point
-        self.dependencies = dependencies
-        self.uploaded_code = None
 
         if self.instance_type in ("local", "local_gpu"):
             if self.instance_type == "local_gpu" and self.instance_count > 1:
@@ -582,21 +437,6 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
         self._ensure_base_job_name()
         return name_from_base(self.base_job_name)
 
-    @staticmethod
-    def _json_encode_hyperparameters(hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
-        """Applies Json encoding for certain Hyperparameter types, returns hyperparameters.
-
-        Args:
-            hyperparameters (dict): Dictionary of hyperparameters.
-        """
-        current_hyperparameters = hyperparameters
-        if current_hyperparameters is not None:
-            hyperparameters = {
-                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
-                for (k, v) in current_hyperparameters.items()
-            }
-        return hyperparameters
-
     def _prepare_for_training(self, job_name=None):
         """Set any values in the estimator that need to be set before training.
 
@@ -616,105 +456,10 @@ class EstimatorBase(with_metaclass(ABCMeta, object)):  # pylint: disable=too-man
             else:
                 self.output_path = "s3://{}/".format(self.sagemaker_session.default_bucket())
 
-        if self.git_config:
-            updated_paths = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updated_paths["entry_point"]
-            self.source_dir = updated_paths["source_dir"]
-            self.dependencies = updated_paths["dependencies"]
-
-        if self.source_dir or self.entry_point or self.dependencies:
-
-            # validate source dir will raise a ValueError if there is something wrong with
-            # the source directory. We are intentionally not handling it because this is a
-            # critical error.
-            if self.source_dir and not self.source_dir.lower().startswith("s3://"):
-                validate_source_dir(self.entry_point, self.source_dir)
-
-            # if we are in local mode with local_code=True. We want the container to just
-            # mount the source dir instead of uploading to S3.
-            local_code = get_config_value("local.local_code", self.sagemaker_session.config)
-
-            if self.sagemaker_session.local_mode and local_code:
-                # if there is no source dir, use the directory containing the entry point.
-                if self.source_dir is None:
-                    self.source_dir = os.path.dirname(self.entry_point)
-                self.entry_point = os.path.basename(self.entry_point)
-
-                code_dir = "file://" + self.source_dir
-                script = self.entry_point
-            elif self.enable_network_isolation() and self.entry_point:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
-                script = self.uploaded_code.script_name
-                self.code_uri = self.uploaded_code.s3_prefix
-            else:
-                self.uploaded_code = self._stage_user_code_in_s3()
-                code_dir = self.uploaded_code.s3_prefix
-                script = self.uploaded_code.script_name
-
-            # Modify hyperparameters in-place to point to the right code directory and
-            # script URIs
-            self._script_mode_hyperparam_update(code_dir, script)
-
         self._prepare_rules()
         self._prepare_debugger_for_training()
         self._prepare_profiler_for_training()
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
-
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
-
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(hyperparams))
-
-    def _stage_user_code_in_s3(self) -> str:
-        """Upload the user training script to s3 and return the s3 URI.
-
-        Returns: s3 uri
-        """
-        local_mode = self.output_path.startswith("file://")
-
-        if self.code_location is None and local_mode:
-            code_bucket = self.sagemaker_session.default_bucket()
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = None
-        elif self.code_location is None:
-            code_bucket, _ = parse_s3_url(self.output_path)
-            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
-            kms_key = self.output_kms_key
-        elif local_mode:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-            kms_key = None
-        else:
-            code_bucket, key_prefix = parse_s3_url(self.code_location)
-            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
-
-            output_bucket, _ = parse_s3_url(self.output_path)
-            kms_key = self.output_kms_key if code_bucket == output_bucket else None
-
-        return tar_and_upload_dir(
-            session=self.sagemaker_session.boto_session,
-            bucket=code_bucket,
-            s3_key_prefix=code_s3_prefix,
-            script=self.entry_point,
-            directory=self.source_dir,
-            dependencies=self.dependencies,
-            kms_key=kms_key,
-            s3_resource=self.sagemaker_session.s3_resource,
-        )
-
     def _prepare_rules(self):
         """Rules list includes both debugger and profiler rules.
 
@@ -1974,12 +1719,6 @@ class Estimator(EstimatorBase):
         disable_profiler=False,
         environment=None,
         max_retry_attempts=None,
-        source_dir=None,
-        git_config=None,
-        container_log_level=logging.INFO,
-        code_location=None,
-        entry_point=None,
-        dependencies=None,
         **kwargs,
     ):
         """Initialize an ``Estimator`` instance.
@@ -2137,127 +1876,9 @@ class Estimator(EstimatorBase):
                 the same number of attempts as the value.
                 You can cap the total duration for your job by setting ``max_wait`` and ``max_run``
                 (default: ``None``)
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git
-                repo.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    and you need 'train.py' as entry point and 'test.py' as
-                    training source code as well, you can assign
-                    entry_point='train.py', source_dir='src'.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            code_location (str): The S3 prefix URI where custom code will be
-                uploaded (default: None) - don't include a trailing slash since
-                a string prepended with a "/" is appended to ``code_location``. The code
-                file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.
-                If not specified, the default ``code location`` is s3://output_bucket/job-name/.
-            entry_point (str): Path (absolute or relative) to the local Python
-                source file which should be executed as the entry point to
-                training. If ``source_dir`` is specified, then ``entry_point``
-                must point to a file located at the root of ``source_dir``.
-                If 'git_config' is provided, 'entry_point' should be
-                a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- train.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/train.py'.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Estimator(entry_point='train.py',
-                    ...           dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ train.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
         """
         self.image_uri = image_uri
-        self._hyperparameters = hyperparameters.copy() if hyperparameters else {}
+        self.hyperparam_dict = hyperparameters.copy() if hyperparameters else {}
         super(Estimator, self).__init__(
             role,
             instance_count,
@@ -2290,13 +1911,6 @@ class Estimator(EstimatorBase):
             disable_profiler=disable_profiler,
             environment=environment,
             max_retry_attempts=max_retry_attempts,
-            container_log_level=container_log_level,
-            source_dir=source_dir,
-            git_config=git_config,
-            code_location=code_location,
-            entry_point=entry_point,
-            dependencies=dependencies,
-            hyperparameters=hyperparameters,
             **kwargs,
         )
 
@@ -2317,7 +1931,7 @@ class Estimator(EstimatorBase):
         training.
         """
         for k, v in kwargs.items():
-            self._hyperparameters[k] = v
+            self.hyperparam_dict[k] = v
 
     def hyperparameters(self):
         """Returns the hyperparameters as a dictionary to use for training.
@@ -2325,7 +1939,7 @@ class Estimator(EstimatorBase):
         The fit() method, that does the model training, calls this method to
         find the hyperparameters you specified.
         """
-        return self._hyperparameters
+        return self.hyperparam_dict
 
     def create_model(
         self,
@@ -2401,6 +2015,15 @@ class Framework(EstimatorBase):
 
     _framework_name = None
 
+    LAUNCH_PS_ENV_NAME = "sagemaker_parameter_server_enabled"
+    LAUNCH_MPI_ENV_NAME = "sagemaker_mpi_enabled"
+    LAUNCH_SM_DDP_ENV_NAME = "sagemaker_distributed_dataparallel_enabled"
+    INSTANCE_TYPE = "sagemaker_instance_type"
+    MPI_NUM_PROCESSES_PER_HOST = "sagemaker_mpi_num_of_processes_per_host"
+    MPI_CUSTOM_MPI_OPTIONS = "sagemaker_mpi_custom_mpi_options"
+    SM_DDP_CUSTOM_MPI_OPTIONS = "sagemaker_distributed_dataparallel_custom_mpi_options"
+    CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = "/opt/ml/input/data/code/sourcedir.tar.gz"
+
     def __init__(
         self,
         entry_point,
@@ -2614,23 +2237,48 @@ class Framework(EstimatorBase):
         """
         super(Framework, self)._prepare_for_training(job_name=job_name)
 
-        self._validate_and_set_debugger_configs()
+        if self.git_config:
+            updated_paths = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updated_paths["entry_point"]
+            self.source_dir = updated_paths["source_dir"]
+            self.dependencies = updated_paths["dependencies"]
 
-    def _script_mode_hyperparam_update(self, code_dir: str, script: str) -> None:
-        """Applies in-place update to hyperparameters required for script mode with training.
+        # validate source dir will raise a ValueError if there is something wrong with the
+        # source directory. We are intentionally not handling it because this is a critical error.
+        if self.source_dir and not self.source_dir.lower().startswith("s3://"):
+            validate_source_dir(self.entry_point, self.source_dir)
+
+        # if we are in local mode with local_code=True. We want the container to just
+        # mount the source dir instead of uploading to S3.
+        local_code = get_config_value("local.local_code", self.sagemaker_session.config)
+        if self.sagemaker_session.local_mode and local_code:
+            # if there is no source dir, use the directory containing the entry point.
+            if self.source_dir is None:
+                self.source_dir = os.path.dirname(self.entry_point)
+            self.entry_point = os.path.basename(self.entry_point)
+
+            code_dir = "file://" + self.source_dir
+            script = self.entry_point
+        elif self.enable_network_isolation() and self.entry_point:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH
+            script = self.uploaded_code.script_name
+            self.code_uri = self.uploaded_code.s3_prefix
+        else:
+            self.uploaded_code = self._stage_user_code_in_s3()
+            code_dir = self.uploaded_code.s3_prefix
+            script = self.uploaded_code.script_name
 
-        Args:
-            code_dir (str): The directory hosting the training scripts.
-            script (str): The relative filepath of the training entry-point script.
-        """
-        hyperparams: Dict[str, str] = {}
-        hyperparams[DIR_PARAM_NAME] = code_dir
-        hyperparams[SCRIPT_PARAM_NAME] = script
-        hyperparams[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
-        hyperparams[JOB_NAME_PARAM_NAME] = self._current_job_name
-        hyperparams[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
+        # Modify hyperparameters in-place to point to the right code directory and script URIs
+        self._hyperparameters[DIR_PARAM_NAME] = code_dir
+        self._hyperparameters[SCRIPT_PARAM_NAME] = script
+        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level
+        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name
+        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name
 
-        self._hyperparameters.update(hyperparams)
+        self._validate_and_set_debugger_configs()
 
     def _validate_and_set_debugger_configs(self):
         """Set defaults for debugging."""
@@ -2660,6 +2308,44 @@ class Framework(EstimatorBase):
                 self.environment = {}
             self.environment[DEBUGGER_FLAG] = "0"
 
+    def _stage_user_code_in_s3(self):
+        """Upload the user training script to s3 and return the location.
+
+        Returns: s3 uri
+        """
+        local_mode = self.output_path.startswith("file://")
+
+        if self.code_location is None and local_mode:
+            code_bucket = self.sagemaker_session.default_bucket()
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = None
+        elif self.code_location is None:
+            code_bucket, _ = parse_s3_url(self.output_path)
+            code_s3_prefix = "{}/{}".format(self._current_job_name, "source")
+            kms_key = self.output_kms_key
+        elif local_mode:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+            kms_key = None
+        else:
+            code_bucket, key_prefix = parse_s3_url(self.code_location)
+            code_s3_prefix = "/".join(filter(None, [key_prefix, self._current_job_name, "source"]))
+
+            output_bucket, _ = parse_s3_url(self.output_path)
+            kms_key = self.output_kms_key if code_bucket == output_bucket else None
+
+        return tar_and_upload_dir(
+            session=self.sagemaker_session.boto_session,
+            bucket=code_bucket,
+            s3_key_prefix=code_s3_prefix,
+            script=self.entry_point,
+            directory=self.source_dir,
+            dependencies=self.dependencies,
+            kms_key=kms_key,
+            s3_resource=self.sagemaker_session.s3_resource,
+            settings=self.sagemaker_session.settings,
+        )
+
     def _model_source_dir(self):
         """Get the appropriate value to pass as ``source_dir`` to a model constructor.
 
@@ -2690,10 +2376,6 @@ class Framework(EstimatorBase):
 
         return None
 
-    def set_hyperparameters(self, **kwargs):
-        """Escape the dict argument as JSON, update the private hyperparameter attribute."""
-        self._hyperparameters.update(EstimatorBase._json_encode_hyperparameters(kwargs))
-
     def hyperparameters(self):
         """Return the hyperparameters as a dictionary to use for training.
 
@@ -2703,7 +2385,7 @@ class Framework(EstimatorBase):
         Returns:
             dict[str, str]: The hyperparameters.
         """
-        return EstimatorBase._json_encode_hyperparameters(self._hyperparameters)
+        return self._json_encode_hyperparameters(self._hyperparameters)
 
     @classmethod
     def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
@@ -2822,6 +2504,17 @@ class Framework(EstimatorBase):
         )
         return estimator
 
+    @staticmethod
+    def _json_encode_hyperparameters(hyperparameters):
+        """Placeholder docstring"""
+        current_hyperparameters = hyperparameters
+        if current_hyperparameters is not None:
+            hyperparameters = {
+                str(k): (v if isinstance(v, (Parameter, Expression, Properties)) else json.dumps(v))
+                for (k, v) in current_hyperparameters.items()
+            }
+        return hyperparameters
+
     @classmethod
     def _update_init_params(cls, hp, tf_arguments):
         """Placeholder docstring"""

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-19 12:55:15[0m
[92mHash: 167b72384249558d7b11301d0ad2ac5acc663e20[0m
[92mFilepath: src/sagemaker/model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: script mode for model class (#2841)

[0m
@@ -18,7 +18,6 @@ import json
 import logging
 import os
 import re
-import copy
 
 import sagemaker
 from sagemaker import (
@@ -33,6 +32,7 @@ from sagemaker import (
 from sagemaker.inputs import CompilationInput
 from sagemaker.deprecations import removed_kwargs
 from sagemaker.predictor import PredictorBase
+from sagemaker.serverless import ServerlessInferenceConfig
 from sagemaker.transformer import Transformer
 
 LOGGER = logging.getLogger("sagemaker")
@@ -58,15 +58,6 @@ class ModelBase(abc.ABC):
         """Destroy resources associated with this model."""
 
 
-SCRIPT_PARAM_NAME = "sagemaker_program"
-DIR_PARAM_NAME = "sagemaker_submit_directory"
-CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
-JOB_NAME_PARAM_NAME = "sagemaker_job_name"
-MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
-SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
-SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
-
-
 class Model(ModelBase):
     """A SageMaker ``Model`` that can be deployed to an ``Endpoint``."""
 
@@ -83,12 +74,6 @@ class Model(ModelBase):
         enable_network_isolation=False,
         model_kms_key=None,
         image_config=None,
-        source_dir=None,
-        code_location=None,
-        entry_point=None,
-        container_log_level=logging.INFO,
-        dependencies=None,
-        git_config=None,
     ):
         """Initialize an SageMaker ``Model``.
 
@@ -130,124 +115,6 @@ class Model(ModelBase):
                 model container is pulled from ECR, or private registry in your
                 VPC. By default it is set to pull model container image from
                 ECR. (default: None).
-            source_dir (str): Path (absolute, relative or an S3 URI) to a directory
-                with any other training source code dependencies aside from the entry
-                point file (default: None). If ``source_dir`` is an S3 URI, it must
-                point to a tar.gz file. Structure within this directory are preserved
-                when training on Amazon SageMaker. If 'git_config' is provided,
-                'source_dir' should be a relative location to a directory in the Git repo.
-                If the directory points to S3, no code will be uploaded and the S3 location
-                will be used instead.
-
-                .. admonition:: Example
-
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='inference.py', source_dir='src'.
-            code_location (str): Name of the S3 bucket where custom code is
-                uploaded (default: None). If not specified, default bucket
-                created by ``sagemaker.session.Session`` is used.
-            entry_point (str): Path (absolute or relative) to the Python source
-                file which should be executed as the entry point to model
-                hosting (default: None). If ``source_dir`` is specified,
-                then ``entry_point`` must point to a file located at the root of
-                ``source_dir``. If 'git_config' is provided, 'entry_point' should
-                be a relative location to the Python source file in the Git repo.
-
-                Example:
-                    With the following GitHub repo directory structure:
-
-                    >>> |----- README.md
-                    >>> |----- src
-                    >>>         |----- inference.py
-                    >>>         |----- test.py
-
-                    You can assign entry_point='src/inference.py'.
-            container_log_level (int): Log level to use within the container
-                (default: logging.INFO). Valid values are defined in the Python
-                logging module.
-            dependencies (list[str]): A list of paths to directories (absolute
-                or relative) with any additional libraries that will be exported
-                to the container (default: []). The library folders will be
-                copied to SageMaker in the same folder where the entrypoint is
-                copied. If 'git_config' is provided, 'dependencies' should be a
-                list of relative locations to directories with any additional
-                libraries needed in the Git repo. If the ```source_dir``` points
-                to S3, code will be uploaded and the S3 location will be used
-                instead.
-
-                .. admonition:: Example
-
-                    The following call
-
-                    >>> Model(entry_point='inference.py',
-                    ...       dependencies=['my/libs/common', 'virtual-env'])
-
-                    results in the following inside the container:
-
-                    >>> $ ls
-
-                    >>> opt/ml/code
-                    >>>     |------ inference.py
-                    >>>     |------ common
-                    >>>     |------ virtual-env
-
-                This is not supported with "local code" in Local Mode.
-            git_config (dict[str, str]): Git configurations used for cloning
-                files, including ``repo``, ``branch``, ``commit``,
-                ``2FA_enabled``, ``username``, ``password`` and ``token``. The
-                ``repo`` field is required. All other fields are optional.
-                ``repo`` specifies the Git repository where your training script
-                is stored. If you don't provide ``branch``, the default value
-                'master' is used. If you don't provide ``commit``, the latest
-                commit in the specified branch is used. .. admonition:: Example
-
-                    The following config:
-
-                    >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',
-                    >>>               'branch': 'test-branch-git-config',
-                    >>>               'commit': '[93m329bfcf884482002c05ff7f44f62599ebc9f445a[0m'}
-
-                    results in cloning the repo specified in 'repo', then
-                    checkout the 'master' branch, and checkout the specified
-                    commit.
-
-                ``2FA_enabled``, ``username``, ``password`` and ``token`` are
-                used for authentication. For GitHub (or other Git) accounts, set
-                ``2FA_enabled`` to 'True' if two-factor authentication is
-                enabled for the account, otherwise set it to 'False'. If you do
-                not provide a value for ``2FA_enabled``, a default value of
-                'False' is used. CodeCommit does not support two-factor
-                authentication, so do not provide "2FA_enabled" with CodeCommit
-                repositories.
-
-                For GitHub and other Git repos, when SSH URLs are provided, it
-                doesn't matter whether 2FA is enabled or disabled; you should
-                either have no passphrase for the SSH key pairs, or have the
-                ssh-agent configured so that you will not be prompted for SSH
-                passphrase when you do 'git clone' command with SSH URLs. When
-                HTTPS URLs are provided: if 2FA is disabled, then either token
-                or username+password will be used for authentication if provided
-                (token prioritized); if 2FA is enabled, only token will be used
-                for authentication if provided. If required authentication info
-                is not provided, python SDK will try to use local credentials
-                storage to authenticate. If that fails either, an error message
-                will be thrown.
-
-                For CodeCommit repos, 2FA is not supported, so '2FA_enabled'
-                should not be provided. There is no token in CodeCommit, so
-                'token' should not be provided too. When 'repo' is an SSH URL,
-                the requirements are the same as GitHub-like repos. When 'repo'
-                is an HTTPS URL, username+password will be used for
-                authentication if they are provided; otherwise, python SDK will
-                try to use either CodeCommit credential helper or local
-                credential storage for authentication.
-
         """
         self.model_data = model_data
         self.image_uri = image_uri
@@ -265,24 +132,6 @@ class Model(ModelBase):
         self._enable_network_isolation = enable_network_isolation
         self.model_kms_key = model_kms_key
         self.image_config = image_config
-        self.entry_point = entry_point
-        self.source_dir = source_dir
-        self.dependencies = dependencies or []
-        self.git_config = git_config
-        self.container_log_level = container_log_level
-        if code_location:
-            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
-        else:
-            self.bucket, self.key_prefix = None, None
-        if self.git_config:
-            updates = git_utils.git_clone_repo(
-                self.git_config, self.entry_point, self.source_dir, self.dependencies
-            )
-            self.entry_point = updates["entry_point"]
-            self.source_dir = updates["source_dir"]
-            self.dependencies = updates["dependencies"]
-        self.uploaded_code = None
-        self.repacked_model_data = None
 
     def register(
         self,
@@ -361,7 +210,7 @@ class Model(ModelBase):
             model_package_arn=model_package.get("ModelPackageArn"),
         )
 
-    def _init_sagemaker_session_if_does_not_exist(self, instance_type):
+    def _init_sagemaker_session_if_does_not_exist(self, instance_type=None):
         """Set ``self.sagemaker_session`` to ``LocalSession`` or ``Session`` if it's not already.
 
         The type of session object is determined by the instance type.
@@ -394,90 +243,10 @@ class Model(ModelBase):
         Returns:
             dict: A container definition object usable with the CreateModel API.
         """
-        deploy_key_prefix = fw_utils.model_code_key_prefix(
-            self.key_prefix, self.name, self.image_uri
-        )
-        deploy_env = copy.deepcopy(self.env)
-        if self.source_dir or self.dependencies or self.entry_point or self.git_config:
-            if self.key_prefix or self.git_config:
-                self._upload_code(deploy_key_prefix, repack=False)
-            elif self.source_dir and self.entry_point:
-                self._upload_code(deploy_key_prefix, repack=True)
-            else:
-                self._upload_code(deploy_key_prefix, repack=False)
-            deploy_env.update(self._script_mode_env_vars())
         return sagemaker.container_def(
-            self.image_uri, self.model_data, deploy_env, image_config=self.image_config
+            self.image_uri, self.model_data, self.env, image_config=self.image_config
         )
 
-    def _upload_code(self, key_prefix: str, repack: bool = False) -> None:
-        """Uploads code to S3 to be used with script mode with SageMaker inference.
-
-        Args:
-            key_prefix (str): The S3 key associated with the ``code_location`` parameter of the
-                ``Model`` class.
-            repack (bool): Optional. Set to ``True`` to indicate that the source code and model
-                artifact should be repackaged into a new S3 object. (default: False).
-        """
-        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
-        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
-            self.uploaded_code = None
-        elif not repack:
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            self.uploaded_code = fw_utils.tar_and_upload_dir(
-                session=self.sagemaker_session.boto_session,
-                bucket=bucket,
-                s3_key_prefix=key_prefix,
-                script=self.entry_point,
-                directory=self.source_dir,
-                dependencies=self.dependencies,
-            )
-
-        if repack and self.model_data is not None and self.entry_point is not None:
-            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
-                # model is not yet there, defer repacking to later during pipeline execution
-                return
-
-            bucket = self.bucket or self.sagemaker_session.default_bucket()
-            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
-
-            utils.repack_model(
-                inference_script=self.entry_point,
-                source_directory=self.source_dir,
-                dependencies=self.dependencies,
-                model_uri=self.model_data,
-                repacked_model_uri=repacked_model_data,
-                sagemaker_session=self.sagemaker_session,
-                kms_key=self.model_kms_key,
-            )
-
-            self.repacked_model_data = repacked_model_data
-            self.uploaded_code = fw_utils.UploadedCode(
-                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
-            )
-
-    def _script_mode_env_vars(self):
-        """Placeholder docstring"""
-        script_name = None
-        dir_name = None
-        if self.uploaded_code:
-            script_name = self.uploaded_code.script_name
-            if self.enable_network_isolation():
-                dir_name = "/opt/ml/model/code"
-            else:
-                dir_name = self.uploaded_code.s3_prefix
-        elif self.entry_point is not None:
-            script_name = self.entry_point
-            if self.source_dir is not None:
-                dir_name = "file://" + self.source_dir
-
-        return {
-            SCRIPT_PARAM_NAME.upper(): script_name or str(),
-            DIR_PARAM_NAME.upper(): dir_name or str(),
-            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
-            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
-        }
-
     def enable_network_isolation(self):
         """Whether to enable network isolation when creating this Model
 
@@ -920,8 +689,8 @@ class Model(ModelBase):
 
     def deploy(
         self,
-        initial_instance_count,
-        instance_type,
+        initial_instance_count=None,
+        instance_type=None,
         serializer=None,
         deserializer=None,
         accelerator_type=None,
@@ -930,6 +699,7 @@ class Model(ModelBase):
         kms_key=None,
         wait=True,
         data_capture_config=None,
+        serverless_inference_config=None,
         **kwargs,
     ):
         """Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.
@@ -947,9 +717,13 @@ class Model(ModelBase):
 
         Args:
             initial_instance_count (int): The initial number of instances to run
-                in the ``Endpoint`` created from this ``Model``.
+                in the ``Endpoint`` created from this ``Model``. If not using
+                serverless inference, then it need to be a number larger or equals
+                to 1 (default: None)
             instance_type (str): The EC2 instance type to deploy this Model to.
-                For example, 'ml.p2.xlarge', or 'local' for local mode.
+                For example, 'ml.p2.xlarge', or 'local' for local mode. If not using
+                serverless inference, then it is required to deploy a model.
+                (default: None)
             serializer (:class:`~sagemaker.serializers.BaseSerializer`): A
                 serializer object, used to encode data for an inference endpoint
                 (default: None). If ``serializer`` is not None, then
@@ -978,7 +752,17 @@ class Model(ModelBase):
             data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies
                 configuration related to Endpoint data capture for use with
                 Amazon SageMaker Model Monitoring. Default: None.
-
+            serverless_inference_config (sagemaker.serverless.ServerlessInferenceConfig):
+                Specifies configuration related to serverless endpoint. Use this configuration
+                when trying to create serverless endpoint and make serverless inference. If
+                empty object passed through, we will use pre-defined values in
+                ``ServerlessInferenceConfig`` class to deploy serverless endpoint (default: None)
+        Raises:
+             ValueError: If arguments combination check failed in these circumstances:
+                - If no role is specified or
+                - If serverless inference config is not specified and instance type and instance
+                    count are also not specified or
+                - If a wrong type of object is provided as serverless inference config
         Returns:
             callable[string, sagemaker.session.Session] or None: Invocation of
                 ``self.predictor_cls`` on the created endpoint name, if ``self.predictor_cls``
@@ -990,27 +774,47 @@ class Model(ModelBase):
         if self.role is None:
             raise ValueError("Role can not be null for deploying a model")
 
-        if instance_type.startswith("ml.inf") and not self._is_compiled_model:
+        is_serverless = serverless_inference_config is not None
+        if not is_serverless and not (instance_type and initial_instance_count):
+            raise ValueError(
+                "Must specify instance type and instance count unless using serverless inference"
+            )
+
+        if is_serverless and not isinstance(serverless_inference_config, ServerlessInferenceConfig):
+            raise ValueError(
+                "serverless_inference_config needs to be a ServerlessInferenceConfig object"
+            )
+
+        if instance_type and instance_type.startswith("ml.inf") and not self._is_compiled_model:
             LOGGER.warning(
                 "Your model is not compiled. Please compile your model before using Inferentia."
             )
 
-        compiled_model_suffix = "-".join(instance_type.split(".")[:-1])
-        if self._is_compiled_model:
+        compiled_model_suffix = None if is_serverless else "-".join(instance_type.split(".")[:-1])
+        if self._is_compiled_model and not is_serverless:
             self._ensure_base_name_if_needed(self.image_uri)
             if self._base_name is not None:
                 self._base_name = "-".join((self._base_name, compiled_model_suffix))
 
         self._create_sagemaker_model(instance_type, accelerator_type, tags)
+
+        serverless_inference_config_dict = (
+            serverless_inference_config._to_request_dict() if is_serverless else None
+        )
         production_variant = sagemaker.production_variant(
-            self.name, instance_type, initial_instance_count, accelerator_type=accelerator_type
+            self.name,
+            instance_type,
+            initial_instance_count,
+            accelerator_type=accelerator_type,
+            serverless_inference_config=serverless_inference_config_dict,
         )
         if endpoint_name:
             self.endpoint_name = endpoint_name
         else:
             base_endpoint_name = self._base_name or utils.base_from_name(self.name)
-            if self._is_compiled_model and not base_endpoint_name.endswith(compiled_model_suffix):
-                base_endpoint_name = "-".join((base_endpoint_name, compiled_model_suffix))
+            if self._is_compiled_model and not is_serverless:
+                if not base_endpoint_name.endswith(compiled_model_suffix):
+                    base_endpoint_name = "-".join((base_endpoint_name, compiled_model_suffix))
             self.endpoint_name = utils.name_from_base(base_endpoint_name)
 
         data_capture_config_dict = None
@@ -1117,6 +921,15 @@ class Model(ModelBase):
         self.sagemaker_session.delete_model(self.name)
 
 
+SCRIPT_PARAM_NAME = "sagemaker_program"
+DIR_PARAM_NAME = "sagemaker_submit_directory"
+CONTAINER_LOG_LEVEL_PARAM_NAME = "sagemaker_container_log_level"
+JOB_NAME_PARAM_NAME = "sagemaker_job_name"
+MODEL_SERVER_WORKERS_PARAM_NAME = "sagemaker_model_server_workers"
+SAGEMAKER_REGION_PARAM_NAME = "sagemaker_region"
+SAGEMAKER_OUTPUT_LOCATION = "sagemaker_s3_output"
+
+
 class FrameworkModel(Model):
     """A Model for working with an SageMaker ``Framework``.
 
@@ -1294,14 +1107,113 @@ class FrameworkModel(Model):
             env=env,
             name=name,
             sagemaker_session=sagemaker_session,
-            source_dir=source_dir,
-            code_location=code_location,
-            entry_point=entry_point,
-            container_log_level=container_log_level,
-            dependencies=dependencies,
-            git_config=git_config,
             **kwargs,
         )
+        self.entry_point = entry_point
+        self.source_dir = source_dir
+        self.dependencies = dependencies or []
+        self.git_config = git_config
+        self.container_log_level = container_log_level
+        if code_location:
+            self.bucket, self.key_prefix = s3.parse_s3_url(code_location)
+        else:
+            self.bucket, self.key_prefix = None, None
+        if self.git_config:
+            updates = git_utils.git_clone_repo(
+                self.git_config, self.entry_point, self.source_dir, self.dependencies
+            )
+            self.entry_point = updates["entry_point"]
+            self.source_dir = updates["source_dir"]
+            self.dependencies = updates["dependencies"]
+        self.uploaded_code = None
+        self.repacked_model_data = None
+
+    def prepare_container_def(self, instance_type=None, accelerator_type=None):
+        """Return a container definition with framework configuration.
+
+        Framework configuration is set in model environment variables.
+        This also uploads user-supplied code to S3.
+
+        Args:
+            instance_type (str): The EC2 instance type to deploy this Model to.
+                For example, 'ml.p2.xlarge'.
+            accelerator_type (str): The Elastic Inference accelerator type to
+                deploy to the instance for loading and making inferences to the
+                model. For example, 'ml.eia1.medium'.
+
+        Returns:
+            dict[str, str]: A container definition object usable with the
+            CreateModel API.
+        """
+        deploy_key_prefix = fw_utils.model_code_key_prefix(
+            self.key_prefix, self.name, self.image_uri
+        )
+        self._upload_code(deploy_key_prefix)
+        deploy_env = dict(self.env)
+        deploy_env.update(self._framework_env_vars())
+        return sagemaker.container_def(self.image_uri, self.model_data, deploy_env)
+
+    def _upload_code(self, key_prefix, repack=False):
+        """Placeholder Docstring"""
+        local_code = utils.get_config_value("local.local_code", self.sagemaker_session.config)
+        if (self.sagemaker_session.local_mode and local_code) or self.entry_point is None:
+            self.uploaded_code = None
+        elif not repack:
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            self.uploaded_code = fw_utils.tar_and_upload_dir(
+                session=self.sagemaker_session.boto_session,
+                bucket=bucket,
+                s3_key_prefix=key_prefix,
+                script=self.entry_point,
+                directory=self.source_dir,
+                dependencies=self.dependencies,
+                settings=self.sagemaker_session.settings,
+            )
+
+        if repack and self.model_data is not None and self.entry_point is not None:
+            if isinstance(self.model_data, sagemaker.workflow.properties.Properties):
+                # model is not yet there, defer repacking to later during pipeline execution
+                return
+
+            bucket = self.bucket or self.sagemaker_session.default_bucket()
+            repacked_model_data = "s3://" + "/".join([bucket, key_prefix, "model.tar.gz"])
+
+            utils.repack_model(
+                inference_script=self.entry_point,
+                source_directory=self.source_dir,
+                dependencies=self.dependencies,
+                model_uri=self.model_data,
+                repacked_model_uri=repacked_model_data,
+                sagemaker_session=self.sagemaker_session,
+                kms_key=self.model_kms_key,
+            )
+
+            self.repacked_model_data = repacked_model_data
+            self.uploaded_code = fw_utils.UploadedCode(
+                s3_prefix=self.repacked_model_data, script_name=os.path.basename(self.entry_point)
+            )
+
+    def _framework_env_vars(self):
+        """Placeholder docstring"""
+        script_name = None
+        dir_name = None
+        if self.uploaded_code:
+            script_name = self.uploaded_code.script_name
+            if self.enable_network_isolation():
+                dir_name = "/opt/ml/model/code"
+            else:
+                dir_name = self.uploaded_code.s3_prefix
+        elif self.entry_point is not None:
+            script_name = self.entry_point
+            if self.source_dir is not None:
+                dir_name = "file://" + self.source_dir
+
+        return {
+            SCRIPT_PARAM_NAME.upper(): script_name or str(),
+            DIR_PARAM_NAME.upper(): dir_name or str(),
+            CONTAINER_LOG_LEVEL_PARAM_NAME.upper(): str(self.container_log_level),
+            SAGEMAKER_REGION_PARAM_NAME.upper(): self.sagemaker_session.boto_region_name,
+        }
 
 
 class ModelPackage(Model):

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-19 12:55:15[0m
[92mHash: 167b72384249558d7b11301d0ad2ac5acc663e20[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: script mode for model class (#2841)

[0m
@@ -11,9 +11,11 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
+import unittest.mock
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
 from sagemaker.lineage.action import Action
+from sagemaker.lineage.lineage_trial_component import LineageTrialComponent
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -32,6 +34,38 @@ def test_lineage_query(sagemaker_session):
         start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
     )
 
+    assert len(response.edges) == 1
+    assert response.edges[0].source_arn == "arn1"
+    assert response.edges[0].destination_arn == "arn2"
+    assert response.edges[0].association_type == "Produced"
+    assert len(response.vertices) == 2
+
+    assert response.vertices[0].arn == "arn1"
+    assert response.vertices[0].lineage_source == "Endpoint"
+    assert response.vertices[0].lineage_entity == "Artifact"
+    assert response.vertices[1].arn == "arn2"
+    assert response.vertices[1].lineage_source == "Model"
+    assert response.vertices[1].lineage_entity == "Context"
+
+
+def test_lineage_query_duplication(sagemaker_session):
+    lineage_query = LineageQuery(sagemaker_session)
+    sagemaker_session.sagemaker_client.query_lineage.return_value = {
+        "Vertices": [
+            {"Arn": "arn1", "Type": "Endpoint", "LineageType": "Artifact"},
+            {"Arn": "arn1", "Type": "Endpoint", "LineageType": "Artifact"},
+            {"Arn": "arn2", "Type": "Model", "LineageType": "Context"},
+        ],
+        "Edges": [
+            {"SourceArn": "arn1", "DestinationArn": "arn2", "AssociationType": "Produced"},
+            {"SourceArn": "arn1", "DestinationArn": "arn2", "AssociationType": "Produced"},
+        ],
+    }
+
+    response = lineage_query.query(
+        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
+    )
+
     assert len(response.edges) == 1
     assert response.edges[0].source_arn == "arn1"
     assert response.edges[0].destination_arn == "arn2"
@@ -254,6 +288,49 @@ def test_vertex_to_object_context(sagemaker_session):
     assert isinstance(context, Context)
 
 
+def test_vertex_to_object_trial_component(sagemaker_session):
+
+    tc_arn = "arn:aws:sagemaker:us-west-2:963951943925:trial-component/abaloneprocess-ixyt08z3ru-aws-processing-job"
+    vertex = Vertex(
+        arn=tc_arn,
+        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
+        lineage_source=LineageSourceEnum.TRANSFORM_JOB.value,
+        sagemaker_session=sagemaker_session,
+    )
+
+    sagemaker_session.sagemaker_client.describe_trial_component.return_value = {
+        "TrialComponentName": "MyTrialComponent",
+        "TrialComponentArn": tc_arn,
+        "Source": {
+            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/my_trial_component",
+            "SourceType": "ARN",
+            "SourceId": "Thu Dec 17 17:16:24 UTC 2020",
+        },
+        "TrialComponentType": "ModelDeployment",
+        "Properties": {
+            "PipelineExecutionArn": "arn:aws:sagemaker:us-west-2:0123456789012:\
+                pipeline/mypipeline/execution/0irnteql64d0",
+            "PipelineStepName": "MyStep",
+            "Status": "Completed",
+        },
+        "CreationTime": 1608225384.0,
+        "CreatedBy": {},
+        "LastModifiedTime": 1608225384.0,
+        "LastModifiedBy": {},
+    }
+
+    trial_component = vertex.to_lineage_object()
+
+    expected_calls = [
+        unittest.mock.call(TrialComponentName="abaloneprocess-ixyt08z3ru-aws-processing-job"),
+    ]
+    assert expected_calls == sagemaker_session.sagemaker_client.describe_trial_component.mock_calls
+
+    assert trial_component.trial_component_arn == tc_arn
+    assert trial_component.trial_component_name == "MyTrialComponent"
+    assert isinstance(trial_component, LineageTrialComponent)
+
+
 def test_vertex_to_object_model_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -285,6 +362,37 @@ def test_vertex_to_object_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
+def test_vertex_to_object_artifact(sagemaker_session):
+    vertex = Vertex(
+        arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
+        lineage_entity=LineageEntityEnum.ARTIFACT.value,
+        lineage_source=LineageSourceEnum.MODEL.value,
+        sagemaker_session=sagemaker_session,
+    )
+
+    sagemaker_session.sagemaker_client.describe_artifact.return_value = {
+        "ArtifactArn": "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
+        "Source": {
+            "SourceUri": "arn:aws:sagemaker:us-west-2:0123456789012:model/mymodel",
+            "SourceTypes": [],
+        },
+        "ArtifactType": None,
+        "Properties": {},
+        "CreationTime": 1608224704.149,
+        "CreatedBy": {},
+        "LastModifiedTime": 1608224704.149,
+        "LastModifiedBy": {},
+    }
+
+    artifact = vertex.to_lineage_object()
+
+    assert (
+        artifact.artifact_arn
+        == "arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m"
+    )
+    assert isinstance(artifact, Artifact)
+
+
 def test_vertex_to_dataset_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
@@ -347,7 +455,7 @@ def test_vertex_to_model_artifact(sagemaker_session):
     assert isinstance(artifact, ModelArtifact)
 
 
-def test_vertex_to_object_artifact(sagemaker_session):
+def test_vertex_to_object_image_artifact(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
         lineage_entity=LineageEntityEnum.ARTIFACT.value,
@@ -409,7 +517,7 @@ def test_vertex_to_object_action(sagemaker_session):
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93m[93m[93m[93m[93m[93m[93me66eef7f19c05e75284089183491bd4f[0m[0m[0m[0m[0m[0m[0m",
-        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
+        lineage_entity=LineageEntityEnum.TRIAL.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-19 12:55:15[0m
[92mHash: 167b72384249558d7b11301d0ad2ac5acc663e20[0m
[92mFilepath: tests/unit/sagemaker/model/test_model.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: script mode for model class (#2841)

[0m
@@ -11,13 +11,12 @@
 # ANY KIND, either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 from __future__ import absolute_import
-from unittest.mock import MagicMock
 
 import pytest
 from mock import Mock, patch
 
 import sagemaker
-from sagemaker.model import FrameworkModel, Model
+from sagemaker.model import Model
 
 MODEL_DATA = "s3://bucket/model.tar.gz"
 MODEL_IMAGE = "mi"
@@ -28,39 +27,10 @@ INSTANCE_COUNT = 2
 INSTANCE_TYPE = "ml.c4.4xlarge"
 ROLE = "some-role"
 
-REGION = "us-west-2"
-BUCKET_NAME = "some-bucket-name"
-GIT_REPO = "https://github.com/aws/sagemaker-python-sdk.git"
-BRANCH = "test-branch-git-config"
-COMMIT = "[93mae15c9d7d5b97ea95ea451e4662ee43da3401d73[0m"
-ENTRY_POINT_INFERENCE = "inference.py"
 
-SCRIPT_URI = "s3://codebucket/someprefix/sourcedir.tar.gz"
-IMAGE_URI = "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.9.0-gpu-py38"
-
-
-class DummyFrameworkModel(FrameworkModel):
-    def __init__(self, **kwargs):
-        super(DummyFrameworkModel, self).__init__(
-            **kwargs,
-        )
-
-
-@pytest.fixture()
+@pytest.fixture
 def sagemaker_session():
-    boto_mock = Mock(name="boto_session", region_name=REGION)
-    sms = MagicMock(
-        name="sagemaker_session",
-        boto_session=boto_mock,
-        boto_region_name=REGION,
-        config=None,
-        local_mode=False,
-        s3_client=None,
-        s3_resource=None,
-    )
-    sms.default_bucket = Mock(name="default_bucket", return_value=BUCKET_NAME)
-
-    return sms
+    return Mock()
 
 
 def test_prepare_container_def_with_model_data():
@@ -375,75 +345,3 @@ def test_delete_model_no_name(sagemaker_session):
     ):
         model.delete_model()
     sagemaker_session.delete_model.assert_not_called()
-
-
-@patch("time.strftime", MagicMock(return_value=TIMESTAMP))
-@patch("sagemaker.utils.repack_model")
-def test_script_mode_model_same_calls_as_framework(repack_model, sagemaker_session):
-    t = Model(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert len(sagemaker_session.create_model.call_args_list) == 1
-    assert len(sagemaker_session.endpoint_from_production_variants.call_args_list) == 1
-    assert len(repack_model.call_args_list) == 1
-
-    generic_model_create_model_args = sagemaker_session.create_model.call_args_list
-    generic_model_endpoint_from_production_variants_args = (
-        sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    generic_model_repack_model_args = repack_model.call_args_list
-
-    sagemaker_session.create_model.reset_mock()
-    sagemaker_session.endpoint_from_production_variants.reset_mock()
-    repack_model.reset_mock()
-
-    t = DummyFrameworkModel(
-        entry_point=ENTRY_POINT_INFERENCE,
-        role=ROLE,
-        sagemaker_session=sagemaker_session,
-        source_dir=SCRIPT_URI,
-        image_uri=IMAGE_URI,
-        model_data=MODEL_DATA,
-    )
-    t.deploy(instance_type=INSTANCE_TYPE, initial_instance_count=INSTANCE_COUNT)
-
-    assert generic_model_create_model_args == sagemaker_session.create_model.call_args_list
-    assert (
-        generic_model_endpoint_from_production_variants_args
-        == sagemaker_session.endpoint_from_production_variants.call_args_list
-    )
-    assert generic_model_repack_model_args == repack_model.call_args_list
-
-
-@patch("sagemaker.git_utils.git_clone_repo")
-@patch("sagemaker.model.fw_utils.tar_and_upload_dir")
-def test_git_support_succeed_model_class(tar_and_upload_dir, git_clone_repo, sagemaker_session):
-    git_clone_repo.side_effect = lambda gitconfig, entrypoint, sourcedir, dependency: {
-        "entry_point": "entry_point",
-        "source_dir": "/tmp/repo_dir/source_dir",
-        "dependencies": ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"],
-    }
-    entry_point = "entry_point"
-    source_dir = "source_dir"
-    dependencies = ["foo", "bar"]
-    git_config = {"repo": GIT_REPO, "branch": BRANCH, "commit": COMMIT}
-    model = Model(
-        sagemaker_session=sagemaker_session,
-        entry_point=entry_point,
-        source_dir=source_dir,
-        dependencies=dependencies,
-        git_config=git_config,
-        image_uri=IMAGE_URI,
-    )
-    model.prepare_container_def(instance_type=INSTANCE_TYPE)
-    git_clone_repo.assert_called_with(git_config, entry_point, source_dir, dependencies)
-    assert model.entry_point == "entry_point"
-    assert model.source_dir == "/tmp/repo_dir/source_dir"
-    assert model.dependencies == ["/tmp/repo_dir/foo", "/tmp/repo_dir/bar"]

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-11 13:36:15[0m
[92mHash: 20740cb609b10060947be969febaf736cdaa8958[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: change: update master from dev (#2836)

Co-authored-by: Basil Beirouti <beirb@amazon.com>
Co-authored-by: Payton Staub <pstaub@amazon.com>
Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>
Co-authored-by: Basil Beirouti <BasilBeirouti@gmail.com>
Co-authored-by: Payton Staub <staubhpa@gmail.com>
Co-authored-by: Shreya Pandit <shreya.pandit25@gmail.com>
Co-authored-by: Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>
Co-authored-by: ci <ci>
Co-authored-by: Jeniya Tabassum <jeniya.tabassum@gmail.com>
Co-authored-by: sreedes <70613743+sreedes@users.noreply.github.com>
Co-authored-by: Navin Soni <navinns@amazon.com>
Co-authored-by: Miyoung <myoung8739@gmail.com>
Co-authored-by: Ameen Khan <ameenmk@amazon.com>
Co-authored-by: Zhankui Lu <zhankuil@amazon.com>
Co-authored-by: Xiaoguang Chen <68292680+xgchena@users.noreply.github.com>
Co-authored-by: Jonathan Guinegagne <12092593+JGuinegagne@users.noreply.github.com>
Co-authored-by: Zhankui Lu <zhankuilv@gmail.com>
Co-authored-by: Yifei Zhu <66866419+yzhu0@users.noreply.github.com>
Co-authored-by: Qingzi-Lan <83724147+Qingzi-Lan@users.noreply.github.com>[0m
@@ -13,7 +13,6 @@
 from __future__ import absolute_import
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
-from sagemaker.lineage.action import Action
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -45,143 +44,6 @@ def test_lineage_query(sagemaker_session):
     assert response.vertices[1].lineage_entity == "Context"
 
 
-def test_lineage_query_cross_account_same_artifact(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-        ],
-        "Edges": [
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-        ],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-    assert len(response.edges) == 0
-    assert len(response.vertices) == 1
-    assert (
-        response.vertices[0].arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-
-
-def test_lineage_query_cross_account(sagemaker_session):
-    lineage_query = LineageQuery(sagemaker_session)
-    sagemaker_session.sagemaker_client.query_lineage.return_value = {
-        "Vertices": [
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-            {
-                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
-                "Type": "Endpoint",
-                "LineageType": "Artifact",
-            },
-        ],
-        "Edges": [
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "SAME_AS",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "AssociationType": "ABC",
-            },
-            {
-                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
-                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
-                "AssociationType": "DEF",
-            },
-        ],
-    }
-
-    response = lineage_query.query(
-        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
-    )
-
-    assert len(response.edges) == 2
-    assert (
-        response.edges[0].source_arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert (
-        response.edges[0].destination_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.edges[0].association_type == "ABC"
-
-    assert (
-        response.edges[1].source_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert (
-        response.edges[1].destination_arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
-    )
-    assert response.edges[1].association_type == "DEF"
-
-    assert len(response.vertices) == 3
-    assert (
-        response.vertices[0].arn
-        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[0].lineage_source == "Endpoint"
-    assert response.vertices[0].lineage_entity == "Artifact"
-    assert (
-        response.vertices[1].arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
-    )
-    assert response.vertices[1].lineage_source == "Endpoint"
-    assert response.vertices[1].lineage_entity == "Artifact"
-    assert (
-        response.vertices[2].arn
-        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
-    )
-    assert response.vertices[2].lineage_source == "Endpoint"
-    assert response.vertices[2].lineage_entity == "Artifact"
-
-
 def test_vertex_to_object_endpoint_context(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
@@ -378,38 +240,10 @@ def test_vertex_to_object_artifact(sagemaker_session):
     assert isinstance(artifact, Artifact)
 
 
-def test_vertex_to_object_action(sagemaker_session):
-    vertex = Vertex(
-        arn="arn:aws:sagemaker:us-west-2:0123456789012:action/cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
-        lineage_entity=LineageEntityEnum.ACTION.value,
-        lineage_source="A",
-        sagemaker_session=sagemaker_session,
-    )
-
-    sagemaker_session.sagemaker_client.describe_action.return_value = {
-        "ActionName": "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
-        "Source": {
-            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
-            "SourceTypes": [],
-        },
-        "ActionType": "A",
-        "Properties": {},
-        "CreationTime": 1608224704.149,
-        "CreatedBy": {},
-        "LastModifiedTime": 1608224704.149,
-        "LastModifiedBy": {},
-    }
-
-    action = vertex.to_lineage_object()
-
-    assert action.action_name == "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint"
-    assert isinstance(action, Action)
-
-
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93me66eef7f19c05e75284089183491bd4f[0m",
-        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
+        lineage_entity=LineageEntityEnum.ACTION.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-11 13:36:15[0m
[92mHash: b689689f89a2001011ab33f0c55393f9f5fd8f95[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_steps.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: fix: Set ProcessingStep upload locations deterministically to avoid c… (#2790)

[0m
@@ -16,7 +16,6 @@ from __future__ import absolute_import
 import pytest
 import sagemaker
 import os
-import warnings
 
 from mock import (
     Mock,
@@ -64,7 +63,8 @@ from sagemaker.workflow.steps import (
 )
 from tests.unit import DATA_DIR
 
-DUMMY_SCRIPT_PATH = os.path.join(DATA_DIR, "dummy_script.py")
+SCRIPT_FILE = "dummy_script.py"
+SCRIPT_PATH = os.path.join(DATA_DIR, SCRIPT_FILE)
 
 REGION = "us-west-2"
 BUCKET = "my-bucket"
@@ -129,31 +129,6 @@ def sagemaker_session(boto_session, client):
     )
 
 
-@pytest.fixture
-def script_processor(sagemaker_session):
-    return ScriptProcessor(
-        role=ROLE,
-        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
-        command=["python3"],
-        instance_type="ml.m4.xlarge",
-        instance_count=1,
-        volume_size_in_gb=100,
-        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
-        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
-        max_runtime_in_seconds=3600,
-        base_job_name="my_sklearn_processor",
-        env={"my_env_variable": "my_env_variable_value"},
-        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
-        network_config=NetworkConfig(
-            subnets=["my_subnet_id"],
-            security_group_ids=["my_security_group_id"],
-            enable_network_isolation=True,
-            encrypt_inter_container_traffic=True,
-        ),
-        sagemaker_session=sagemaker_session,
-    )
-
-
 def test_custom_step():
     step = CustomStep(
         name="MyStep", display_name="CustomStepDisplayName", description="CustomStepDescription"
@@ -351,7 +326,7 @@ def test_training_step_tensorflow(sagemaker_session):
     training_epochs_parameter = ParameterInteger(name="TrainingEpochs", default_value=5)
     training_batch_size_parameter = ParameterInteger(name="TrainingBatchSize", default_value=500)
     estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
+        entry_point=os.path.join(DATA_DIR, SCRIPT_FILE),
         role=ROLE,
         model_dir=False,
         image_uri=IMAGE_URI,
@@ -428,75 +403,6 @@ def test_training_step_tensorflow(sagemaker_session):
     assert step.properties.TrainingJobName.expr == {"Get": "Steps.MyTrainingStep.TrainingJobName"}
 
 
-def test_training_step_profiler_warning(sagemaker_session):
-    estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
-        role=ROLE,
-        model_dir=False,
-        image_uri=IMAGE_URI,
-        source_dir="s3://mybucket/source",
-        framework_version="2.4.1",
-        py_version="py37",
-        disable_profiler=False,
-        instance_count=1,
-        instance_type="ml.p3.16xlarge",
-        sagemaker_session=sagemaker_session,
-        hyperparameters={
-            "batch-size": 500,
-            "epochs": 5,
-        },
-        debugger_hook_config=False,
-        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
-    )
-
-    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    with warnings.catch_warnings(record=True) as w:
-        TrainingStep(
-            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
-        )
-        assert len(w) == 1
-        assert issubclass(w[-1].category, UserWarning)
-        assert "Profiling is enabled on the provided estimator" in str(w[-1].message)
-
-
-def test_training_step_no_profiler_warning(sagemaker_session):
-    estimator = TensorFlow(
-        entry_point=DUMMY_SCRIPT_PATH,
-        role=ROLE,
-        model_dir=False,
-        image_uri=IMAGE_URI,
-        source_dir="s3://mybucket/source",
-        framework_version="2.4.1",
-        py_version="py37",
-        disable_profiler=True,
-        instance_count=1,
-        instance_type="ml.p3.16xlarge",
-        sagemaker_session=sagemaker_session,
-        hyperparameters={
-            "batch-size": 500,
-            "epochs": 5,
-        },
-        debugger_hook_config=False,
-        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
-    )
-
-    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    with warnings.catch_warnings(record=True) as w:
-        # profiler disabled, cache config not None
-        TrainingStep(
-            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
-        )
-        assert len(w) == 0
-
-    with warnings.catch_warnings(record=True) as w:
-        # profiler enabled, cache config is None
-        estimator.disable_profiler = False
-        TrainingStep(name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=None)
-        assert len(w) == 0
-
-
 def test_processing_step(sagemaker_session):
     processing_input_data_uri_parameter = ParameterString(
         name="ProcessingInputDataUri", default_value=f"s3://{BUCKET}/processing_manifest"
@@ -567,42 +473,28 @@ def test_processing_step(sagemaker_session):
 
 
 @patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_local_code(mock_normalize_args, script_processor):
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    inputs = [
-        ProcessingInput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    outputs = [
-        ProcessingOutput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    step = ProcessingStep(
-        name="MyProcessingStep",
-        processor=script_processor,
-        code=DUMMY_SCRIPT_PATH,
-        inputs=inputs,
-        outputs=outputs,
-        job_arguments=["arg1", "arg2"],
-        cache_config=cache_config,
-    )
-    mock_normalize_args.return_value = [step.inputs, step.outputs]
-    step.to_request()
-    mock_normalize_args.assert_called_with(
-        job_name="MyProcessingStep-[93m3e89f0c7e101c356cbedf27d9d27e9db[0m",
-        arguments=step.job_arguments,
-        inputs=step.inputs,
-        outputs=step.outputs,
-        code=step.code,
+def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session):
+    processor = ScriptProcessor(
+        role=ROLE,
+        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
+        command=["python3"],
+        instance_type="ml.m4.xlarge",
+        instance_count=1,
+        volume_size_in_gb=100,
+        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
+        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
+        max_runtime_in_seconds=3600,
+        base_job_name="my_sklearn_processor",
+        env={"my_env_variable": "my_env_variable_value"},
+        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
+        network_config=NetworkConfig(
+            subnets=["my_subnet_id"],
+            security_group_ids=["my_security_group_id"],
+            enable_network_isolation=True,
+            encrypt_inter_container_traffic=True,
+        ),
+        sagemaker_session=sagemaker_session,
     )
-
-
-@patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, script_processor):
     cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
     inputs = [
         ProcessingInput(
@@ -618,8 +510,8 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     ]
     step = ProcessingStep(
         name="MyProcessingStep",
-        processor=script_processor,
-        code="s3://foo",
+        processor=processor,
+        code="foo.py",
         inputs=inputs,
         outputs=outputs,
         job_arguments=["arg1", "arg2"],
@@ -628,7 +520,6 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     mock_normalize_args.return_value = [step.inputs, step.outputs]
     step.to_request()
     mock_normalize_args.assert_called_with(
-        job_name=None,
         arguments=step.job_arguments,
         inputs=step.inputs,
         outputs=step.outputs,
@@ -636,40 +527,6 @@ def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, scrip
     )
 
 
-@patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args_with_no_code(mock_normalize_args, script_processor):
-    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
-    inputs = [
-        ProcessingInput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    outputs = [
-        ProcessingOutput(
-            source=f"s3://{BUCKET}/processing_manifest",
-            destination="processing_manifest",
-        )
-    ]
-    step = ProcessingStep(
-        name="MyProcessingStep",
-        processor=script_processor,
-        inputs=inputs,
-        outputs=outputs,
-        job_arguments=["arg1", "arg2"],
-        cache_config=cache_config,
-    )
-    mock_normalize_args.return_value = [step.inputs, step.outputs]
-    step.to_request()
-    mock_normalize_args.assert_called_with(
-        job_name=None,
-        arguments=step.job_arguments,
-        inputs=step.inputs,
-        outputs=step.outputs,
-        code=None,
-    )
-
-
 def test_create_model_step(sagemaker_session):
     model = Model(
         image_uri=IMAGE_URI,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-11 13:36:15[0m
[92mHash: 1da6b98a9a15df8f8b177e158ff89de8932f542e[0m
[92mFilepath: tests/unit/sagemaker/lineage/test_query.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: allow conditional parellel builds (#2727)

[0m
@@ -13,6 +13,7 @@
 from __future__ import absolute_import
 from sagemaker.lineage.artifact import DatasetArtifact, ModelArtifact, Artifact
 from sagemaker.lineage.context import EndpointContext, Context
+from sagemaker.lineage.action import Action
 from sagemaker.lineage.query import LineageEntityEnum, LineageSourceEnum, Vertex, LineageQuery
 import pytest
 
@@ -44,6 +45,143 @@ def test_lineage_query(sagemaker_session):
     assert response.vertices[1].lineage_entity == "Context"
 
 
+def test_lineage_query_cross_account_same_artifact(sagemaker_session):
+    lineage_query = LineageQuery(sagemaker_session)
+    sagemaker_session.sagemaker_client.query_lineage.return_value = {
+        "Vertices": [
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+        ],
+        "Edges": [
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "AssociationType": "SAME_AS",
+            },
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "AssociationType": "SAME_AS",
+            },
+        ],
+    }
+
+    response = lineage_query.query(
+        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
+    )
+    assert len(response.edges) == 0
+    assert len(response.vertices) == 1
+    assert (
+        response.vertices[0].arn
+        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
+    )
+    assert response.vertices[0].lineage_source == "Endpoint"
+    assert response.vertices[0].lineage_entity == "Artifact"
+
+
+def test_lineage_query_cross_account(sagemaker_session):
+    lineage_query = LineageQuery(sagemaker_session)
+    sagemaker_session.sagemaker_client.query_lineage.return_value = {
+        "Vertices": [
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+            {
+                "Arn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
+                "Type": "Endpoint",
+                "LineageType": "Artifact",
+            },
+        ],
+        "Edges": [
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "AssociationType": "SAME_AS",
+            },
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "AssociationType": "SAME_AS",
+            },
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678902:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
+                "AssociationType": "ABC",
+            },
+            {
+                "SourceArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m",
+                "DestinationArn": "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh",
+                "AssociationType": "DEF",
+            },
+        ],
+    }
+
+    response = lineage_query.query(
+        start_arns=["arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext"]
+    )
+
+    assert len(response.edges) == 2
+    assert (
+        response.edges[0].source_arn
+        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
+    )
+    assert (
+        response.edges[0].destination_arn
+        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
+    )
+    assert response.edges[0].association_type == "ABC"
+
+    assert (
+        response.edges[1].source_arn
+        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
+    )
+    assert (
+        response.edges[1].destination_arn
+        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
+    )
+    assert response.edges[1].association_type == "DEF"
+
+    assert len(response.vertices) == 3
+    assert (
+        response.vertices[0].arn
+        == "arn:aws:sagemaker:us-east-2:012345678901:artifact/[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9d2a0[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m[0m"
+    )
+    assert response.vertices[0].lineage_source == "Endpoint"
+    assert response.vertices[0].lineage_entity == "Artifact"
+    assert (
+        response.vertices[1].arn
+        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93m[93m[93me1f29799189751939405b0f2b5b9abcd[0m[0m[0m[0m[0m[0m"
+    )
+    assert response.vertices[1].lineage_source == "Endpoint"
+    assert response.vertices[1].lineage_entity == "Artifact"
+    assert (
+        response.vertices[2].arn
+        == "arn:aws:sagemaker:us-east-2:012345678903:artifact/[93m[93m[93m[93me1f29799189751939405b0f2b5b9ef[0m[0m[0m[0mgh"
+    )
+    assert response.vertices[2].lineage_source == "Endpoint"
+    assert response.vertices[2].lineage_entity == "Artifact"
+
+
 def test_vertex_to_object_endpoint_context(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:context/mycontext",
@@ -240,10 +378,38 @@ def test_vertex_to_object_artifact(sagemaker_session):
     assert isinstance(artifact, Artifact)
 
 
+def test_vertex_to_object_action(sagemaker_session):
+    vertex = Vertex(
+        arn="arn:aws:sagemaker:us-west-2:0123456789012:action/cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
+        lineage_entity=LineageEntityEnum.ACTION.value,
+        lineage_source="A",
+        sagemaker_session=sagemaker_session,
+    )
+
+    sagemaker_session.sagemaker_client.describe_action.return_value = {
+        "ActionName": "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint",
+        "Source": {
+            "SourceUri": "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
+            "SourceTypes": [],
+        },
+        "ActionType": "A",
+        "Properties": {},
+        "CreationTime": 1608224704.149,
+        "CreatedBy": {},
+        "LastModifiedTime": 1608224704.149,
+        "LastModifiedBy": {},
+    }
+
+    action = vertex.to_lineage_object()
+
+    assert action.action_name == "cp-m5-20210424t041405868z-1619237657-1-aws-endpoint"
+    assert isinstance(action, Action)
+
+
 def test_vertex_to_object_unconvertable(sagemaker_session):
     vertex = Vertex(
         arn="arn:aws:sagemaker:us-west-2:0123456789012:artifact/[93me66eef7f19c05e75284089183491bd4f[0m",
-        lineage_entity=LineageEntityEnum.ACTION.value,
+        lineage_entity=LineageEntityEnum.TRIAL_COMPONENT.value,
         lineage_source=LineageSourceEnum.TENSORBOARD.value,
         sagemaker_session=sagemaker_session,
     )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-01-11 13:36:15[0m
[92mHash: 1da6b98a9a15df8f8b177e158ff89de8932f542e[0m
[92mFilepath: tests/unit/sagemaker/workflow/test_steps.py[0m
[92mBranch: origin/master-jumpstart[0m
[92mCommit: feature: allow conditional parellel builds (#2727)

[0m
@@ -16,6 +16,7 @@ from __future__ import absolute_import
 import pytest
 import sagemaker
 import os
+import warnings
 
 from mock import (
     Mock,
@@ -63,8 +64,7 @@ from sagemaker.workflow.steps import (
 )
 from tests.unit import DATA_DIR
 
-SCRIPT_FILE = "dummy_script.py"
-SCRIPT_PATH = os.path.join(DATA_DIR, SCRIPT_FILE)
+DUMMY_SCRIPT_PATH = os.path.join(DATA_DIR, "dummy_script.py")
 
 REGION = "us-west-2"
 BUCKET = "my-bucket"
@@ -129,6 +129,31 @@ def sagemaker_session(boto_session, client):
     )
 
 
+@pytest.fixture
+def script_processor(sagemaker_session):
+    return ScriptProcessor(
+        role=ROLE,
+        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
+        command=["python3"],
+        instance_type="ml.m4.xlarge",
+        instance_count=1,
+        volume_size_in_gb=100,
+        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
+        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
+        max_runtime_in_seconds=3600,
+        base_job_name="my_sklearn_processor",
+        env={"my_env_variable": "my_env_variable_value"},
+        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
+        network_config=NetworkConfig(
+            subnets=["my_subnet_id"],
+            security_group_ids=["my_security_group_id"],
+            enable_network_isolation=True,
+            encrypt_inter_container_traffic=True,
+        ),
+        sagemaker_session=sagemaker_session,
+    )
+
+
 def test_custom_step():
     step = CustomStep(
         name="MyStep", display_name="CustomStepDisplayName", description="CustomStepDescription"
@@ -326,7 +351,7 @@ def test_training_step_tensorflow(sagemaker_session):
     training_epochs_parameter = ParameterInteger(name="TrainingEpochs", default_value=5)
     training_batch_size_parameter = ParameterInteger(name="TrainingBatchSize", default_value=500)
     estimator = TensorFlow(
-        entry_point=os.path.join(DATA_DIR, SCRIPT_FILE),
+        entry_point=DUMMY_SCRIPT_PATH,
         role=ROLE,
         model_dir=False,
         image_uri=IMAGE_URI,
@@ -403,6 +428,75 @@ def test_training_step_tensorflow(sagemaker_session):
     assert step.properties.TrainingJobName.expr == {"Get": "Steps.MyTrainingStep.TrainingJobName"}
 
 
+def test_training_step_profiler_warning(sagemaker_session):
+    estimator = TensorFlow(
+        entry_point=DUMMY_SCRIPT_PATH,
+        role=ROLE,
+        model_dir=False,
+        image_uri=IMAGE_URI,
+        source_dir="s3://mybucket/source",
+        framework_version="2.4.1",
+        py_version="py37",
+        disable_profiler=False,
+        instance_count=1,
+        instance_type="ml.p3.16xlarge",
+        sagemaker_session=sagemaker_session,
+        hyperparameters={
+            "batch-size": 500,
+            "epochs": 5,
+        },
+        debugger_hook_config=False,
+        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
+    )
+
+    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
+    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
+    with warnings.catch_warnings(record=True) as w:
+        TrainingStep(
+            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
+        )
+        assert len(w) == 1
+        assert issubclass(w[-1].category, UserWarning)
+        assert "Profiling is enabled on the provided estimator" in str(w[-1].message)
+
+
+def test_training_step_no_profiler_warning(sagemaker_session):
+    estimator = TensorFlow(
+        entry_point=DUMMY_SCRIPT_PATH,
+        role=ROLE,
+        model_dir=False,
+        image_uri=IMAGE_URI,
+        source_dir="s3://mybucket/source",
+        framework_version="2.4.1",
+        py_version="py37",
+        disable_profiler=True,
+        instance_count=1,
+        instance_type="ml.p3.16xlarge",
+        sagemaker_session=sagemaker_session,
+        hyperparameters={
+            "batch-size": 500,
+            "epochs": 5,
+        },
+        debugger_hook_config=False,
+        distribution={"smdistributed": {"dataparallel": {"enabled": True}}},
+    )
+
+    inputs = TrainingInput(s3_data=f"s3://{BUCKET}/train_manifest")
+    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
+    with warnings.catch_warnings(record=True) as w:
+        # profiler disabled, cache config not None
+        TrainingStep(
+            name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=cache_config
+        )
+        assert len(w) == 0
+
+    with warnings.catch_warnings(record=True) as w:
+        # profiler enabled, cache config is None
+        estimator.disable_profiler = False
+        TrainingStep(name="MyTrainingStep", estimator=estimator, inputs=inputs, cache_config=None)
+        assert len(w) == 0
+
+
 def test_processing_step(sagemaker_session):
     processing_input_data_uri_parameter = ParameterString(
         name="ProcessingInputDataUri", default_value=f"s3://{BUCKET}/processing_manifest"
@@ -473,28 +567,42 @@ def test_processing_step(sagemaker_session):
 
 
 @patch("sagemaker.processing.ScriptProcessor._normalize_args")
-def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session):
-    processor = ScriptProcessor(
-        role=ROLE,
-        image_uri="012345678901.dkr.ecr.us-west-2.amazonaws.com/my-custom-image-uri",
-        command=["python3"],
-        instance_type="ml.m4.xlarge",
-        instance_count=1,
-        volume_size_in_gb=100,
-        volume_kms_key="arn:aws:kms:us-west-2:012345678901:key/volume-kms-key",
-        output_kms_key="arn:aws:kms:us-west-2:012345678901:key/output-kms-key",
-        max_runtime_in_seconds=3600,
-        base_job_name="my_sklearn_processor",
-        env={"my_env_variable": "my_env_variable_value"},
-        tags=[{"Key": "my-tag", "Value": "my-tag-value"}],
-        network_config=NetworkConfig(
-            subnets=["my_subnet_id"],
-            security_group_ids=["my_security_group_id"],
-            enable_network_isolation=True,
-            encrypt_inter_container_traffic=True,
-        ),
-        sagemaker_session=sagemaker_session,
+def test_processing_step_normalizes_args_with_local_code(mock_normalize_args, script_processor):
+    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
+    inputs = [
+        ProcessingInput(
+            source=f"s3://{BUCKET}/processing_manifest",
+            destination="processing_manifest",
+        )
+    ]
+    outputs = [
+        ProcessingOutput(
+            source=f"s3://{BUCKET}/processing_manifest",
+            destination="processing_manifest",
+        )
+    ]
+    step = ProcessingStep(
+        name="MyProcessingStep",
+        processor=script_processor,
+        code=DUMMY_SCRIPT_PATH,
+        inputs=inputs,
+        outputs=outputs,
+        job_arguments=["arg1", "arg2"],
+        cache_config=cache_config,
     )
+    mock_normalize_args.return_value = [step.inputs, step.outputs]
+    step.to_request()
+    mock_normalize_args.assert_called_with(
+        job_name="MyProcessingStep-[93m3e89f0c7e101c356cbedf27d9d27e9db[0m",
+        arguments=step.job_arguments,
+        inputs=step.inputs,
+        outputs=step.outputs,
+        code=step.code,
+    )
+
+
+@patch("sagemaker.processing.ScriptProcessor._normalize_args")
+def test_processing_step_normalizes_args_with_s3_code(mock_normalize_args, script_processor):
     cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
     inputs = [
         ProcessingInput(
@@ -510,8 +618,8 @@ def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session)
     ]
     step = ProcessingStep(
         name="MyProcessingStep",
-        processor=processor,
-        code="foo.py",
+        processor=script_processor,
+        code="s3://foo",
         inputs=inputs,
         outputs=outputs,
         job_arguments=["arg1", "arg2"],
@@ -520,6 +628,7 @@ def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session)
     mock_normalize_args.return_value = [step.inputs, step.outputs]
     step.to_request()
     mock_normalize_args.assert_called_with(
+        job_name=None,
         arguments=step.job_arguments,
         inputs=step.inputs,
         outputs=step.outputs,
@@ -527,6 +636,40 @@ def test_processing_step_normalizes_args(mock_normalize_args, sagemaker_session)
     )
 
 
+@patch("sagemaker.processing.ScriptProcessor._normalize_args")
+def test_processing_step_normalizes_args_with_no_code(mock_normalize_args, script_processor):
+    cache_config = CacheConfig(enable_caching=True, expire_after="PT1H")
+    inputs = [
+        ProcessingInput(
+            source=f"s3://{BUCKET}/processing_manifest",
+            destination="processing_manifest",
+        )
+    ]
+    outputs = [
+        ProcessingOutput(
+            source=f"s3://{BUCKET}/processing_manifest",
+            destination="processing_manifest",
+        )
+    ]
+    step = ProcessingStep(
+        name="MyProcessingStep",
+        processor=script_processor,
+        inputs=inputs,
+        outputs=outputs,
+        job_arguments=["arg1", "arg2"],
+        cache_config=cache_config,
+    )
+    mock_normalize_args.return_value = [step.inputs, step.outputs]
+    step.to_request()
+    mock_normalize_args.assert_called_with(
+        job_name=None,
+        arguments=step.job_arguments,
+        inputs=step.inputs,
+        outputs=step.outputs,
+        code=None,
+    )
+
+
 def test_create_model_step(sagemaker_session):
     model = Model(
         image_uri=IMAGE_URI,

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2022-12-05 16:11:34[0m
[92mHash: b68bcd9344deba8e3bedf7ccb0adb31498735b13[0m
[92mFilepath: src/sagemaker/processing.py[0m
[92mBranch: origin/revert-3354-master[0m
[92mCommit: fix: support idempotency for framework and spark processors (#3460)

Co-authored-by: Brock Wade <bwayde@amazon.com>
Co-authored-by: Mufaddal Rohawala <89424143+mufaddal-rohawala@users.noreply.github.com>[0m
@@ -23,7 +23,6 @@ import pathlib
 import logging
 from textwrap import dedent
 from typing import Dict, List, Optional, Union
-from copy import copy
 
 import attr
 
@@ -1831,17 +1830,14 @@ class FrameworkProcessor(ScriptProcessor):
         #   [93ma7399455f5386d83ddc5cb15c0db00c04bd518ec[0m/src/sagemaker/processing.py#L425-L426
         if inputs is None:
             inputs = []
-
-        # make a shallow copy of user inputs
-        patched_inputs = copy(inputs)
-        patched_inputs.append(
+        inputs.append(
             ProcessingInput(
                 input_name="code",
                 source=s3_payload,
                 destination="/opt/ml/processing/input/code/",
             )
         )
-        return patched_inputs
+        return inputs
 
     def _set_entrypoint(self, command, user_script_name):
         """Framework processor override for setting processing job entrypoint.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: CONTRIBUTING.md[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,102 @@
+# Contributing Guidelines
+
+Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional
+documentation, we greatly value feedback and contributions from our community.
+
+Please read through this document before submitting any issues or pull requests to ensure we have all the necessary
+information to effectively respond to your bug report or contribution.
+
+
+## Reporting Bugs/Feature Requests
+
+We welcome you to use the GitHub issue tracker to report bugs or suggest features.
+
+When filing an issue, please check [existing open](https://github.com/aws/sagemaker-python-sdk/issues), or [recently closed](https://github.com/aws/sagemaker-python-sdk/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aclosed%20), issues to make sure somebody else hasn't already
+reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:
+
+* A reproducible test case or series of steps
+* The version of our code being used
+* Any modifications you've made relevant to the bug
+* A description of your environment or deployment
+
+
+## Contributing via Pull Requests
+Contributions via pull requests are much appreciated. 
+
+You can use the following commands to setup your developing and testing environment after you fork sagemaker-python-sdk repository:
+
+```bash
+git clone git@github.com:<your-github-username>/sagemaker-python-sdk.git
+cd sagemaker-python-sdk
+pip install -U .
+pip install -U .[test]
+```
+
+Before sending us a pull request, please ensure that:
+
+1. You are working against the latest source on the *master* branch.
+2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.
+3. You open an issue to discuss any significant work - we would hate for your time to be wasted.
+
+To send us a pull request, please:
+
+1. Fork the repository.
+2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.
+3. Include unit tests when you contribute new features or make bug fixes, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost.
+4. Ensure local tests pass.
+5. Use commit messages (and PR titles) that follow [these guidelines](#commit-message-guidelines).
+6. Send us a pull request, answering any default questions in the pull request interface.
+7. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.
+
+GitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and
+[creating a pull request](https://help.github.com/articles/creating-a-pull-request/).
+
+### Commit message guidelines
+
+We use commit messages to update the project version number and generate changelog entries, so it's important for them to follow the right format. Valid commit messages include a prefix, separated from the rest of the message by a colon and a space. Here are a few examples:
+
+```
+feature: support VPC config for hyperparameter tuning
+fix: fix flake8 errors
+documentation: add MXNet documentation
+```
+
+Valid prefixes are listed in the table below.
+
+| Prefix          | Use for...                                                                                     |
+|-----------------|------------------------------------------------------------------------------------------------|
+| `breaking`      | Incompatible API changes.                                                                      |
+| `deprecation`   | Deprecating an existing API or feature, or removing something that was previously deprecated.  |
+| `feature`       | Adding a new feature.                                                                          |
+| `fix`           | Bug fixes.                                                                                     |
+| `change`        | Any other code change.                                                                         |
+| `documentation` | Documentation changes.                                                                         |
+
+Some of the prefixes allow abbreviation -- `break`, `feat`, `depr`, and `doc` are all valid. If you omit a prefix, the commit will be treated as a `change`.
+
+For the rest of the message, use imperative style and keep things concise but informative. See [How to Write a Git Commit Message](https://chris.beams.io/posts/git-commit/) for guidance.
+
+### Integration tests
+
+Our CI system runs integration tests (the ones in the `tests/integ` directory) in parallel. If you are writing or modifying a test that creates a SageMaker job (training, tuner, or transform) or an endpoint, it's important to assign a concurrency-friendly `job_name` (or `endpoint_name`), or your tests may fail randomly due to name collisions. We have a helper method `sagemaker.utils.unique_name_from_base(base, max_length)` that makes test-friendly names. You can find examples of how to use it [here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tfs.py#L37) and 
+[here](https://github.com/aws/sagemaker-python-sdk/blob/[93m[93m3816a5658d3737c9767e01bc8d37fc3ed5551593[0m[0m/tests/integ/test_tuner.py#L616), or by searching for "unique\_name\_from\_base" in our test code.
+
+## Finding contributions to work on
+Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any ['help wanted'](https://github.com/aws/sagemaker-python-sdk/labels/help%20wanted) issues is a great place to start.
+
+
+## Code of Conduct
+This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).
+For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact
+opensource-codeofconduct@amazon.com with any additional questions or comments.
+
+
+## Security issue notifications
+If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.
+
+
+## Licensing
+
+See the [LICENSE](https://github.com/aws/sagemaker-python-sdk/blob/master/LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.
+
+We may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: doc/using_mxnet.rst[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,832 @@
+=========================================
+Using MXNet with the SageMaker Python SDK
+=========================================
+
+.. contents::
+
+With the SageMaker Python SDK, you can train and host MXNet models on Amazon SageMaker.
+
+Supported versions of MXNet: ``1.4.0``, ``1.3.0``, ``1.2.1``, ``1.1.0``, ``1.0.0``, ``0.12.1``.
+
+Supported versions of MXNet for Elastic Inference: ``1.4.0``, ``1.3.0``.
+
+Training with MXNet
+-------------------
+
+Training MXNet models using ``MXNet`` Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an ``MXNet`` Estimator. You should prepare your script in a separate source file than the notebook, terminal session, or source file you're using to submit the script to SageMaker via an ``MXNet`` Estimator.
+
+Suppose that you already have an MXNet training script called
+``mxnet-train.py``. You can run this script in SageMaker as follows:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+    mxnet_estimator = MXNet('mxnet-train.py',
+                            role='SageMakerRole',
+                            train_instance_type='ml.p3.2xlarge',
+                            train_instance_count=1,
+                            framework_version='1.3.0')
+    mxnet_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The constructor keyword arguments define how SageMaker runs your training script and are discussed, in detail, in a later section.
+
+In the following sections, we'll discuss how to prepare a training script for execution on SageMaker, then how to run that script on SageMaker using an ``MXNet`` Estimator.
+
+Preparing the MXNet training script
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+| WARNING                                                                                                                                                  |
++==========================================================================================================================================================+
+| The structure for training scripts changed starting at MXNet version 1.3.                                                                                |
+| Make sure you refer to the correct section of this README when you prepare your script.                                                                  |
+| For information on how to upgrade an old script to the new format, see `"Updating your MXNet training script" <#updating-your-mxnet-training-script>`__. |
++----------------------------------------------------------------------------------------------------------------------------------------------------------+
+
+For versions 1.3 and higher
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the path where the training job writes the model artifacts to.
+  After training, artifacts in this directory are uploaded to S3 for model hosting.
+* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the MXNet estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+* ``SM_HPS``: A json dump of the hyperparameters preserving json types (boolean, integer, etc.)
+
+For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``model_dir`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+    import json
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # an alternative way to load hyperparameters via SM_HPS environment variable.
+        parser.add_argument('--sm-hps', type=json.loads, default=os.environ['SM_HPS'])
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model,
+so that SageMaker does not inadvertently run your training code at the wrong point in execution.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your MXNet estimator.
+
+For more on training environment variables, please visit `SageMaker Containers <https://github.com/aws/sagemaker-containers>`_.
+
+For versions 1.2 and lower
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your MXNet training script must be a Python 2.7 or 3.5 compatible source file. The MXNet training script must contain a function ``train``, which SageMaker invokes to run training. You can include other functions as well, but it must contain a ``train`` function.
+
+When you run your script on SageMaker via the ``MXNet`` Estimator, SageMaker injects information about the training environment into your training function via Python keyword arguments. You can choose to take advantage of these by including them as keyword arguments in your train function. The full list of arguments is:
+
+-  ``hyperparameters (dict[string,string])``: The hyperparameters passed
+   to SageMaker TrainingJob that runs your MXNet training script. You
+   can use this to pass hyperparameters to your training script.
+-  ``input_data_config (dict[string,dict])``: The SageMaker TrainingJob
+   InputDataConfig object, that's set when the SageMaker TrainingJob is
+   created. This is discussed in more detail below.
+-  ``channel_input_dirs (dict[string,string])``: A collection of
+   directories containing training data. When you run training, you can
+   partition your training data into different logical "channels".
+   Depending on your problem, some common channel ideas are: "train",
+   "test", "evaluation" or "images',"labels".
+-  ``output_data_dir (str)``: A directory where your training script can
+   write data that will be moved to S3 after training is complete.
+-  ``num_gpus (int)``: The number of GPU devices available on your
+   training instance.
+-  ``num_cpus (int)``: The number of CPU devices available on your training instance.
+-  ``hosts (list[str])``: The list of host names running in the
+   SageMaker Training Job cluster.
+-  ``current_host (str)``: The name of the host executing the script.
+   When you use SageMaker for MXNet training, the script is run on each
+   host in the cluster.
+
+A training script that takes advantage of all arguments would have the following definition:
+
+.. code:: python
+
+    def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,
+              num_gpus, num_cpus, hosts, current_host):
+        pass
+
+You don't have to use all the arguments, arguments you don't care about can be ignored by including ``**kwargs``.
+
+.. code:: python
+
+    # Only work with hyperparameters and num_gpus, ignore all other hyperparameters
+    def train(hyperparameters, num_gpus, **kwargs):
+        pass
+
+**Note: Writing a training script that imports correctly**
+When SageMaker runs your training script, it imports it as a Python module and then invokes ``train`` on the imported module. Consequently, you should not include any statements that won't execute successfully in SageMaker when your module is imported. For example, don't attempt to open any local files in top-level statements in your training script.
+
+If you want to run your training script locally via the Python interpreter, look at using a ``___name__ == '__main__'`` guard, discussed in more detail here: https://stackoverflow.com/questions/419163/what-does-if-name-main-do .
+
+Distributed training
+''''''''''''''''''''
+
+When writing a distributed training script, you will want to use an MXNet kvstore to store and share model parameters.
+During training, SageMaker automatically starts an MXNet kvstore server and scheduler processes on hosts in your training job cluster.
+Your script runs as an MXNet worker task, with one server process on each host in your cluster.
+One host is selected arbitrarily to run the scheduler process.
+
+To learn more about writing distributed MXNet programs, please see `Distributed Training <https://mxnet.incubator.apache.org/versions/master/faq/distributed_training.html>`__ in the MXNet docs.
+
+Saving models
+'''''''''''''
+
+Just as you enable training by defining a ``train`` function in your training script, you enable model saving by defining a ``save`` function in your script. If your script includes a ``save`` function, SageMaker will invoke it with the return-value of ``train``. Model saving is a two-step process, firstly you return the model you want to save from
+``train``, then you define your model-serialization logic in ``save``.
+
+SageMaker provides a default implementation of ``save`` that works with MXNet Module API ``Module`` objects. If your training script does not define a ``save`` function, then the default ``save`` function will be invoked on the return-value of your ``train`` function.
+
+The default serialization system generates three files:
+
+-  ``model-shapes.json``: A json list, containing a serialization of the
+   ``Module`` ``data_shapes`` property. Each object in the list contains
+   the serialization of one ``DataShape`` in the returned ``Module``.
+   Each object has a ``name`` property, containing the ``DataShape``
+   name and a ``shape`` property, which is a list of that dimensions for
+   the shape of that ``DataShape``. For example:
+
+.. code:: javascript
+
+    [
+        {"name":"images", "shape":[100, 1, 28, 28]},
+        {"name":"labels", "shape":[100, 1]}
+    ]
+
+-  ``model-symbol.json``: The MXNet ``Module`` ``Symbol`` serialization,
+   produced by invoking ``save`` on the ``symbol`` property of the
+   ``Module`` being saved.
+-  ``modle.params``: The MXNet ``Module`` parameters. Produced by
+   invoking ``save_params`` on the ``Module`` being saved.
+
+You can provide your own save function. This is useful if you are not working with the ``Module`` API or you need special processing.
+
+To provide your own save function, define a ``save`` function in your training script:
+
+.. code:: python
+
+    def save(model, model_dir):
+        pass
+
+The function should take two arguments:
+
+-  ``model``: This is the object that was returned from your ``train``
+   function. If your ``train`` function does not return an object, it
+   will be ``None``. You are free to return an object of any type from
+   ``train``, you do not have to return ``Module`` or ``Gluon`` API
+   specific objects.
+-  ``model_dir``: This is the string path on the SageMaker training host
+   where you save your model. Files created in this directory will be
+   accessible in S3 after your SageMaker Training Job completes.
+
+After your ``train`` function completes, SageMaker will invoke ``save`` with the object returned from ``train``.
+
+**Note: How to save Gluon models with SageMaker**
+
+If your train function returns a Gluon API ``net`` object as its model, you'll need to write your own ``save`` function. You will want to serialize the ``net`` parameters. Saving ``net`` parameters is covered in the `Serialization section <http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html>`__ of the collaborative Gluon deep-learning book `"The Straight Dope" <http://gluon.mxnet.io/index.html>`__.
+
+Updating your MXNet training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The structure for training scripts changed with MXNet version 1.3.
+The ``train`` function is no longer be required; instead the training script must be able to be run as a standalone script.
+In this way, the training script is similar to a training script you might run outside of SageMaker.
+
+There are a few steps needed to make a training script with the old format compatible with the new format.
+
+First, add a `main guard <https://docs.python.org/3/library/__main__.html>`__ (``if __name__ == '__main__':``).
+The code executed from your main guard needs to:
+
+1. Set hyperparameters and directory locations
+2. Initiate training
+3. Save the model
+
+Hyperparameters will be passed as command-line arguments to your training script.
+In addition, the container will define the locations of input data and where to save the model artifacts and output data as environment variables rather than passing that information as arguments to the ``train`` function.
+You can find the full list of available environment variables in the `SageMaker Containers README <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+We recommend using `an argument parser <https://docs.python.org/3.5/howto/argparse.html>`__ for this part.
+Using the ``argparse`` library as an example, the code would look something like this:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ == '__main__':
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch-size', type=int, default=100)
+        parser.add_argument('--learning-rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
+        parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
+        parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
+
+        args, _ = parser.parse_known_args()
+
+The code in the main guard should also take care of training and saving the model.
+This can be as simple as just calling the ``train`` and ``save`` methods used in the previous training script format:
+
+.. code:: python
+
+    if __name__ == '__main__':
+        # arg parsing (shown above) goes here
+
+        model = train(args.batch_size, args.epochs, args.learning_rate, args.train, args.test)
+        save(args.model_dir, model)
+
+Note that saving the model will no longer be done by default; this must be done by the training script.
+If you were previously relying on the default save method, you can now import one from the container:
+
+.. code:: python
+
+    from sagemaker_mxnet_container.training_utils import save
+
+    if __name__ == '__main__':
+        # arg parsing and training (shown above) goes here
+
+        save(args.model_dir, model)
+
+Lastly, if you were relying on the container launching a parameter server for use with distributed training, you must now set ``distributions`` to the following dictionary when creating an MXNet estimator:
+
+.. code:: python
+
+    from sagemaker.mxnet import MXNet
+
+    estimator = MXNet('path-to-distributed-training-script.py',
+                      ...,
+                      distributions={'parameter_server': {'enabled': True}})
+
+
+Using third-party libraries
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+When running your training script on SageMaker, it will have access to some pre-installed third-party libraries including ``mxnet``, ``numpy``, ``onnx``, and ``keras-mxnet``.
+For more information on the runtime environment, including specific package versions, see `SageMaker MXNet Containers <#sagemaker-mxnet-containers>`__.
+
+If there are other packages you want to use with your script, you can include a ``requirements.txt`` file in the same directory as your training script to install other dependencies at runtime.
+A ``requirements.txt`` file is a text file that contains a list of items that are installed by using ``pip install``. You can also specify the version of an item to install.
+For information about the format of a ``requirements.txt`` file, see `Requirements Files <https://pip.pypa.io/en/stable/user_guide/#requirements-files>`__ in the pip documentation.
+
+Running an MXNet training script in SageMaker
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You run MXNet training scripts on SageMaker by creating an ``MXNet`` estimators.
+When you call ``fit`` on an ``MXNet`` estimator, a SageMaker training job with your script is started.
+The following code sample shows how you train a custom MXNet script "train.py".
+
+.. code:: python
+
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.3.0',
+                            hyperparameters={'batch-size': 100,
+                                             'epochs': 10,
+                                             'learning-rate': 0.1})
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+MXNet Estimators
+^^^^^^^^^^^^^^^^
+
+The ``MXNet`` constructor takes both required and optional arguments.
+
+Required arguments
+''''''''''''''''''
+
+The following are required arguments to the ``MXNet`` constructor. When you create an MXNet object, you must include these in the constructor, either positionally or as keyword arguments.
+
+-  ``entry_point`` Path (absolute or relative) to the Python file which
+   should be executed as the entry point to training.
+-  ``role`` An AWS IAM role (either name or full ARN). The Amazon
+   SageMaker training jobs and APIs that create Amazon SageMaker
+   endpoints use this role to access training data and model artifacts.
+   After the endpoint is created, the inference code might use the IAM
+   role, if accessing AWS resource.
+-  ``train_instance_count`` Number of Amazon EC2 instances to use for
+   training.
+-  ``train_instance_type`` Type of EC2 instance to use for training, for
+   example, 'ml.c4.xlarge'.
+
+Optional arguments
+''''''''''''''''''
+
+The following are optional arguments. When you create an ``MXNet`` object, you can specify these as keyword arguments.
+
+-  ``source_dir`` Path (absolute or relative) to a directory with any
+   other training source code dependencies including the entry point
+   file. Structure within this directory will be preserved when training
+   on SageMaker.
+-  ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+   any additional libraries that will be exported to the container (default: ``[]``).
+   The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+   If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+   instead. For example, the following call
+
+   >>> MXNet(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+   results in the following inside the container:
+
+   .. code::
+
+       opt/ml/code
+         ├── train.py
+         ├── common
+         └── virtual-env
+
+-  ``hyperparameters`` Hyperparameters that will be used for training.
+   Will be made accessible as a dict[str, str] to the training code on
+   SageMaker. For convenience, accepts other types besides str, but
+   str() will be called on keys and values to convert them before
+   training.
+-  ``py_version`` Python version you want to use for executing your
+   model training code. Valid values: 'py2' and 'py3'.
+-  ``train_volume_size`` Size in GB of the EBS volume to use for storing
+   input data during training. Must be large enough to store training
+   data if input_mode='File' is used (which is the default).
+-  ``train_max_run`` Timeout in seconds for training, after which Amazon
+   SageMaker terminates the job regardless of its current status.
+-  ``input_mode`` The input mode that the algorithm supports. Valid
+   modes: 'File' - Amazon SageMaker copies the training dataset from the
+   S3 location to a directory in the Docker container. 'Pipe' - Amazon
+   SageMaker streams data directly from S3 to the container via a Unix
+   named pipe.
+-  ``output_path`` Location where you want the training result (model artifacts and optional output files) saved.
+   This should be an S3 location unless you're using Local Mode, which also supports local output paths.
+   If not specified, results are stored to a default S3 bucket.
+-  ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+   output with.
+-  ``job_name`` Name to assign for the training job that the fit()
+   method launches. If not specified, the estimator generates a default
+   job name, based on the training image name and current timestamp
+-  ``image_name`` An alternative docker image to use for training and
+   serving.  If specified, the estimator will use this image for training and
+   hosting, instead of selecting the appropriate SageMaker official image based on
+   framework_version and py_version. Refer to: `SageMaker MXNet Docker Containers
+   <#sagemaker-mxnet-docker-containers>`_ for details on what the Official images support
+   and where to find the source code to build your custom image.
+-  ``distributions`` For versions 1.3 and above only.
+   Specifies information for how to run distributed training.
+   To launch a parameter server during training, set this argument to:
+
+.. code::
+
+    {
+      'parameter_server': {
+        'enabled': True
+      }
+    }
+
+Calling fit
+^^^^^^^^^^^
+
+You start your training script by calling ``fit`` on an ``MXNet`` Estimator. ``fit`` takes both required and optional arguments.
+
+Required argument
+'''''''''''''''''
+
+-  ``inputs``: This can take one of the following forms: A string
+   S3 URI, for example ``s3://my-bucket/my-training-data``. In this
+   case, the S3 objects rooted at the ``my-training-data`` prefix will
+   be available in the default ``training`` channel. A dict from
+   string channel names to S3 URIs. In this case, the objects rooted at
+   each S3 prefix will available as files in each channel directory.
+
+For example:
+
+.. code:: python
+
+    {'train':'s3://my-bucket/my-training-data',
+     'eval':'s3://my-bucket/my-evaluation-data'}
+
+.. optional-arguments-1:
+
+Optional arguments
+''''''''''''''''''
+
+-  ``wait``: Defaults to True, whether to block and wait for the
+   training script to complete before returning.
+-  ``logs``: Defaults to True, whether to show logs produced by training
+   job in the Python session. Only meaningful when wait is True.
+
+
+Deploying MXNet models
+----------------------
+
+After an MXNet Estimator has been fit, you can host the newly created model in SageMaker.
+
+After calling ``fit``, you can call ``deploy`` on an ``MXNet`` Estimator to create a SageMaker Endpoint. The Endpoint runs a SageMaker-provided MXNet model server and hosts the model produced by your training script, which was run when you called ``fit``. This was the model object you returned from ``train`` and saved with either a custom save function or the default save function.
+
+``deploy`` returns a ``Predictor`` object, which you can use to do inference on the Endpoint hosting your MXNet model. Each ``Predictor`` provides a ``predict`` method which can do inference with numpy arrays or Python lists. Inference arrays or lists are serialized and sent to the MXNet model server by an ``InvokeEndpoint`` SageMaker operation.
+
+``predict`` returns the result of inference against your model. By default, the inference result is either a Python list or dictionary.
+
+.. code:: python
+
+    # Train my estimator
+    mxnet_estimator = MXNet('train.py',
+                            train_instance_type='ml.p2.xlarge',
+                            train_instance_count=1,
+                            framework_version='1.2.1')
+    mxnet_estimator.fit('s3://my_bucket/my_training_data/')
+
+    # Deploy my estimator to a SageMaker Endpoint and get a Predictor
+    predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                       initial_instance_count=1)
+
+You use the SageMaker MXNet model server to host your MXNet model when you call ``deploy`` on an ``MXNet`` Estimator. The model server runs inside a SageMaker Endpoint, which your call to ``deploy`` creates. You can access the name of the Endpoint by the ``name`` property on the returned ``Predictor``.
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`_, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to ``accelerator_type`` to your ``deploy`` call.
+
+.. code:: python
+
+  predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge',
+                                     initial_instance_count=1,
+                                     accelerator_type='ml.eia1.medium')
+
+The SageMaker MXNet Model Server
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The MXNet Endpoint you create with ``deploy`` runs a SageMaker MXNet model server. The model server loads the model that was saved by your training script and performs inference on the model in response to SageMaker InvokeEndpoint API calls.
+
+You can configure two components of the SageMaker MXNet model server: Model loading and model serving. Model loading is the process of deserializing your saved model back into an MXNet model. Serving is the process of translating InvokeEndpoint requests to inference calls on the loaded model.
+
+As with MXNet training, you configure the MXNet model server by defining functions in the Python source file you passed to the MXNet constructor.
+
+Model loading
+^^^^^^^^^^^^^
+
+Before a model can be served, it must be loaded. The SageMaker model server loads your model by invoking a ``model_fn`` function on your training script. If you don't provide a ``model_fn`` function, SageMaker will use a default ``model_fn`` function. The default function works with MXNet Module model objects, saved via the default ``save`` function.
+
+If you wrote a custom ``save`` function then you may need to write a custom ``model_fn`` function. If your save function serializes ``Module`` objects under the same format as the default ``save`` function, then you won't need to write a custom model_fn function. If you do write a ``model_fn`` function must have the following signature:
+
+.. code:: python
+
+    def model_fn(model_dir)
+
+SageMaker will inject the directory where your model files and sub-directories, saved by ``save``, have been mounted. Your model function should return a model object that can be used for model serving. SageMaker provides automated serving functions that work with Gluon API ``net`` objects and Module API ``Module`` objects. If you return either of these types of objects, then you will be able to use the default serving request handling functions.
+
+The following code-snippet shows an example custom ``model_fn`` implementation. This loads returns an MXNet Gluon net model for resnet-34 inference. It loads the model parameters from a ``model.params`` file in the SageMaker model directory.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.cpu(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())
+        return net
+
+MXNet on SageMaker has support for `Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to load and serve your MXNet model through Amazon Elastic Inference, the MXNet context passed to your MXNet Symbol or Module object within your ``model_fn`` needs to be set to ``eia``, as shown `here <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html#ei-mxnet>`__.
+
+Based on the example above, the following code-snippet shows an example custom ``model_fn`` implementation, which enables loading and serving our MXNet model through Amazon Elastic Inference.
+
+.. code:: python
+
+    def model_fn(model_dir):
+        """
+        Load the gluon model in an Elastic Inference context. Called once when hosting service starts.
+        :param: model_dir The directory where model files are stored.
+        :return: a model (in this case a Gluon network)
+        """
+        net = models.get_model('resnet34_v2', ctx=mx.eia(), pretrained=False, classes=10)
+        net.load_params('%s/model.params' % model_dir, ctx=mx.eia())
+        return net
+
+The `default_model_fn <https://github.com/aws/sagemaker-mxnet-container/pull/55/files#diff-[93maabf018d906ed282a3c738377d19a8de[0mR71>`__ will load and serve your model through Elastic Inference, if applicable, within the SageMaker MXNet containers.
+
+For more information on how to enable MXNet to interact with Amazon Elastic Inference, see `Use Elastic Inference with MXNet <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-mxnet-elastic-inference.html>`__.
+
+Model serving
+^^^^^^^^^^^^^
+
+After the SageMaker model server loads your model by calling either the default ``model_fn`` or the implementation in your script, SageMaker serves your model.
+Model serving is the process of responding to inference requests received by SageMaker ``InvokeEndpoint`` API calls.
+Defining how to handle these requests can be done in one of two ways:
+
+- using ``input_fn``, ``predict_fn``, and ``output_fn``, some of which may be your own implementations
+- writing your own ``transform_fn`` for handling input processing, prediction, and output processing
+
+Using ``input_fn``, ``predict_fn``, and ``output_fn``
+'''''''''''''''''''''''''''''''''''''''''''''''''''''
+
+The SageMaker MXNet model server breaks request handling into three steps:
+
+-  input processing
+-  prediction
+-  output processing
+
+Just like with ``model_fn``, you configure these steps by defining functions in your Python source file.
+
+Each step has its own Python function, which takes in information about the request and the return value from the previous function in the chain.
+Inside the SageMaker MXNet model server, the process looks like:
+
+.. code:: python
+
+    # Deserialize the Invoke request body into an object we can perform prediction on
+    input_object = input_fn(request_body, request_content_type)
+
+    # Perform prediction on the deserialized object, with the loaded model
+    prediction = predict_fn(input_object, model)
+
+    # Serialize the prediction result into the desired response content type
+    ouput = output_fn(prediction, response_content_type)
+
+The above code sample shows the three function definitions that correlate to the three steps mentioned above:
+
+-  ``input_fn``: Takes request data and deserializes the data into an
+   object for prediction.
+-  ``predict_fn``: Takes the deserialized request object and performs
+   inference against the loaded model.
+-  ``output_fn``: Takes the result of prediction and serializes this
+   according to the response content type.
+
+The SageMaker MXNet model server provides default implementations of these functions.
+These work with both Gluon API and Module API model objects.
+The following content types are supported:
+
+- Gluon API: 'application/json', 'application/x-npy'
+- Module API: 'application/json', 'application/x-npy', 'text-csv'
+
+You can also provide your own implementations for these functions in your training script.
+If you omit any definition then the SageMaker MXNet model server will use its default implementation for that function.
+
+If you rely solely on the SageMaker MXNet model server defaults, you get the following functionality:
+
+-  Prediction on MXNet Gluon API ``net`` and Module API ``Module``
+   objects.
+-  Deserialization from CSV and JSON to NDArrayIters.
+-  Serialization of NDArrayIters to CSV or JSON.
+
+In the following sections we describe the default implementations of input_fn, predict_fn, and output_fn. We describe the input arguments and expected return types of each, so you can define your own implementations.
+
+Input processing
+""""""""""""""""
+
+When an InvokeEndpoint operation is made against an Endpoint running a SageMaker MXNet model server, the model server receives two pieces of information:
+
+-  The request's content type, for example "application/json"
+-  The request data body as a byte array
+
+The SageMaker MXNet model server will invoke ``input_fn``, passing in this information. If you define an ``input_fn`` function definition, it should return an object that can be passed to ``predict_fn`` and have the following signature:
+
+.. code:: python
+
+    def input_fn(request_body, request_content_type)
+
+Where ``request_body`` is a byte buffer and ``request_content_type`` is the content type of the request.
+
+The SageMaker MXNet model server provides a default implementation of ``input_fn``. This function deserializes JSON or CSV encoded data into an MXNet ``NDArrayIter`` `(external API docs) <https://mxnet.incubator.apache.org/api/python/io.html#mxnet.io.NDArrayIter>`__ multi-dimensional array iterator. This works with the default ``predict_fn`` implementation, which expects an ``NDArrayIter`` as input.
+
+Default JSON deserialization requires ``request_body`` contain a single json list. Sending multiple json objects within the same ``request_body`` is not supported. The list must have a dimensionality compatible with the MXNet ``net`` or ``Module`` object. Specifically, after the list is loaded, it's either padded or split to fit the first dimension of the model input shape. The list's shape must be identical to the model's input shape, for all dimensions after the first.
+
+Default CSV deserialization requires ``request_body`` contain one or more lines of CSV numerical data. The data is loaded into a two-dimensional array, where each line break defines the boundaries of the first dimension. This two-dimensional array is then re-shaped to be compatible with the shape expected by the model object. Specifically, the first dimension is kept unchanged, but the second dimension is reshaped to be consistent with the shape of all dimensions in the model, following the first dimension.
+
+If you provide your own implementation of input_fn, you should abide by the ``input_fn`` signature. If you want to use this with the default
+``predict_fn``, then you should return an ``NDArrayIter``. The ``NDArrayIter`` should have a shape identical to the shape of the model being predicted on. The example below shows a custom ``input_fn`` for preparing pickled numpy arrays.
+
+.. code:: python
+
+    import numpy as np
+    import mxnet as mx
+
+    def input_fn(request_body, request_content_type):
+        """An input_fn that loads a pickled numpy array"""
+        if request_content_type == 'application/python-pickle':
+            array = np.load(StringIO(request_body))
+            array.reshape(model.data_shpaes[0])
+            return mx.io.NDArrayIter(mx.ndarray(array))
+        else:
+            # Handle other content-types here or raise an Exception
+            # if the content type is not supported.
+            pass
+
+Prediction
+""""""""""
+
+After the inference request has been deserialized by ``input_fn``, the SageMaker MXNet model server invokes ``predict_fn``. As with ``input_fn``, you can define your own ``predict_fn`` or use the SageMaker Mxnet default.
+
+The ``predict_fn`` function has the following signature:
+
+.. code:: python
+
+    def predict_fn(input_object, model)
+
+Where ``input_object`` is the object returned from ``input_fn`` and
+``model`` is the model loaded by ``model_fn``.
+
+The default implementation of ``predict_fn`` requires ``input_object`` be an ``NDArrayIter``, which is the return-type of the default
+``input_fn``. It also requires that ``model`` be either an MXNet Gluon API ``net`` object or a Module API ``Module`` object.
+
+The default implementation performs inference with the input
+``NDArrayIter`` on the Gluon or Module object. If the model is a Gluon
+``net`` it performs: ``net.forward(input_object)``. If the model is a Module object it performs ``module.predict(input_object)``. In both cases, it returns the result of that call.
+
+If you implement your own prediction function, you should take care to ensure that:
+
+-  The first argument is expected to be the return value from input_fn.
+   If you use the default input_fn, this will be an ``NDArrayIter``.
+-  The second argument is the loaded model. If you use the default
+   ``model_fn`` implementation, this will be an MXNet Module object.
+   Otherwise, it will be the return value of your ``model_fn``
+   implementation.
+-  The return value should be of the correct type to be passed as the
+   first argument to ``output_fn``. If you use the default
+   ``output_fn``, this should be an ``NDArrayIter``.
+
+Output processing
+"""""""""""""""""
+
+After invoking ``predict_fn``, the model server invokes ``output_fn``, passing in the return value from ``predict_fn`` and the InvokeEndpoint requested response content type.
+
+The ``output_fn`` has the following signature:
+
+.. code:: python
+
+    def output_fn(prediction, content_type)
+
+Where ``prediction`` is the result of invoking ``predict_fn`` and ``content_type`` is the requested response content type for ``InvokeEndpoint``.
+The function should return an array of bytes serialized to the expected content type.
+
+The default implementation expects ``prediction`` to be an ``NDArray`` and can serialize the result to either JSON or CSV. It accepts response content types of "application/json" and "text/csv".
+
+Using ``transform_fn``
+''''''''''''''''''''''
+
+If you would rather not structure your code around the three methods described above, you can instead define your own ``transform_fn`` to handle inference requests. An error will be thrown if a ``transform_fn`` is present in conjunction with any ``input_fn``, ``predict_fn``, and/or ``output_fn``.
+``transform_fn`` has the following signature:
+
+.. code:: python
+
+    def transform_fn(model, request_body, content_type, accept_type)
+
+Where ``model`` is the model objected loaded by ``model_fn``, ``request_body`` is the data from the inference request, ``content_type`` is the content type of the request, and ``accept_type`` is the request content type for the response.
+
+This one function should handle processing the input, performing a prediction, and processing the output.
+The return object should be one of the following:
+
+For versions 1.4 and higher:
+----------------------------
+- a tuple with two items: the response data and ``accept_type`` (the content type of the response data), or
+- the response data: (the content type of the response will be set to either the accept header in the initial request or default to application/json)
+
+For versions 1.3 and lower:
+---------------------------
+- a tuple with two items: the response data and ``accept_type`` (the content type of the response data), or
+- a Flask response object: http://flask.pocoo.org/docs/1.0/api/#response-objects
+
+You can find examples of hosting scripts using this structure in the example notebooks, such as the `mxnet_gluon_sentiment <https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_sentiment/sentiment.py#L344-L387>`__ notebook.
+
+Working with existing model data and training jobs
+--------------------------------------------------
+
+Attaching to existing training jobs
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+You can attach an MXNet Estimator to an existing training job using the
+``attach`` method.
+
+.. code:: python
+
+    my_training_job_name = 'MyAwesomeMXNetTrainingJob'
+    mxnet_estimator = MXNet.attach(my_training_job_name)
+
+After attaching, if the training job is in a Complete status, it can be
+``deploy``\ ed to create a SageMaker Endpoint and return a
+``Predictor``. If the training job is in progress, attach will block and display log messages from the training job, until the training job completes.
+
+The ``attach`` method accepts the following arguments:
+
+-  ``training_job_name (str):`` The name of the training job to attach
+   to.
+-  ``sagemaker_session (sagemaker.Session or None):`` The Session used
+   to interact with SageMaker
+
+Deploying Endpoints from model data
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+As well as attaching to existing training jobs, you can deploy models directly from model data in S3. The following code sample shows how to do this, using the ``MXNetModel`` class.
+
+.. code:: python
+
+    mxnet_model = MXNetModel(model_data='s3://bucket/model.tar.gz', role='SageMakerRole', entry_point='trasform_script.py')
+
+    predictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
+
+The MXNetModel constructor takes the following arguments:
+
+-  ``model_data (str):`` An S3 location of a SageMaker model data
+   .tar.gz file
+-  ``image (str):`` A Docker image URI
+-  ``role (str):`` An IAM role name or Arn for SageMaker to access AWS
+   resources on your behalf.
+-  ``predictor_cls (callable[string,sagemaker.Session]):`` A function to
+   call to create a predictor. If not None, ``deploy`` will return the
+   result of invoking this function on the created endpoint name
+-  ``env (dict[string,string]):`` Environment variables to run with
+   ``image`` when hosted in SageMaker.
+-  ``name (str):`` The model name. If None, a default model name will be
+   selected on each ``deploy.``
+-  ``entry_point (str):`` Path (absolute or relative) to the Python file
+   which should be executed as the entry point to model hosting.
+-  ``source_dir (str):`` Optional. Path (absolute or relative) to a
+   directory with any other training source code dependencies including
+   tne entry point file. Structure within this directory will be
+   preserved when training on SageMaker.
+-  ``container_log_level (int):`` Log level to use within the container.
+   Valid values are defined in the Python logging module.
+-  ``code_location (str):`` Optional. Name of the S3 bucket where your
+   custom code will be uploaded to. If not specified, will use the
+   SageMaker default bucket created by sagemaker.Session.
+-  ``sagemaker_session (sagemaker.Session):`` The SageMaker Session
+   object, used for SageMaker interaction
+
+Your model data must be a .tar.gz file in S3. SageMaker Training Job model data is saved to .tar.gz files in S3, however if you have local data you want to deploy, you can prepare the data yourself.
+
+Assuming you have a local directory containg your model data named "my_model" you can tar and gzip compress the file and upload to S3 using the following commands:
+
+::
+
+    tar -czf model.tar.gz my_model
+    aws s3 cp model.tar.gz s3://my-bucket/my-path/model.tar.gz
+
+This uploads the contents of my_model to a gzip compressed tar file to S3 in the bucket "my-bucket", with the key "my-path/model.tar.gz".
+
+To run this command, you'll need the aws cli tool installed. Please refer to our `FAQ <#FAQ>`__ for more information on installing this.
+
+Examples
+--------
+
+Amazon provides several example Jupyter notebooks that demonstrate end-to-end training on Amazon SageMaker using MXNet. Please refer to:
+
+https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk
+
+These are also available in SageMaker Notebook Instance hosted Jupyter notebooks under the "sample notebooks" folder.
+
+SageMaker MXNet Containers
+--------------------------
+
+When training and deploying training scripts, SageMaker runs your Python script in a Docker container with several libraries installed. When creating the Estimator and calling deploy to create the SageMaker Endpoint, you can control the environment your script runs in.
+
+SageMaker runs MXNet Estimator scripts in either Python 2.7 or Python 3.5. You can select the Python version by passing a ``py_version`` keyword arg to the MXNet Estimator constructor. Setting this to ``py2`` (the default) will cause your training script to be run on Python 2.7. Setting this to ``py3`` will cause your training script to be run on Python 3.5. This Python version applies to both the Training Job, created by fit, and the Endpoint, created by deploy.
+
+Your MXNet training script will be run on version 1.2.1 by default. (See below for how to choose a different version, and currently supported versions.) The decision to use the GPU or CPU version of MXNet is made by the ``train_instance_type``, set on the MXNet constructor. If you choose a GPU instance type, your training job will be run on a GPU version of MXNet. If you choose a CPU instance type, your training job will be run on a CPU version of MXNet. Similarly, when you call deploy, specifying a GPU or CPU deploy_instance_type, will control which MXNet build your Endpoint runs.
+
+The Docker images have the following dependencies installed:
+
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| Dependencies            | MXNet 0.12.1 | MXNet 1.0.0 | MXNet 1.1.0 | MXNet 1.2.1 | MXNet 1.3.0 | MXNet 1.4.0 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| Python                  |   2.7 or 3.5 |   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.5|   2.7 or 3.6|
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| CUDA (GPU image only)   |          9.0 |         9.0 |         9.0 |         9.0 |         9.0 |         9.2 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| numpy                   |       1.13.3 |      1.13.3 |      1.13.3 |      1.14.5 |      1.14.6 |      1.16.3 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| onnx                    |          N/A |         N/A |         N/A |       1.2.1 |       1.2.1 |       1.4.1 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+| keras-mxnet             |          N/A |         N/A |         N/A |         N/A |       2.2.2 |     2.2.4.1 |
++-------------------------+--------------+-------------+-------------+-------------+-------------+-------------+
+
+The Docker images extend Ubuntu 16.04.
+
+You can select version of MXNet by passing a ``framework_version`` keyword arg to the MXNet Estimator constructor. Currently supported versions are listed in the above table. You can also set ``framework_version`` to only specify major and minor version, e.g ``1.2``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.2.1.
+Alternatively, you can build your own image by following the instructions in the SageMaker MXNet containers repository, and passing ``image_name`` to the MXNet Estimator constructor.
+
+You can visit the SageMaker MXNet training containers repository here: https://github.com/aws/sagemaker-mxnet-container
+You can visit the SageMaker MXNet serving containers repository here: https://github.com/aws/sagemaker-mxnet-serving-container

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: doc/using_tf.rst[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,510 @@
+==============================================
+Using TensorFlow with the SageMaker Python SDK
+==============================================
+
+TensorFlow SageMaker Estimators allow you to run your own TensorFlow
+training algorithms on SageMaker Learner, and to host your own TensorFlow
+models on SageMaker Hosting.
+
+**Note:** This topic describes how to use script mode for TensorFlow versions 1.11 and later.
+For Documentation of the previous Legacy Mode versions, see:
+* `1.4.1 <https://github.com/aws/sagemaker-python-sdk/tree/v1.0.0#tensorflow-sagemaker-estimators>`_
+* `1.5.0 <https://github.com/aws/sagemaker-python-sdk/tree/v1.1.0#tensorflow-sagemaker-estimators>`_
+* `1.6.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.7.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.8.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.5.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.9.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.9.2/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+* `1.10.0 <https://github.com/aws/sagemaker-python-sdk/blob/v1.10.0/src/sagemaker/tensorflow/README.rst#tensorflow-sagemaker-estimators-and-models>`_
+
++-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
+| WARNING                                                                                                                                                                     |
++=============================================================================================================================================================================+
+| We have added a new format of your TensorFlow training script with TensorFlow version 1.11.                                                                                 |
+| This new way gives the user script more flexibility.                                                                                                                        |
+| This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions.                                         |
+| In addition we are adding Python 3 support with Script Mode.                                                                                                                |
+| Last supported version of Legacy Mode will be TensorFlow 1.12.                                                                                                              |
+| Script Mode is available with TensorFlow version 1.11 and newer.                                                                                                            |
+| Make sure you refer to the correct version of this README when you prepare your script.                                                                                     |
+| You can find the Legacy Mode README `here <https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#tensorflow-sagemaker-estimators-and-models>`_. |
++-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
+
+.. contents::
+
+Supported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``.
+
+Training with TensorFlow
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Training TensorFlow models using ``sagemaker.tensorflow.TensorFlow`` is a two-step process.
+First, you prepare your training script, then second, you run it on
+SageMaker Learner via the ``sagemaker.tensorflow.TensorFlow`` estimator.
+
+Preparing a Script Mode training script
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Your TensorFlow training script must be a Python 2.7- or 3.6-compatible source file.
+
+The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:
+
+* ``SM_MODEL_DIR``: A string that represents the local path where the training job can write the model artifacts to.
+  After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the ``model_dir``
+  argument passed in your training script which is a S3 location. ``SM_MODEL_DIR`` is always set to ``/opt/ml/model``.
+* ``SM_NUM_GPUS``: An integer representing the number of GPUs available to the host.
+* ``SM_OUTPUT_DATA_DIR``: A string that represents the path to the directory to write output artifacts to.
+  Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts.
+  These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.
+* ``SM_CHANNEL_XXXX``: A string that represents the path to the directory that contains the input data for the specified channel.
+  For example, if you specify two input channels in the TensorFlow estimator's ``fit`` call, named 'train' and 'test', the environment variables ``SM_CHANNEL_TRAIN`` and ``SM_CHANNEL_TEST`` are set.
+
+For the exhaustive list of available environment variables, see the `SageMaker Containers documentation <https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers>`__.
+
+A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to ``SM_CHANNEL_TRAIN`` so that it can be deployed for inference later.
+Hyperparameters are passed to your script as arguments and can be retrieved with an ``argparse.ArgumentParser`` instance.
+For example, a training script might start with the following:
+
+.. code:: python
+
+    import argparse
+    import os
+
+    if __name__ =='__main__':
+
+        parser = argparse.ArgumentParser()
+
+        # hyperparameters sent by the client are passed as command-line arguments to the script.
+        parser.add_argument('--epochs', type=int, default=10)
+        parser.add_argument('--batch_size', type=int, default=100)
+        parser.add_argument('--learning_rate', type=float, default=0.1)
+
+        # input data and model directories
+        parser.add_argument('--model_dir', type=str)
+        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
+
+        args, _ = parser.parse_known_args()
+
+        # ... load from args.train and args.test, train a model, write model to args.model_dir.
+
+Because the SageMaker imports your training script, putting your training launching code in a main guard (``if __name__=='__main__':``)
+is good practice.
+
+Note that SageMaker doesn't support argparse actions.
+If you want to use, for example, boolean hyperparameters, you need to specify ``type`` as ``bool`` in your script and provide an explicit ``True`` or ``False`` value for this hyperparameter when instantiating your TensorFlow estimator.
+
+Adapting your local TensorFlow script
+'''''''''''''''''''''''''''''''''''''
+
+If you have a TensorFlow training script that runs outside of SageMaker please follow the directions here:
+
+1. Make sure your script can handle ``--model_dir`` as an additional command line argument. If you did not specify a
+location when the TensorFlow estimator is constructed a S3 location under the default training job bucket will be passed
+in here. Distributed training with parameter servers requires you use the ``tf.estimator.train_and_evaluate`` API and
+a S3 location is needed as the model directory during training. Here is an example:
+
+.. code:: python
+
+    estimator = tf.estimator.Estimator(model_fn=my_model_fn, model_dir=args.model_dir)
+    ...
+    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
+    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
+    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)
+
+2. Load input data from the input channels. The input channels are defined when ``fit`` is called. For example:
+
+.. code:: python
+
+    estimator.fit({'train':'s3://my-bucket/my-training-data',
+                  'eval':'s3://my-bucket/my-evaluation-data'})
+
+In your training script the channels will be stored in environment variables ``SM_CHANNEL_TRAIN`` and
+``SM_CHANNEL_EVAL``. You can add them to your argument parsing logic like this:
+
+.. code:: python
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
+    parser.add_argument('--eval', type=str, default=os.environ.get('SM_CHANNEL_EVAL'))
+
+3. Export your final model to path stored in environment variable ``SM_MODEL_DIR`` which should always be
+   ``/opt/ml/model``. At end of training SageMaker will upload the model file under ``/opt/ml/model`` to
+   ``output_path``.
+
+
+Training with TensorFlow estimator
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Calling fit
+'''''''''''
+
+To use Script Mode, set at least one of these args
+
+- ``py_version='py3'``
+- ``script_mode=True``
+
+Please note that when using Script Mode, your training script need to accept the following args:
+
+- ``model_dir``
+
+Please note that the following args are not permitted when using Script Mode:
+
+- ``checkpoint_path``
+- ``training_steps``
+- ``evaluation_steps``
+- ``requirements_file``
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.12', py_version='py3')
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Where the S3 url is a path to your training data, within Amazon S3. The
+constructor keyword arguments define how SageMaker runs your training
+script which we discussed earlier.
+
+You start your training script by calling ``fit`` on a ``TensorFlow`` estimator. ``fit`` takes
+both required and optional arguments.
+
+Required argument
+"""""""""""""""""
+
+- ``inputs``: The S3 location(s) of datasets to be used for training. This can take one of two forms:
+
+  - ``str``: An S3 URI, for example ``s3://my-bucket/my-training-data``, which indicates the dataset's location.
+  - ``dict[str, str]``: A dictionary mapping channel names to S3 locations, for example ``{'train': 's3://my-bucket/my-training-data/train', 'test': 's3://my-bucket/my-training-data/test'}``
+  - ``sagemaker.session.s3_input``: channel configuration for S3 data sources that can provide additional information as well as the path to the training dataset. See `the API docs <https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input>`_ for full details.
+
+Optional arguments
+""""""""""""""""""
+
+- ``wait (bool)``: Defaults to True, whether to block and wait for the
+  training script to complete before returning.
+  If set to False, it will return immediately, and can later be attached to.
+- ``logs (bool)``: Defaults to True, whether to show logs produced by training
+  job in the Python session. Only meaningful when wait is True.
+- ``run_tensorboard_locally (bool)``: Defaults to False. If set to True a Tensorboard command will be printed out.
+- ``job_name (str)``: Training job name. If not specified, the estimator generates a default job name,
+  based on the training image name and current timestamp.
+
+What happens when fit is called
+"""""""""""""""""""""""""""""""
+
+Calling ``fit`` starts a SageMaker training job. The training job will execute the following.
+
+- Starts ``train_instance_count`` EC2 instances of the type ``train_instance_type``.
+- On each instance, it will do the following steps:
+
+  - starts a Docker container optimized for TensorFlow.
+  - downloads the dataset.
+  - setup up training related environment varialbes
+  - setup up distributed training environment if configured to use parameter server
+  - starts asynchronous training
+
+If the ``wait=False`` flag is passed to ``fit``, then it will return immediately. The training job will continue running
+asynchronously. At a later time, a Tensorflow Estimator can be obtained by attaching to the existing training job. If
+the training job is not finished it will start showing the standard output of training and wait until it completes.
+After attaching, the estimator can be deployed as usual.
+
+.. code:: python
+
+    tf_estimator.fit(your_input_data, wait=False)
+    training_job_name = tf_estimator.latest_training_job.name
+
+    # after some time, or in a separate Python notebook, we can attach to it again.
+
+    tf_estimator = TensorFlow.attach(training_job_name=training_job_name)
+
+Distributed Training
+''''''''''''''''''''
+
+To run your training job with multiple instances in a distributed fashion, set ``train_instance_count``
+to a number larger than 1. We support two different types of distributed training, parameter server and Horovod.
+The ``distributions`` parameter is used to configure which distributed training strategy to use.
+
+Training with parameter servers
+"""""""""""""""""""""""""""""""
+
+If you specify parameter_server as the value of the distributions parameter, the container launches a parameter server
+thread on each instance in the training cluster, and then executes your training code. You can find more information on
+TensorFlow distributed training at `TensorFlow docs <https://www.tensorflow.org/deploy/distributed>`__.
+To enable parameter server training:
+
+.. code:: python
+
+  from sagemaker.tensorflow import TensorFlow
+
+  tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                            train_instance_count=2, train_instance_type='ml.p2.xlarge',
+                            framework_version='1.11', py_version='py3',
+                            distributions={'parameter_server': {'enabled': True}})
+  tf_estimator.fit('s3://bucket/path/to/training/data')
+
+Training with Horovod
+"""""""""""""""""""""
+
+Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version ``1.12`` or newer.
+You can find more details at `Horovod README <https://github.com/uber/horovod>`__.
+
+The container sets up the MPI environment and executes the ``mpirun`` command enabling you to run any Horovod
+training script with Script Mode.
+
+Training with ``MPI`` is configured by specifying following fields in ``distributions``:
+
+- ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.
+- ``processes_per_host (int)``: Number of processes MPI should launch on each host. Note, this should not be
+  greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu
+  training.
+- ``custom_mpi_options (str)``:  Any `mpirun` flag(s) can be passed in this field that will be added to the `mpirun`
+  command executed by SageMaker to launch distributed horovod training.
+
+
+In the below example we create an estimator to launch Horovod distributed training with 2 processes on one host:
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.12', py_version='py3',
+                              distributions={
+                                  'mpi': {
+                                      'enabled': True,
+                                      'processes_per_host': 2,
+                                      'custom_mpi_options': '--NCCL_DEBUG INFO'
+                                  }
+                              })
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+sagemaker.tensorflow.TensorFlow class
+'''''''''''''''''''''''''''''''''''''
+
+The ``TensorFlow`` constructor takes both required and optional arguments.
+
+Required:
+
+- ``entry_point (str)`` Path (absolute or relative) to the Python file which
+  should be executed as the entry point to training.
+- ``role (str)`` An AWS IAM role (either name or full ARN). The Amazon
+  SageMaker training jobs and APIs that create Amazon SageMaker
+  endpoints use this role to access training data and model artifacts.
+  After the endpoint is created, the inference code might use the IAM
+  role, if accessing AWS resource.
+- ``train_instance_count (int)`` Number of Amazon EC2 instances to use for
+  training.
+- ``train_instance_type (str)`` Type of EC2 instance to use for training, for
+  example, 'ml.c4.xlarge'.
+
+Optional:
+
+- ``source_dir (str)`` Path (absolute or relative) to a directory with any
+  other training source code dependencies including the entry point
+  file. Structure within this directory will be preserved when training
+  on SageMaker.
+- ``dependencies (list[str])`` A list of paths to directories (absolute or relative) with
+  any additional libraries that will be exported to the container (default: ``[]``).
+  The library folders will be copied to SageMaker in the same folder where the entrypoint is copied.
+  If the ``source_dir`` points to S3, code will be uploaded and the S3 location will be used
+  instead. Example:
+
+  The following call
+
+  >>> TensorFlow(entry_point='train.py', dependencies=['my/libs/common', 'virtual-env'])
+
+  results in the following inside the container:
+
+  >>> opt/ml/code
+  >>>     ├── train.py
+  >>>     ├── common
+  >>>     └── virtual-env
+
+- ``hyperparameters (dict[str, ANY])`` Hyperparameters that will be used for training.
+  Will be made accessible as command line arguments.
+- ``train_volume_size (int)`` Size in GB of the EBS volume to use for storing
+  input data during training. Must be large enough to the store training
+  data.
+- ``train_max_run (int)`` Timeout in seconds for training, after which Amazon
+  SageMaker terminates the job regardless of its current status.
+- ``output_path (str)`` S3 location where you want the training result (model
+  artifacts and optional output files) saved. If not specified, results
+  are stored to a default bucket. If the bucket with the specific name
+  does not exist, the estimator creates the bucket during the ``fit``
+  method execution.
+- ``output_kms_key`` Optional KMS key ID to optionally encrypt training
+  output with.
+- ``base_job_name`` Name to assign for the training job that the ``fit``
+  method launches. If not specified, the estimator generates a default
+  job name, based on the training image name and current timestamp.
+- ``image_name`` An alternative docker image to use for training and
+  serving.  If specified, the estimator will use this image for training and
+  hosting, instead of selecting the appropriate SageMaker official image based on
+  ``framework_version`` and ``py_version``. Refer to: `SageMaker TensorFlow Docker Containers
+  <#sagemaker-tensorflow-docker-containers>`_ for details on what the official images support
+  and where to find the source code to build your custom image.
+- ``script_mode (bool)`` Whether to use Script Mode or not. Script mode is the only available training mode in Python 3,
+  setting ``py_version`` to ``py3`` automatically sets ``script_mode`` to True.
+- ``model_dir (str)`` Location where model data, checkpoint data, and TensorBoard checkpoints should be saved during training.
+  If not specified a S3 location will be generated under the training job's default bucket. And ``model_dir`` will be
+  passed in your training script as one of the command line arguments.
+- ``distributions (dict)`` Configure your distribution strategy with this argument.
+
+Training with Pipe Mode using PipeModeDataset
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Amazon SageMaker allows users to create training jobs using Pipe input mode.
+With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first.
+This means that your training jobs start sooner, finish quicker, and need less disk space.
+
+SageMaker TensorFlow provides an implementation of ``tf.data.Dataset`` that makes it easy to take advantage of Pipe
+input mode in SageMaker. You can replace your ``tf.data.Dataset`` with a ``sagemaker_tensorflow.PipeModeDataset`` to
+read TFRecords as they are streamed to your training instances.
+
+In your ``entry_point`` script, you can use ``PipeModeDataset`` like a ``Dataset``. In this example, we create a
+``PipeModeDataset`` to read TFRecords from the 'training' channel:
+
+
+.. code:: python
+
+    from sagemaker_tensorflow import PipeModeDataset
+
+    features = {
+        'data': tf.FixedLenFeature([], tf.string),
+        'labels': tf.FixedLenFeature([], tf.int64),
+    }
+
+    def parse(record):
+        parsed = tf.parse_single_example(record, features)
+        return ({
+            'data': tf.decode_raw(parsed['data'], tf.float64)
+        }, parsed['labels'])
+
+    def train_input_fn(training_dir, hyperparameters):
+        ds = PipeModeDataset(channel='training', record_format='TFRecord')
+        ds = ds.repeat(20)
+        ds = ds.prefetch(10)
+        ds = ds.map(parse, num_parallel_calls=10)
+        ds = ds.batch(64)
+        return ds
+
+
+To run training job with Pipe input mode, pass in ``input_mode='Pipe'`` to your TensorFlow Estimator:
+
+
+.. code:: python
+
+    from sagemaker.tensorflow import TensorFlow
+
+    tf_estimator = TensorFlow(entry_point='tf-train-with-pipemodedataset.py', role='SageMakerRole',
+                              training_steps=10000, evaluation_steps=100,
+                              train_instance_count=1, train_instance_type='ml.p2.xlarge',
+                              framework_version='1.10.0', input_mode='Pipe')
+
+    tf_estimator.fit('s3://bucket/path/to/training/data')
+
+
+If your TFRecords are compressed, you can train on Gzipped TF Records by passing in ``compression='Gzip'`` to the call to
+``fit()``, and SageMaker will automatically unzip the records as data is streamed to your training instances:
+
+.. code:: python
+
+    from sagemaker.session import s3_input
+
+    train_s3_input = s3_input('s3://bucket/path/to/training/data', compression='Gzip')
+    tf_estimator.fit(train_s3_input)
+
+
+You can learn more about ``PipeModeDataset`` in the sagemaker-tensorflow-extensions repository: https://github.com/aws/sagemaker-tensorflow-extensions
+
+
+Training with MKL-DNN disabled
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+SageMaker TensorFlow CPU images use TensorFlow built with Intel® MKL-DNN optimization.
+
+In certain cases you might be able to get a better performance by disabling this optimization
+(`for example when using small models <https://github.com/awslabs/amazon-sagemaker-examples/blob/[93md88d1c19861fb7733941969f5a68821d9da2982e[0m/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/iris_dnn_classifier.py#L7-L9>`_)
+
+You can disable MKL-DNN optimization for TensorFlow ``1.8.0`` and above by setting two following environment variables:
+
+.. code:: python
+
+    import os
+
+    os.environ['TF_DISABLE_MKL'] = '1'
+    os.environ['TF_DISABLE_POOL_ALLOCATOR'] = '1'
+
+
+Deploying TensorFlow Serving models
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+After a TensorFlow estimator has been fit, it saves a TensorFlow SavedModel in
+the S3 location defined by ``output_path``. You can call ``deploy`` on a TensorFlow
+estimator to create a SageMaker Endpoint.
+
+SageMaker provides two different options for deploying TensorFlow models to a SageMaker
+Endpoint:
+
+- The first option uses a Python-based server that allows you to specify your own custom
+  input and output handling functions in a Python script. This is the default option.
+
+  See `Deploying to Python-based Endpoints <https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_python.rst>`_ to learn how to use this option.
+
+
+- The second option uses a TensorFlow Serving-based server to provide a super-set of the
+  `TensorFlow Serving REST API <https://www.tensorflow.org/serving/api_rest>`_. This option
+  does not require (or allow) a custom python script.
+
+  See `Deploying to TensorFlow Serving Endpoints <https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst>`_ to learn how to use this option.
+
+
+SageMaker TensorFlow Docker containers
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The containers include the following Python packages:
+
++--------------------------------+---------------+-------------------+
+| Dependencies                   | Script Mode   | Legacy Mode       |
++--------------------------------+---------------+-------------------+
+| boto3                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| botocore                       | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| CUDA (GPU image only)          | 9.0           | 9.0               |
++--------------------------------+---------------+-------------------+
+| numpy                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| Pillow                         | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| scipy                          | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| sklean                         | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| h5py                           | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| pip                            | 18.1          | 18.1              |
++--------------------------------+---------------+-------------------+
+| curl                           | Latest        | Latest            |
++--------------------------------+---------------+-------------------+
+| tensorflow                     | 1.12.0        | 1.12.0            |
++--------------------------------+---------------+-------------------+
+| tensorflow-serving-api         | 1.12.0        | None              |
++--------------------------------+---------------+-------------------+
+| sagemaker-containers           | >=2.3.5       | >=2.3.5           |
++--------------------------------+---------------+-------------------+
+| sagemaker-tensorflow-container | 1.0           | 1.0               |
++--------------------------------+---------------+-------------------+
+| Python                         | 2.7 or 3.6    | 2.7               |
++--------------------------------+---------------+-------------------+
+
+Legacy Mode TensorFlow Docker images support Python 2.7. Script Mode TensorFlow Docker images support both Python 2.7
+and Python 3.6. The Docker images extend Ubuntu 16.04.
+
+You can select version of TensorFlow by passing a ``framework_version`` keyword arg to the TensorFlow Estimator constructor. Currently supported versions are listed in the table above. You can also set ``framework_version`` to only specify major and minor version, e.g ``'1.6'``, which will cause your training script to be run on the latest supported patch version of that minor version, which in this example would be 1.6.0.
+Alternatively, you can build your own image by following the instructions in the SageMaker TensorFlow containers
+repository, and passing ``image_name`` to the TensorFlow Estimator constructor.
+
+For more information on the contents of the images, see the SageMaker TensorFlow containers repository here: https://github.com/aws/sagemaker-tensorflow-containers/
\ No newline at end of file

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: tests/integ/test_marketplace.py[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,238 @@
+# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"). You
+# may not use this file except in compliance with the License. A copy of
+# the License is located at
+#
+#     http://aws.amazon.com/apache2.0/
+#
+# or in the "license" file accompanying this file. This file is
+# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
+# ANY KIND, either express or implied. See the License for the specific
+# language governing permissions and limitations under the License.
+from __future__ import absolute_import
+
+import itertools
+import os
+import time
+
+import pandas
+import pytest
+
+import sagemaker
+from sagemaker import AlgorithmEstimator, ModelPackage
+from sagemaker.tuner import IntegerParameter, HyperparameterTuner
+from sagemaker.utils import sagemaker_timestamp
+from tests.integ import DATA_DIR
+from tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name
+from tests.integ.marketplace_utils import REGION_ACCOUNT_MAP
+
+
+# All these tests require a manual 1 time subscription to the following Marketplace items:
+# Algorithm: Scikit Decision Trees
+# https://aws.amazon.com/marketplace/pp/prodview-ha4f3kqugba3u
+#
+# Pre-Trained Model: Scikit Decision Trees - Pretrained Model
+# https://aws.amazon.com/marketplace/pp/prodview-7qop4x5ahrdhe
+#
+# Both are  written by Amazon and are free to subscribe.
+
+ALGORITHM_ARN = 'arn:aws:sagemaker:%s:%s:algorithm/scikit-decision-trees-' \
+                '15423055-[93m57b73412d2e93e9239e4e16f83298b8f[0m'
+
+MODEL_PACKAGE_ARN = 'arn:aws:sagemaker:%s:%s:model-package/scikit-iris-detector-' \
+                    '154230595-[93m8f00905c1f927a512b73ea29dd09ae30[0m'
+
+
+@pytest.mark.canary_quick
+def test_marketplace_estimator(sagemaker_session):
+    with timeout(minutes=15):
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+        region = sagemaker_session.boto_region_name
+        account = REGION_ACCOUNT_MAP[region]
+        algorithm_arn = ALGORITHM_ARN % (region, account)
+
+        algo = AlgorithmEstimator(
+            algorithm_arn=algorithm_arn,
+            role='SageMakerRole',
+            train_instance_count=1,
+            train_instance_type='ml.c4.xlarge',
+            sagemaker_session=sagemaker_session)
+
+        train_input = algo.sagemaker_session.upload_data(
+            path=data_path, key_prefix='integ-test-data/marketplace/train')
+
+        algo.fit({'training': train_input})
+
+    endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
+    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
+        predictor = algo.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
+
+        a = [50 * i for i in range(3)]
+        b = [40 + i for i in range(10)]
+        indices = [i + j for i, j in itertools.product(a, b)]
+
+        test_data = shape.iloc[indices[:-1]]
+        test_x = test_data.iloc[:, 1:]
+
+        print(predictor.predict(test_x.values).decode('utf-8'))
+
+
+def test_marketplace_attach(sagemaker_session):
+    with timeout(minutes=15):
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+        region = sagemaker_session.boto_region_name
+        account = REGION_ACCOUNT_MAP[region]
+        algorithm_arn = ALGORITHM_ARN % (region, account)
+
+        mktplace = AlgorithmEstimator(
+            algorithm_arn=algorithm_arn,
+            role='SageMakerRole',
+            train_instance_count=1,
+            train_instance_type='ml.c4.xlarge',
+            sagemaker_session=sagemaker_session,
+            base_job_name='test-marketplace')
+
+        train_input = mktplace.sagemaker_session.upload_data(
+            path=data_path, key_prefix='integ-test-data/marketplace/train')
+
+        mktplace.fit({'training': train_input}, wait=False)
+        training_job_name = mktplace.latest_training_job.name
+
+        print('Waiting to re-attach to the training job: %s' % training_job_name)
+        time.sleep(20)
+        endpoint_name = 'test-marketplace-estimator{}'.format(sagemaker_timestamp())
+
+    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
+        print('Re-attaching now to: %s' % training_job_name)
+        estimator = AlgorithmEstimator.attach(training_job_name=training_job_name,
+                                              sagemaker_session=sagemaker_session)
+        predictor = estimator.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name,
+                                     serializer=sagemaker.predictor.csv_serializer)
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
+        a = [50 * i for i in range(3)]
+        b = [40 + i for i in range(10)]
+        indices = [i + j for i, j in itertools.product(a, b)]
+
+        test_data = shape.iloc[indices[:-1]]
+        test_x = test_data.iloc[:, 1:]
+
+        print(predictor.predict(test_x.values).decode('utf-8'))
+
+
+@pytest.mark.canary_quick
+def test_marketplace_model(sagemaker_session):
+    region = sagemaker_session.boto_region_name
+    account = REGION_ACCOUNT_MAP[region]
+    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
+
+    def predict_wrapper(endpoint, session):
+        return sagemaker.RealTimePredictor(
+            endpoint, session, serializer=sagemaker.predictor.csv_serializer
+        )
+
+    model = ModelPackage(role='SageMakerRole',
+                         model_package_arn=model_package_arn,
+                         sagemaker_session=sagemaker_session,
+                         predictor_cls=predict_wrapper)
+
+    endpoint_name = 'test-marketplace-model-endpoint{}'.format(sagemaker_timestamp())
+    with timeout_and_delete_endpoint_by_name(endpoint_name, sagemaker_session, minutes=20):
+        predictor = model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)
+        data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+        shape = pandas.read_csv(os.path.join(data_path, 'iris.csv'), header=None)
+        a = [50 * i for i in range(3)]
+        b = [40 + i for i in range(10)]
+        indices = [i + j for i, j in itertools.product(a, b)]
+
+        test_data = shape.iloc[indices[:-1]]
+        test_x = test_data.iloc[:, 1:]
+
+        print(predictor.predict(test_x.values).decode('utf-8'))
+
+
+def test_marketplace_tuning_job(sagemaker_session):
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+    region = sagemaker_session.boto_region_name
+    account = REGION_ACCOUNT_MAP[region]
+    algorithm_arn = ALGORITHM_ARN % (region, account)
+
+    mktplace = AlgorithmEstimator(
+        algorithm_arn=algorithm_arn,
+        role='SageMakerRole',
+        train_instance_count=1,
+        train_instance_type='ml.c4.xlarge',
+        sagemaker_session=sagemaker_session,
+        base_job_name='test-marketplace')
+
+    train_input = mktplace.sagemaker_session.upload_data(
+        path=data_path, key_prefix='integ-test-data/marketplace/train')
+
+    mktplace.set_hyperparameters(max_leaf_nodes=10)
+
+    hyperparameter_ranges = {'max_leaf_nodes': IntegerParameter(1, 100000)}
+
+    tuner = HyperparameterTuner(estimator=mktplace, base_tuning_job_name='byo',
+                                objective_metric_name='validation:accuracy',
+                                hyperparameter_ranges=hyperparameter_ranges,
+                                max_jobs=2, max_parallel_jobs=2)
+
+    tuner.fit({'training': train_input}, include_cls_metadata=False)
+    time.sleep(15)
+    tuner.wait()
+
+
+def test_marketplace_transform_job(sagemaker_session):
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+    region = sagemaker_session.boto_region_name
+    account = REGION_ACCOUNT_MAP[region]
+    algorithm_arn = ALGORITHM_ARN % (region, account)
+
+    algo = AlgorithmEstimator(
+        algorithm_arn=algorithm_arn,
+        role='SageMakerRole',
+        train_instance_count=1,
+        train_instance_type='ml.c4.xlarge',
+        sagemaker_session=sagemaker_session,
+        base_job_name='test-marketplace')
+
+    train_input = algo.sagemaker_session.upload_data(
+        path=data_path, key_prefix='integ-test-data/marketplace/train')
+
+    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
+
+    transform_workdir = DATA_DIR + '/marketplace/transform'
+    shape.to_csv(transform_workdir + '/batchtransform_test.csv', index=False, header=False)
+    transform_input = algo.sagemaker_session.upload_data(
+        transform_workdir,
+        key_prefix='integ-test-data/marketplace/transform')
+
+    algo.fit({'training': train_input})
+
+    transformer = algo.transformer(1, 'ml.m4.xlarge')
+    transformer.transform(transform_input, content_type='text/csv')
+    transformer.wait()
+
+
+def test_marketplace_transform_job_from_model_package(sagemaker_session):
+    data_path = os.path.join(DATA_DIR, 'marketplace', 'training')
+    shape = pandas.read_csv(data_path + '/iris.csv', header=None).drop([0], axis=1)
+
+    TRANSFORM_WORKDIR = DATA_DIR + '/marketplace/transform'
+    shape.to_csv(TRANSFORM_WORKDIR + '/batchtransform_test.csv', index=False, header=False)
+    transform_input = sagemaker_session.upload_data(
+        TRANSFORM_WORKDIR,
+        key_prefix='integ-test-data/marketplace/transform')
+
+    region = sagemaker_session.boto_region_name
+    account = REGION_ACCOUNT_MAP[region]
+    model_package_arn = MODEL_PACKAGE_ARN % (region, account)
+
+    model = ModelPackage(role='SageMakerRole',
+                         model_package_arn=model_package_arn,
+                         sagemaker_session=sagemaker_session)
+
+    transformer = model.transformer(1, 'ml.m4.xlarge')
+    transformer.transform(transform_input, content_type='text/csv')
+    transformer.wait()

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: tests/unit/test_algorithm.py[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,955 @@
+# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"). You
+# may not use this file except in compliance with the License. A copy of
+# the License is located at
+#
+#     http://aws.amazon.com/apache2.0/
+#
+# or in the "license" file accompanying this file. This file is
+# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
+# ANY KIND, either express or implied. See the License for the specific
+# language governing permissions and limitations under the License.
+from __future__ import absolute_import
+
+import copy
+import datetime
+
+import pytest
+from mock import Mock, patch
+
+from sagemaker.algorithm import AlgorithmEstimator
+from sagemaker.estimator import _TrainingJob
+from sagemaker.transformer import Transformer
+
+DESCRIBE_ALGORITHM_RESPONSE = {
+    'AlgorithmName': 'scikit-decision-trees',
+    'AlgorithmArn': 'arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+    'AlgorithmDescription': 'Decision trees using Scikit',
+    'CreationTime': datetime.datetime(2018, 8, 3, 22, 44, 54, 437000),
+    'TrainingSpecification': {
+        'TrainingImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:12345',
+        'TrainingImageDigest': 'sha256:[93m206854b6ea2f0020d216311da732010515169820b898ec29720bcf1d2b46806a[0m',
+        'SupportedHyperParameters': [
+            {
+                'Name': 'max_leaf_nodes',
+                'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+                'Type': 'Integer',
+                'Range': {
+                    'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
+                },
+                'IsTunable': True,
+                'IsRequired': False,
+                'DefaultValue': '100',
+            },
+            {
+                'Name': 'free_text_hp1',
+                'Description': 'You can write anything here',
+                'Type': 'FreeText',
+                'IsTunable': False,
+                'IsRequired': True
+            }
+        ],
+        'SupportedTrainingInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge', 'ml.m4.4xlarge'],
+        'SupportsDistributedTraining': False,
+        'MetricDefinitions': [
+            {'Name': 'validation:accuracy', 'Regex': 'validation-accuracy: (\\S+)'}
+        ],
+        'TrainingChannels': [
+            {
+                'Name': 'training',
+                'Description': 'Input channel that provides training data',
+                'IsRequired': True,
+                'SupportedContentTypes': ['text/csv'],
+                'SupportedCompressionTypes': ['None'],
+                'SupportedInputModes': ['File'],
+            }
+        ],
+        'SupportedTuningJobObjectiveMetrics': [
+            {'Type': 'Maximize', 'MetricName': 'validation:accuracy'}
+        ],
+    },
+    'InferenceSpecification': {
+        'InferenceImage': '123.dkr.ecr.us-east-2.amazonaws.com/decision-trees-sample@sha256:123',
+        'SupportedTransformInstanceTypes': ['ml.m4.xlarge', 'ml.m4.2xlarge'],
+        'SupportedContentTypes': ['text/csv'],
+        'SupportedResponseMIMETypes': ['text'],
+    },
+    'ValidationSpecification': {
+        'ValidationRole': 'arn:aws:iam::764419575721:role/SageMakerRole',
+        'ValidationProfiles': [
+            {
+                'ProfileName': 'ValidationProfile1',
+                'TrainingJobDefinition': {
+                    'TrainingInputMode': 'File',
+                    'HyperParameters': {},
+                    'InputDataConfig': [
+                        {
+                            'ChannelName': 'training',
+                            'DataSource': {
+                                'S3DataSource': {
+                                    'S3DataType': 'S3Prefix',
+                                    'S3Uri': 's3://sagemaker-us-east-2-7123/-scikit-byo-iris/training-input-data',
+                                    'S3DataDistributionType': 'FullyReplicated',
+                                }
+                            },
+                            'ContentType': 'text/csv',
+                            'CompressionType': 'None',
+                            'RecordWrapperType': 'None',
+                        }
+                    ],
+                    'OutputDataConfig': {
+                        'KmsKeyId': '',
+                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/training-output',
+                    },
+                    'ResourceConfig': {
+                        'InstanceType': 'ml.c4.xlarge',
+                        'InstanceCount': 1,
+                        'VolumeSizeInGB': 10,
+                    },
+                    'StoppingCondition': {'MaxRuntimeInSeconds': 3600},
+                },
+                'TransformJobDefinition': {
+                    'MaxConcurrentTransforms': 0,
+                    'MaxPayloadInMB': 0,
+                    'TransformInput': {
+                        'DataSource': {
+                            'S3DataSource': {
+                                'S3DataType': 'S3Prefix',
+                                'S3Uri': 's3://sagemaker-us-east-2/scikit-byo-iris/batch-inference/transform_test.csv',
+                            }
+                        },
+                        'ContentType': 'text/csv',
+                        'CompressionType': 'None',
+                        'SplitType': 'Line',
+                    },
+                    'TransformOutput': {
+                        'S3OutputPath': 's3://sagemaker-us-east-2-764419575721/scikit-byo-iris/batch-transform-output',
+                        'Accept': 'text/csv',
+                        'AssembleWith': 'Line',
+                        'KmsKeyId': '',
+                    },
+                    'TransformResources': {'InstanceType': 'ml.c4.xlarge', 'InstanceCount': 1},
+                },
+            }
+        ],
+        'ValidationOutputS3Prefix': 's3://sagemaker-us-east-2-764419575721/DEMO-scikit-byo-iris/validation-output',
+        'ValidateForMarketplace': True,
+    },
+    'AlgorithmStatus': 'Completed',
+    'AlgorithmStatusDetails': {
+        'ValidationStatuses': [{'ProfileName': 'ValidationProfile1', 'Status': 'Completed'}]
+    },
+    'ResponseMetadata': {
+        'RequestId': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
+        'HTTPStatusCode': 200,
+        'HTTPHeaders': {
+            'x-amzn-requestid': 'e04bc28b-61b6-4486-9106-0edf07f5649c',
+            'content-type': 'application/x-amz-json-1.1',
+            'content-length': '3949',
+            'date': 'Fri, 03 Aug 2018 23:08:43 GMT',
+        },
+        'RetryAttempts': 0,
+    },
+}
+
+
+@patch('sagemaker.Session')
+def test_algorithm_supported_input_mode_with_valid_input_types(session):
+    # verify that the Estimator verifies the
+    # input mode that an Algorithm supports.
+
+    file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=file_mode_algo)
+
+    # Creating a File mode Estimator with a File mode algorithm should work
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['Pipe'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=pipe_mode_algo)
+
+    # Creating a Pipe mode Estimator with a Pipe mode algorithm should work.
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        input_mode='Pipe',
+        sagemaker_session=session,
+    )
+
+    any_input_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    any_input_algo['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=any_input_algo)
+
+    # Creating a File mode Estimator with an algorithm that supports both input modes
+    # should work.
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+
+@patch('sagemaker.Session')
+def test_algorithm_supported_input_mode_with_bad_input_types(session):
+    # verify that the Estimator verifies raises exceptions when
+    # attempting to train with an incorrect input type
+
+    file_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    file_mode_algo['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=file_mode_algo)
+
+    # Creating a Pipe mode Estimator with a File mode algorithm should fail.
+    with pytest.raises(ValueError):
+        AlgorithmEstimator(
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.xlarge',
+            train_instance_count=1,
+            input_mode='Pipe',
+            sagemaker_session=session,
+        )
+
+    pipe_mode_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    pipe_mode_algo['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['Pipe'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File', 'Pipe'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=pipe_mode_algo)
+
+    # Creating a File mode Estimator with a Pipe mode algorithm should fail.
+    with pytest.raises(ValueError):
+        AlgorithmEstimator(
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.xlarge',
+            train_instance_count=1,
+            sagemaker_session=session,
+        )
+
+
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
+@patch('sagemaker.Session')
+def test_algorithm_trainining_channels_with_expected_channels(session):
+    training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+
+    training_channels['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    # Pass training and validation channels. This should work
+    estimator.fit({'training': 's3://some/place', 'validation': 's3://some/other'})
+
+    # Passing only the training channel. Validation is optional so this should also work.
+    estimator.fit({'training': 's3://some/place'})
+
+
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
+@patch('sagemaker.Session')
+def test_algorithm_trainining_channels_with_invalid_channels(session):
+    training_channels = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+
+    training_channels['TrainingSpecification']['TrainingChannels'] = [
+        {
+            'Name': 'training',
+            'Description': 'Input channel that provides training data',
+            'IsRequired': True,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+        {
+            'Name': 'validation',
+            'Description': 'Input channel that provides validation data',
+            'IsRequired': False,
+            'SupportedContentTypes': ['text/csv'],
+            'SupportedCompressionTypes': ['None'],
+            'SupportedInputModes': ['File'],
+        },
+    ]
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=training_channels)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    # Passing only validation should fail as training is required.
+    with pytest.raises(ValueError):
+        estimator.fit({'validation': 's3://some/thing'})
+
+    # Passing an unknown channel should fail???
+    with pytest.raises(ValueError):
+        estimator.fit({'training': 's3://some/data', 'training2': 's3://some/other/data'})
+
+
+@patch('sagemaker.Session')
+def test_algorithm_train_instance_types_valid_instance_types(session):
+    describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
+
+    describe_algo_response['TrainingSpecification'][
+        'SupportedTrainingInstanceTypes'
+    ] = train_instance_types
+
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=describe_algo_response
+    )
+
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m5.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+
+@patch('sagemaker.Session')
+def test_algorithm_train_instance_types_invalid_instance_types(session):
+    describe_algo_response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    train_instance_types = ['ml.m4.xlarge', 'ml.m5.2xlarge']
+
+    describe_algo_response['TrainingSpecification'][
+        'SupportedTrainingInstanceTypes'
+    ] = train_instance_types
+
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=describe_algo_response
+    )
+
+    # invalid instance type, should fail
+    with pytest.raises(ValueError):
+        AlgorithmEstimator(
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m4.8xlarge',
+            train_instance_count=1,
+            sagemaker_session=session,
+        )
+
+
+@patch('sagemaker.Session')
+def test_algorithm_distributed_training_validation(session):
+    distributed_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    distributed_algo['TrainingSpecification']['SupportsDistributedTraining'] = True
+
+    single_instance_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    single_instance_algo['TrainingSpecification']['SupportsDistributedTraining'] = False
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=distributed_algo)
+
+    # Distributed training should work for Distributed and Single instance.
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=2,
+        sagemaker_session=session,
+    )
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=single_instance_algo)
+
+    # distributed training on a single instance algorithm should fail.
+    with pytest.raises(ValueError):
+        AlgorithmEstimator(
+            algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+            role='SageMakerRole',
+            train_instance_type='ml.m5.2xlarge',
+            train_instance_count=2,
+            sagemaker_session=session,
+        )
+
+
+@patch('sagemaker.Session')
+def test_algorithm_hyperparameter_integer_range_valid_range(session):
+    hyperparameters = [
+        {
+            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+            'Type': 'Integer',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
+            },
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    estimator.set_hyperparameters(max_leaf_nodes=1)
+    estimator.set_hyperparameters(max_leaf_nodes=100000)
+
+
+@patch('sagemaker.Session')
+def test_algorithm_hyperparameter_integer_range_invalid_range(session):
+    hyperparameters = [
+        {
+            'Description': 'Grow a tree with max_leaf_nodes in best-first fashion.',
+            'Type': 'Integer',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'IntegerParameterRangeSpecification': {'MinValue': '1', 'MaxValue': '100000'}
+            },
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(max_leaf_nodes=0)
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(max_leaf_nodes=100001)
+
+
+@patch('sagemaker.Session')
+def test_algorithm_hyperparameter_continuous_range_valid_range(session):
+    hyperparameters = [
+        {
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Continuous',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
+            },
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    estimator.set_hyperparameters(max_leaf_nodes=0)
+    estimator.set_hyperparameters(max_leaf_nodes=1.0)
+    estimator.set_hyperparameters(max_leaf_nodes=0.5)
+    estimator.set_hyperparameters(max_leaf_nodes=1)
+
+
+@patch('sagemaker.Session')
+def test_algorithm_hyperparameter_continuous_range_invalid_range(session):
+    hyperparameters = [
+        {
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Continuous',
+            'Name': 'max_leaf_nodes',
+            'Range': {
+                'ContinuousParameterRangeSpecification': {'MinValue': '0.0', 'MaxValue': '1.0'}
+            },
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(max_leaf_nodes=1.1)
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(max_leaf_nodes=-0.1)
+
+
+@patch('sagemaker.Session')
+def test_algorithm_hyperparameter_categorical_range(session):
+    hyperparameters = [
+        {
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': False,
+            'DefaultValue': '100',
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    estimator.set_hyperparameters(hp1='MXNet')
+    estimator.set_hyperparameters(hp1='TF')
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(hp1='Chainer')
+
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(hp1='MxNET')
+
+
+@patch('sagemaker.Session')
+def test_algorithm_required_hyperparameters_not_provided(session):
+    hyperparameters = [
+        {
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': True,
+        },
+        {
+            'Name': 'hp2',
+            'Description': 'A continuous hyperparameter',
+            'Type': 'Categorical',
+            'IsTunable': False,
+            'IsRequired': True
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    # hp1 is required and was not provided
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(hp2='TF2')
+
+    # Calling fit with unset required hyperparameters should fail
+    # this covers the use case of not calling set_hyperparameters() explicitly
+    with pytest.raises(ValueError):
+        estimator.fit({'training': 's3://some/place'})
+
+
+@patch('sagemaker.Session')
+@patch('sagemaker.estimator.EstimatorBase.fit', Mock())
+def test_algorithm_required_hyperparameters_are_provided(session):
+    hyperparameters = [
+        {
+            'Description': 'A categorical hyperparameter',
+            'Type': 'Categorical',
+            'Name': 'hp1',
+            'Range': {'CategoricalParameterRangeSpecification': {'Values': ['TF', 'MXNet']}},
+            'IsTunable': True,
+            'IsRequired': True,
+        },
+        {
+            'Name': 'hp2',
+            'Description': 'A categorical hyperparameter',
+            'Type': 'Categorical',
+            'IsTunable': False,
+            'IsRequired': True
+        },
+        {
+            'Name': 'free_text_hp1',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': True
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    # All 3 Hyperparameters are provided
+    estimator.set_hyperparameters(hp1='TF', hp2='TF2', free_text_hp1='Hello!')
+
+
+@patch('sagemaker.Session')
+def test_algorithm_required_free_text_hyperparameter_not_provided(session):
+    hyperparameters = [
+        {
+            'Name': 'free_text_hp1',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': True
+        },
+        {
+            'Name': 'free_text_hp2',
+            'Description': 'You can write anything here',
+            'Type': 'FreeText',
+            'IsTunable': False,
+            'IsRequired': False
+        }
+    ]
+
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    some_algo['TrainingSpecification']['SupportedHyperParameters'] = hyperparameters
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    # Calling fit with unset required hyperparameters should fail
+    # this covers the use case of not calling set_hyperparameters() explicitly
+    with pytest.raises(ValueError):
+        estimator.fit({'training': 's3://some/place'})
+
+    # hp1 is required and was not provided
+    with pytest.raises(ValueError):
+        estimator.set_hyperparameters(free_text_hp2='some text')
+
+
+@patch('sagemaker.Session')
+@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
+def test_algorithm_create_transformer(create_model, session):
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    estimator.latest_training_job = _TrainingJob(session, 'some-job-name')
+    model = Mock()
+    model.name = 'my-model'
+    create_model.return_value = model
+
+    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
+
+    assert isinstance(transformer, Transformer)
+    create_model.assert_called()
+    assert transformer.model_name == 'my-model'
+
+
+@patch('sagemaker.Session')
+def test_algorithm_create_transformer_without_completed_training_job(session):
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    with pytest.raises(RuntimeError) as error:
+        estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
+        assert 'No finished training job found associated with this estimator' in str(error)
+
+
+@patch('sagemaker.algorithm.AlgorithmEstimator.create_model')
+@patch('sagemaker.Session')
+def test_algorithm_create_transformer_with_product_id(create_model, session):
+    response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    response['ProductId'] = 'some-product-id'
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    estimator.latest_training_job = _TrainingJob(session, 'some-job-name')
+    model = Mock()
+    model.name = 'my-model'
+    create_model.return_value = model
+
+    transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')
+    assert transformer.env is None
+
+
+@patch('sagemaker.Session')
+def test_algorithm_enable_network_isolation_no_product_id(session):
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=DESCRIBE_ALGORITHM_RESPONSE)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    network_isolation = estimator.enable_network_isolation()
+    assert network_isolation is False
+
+
+@patch('sagemaker.Session')
+def test_algorithm_enable_network_isolation_with_product_id(session):
+    response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    response['ProductId'] = 'some-product-id'
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )
+
+    network_isolation = estimator.enable_network_isolation()
+    assert network_isolation is True
+
+
+@patch('sagemaker.Session')
+def test_algorithm_encrypt_inter_container_traffic(session):
+    response = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    response['encrypt_inter_container_traffic'] = True
+    session.sagemaker_client.describe_algorithm = Mock(
+        return_value=response)
+
+    estimator = AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+        encrypt_inter_container_traffic=True
+    )
+
+    encrypt_inter_container_traffic = estimator.encrypt_inter_container_traffic
+    assert encrypt_inter_container_traffic is True
+
+
+@patch('sagemaker.Session')
+def test_algorithm_no_required_hyperparameters(session):
+    some_algo = copy.deepcopy(DESCRIBE_ALGORITHM_RESPONSE)
+    del some_algo['TrainingSpecification']['SupportedHyperParameters']
+
+    session.sagemaker_client.describe_algorithm = Mock(return_value=some_algo)
+
+    # Calling AlgorithmEstimator() with unset required hyperparameters
+    # should fail if they are required.
+    # Pass training and hyperparameters channels. This should work
+    assert AlgorithmEstimator(
+        algorithm_arn='arn:aws:sagemaker:us-east-2:1234:algorithm/scikit-decision-trees',
+        role='SageMakerRole',
+        train_instance_type='ml.m4.2xlarge',
+        train_instance_count=1,
+        sagemaker_session=session,
+    )

~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
[92mReason: High Entropy[0m
[92mDate: 2019-06-17 16:02:45[0m
[92mHash: 3b267125c6a3eb52be40a6477e51e23e42d081f8[0m
[92mFilepath: tox.ini[0m
[92mBranch: origin/test-branch-git-config[0m
[92mCommit: Create test branch
[0m
@@ -0,0 +1,114 @@
+# Tox (http://tox.testrun.org/) is a tool for running tests
+# in multiple virtualenvs. This configuration file will run the
+# test suite on all supported python versions. To use it, "pip install tox"
+# and then run "tox" from this directory.
+
+[tox]
+envlist = flake8,pylint,twine,sphinx,py27,py36
+
+skip_missing_interpreters = False
+
+
+[flake8]
+max-line-length = 120
+exclude =
+    build/
+    .git
+    __pycache__
+    examples/
+    *pb2.py
+    .tox
+    tests/data/
+    venv/
+    *service_pb2_grpc.py
+
+max-complexity = 10
+
+ignore =
+    FI10,
+    FI12,
+    FI13,
+    FI14,
+    FI15,
+    FI16,
+    FI17,
+    FI50,
+    FI51,
+    FI52,
+    FI53,
+    FI54,
+    FI55,
+    FI56,
+    FI57,
+    W503
+
+require-code = True
+
+[testenv]
+passenv =
+    AWS_ACCESS_KEY_ID
+    AWS_SECRET_ACCESS_KEY
+    AWS_SESSION_TOKEN
+    AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
+    AWS_DEFAULT_REGION
+
+# {posargs} can be passed in by additional arguments specified when invoking tox.
+# Can be used to specify which tests to run, e.g.: tox -- -s
+commands =
+    coverage run --source sagemaker -m pytest {posargs}
+    {env:IGNORE_COVERAGE:} coverage report --fail-under=90 --omit */tensorflow/tensorflow_serving/*
+deps = .[test]
+
+[testenv:flake8]
+basepython = python3
+skipdist = true
+skip_install = true
+deps =
+    flake8
+    flake8-future-import
+commands = flake8
+
+[testenv:pylint]
+basepython = python3
+skipdist = true
+skip_install = true
+deps =
+    pylint==2.3.1
+commands =
+    python -m pylint --rcfile=.pylintrc -j 0 src/sagemaker
+
+[testenv:twine]
+basepython = python3
+# twine check was added starting in 1.12.0
+# https://github.com/pypa/twine/blob/master/docs/changelog.rst
+deps =
+    twine>=1.12.0
+# https://packaging.python.org/guides/making-a-pypi-friendly-readme/#validating-restructuredtext-markup
+commands =
+    python setup.py sdist
+    twine check dist/*.tar.gz
+
+[testenv:sphinx]
+basepython = python3
+changedir = doc
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L263
+install_command = python -m pip install --upgrade -I {packages}
+# Based on: https://github.com/rtfd/readthedocs.org/blob/[93m[93m8f0c78dde5edcc85acf90462a8518735a25482d3[0m[0m/readthedocs/doc_builder/python_environments.py#L280
+deps =
+    Pygments==2.2.0
+    setuptools<40
+    docutils==0.13.1
+    mock==1.0.1
+    pillow==2.6.1
+    alabaster>=0.7,<0.8,!=0.7.5
+    commonmark==0.5.4
+    recommonmark==0.4.0
+    sphinx<1.8
+    sphinx-rtd-theme<0.5
+    readthedocs-sphinx-ext<0.6
+# pip install requirements.txt is separate as RTD does it in separate steps
+# having the requirements.txt installed in deps above results in Double Requirement exception
+# https://github.com/pypa/pip/issues/988
+commands =
+    pip install --exists-action=w -r requirements.txt
+    sphinx-build -T -W -b html -d _build/doctrees-readthedocs -D language=en . _build/html

~~~~~~~~~~~~~~~~~~~~~
